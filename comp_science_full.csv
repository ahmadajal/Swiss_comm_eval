,title,pageid,length,content
0,Artificial intelligence,1164,231620,"Artificial intelligence (AI, also machine intelligence, MI) is intelligence demonstrated by machines, in contrast to the natural intelligence (NI) displayed by humans and other animals. In computer science AI research is defined as the study of ""intelligent agents"": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals. Colloquially, the term ""artificial intelligence"" is applied when a machine mimics ""cognitive"" functions that humans associate with other human minds, such as ""learning"" and ""problem solving"". See glossary of artificial intelligence.
The scope of AI is disputed: as machines become increasingly capable, tasks considered as requiring ""intelligence"" are often removed from the definition, a phenomenon known as the AI effect, leading to the quip ""AI is whatever hasn't been done yet."" For instance, optical character recognition is frequently excluded from ""artificial intelligence"", having become a routine technology. Capabilities generally classified as AI as of 2017 include successfully understanding human speech, competing at the highest level in strategic game systems (such as chess and Go), autonomous cars, intelligent routing in content delivery network and military simulations.
Artificial intelligence was founded as an academic discipline in 1956, and in the years since has experienced several waves of optimism, followed by disappointment and the loss of funding (known as an ""AI winter""), followed by new approaches, success and renewed funding. For most of its history, AI research has been divided into subfields that often fail to communicate with each other. These sub-fields are based on technical considerations, such as particular goals (e.g. ""robotics"" or ""machine learning""), the use of particular tools (""logic"" or ""neural networks""), or deep philosophical differences. Subfields have also been based on social factors (particular institutions or the work of particular researchers).
The traditional problems (or goals) of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception and the ability to move and manipulate objects. General intelligence is among the field's long-term goals. Approaches include statistical methods, computational intelligence, and traditional symbolic AI. Many tools are used in AI, including versions of search and mathematical optimization, neural networks and methods based on statistics, probability and economics. The AI field draws upon computer science, mathematics, psychology, linguistics, philosophy and many others.
The field was founded on the claim that human intelligence ""can be so precisely described that a machine can be made to simulate it"". This raises philosophical arguments about the nature of the mind and the ethics of creating artificial beings endowed with human-like intelligence, issues which have been explored by myth, fiction and philosophy since antiquity. Some people also consider AI to be a danger to humanity if it progresses unabatedly. Others believe that AI, unlike previous technological revolutions, will create a risk of mass unemployment.
In the twenty-first century, AI techniques have experienced a resurgence following concurrent advances in computer power, large amounts of data, and theoretical understanding; and AI techniques have become an essential part of the technology industry, helping to solve many challenging problems in computer science.


== History ==

While thought-capable artificial beings appeared as storytelling devices in antiquity, the idea of actually trying to build a machine to perform useful reasoning may have begun with Ramon Llull (c. 1300 CE). With his Calculus ratiocinator, Gottfried Leibniz extended the concept of the calculating machine (Wilhelm Schickard engineered the first one around 1623), intending to perform operations on concepts rather than numbers. Since the 19th century, artificial beings are common in fiction, as in Mary Shelley's Frankenstein or Karel Čapek's R.U.R. (Rossum's Universal Robots).
The study of mechanical or ""formal"" reasoning began with philosophers and mathematicians in antiquity. The study of mathematical logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as ""0"" and ""1"", could simulate any conceivable act of mathematical deduction. This insight, that digital computers can simulate any process of formal reasoning, is known as the Church–Turing thesis. Along with concurrent discoveries in neurobiology, information theory and cybernetics, this led researchers to consider the possibility of building an electronic brain. The first work that is now generally recognized as AI was McCullouch and Pitts' 1943 formal design for Turing-complete ""artificial neurons"".
The field of AI research was born at a workshop at Dartmouth College in 1956. Attendees Allen Newell (CMU), Herbert Simon (CMU), John McCarthy (MIT), Marvin Minsky (MIT) and Arthur Samuel (IBM) became the founders and leaders of AI research. They and their students produced programs that the press described as ""astonishing"": computers were learning checkers strategies (c. 1954) (and by 1959 were reportedly playing better than the average human), solving word problems in algebra, proving logical theorems (Logic Theorist, first run c. 1956) and speaking English. By the middle of the 1960s, research in the U.S. was heavily funded by the Department of Defense and laboratories had been established around the world. AI's founders were optimistic about the future: Herbert Simon predicted, ""machines will be capable, within twenty years, of doing any work a man can do"". Marvin Minsky agreed, writing, ""within a generation ... the problem of creating 'artificial intelligence' will substantially be solved"".
They failed to recognize the difficulty of some of the remaining tasks. Progress slowed and in 1974, in response to the criticism of Sir James Lighthill and ongoing pressure from the US Congress to fund more productive projects, both the U.S. and British governments cut off exploratory research in AI. The next few years would later be called an ""AI winter"", a period when obtaining funding for AI projects was difficult.
In the early 1980s, AI research was revived by the commercial success of expert systems, a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985 the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S and British governments to restore funding for academic research. However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting hiatus began.
In the late 1990s and early 21st century, AI began to be used for logistics, data mining, medical diagnosis and other areas. The success was due to increasing computational power (see Moore's law), greater emphasis on solving specific problems, new ties between AI and other fields (such as statistics, economics and mathematics), and a commitment by researchers to mathematical methods and scientific standards. Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov on 11 May 1997.
Advanced statistical techniques (loosely known as deep learning), access to large amounts of data and faster computers enabled advances in machine learning and perception. By the mid 2010s, machine learning applications were used throughout the world. In a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy champions, Brad Rutter and Ken Jennings, by a significant margin. The Kinect, which provides a 3D body–motion interface for the Xbox 360 and the Xbox One use algorithms that emerged from lengthy AI research as do intelligent personal assistants in smartphones. In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps. In the 2017 Future of Go Summit, AlphaGo won a three-game match with Ke Jie, who at the time continuously held the world No. 1 ranking for two years. This marked the completion of a significant milestone in the development of Artificial Intelligence as Go is an extremely complex game, more so than Chess.
According to Bloomberg's Jack Clark, 2015 was a landmark year for artificial intelligence, with the number of software projects that use AI within Google increased from a ""sporadic usage"" in 2012 to more than 2,700 projects. Clark also presents factual data indicating that error rates in image processing tasks have fallen significantly since 2011. He attributes this to an increase in affordable neural networks, due to a rise in cloud computing infrastructure and to an increase in research tools and datasets. Other cited examples include Microsoft's development of a Skype system that can automatically translate from one language to another and Facebook's system that can describe images to blind people.


== Basics ==
A typical AI perceives its environment and takes actions that maximize its chance of successfully achieving its goals. An AI's intended goal function can be simple (""1 if the AI wins a game of Go, 0 otherwise"") or complex (""Do actions mathematically similar to the actions that got you rewards in the past""). Goals can be explicitly defined, or can be induced. If the AI is programmed for ""reinforcement learning"", goals can be implicitly induced by rewarding some types of behavior and punishing others. Alternatively, an evolutionary system can induce goals by using a ""fitness function"" to mutate and preferentially replicate high-scoring AI systems; this is similar to how animals evolved to innately desire certain goals such as finding food, or how dogs can be bred via artificial selection to possess desired traits. Some AI systems, such as nearest-neighbor, instead reason by analogy; these systems are not generally given goals, except to the degree that goals are somehow implicit in their training data. Such systems can still be benchmarked if the non-goal system is framed as a system whose ""goal"" is to successfully accomplish its narrow classification task.
AI often revolves around the use of algorithms. An algorithm is a set of unambiguous instructions that a mechanical computer can execute. A complex algorithm is often built on top of other, simpler, algorithms. A simple example of an algorithm is the following recipe for optimal play at tic-tac-toe:
If someone has a ""threat"" (that is, two in a row), take the remaining square. Otherwise,
if a move ""forks"" to create two threats at once, play that move. Otherwise,
take the center square if it is free. Otherwise,
if your opponent has played in a corner, take the opposite corner. Otherwise,
take an empty corner if one exists. Otherwise,
take any empty square.
Many AI algorithms are capable of learning from data; they can enhance themselves by learning new heuristics (strategies, or ""rules of thumb"", that have worked well in the past), or can themselves write other algorithms. Some of the ""learners"" described below, including Bayesian networks, decision trees, and nearest-neighbor, could theoretically, if given infinite data, time, and memory, learn to approximate any function, including whatever combination of mathematical functions would best describe the entire world. These learners could therefore, in theory, derive all possible knowledge, by considering every possible hypothesis and matching it against the data. In practice, it is almost never possible to consider every possibility, because of the phenomenon of ""combinatorial explosion"", where the amount of time needed to solve a problem grows exponentially. Much of AI research involves figuring out how to identify and avoid considering broad swaths of possibililities that are unlikely to be fruitful. For example, when viewing a map and looking for the shortest driving route from Denver to New York in the East, one can in most cases skip looking at any path through San Francisco or other areas far to the West; thus, an AI wielding an pathfinding algorithm like A* can avoid the combinatorial explosion that would ensue if every possible route had to be ponderously considered in turn.
The earliest (and easiest to understand) approach to AI was symbolism (such as formal logic): ""If an otherwise healthy adult has a fever, then they may have influenza"". A second, more general, approach is Bayesian inference: ""If the current patient has a fever, adjust the probability they have influenza in such-and-such way"". The third major approach, extremely popular in routine business AI applications, is analogizers such as SVM and nearest-neighbor: ""After examining the records of known past patients whose temperature, symptoms, age, and other factors mostly match the current patient, X% of those patients turned out to have influenza"". A fourth approach is harder to intuitively understand, but is inspired by how the brain's machinery works: the neural network approach uses artificial ""neurons"" that can learn by comparing itself to the desired output and altering the strengths of the connections between its internal neurons to ""reinforce"" connections that seemed to be useful. These four main approaches can overlap with each other and with evolutionary systems; for example, neural nets can learn to make inferences, to generalize, and to make analogies. Some systems implicitly or explicitly use multiple of these approaches, alongside many other AI and non-AI algorithms; the best approach is often different depending on the problem.
Learning algorithms work on the basis that strategies, algorithms, and inferences that worked well in the past are likely to continue working well in the future. These inferences can be obvious, such as ""since the sun rose every morning for the last 10,000 days, it will probably rise tomorrow morning as well"". They can be nuanced, such as ""X% of families have geographically separate species with color variants, so there is an Y% chance that undiscovered black swans exist"". Learners also work on the basis of ""Occam's razor"": The simplest theory that explains the data is the likeliest. Therefore, to be successful, a learner must be designed such that it prefers simpler theories to complex theories, except in cases where the complex theory is proven substantially better. Settling on a bad, overly complex theory gerrymandered to fit all the past training data is known as overfitting. Many systems attempt to reduce overfitting by rewarding a theory in accordance with how well it fits the data, but penalizing the theory in accordance with how complex the theory is. Besides classic overfitting, learners can also disappoint by ""learning the wrong lesson"". A toy example is that an image classifier trained only on pictures of brown horses and black cats might conclude that all brown patches are likely to be horses. A real-world example is that, unlike humans, current image classifiers don't determine the spatial relationship between components of the picture; instead, they learn abstract patterns of pixels that humans are oblivious to, but that linearly correlate with images of certain types of real objects. Faintly superimposing such a pattern on a legitimate image results in an ""adversarial"" image that the system misclassifies.

Compared with humans, existing AI lacks several features of human ""commonsense reasoning""; most notably, humans have powerful mechanisms for reasoning about ""naïve physics"" such as space, time, and physical interactions. This enables even young children to easily make inferences like ""If I roll this pen off a table, it will fall on the floor"". Humans also have a powerful mechanism of ""folk psychology"" that helps them to interpret natural-language sentences such as ""The city councilmen refused the demonstrators a permit because they advocated violence"". (A generic AI has difficulty inferring whether the councilmen or the demonstrators are the ones alleged to be advocating violence.) This lack of ""common knowledge"" means that AI often makes different mistakes than humans make, in ways that can seem incomprehensible. For example, existing self-driving cars cannot reason about the location nor the intentions of pedestrians in the exact way that humans do, and instead must use non-human modes of reasoning to avoid accidents.


== Problems ==
The overall research goal of artificial intelligence is to create technology that allows computers and machines to function in an intelligent manner. The general problem of simulating (or creating) intelligence has been broken down into sub-problems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention.


=== Reasoning, problem solving ===
Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, AI research had developed methods for dealing with uncertain or incomplete information, employing concepts from probability and economics.
These algorithms proved to be insufficient for solving large reasoning problems, because they experienced a ""combinatorial explosion"": they became exponentially slower as the problems grew larger. In fact, even humans rarely use the step-by-step deduction that early AI research was able to model. They solve most of their problems using fast, intuitive judgements. Modern statistical approaches to AI (e.g. neural networks) mimic this human ability to make a quick guess based on experience, solving many problems as people do. However, they are not capable of step-by-step deduction.


=== Knowledge representation ===

Knowledge representation and knowledge engineering are central to AI research. Many of the problems machines are expected to solve will require extensive knowledge about the world. Among the things that AI needs to represent are: objects, properties, categories and relations between objects; situations, events, states and time; causes and effects; knowledge about knowledge (what we know about what other people know); and many other, less well researched domains. A representation of ""what exists"" is an ontology: the set of objects, relations, concepts, and properties formally described so that software agents can interpret them. The semantics of these are captured as description logic concepts, roles, and individuals, and typically implemented as classes, properties, and individuals in the Web Ontology Language. The most general ontologies are called upper ontologies, which attempt to provide a foundation for all other knowledge by acting as mediators between domain ontologies that cover specific knowledge about a particular knowledge domain (field of interest or area of concern). Such formal knowledge representations are suitable for content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery via automated reasoning (inferring new statements based on explicitly stated knowledge), etc. Video events are often represented as SWRL rules, which can be used, among others, to automatically generate subtitles for constrained videos.
Among the most difficult problems in knowledge representation are:
Default reasoning and the qualification problem
Many of the things people know take the form of ""working assumptions"". For example, if a bird comes up in conversation, people typically picture an animal that is fist sized, sings, and flies. None of these things are true about all birds. John McCarthy identified this problem in 1969 as the qualification problem: for any commonsense rule that AI researchers care to represent, there tend to be a huge number of exceptions. Almost nothing is simply true or false in the way that abstract logic requires. AI research has explored a number of solutions to this problem.
The breadth of commonsense knowledge
The number of atomic facts that the average person knows is very large. Research projects that attempt to build a complete knowledge base of commonsense knowledge (e.g., Cyc) require enormous amounts of laborious ontological engineering—they must be built, by hand, one complicated concept at a time.
The subsymbolic form of some commonsense knowledge
Much of what people know is not represented as ""facts"" or ""statements"" that they could express verbally. For example, a chess master will avoid a particular chess position because it ""feels too exposed"" or an art critic can take one look at a statue and realize that it is a fake. These are non-conscious and sub-symbolic intuitions or tendencies in the human brain. Knowledge like this informs, supports and provides a context for symbolic, conscious knowledge. As with the related problem of sub-symbolic reasoning, it is hoped that situated AI, computational intelligence, or statistical AI will provide ways to represent this kind of knowledge.


=== Planning ===

Intelligent agents must be able to set goals and achieve them. They need a way to visualize the future—a representation of the state of the world and be able to make predictions about how their actions will change it—and be able to make choices that maximize the utility (or ""value"") of available choices.
In classical planning problems, the agent can assume that it is the only system acting in the world, allowing the agent to be certain of the consequences of its actions. However, if the agent is not the only actor, then it requires that the agent can reason under uncertainty. This calls for an agent that can not only assess its environment and make predictions, but also evaluate its predictions and adapt based on its assessment.
Multi-agent planning uses the cooperation and competition of many agents to achieve a given goal. Emergent behavior such as this is used by evolutionary algorithms and swarm intelligence.


=== Learning ===

Machine learning, a fundamental concept of AI research since the field's inception, is the study of computer algorithms that improve automatically through experience.
Unsupervised learning is the ability to find patterns in a stream of input. Supervised learning includes both classification and numerical regression. Classification is used to determine what category something belongs in, after seeing a number of examples of things from several categories. Regression is the attempt to produce a function that describes the relationship between inputs and outputs and predicts how the outputs should change as the inputs change. In reinforcement learning the agent is rewarded for good responses and punished for bad ones. The agent uses this sequence of rewards and punishments to form a strategy for operating in its problem space. These three types of learning can be analyzed in terms of decision theory, using concepts like utility. The mathematical analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory.
Within developmental robotics, developmental learning approaches are elaborated upon to allow robots to accumulate repertoires of novel skills through autonomous self-exploration, social interaction with human teachers, and the use of guidance mechanisms (active learning, maturation, motor synergies, etc.).


=== Natural language processing ===

Natural language processing gives machines the ability to read and understand human language. A sufficiently powerful natural language processing system would enable natural language user interfaces and the acquisition of knowledge directly from human-written sources, such as newswire texts. Some straightforward applications of natural language processing include information retrieval, text mining, question answering and machine translation.
A common method of processing and extracting meaning from natural language is through semantic indexing. Although these indexes require a large volume of user input, it is expected that increases in processor speeds and decreases in data storage costs will result in greater efficiency.


=== Perception ===

Machine perception is the ability to use input from sensors (such as cameras, microphones, tactile sensors, sonar and others) to deduce aspects of the world. Computer vision is the ability to analyze visual input. A few selected subproblems are speech recognition, facial recognition and object recognition.


=== Motion and manipulation ===

The field of robotics is closely related to AI. Intelligence is required for robots to handle tasks such as object manipulation and navigation, with sub-problems such as localization, mapping, and motion planning. These systems require that an agent is able to: Be spatially cognizant of its surroundings, learn from and build a map of its environment, figure out how to get from one point in space to another, and execute that movement (which often involves compliant motion, a process where movement requires maintaining physical contact with an object).


=== Social intelligence ===

Affective computing is the study and development of systems that can recognize, interpret, process, and simulate human affects. It is an interdisciplinary field spanning computer sciences, psychology, and cognitive science. While the origins of the field may be traced as far back as the early philosophical inquiries into emotion, the more modern branch of computer science originated with Rosalind Picard's 1995 paper on ""affective computing"". A motivation for the research is the ability to simulate empathy, where the machine would be able to interpret human emotions and adapts its behavior to give an appropriate response to those emotions.
Emotion and social skills are important to an intelligent agent for two reasons. First, being able to predict the actions of others by understanding their motives and emotional states allow an agent to make better decisions. Concepts such as game theory, decision theory, necessitate that an agent be able to detect and model human emotions. Second, in an effort to facilitate human–computer interaction, an intelligent machine may want to display emotions (even if it does not experience those emotions itself) to appear more sensitive to the emotional dynamics of human interaction.


=== Creativity ===

A sub-field of AI addresses creativity both theoretically (the philosophical psychological perspective) and practically (the specific implementation of systems that generate novel and useful outputs).


=== General intelligence ===

Many researchers think that their work will eventually be incorporated into a machine with artificial general intelligence, combining all the skills mentioned above and even exceeding human ability in most or all these areas. A few believe that anthropomorphic features like artificial consciousness or an artificial brain may be required for such a project.
Many of the problems above may also require general intelligence, if machines are to solve the problems as well as people do. For example, even specific straightforward tasks, like machine translation, require that a machine read and write in both languages (NLP), follow the author's argument (reason), know what is being talked about (knowledge), and faithfully reproduce the author's original intent (social intelligence). A problem like machine translation is considered ""AI-complete"", because all of these problems need to be solved simultaneously in order to reach human-level machine performance.


== Approaches ==
There is no established unifying theory or paradigm that guides AI research. Researchers disagree about many issues. A few of the most long standing questions that have remained unanswered are these: should artificial intelligence simulate natural intelligence by studying psychology or neurobiology? Or is human biology as irrelevant to AI research as bird biology is to aeronautical engineering? Can intelligent behavior be described using simple, elegant principles (such as logic or optimization)? Or does it necessarily require solving a large number of completely unrelated problems? Can intelligence be reproduced using high-level symbols, similar to words and ideas? Or does it require ""sub-symbolic"" processing? John Haugeland, who coined the term GOFAI (Good Old-Fashioned Artificial Intelligence), also proposed that AI should more properly be referred to as synthetic intelligence, a term which has since been adopted by some non-GOFAI researchers.
Stuart Shapiro divides AI research into three approaches, which he calls computational psychology, computational philosophy, and computer science. Computational psychology is used to make computer programs that mimic human behavior. Computational philosophy, is used to develop an adaptive, free-flowing computer mind. Implementing computer science serves the goal of creating computers that can perform tasks that only people could previously accomplish. Together, the humanesque behavior, mind, and actions make up artificial intelligence.


=== Cybernetics and brain simulation ===

In the 1940s and 1950s, a number of researchers explored the connection between neurobiology, information theory, and cybernetics. Some of them built machines that used electronic networks to exhibit rudimentary intelligence, such as W. Grey Walter's turtles and the Johns Hopkins Beast. Many of these researchers gathered for meetings of the Teleological Society at Princeton University and the Ratio Club in England. By 1960, this approach was largely abandoned, although elements of it would be revived in the 1980s.


=== Symbolic ===

When access to digital computers became possible in the middle 1950s, AI research began to explore the possibility that human intelligence could be reduced to symbol manipulation. The research was centered in three institutions: Carnegie Mellon University, Stanford and MIT, and each one developed its own style of research. John Haugeland named these approaches to AI ""good old fashioned AI"" or ""GOFAI"". During the 1960s, symbolic approaches had achieved great success at simulating high-level thinking in small demonstration programs. Approaches based on cybernetics or neural networks were abandoned or pushed into the background. Researchers in the 1960s and the 1970s were convinced that symbolic approaches would eventually succeed in creating a machine with artificial general intelligence and considered this the goal of their field.


==== Cognitive simulation ====
Economist Herbert Simon and Allen Newell studied human problem-solving skills and attempted to formalize them, and their work laid the foundations of the field of artificial intelligence, as well as cognitive science, operations research and management science. Their research team used the results of psychological experiments to develop programs that simulated the techniques that people used to solve problems. This tradition, centered at Carnegie Mellon University would eventually culminate in the development of the Soar architecture in the middle 1980s.


==== Logic-based ====
Unlike Newell and Simon, John McCarthy felt that machines did not need to simulate human thought, but should instead try to find the essence of abstract reasoning and problem solving, regardless of whether people used the same algorithms. His laboratory at Stanford (SAIL) focused on using formal logic to solve a wide variety of problems, including knowledge representation, planning and learning. Logic was also the focus of the work at the University of Edinburgh and elsewhere in Europe which led to the development of the programming language Prolog and the science of logic programming.


==== Anti-logic or scruffy ====
Researchers at MIT (such as Marvin Minsky and Seymour Papert) found that solving difficult problems in vision and natural language processing required ad-hoc solutions – they argued that there was no simple and general principle (like logic) that would capture all the aspects of intelligent behavior. Roger Schank described their ""anti-logic"" approaches as ""scruffy"" (as opposed to the ""neat"" paradigms at CMU and Stanford). Commonsense knowledge bases (such as Doug Lenat's Cyc) are an example of ""scruffy"" AI, since they must be built by hand, one complicated concept at a time.


==== Knowledge-based ====
When computers with large memories became available around 1970, researchers from all three traditions began to build knowledge into AI applications. This ""knowledge revolution"" led to the development and deployment of expert systems (introduced by Edward Feigenbaum), the first truly successful form of AI software. The knowledge revolution was also driven by the realization that enormous amounts of knowledge would be required by many simple AI applications.


=== Sub-symbolic ===
By the 1980s progress in symbolic AI seemed to stall and many believed that symbolic systems would never be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition. A number of researchers began to look into ""sub-symbolic"" approaches to specific AI problems. Sub-symbolic methods manage to approach intelligence without specific representations of knowledge.


==== Embodied intelligence ====
This includes embodied, situated, behavior-based, and nouvelle AI. Researchers from the related field of robotics, such as Rodney Brooks, rejected symbolic AI and focused on the basic engineering problems that would allow robots to move and survive. Their work revived the non-symbolic viewpoint of the early cybernetics researchers of the 1950s and reintroduced the use of control theory in AI. This coincided with the development of the embodied mind thesis in the related field of cognitive science: the idea that aspects of the body (such as movement, perception and visualization) are required for higher intelligence.


==== Computational intelligence and soft computing ====
Interest in neural networks and ""connectionism"" was revived by David Rumelhart and others in the middle of the 1980s. Neural networks are an example of soft computing --- they are solutions to problems which cannot be solved with complete logical certainty, and where an approximate solution is often sufficient. Other soft computing approaches to AI include fuzzy systems, evolutionary computation and many statistical tools. The application of soft computing to AI is studied collectively by the emerging discipline of computational intelligence.


=== Statistical ===
In the 1990s, AI researchers developed sophisticated mathematical tools to solve specific subproblems. These tools are truly scientific, in the sense that their results are both measurable and verifiable, and they have been responsible for many of AI's recent successes. The shared mathematical language has also permitted a high level of collaboration with more established fields (like mathematics, economics or operations research). Stuart Russell and Peter Norvig describe this movement as nothing less than a ""revolution"" and ""the victory of the neats"". Critics argue that these techniques (with few exceptions) are too focused on particular problems and have failed to address the long-term goal of general intelligence. There is an ongoing debate about the relevance and validity of statistical approaches in AI, exemplified in part by exchanges between Peter Norvig and Noam Chomsky.


=== Integrating the approaches ===
Intelligent agent paradigm
An intelligent agent is a system that perceives its environment and takes actions which maximize its chances of success. The simplest intelligent agents are programs that solve specific problems. More complicated agents include human beings and organizations of human beings (such as firms). The paradigm gives researchers license to study isolated problems and find solutions that are both verifiable and useful, without agreeing on one single approach. An agent that solves a specific problem can use any approach that works – some agents are symbolic and logical, some are sub-symbolic neural networks and others may use new approaches. The paradigm also gives researchers a common language to communicate with other fields—such as decision theory and economics—that also use concepts of abstract agents. The intelligent agent paradigm became widely accepted during the 1990s.
Agent architectures and cognitive architectures
Researchers have designed systems to build intelligent systems out of interacting intelligent agents in a multi-agent system. A system with both symbolic and sub-symbolic components is a hybrid intelligent system, and the study of such systems is artificial intelligence systems integration. A hierarchical control system provides a bridge between sub-symbolic AI at its lowest, reactive levels and traditional symbolic AI at its highest levels, where relaxed time constraints permit planning and world modelling. Rodney Brooks' subsumption architecture was an early proposal for such a hierarchical system.


== Tools ==
In the course of 60 or so years of research, AI has developed a large number of tools to solve the most difficult problems in computer science. A few of the most general of these methods are discussed below.


=== Search and optimization ===

Many problems in AI can be solved in theory by intelligently searching through many possible solutions: Reasoning can be reduced to performing a search. For example, logical proof can be viewed as searching for a path that leads from premises to conclusions, where each step is the application of an inference rule. Planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis. Robotics algorithms for moving limbs and grasping objects use local searches in configuration space. Many learning algorithms use search algorithms based on optimization.
Simple exhaustive searches are rarely sufficient for most real world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes. The solution, for many problems, is to use ""heuristics"" or ""rules of thumb"" that prioritize choices in favor of those that are more likely to reach a goal, and to do so in a shorter number of steps. In some search methodologies heuristics can also serve to entirely eliminate some choices that are unlikely to lead to a goal (called ""pruning the search tree""). Heuristics supply the program with a ""best guess"" for the path on which the solution lies. Heuristics limit the search for solutions into a smaller sample size.
A very different kind of search came to prominence in the 1990s, based on the mathematical theory of optimization. For many problems, it is possible to begin the search with some form of a guess and then refine the guess incrementally until no more refinements can be made. These algorithms can be visualized as blind hill climbing: we begin the search at a random point on the landscape, and then, by jumps or steps, we keep moving our guess uphill, until we reach the top. Other optimization algorithms are simulated annealing, beam search and random optimization.
Evolutionary computation uses a form of optimization search. For example, they may begin with a population of organisms (the guesses) and then allow them to mutate and recombine, selecting only the fittest to survive each generation (refining the guesses). Forms of evolutionary computation include swarm intelligence algorithms (such as ant colony or particle swarm optimization) and evolutionary algorithms (such as genetic algorithms, gene expression programming, and genetic programming).


=== Logic ===

Logic is used for knowledge representation and problem solving, but it can be applied to other problems as well. For example, the satplan algorithm uses logic for planning and inductive logic programming is a method for learning.
Several different forms of logic are used in AI research. Propositional or sentential logic is the logic of statements which can be true or false. First-order logic also allows the use of quantifiers and predicates, and can express facts about objects, their properties, and their relations with each other. Fuzzy logic, is a version of first-order logic which allows the truth of a statement to be represented as a value between 0 and 1, rather than simply True (1) or False (0). Fuzzy systems can be used for uncertain reasoning and have been widely used in modern industrial and consumer product control systems. Subjective logic models uncertainty in a different and more explicit manner than fuzzy-logic: a given binomial opinion satisfies belief + disbelief + uncertainty = 1 within a Beta distribution. By this method, ignorance can be distinguished from probabilistic statements that an agent makes with high confidence.
Default logics, non-monotonic logics and circumscription are forms of logic designed to help with default reasoning and the qualification problem. Several extensions of logic have been designed to handle specific domains of knowledge, such as: description logics; situation calculus, event calculus and fluent calculus (for representing events and time); causal calculus; belief calculus; and modal logics.


=== Probabilistic methods for uncertain reasoning ===

Many problems in AI (in reasoning, planning, learning, perception and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of powerful tools to solve these problems using methods from probability theory and economics.
Bayesian networks are a very general tool that can be used for a large number of problems: reasoning (using the Bayesian inference algorithm), learning (using the expectation-maximization algorithm), planning (using decision networks) and perception (using dynamic Bayesian networks). Bayesian networks are used in AdSense to choose what ads to place and on XBox Live to rate and match players. Probabilistic algorithms can also be used for filtering, prediction, smoothing and finding explanations for streams of data, helping perception systems to analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).
A key concept from the science of economics is ""utility"": a measure of how valuable something is to an intelligent agent. Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis, and information value theory. These tools include models such as Markov decision processes, dynamic decision networks, game theory and mechanism design.


=== Classifiers and statistical learning methods ===

The simplest AI applications can be divided into two types: classifiers (""if shiny then diamond"") and controllers (""if shiny then pick up""). Controllers do, however, also classify conditions before inferring actions, and therefore classification forms a central part of many AI systems. Classifiers are functions that use pattern matching to determine a closest match. They can be tuned according to examples, making them very attractive for use in AI. These examples are known as observations or patterns. In supervised learning, each pattern belongs to a certain predefined class. A class can be seen as a decision that has to be made. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.
A classifier can be trained in various ways; there are many statistical and machine learning approaches. The decision tree is perhaps the most widely used machine learning algorithm. Other widely used classifiers are the neural network, k-nearest neighbor algorithm, kernel methods such as the support vector machine (SVM), Gaussian mixture model and the extremely popular naive Bayes classifier. The performance of these classifiers have been compared over a wide range of tasks. Classifier performance depends greatly on the characteristics of the data to be classified. There is no single classifier that works best on all given problems; this is also referred to as the ""no free lunch"" theorem. Determining a suitable classifier for a given problem is still more an art than science.


=== Artificial neural networks ===

Neural networks, or neural nets, were inspired by the architecture of neurons in the human brain. A simple ""neuron"" N accepts input from multiple other neurons, each of which, when activated (or ""fired""), cast a weighted ""vote"" for or against whether neuron N should itself activate. Learning requires an algorithm to adjust these weights based on the training data; one simple algorithm (dubbed ""fire together, wire together"") is to increase the weight between two connected neurons when the activation of one triggers the successful activation of another. The net forms ""concepts"" that are distributed among a subnetwork of shared neurons that tend to fire together; a concept meaning ""leg"" might be coupled with a subnetwork meaning ""foot"" that includes the sound for ""foot"". Neurons have a continuous spectrum of activation; in addition, neurons can process inputs in a nonlinear way rather than weighing straightforward votes. Modern neural nets can learn both continuous functions and, surprisingly, digital logical operations. Neural networks' early successes included predicting the stock market and (in 1995) a mostly self-driving car. In the 2010s, advances in neural networks using deep learning thrust AI into widespread public consciousness and contributed to an enormous upshift in corporate AI spending; for example, AI-related M&A in 2017 was over 25 times as large as in 2015.
The study of non-learning artificial neural networks began in the decade before the field of AI research was founded, in the work of Walter Pitts and Warren McCullouch. Frank Rosenblatt invented the perceptron, a learning network with a single layer, similar to the old concept of linear regression. Early pioneers also include Alexey Grigorevich Ivakhnenko, Teuvo Kohonen, Stephen Grossberg, Kunihiko Fukushima, Christoph von der Malsburg, David Willshaw, Shun-Ichi Amari, Bernard Widrow, John Hopfield, Eduardo R. Caianiello, and others.
The main categories of networks are acyclic or feedforward neural networks (where the signal passes in only one direction) and recurrent neural networks (which allow feedback and short-term memories of previous input events). Among the most popular feedforward networks are perceptrons, multi-layer perceptrons and radial basis networks. Neural networks can be applied to the problem of intelligent control (for robotics) or learning, using such techniques as Hebbian learning (""fire together, wire together""), GMDH or competitive learning.
Today, neural networks are often trained by the backpropagation algorithm, which had been around since 1970 as the reverse mode of automatic differentiation published by Seppo Linnainmaa, and was introduced to neural networks by Paul Werbos.
Hierarchical temporal memory is an approach that models some of the structural and algorithmic properties of the neocortex.
In short, most neural networks use some form of gradient descent on a hand-created neural topology. However, some research groups, such as Uber, argue that simple neuroevolution to mutate new neural network topologies and weights may be competitive with sophisticated gradient descent approaches. One advantage of neuroevolution is that it may be less prone to get caught in ""dead ends"".


==== Deep feedforward neural networks ====

Deep learning is any artificial neural network that can learn a long chain of causal links. For example, a feedforward network with six hidden layers can learn a seven-link causal chain (six hidden layers + output layer) and has a ""credit assignment path"" (CAP) depth of seven. Many deep learning systems need to be able to learn chains ten or more causal links in length. Deep learning has transformed many important subfields of artificial intelligence, including computer vision, speech recognition, natural language processing and others.
According to one overview, the expression ""Deep Learning"" was introduced to the Machine Learning community by Rina Dechter in 1986 and gained traction after Igor Aizenberg and colleagues introduced it to Artificial Neural Networks in 2000. The first functional Deep Learning networks were published by Alexey Grigorevich Ivakhnenko and V. G. Lapa in 1965. These networks are trained one layer at a time. Ivakhnenko's 1971 paper describes the learning of a deep feedforward multilayer perceptron with eight layers, already much deeper than many later networks. In 2006, a publication by Geoffrey Hinton and Ruslan Salakhutdinov introduced another way of pre-training many-layered feedforward neural networks (FNNs) one layer at a time, treating each layer in turn as an unsupervised restricted Boltzmann machine, then using supervised backpropagation for fine-tuning. Similar to shallow artificial neural networks, deep neural networks can model complex non-linear relationships. Over the last few years, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer.
Deep learning often uses convolutional neural networks (CNNs), whose origins can be traced back to the Neocognitron introduced by Kunihiko Fukushima in 1980. In 1989, Yann LeCun and colleagues applied backpropagation to such an architecture. In the early 2000s, in an industrial application CNNs already processed an estimated 10% to 20% of all the checks written in the US. Since 2011, fast implementations of CNNs on GPUs have won many visual pattern recognition competitions.
CNNs with 12 convolutional layers were used in conjunction with reinforcement learning by Deepmind's ""AlphaGo Lee"", the program that beat a top Go champion in 2016.


==== Deep recurrent neural networks ====

Early on, deep learning was also applied to sequence learning with recurrent neural networks (RNNs) which are in theory Turing complete and can run arbitrary programs to process arbitrary sequences of inputs. The depth of an RNN is unlimited and depends on the length of its input sequence; thus, an RNN is an example of deep learning. RNNs can be trained by gradient descent but suffer from the vanishing gradient problem. In 1992, it was shown that unsupervised pre-training of a stack of recurrent neural networks can speed up subsequent supervised learning of deep sequential problems.
Numerous researchers now use variants of a deep learning recurrent NN called the long short-term memory (LSTM) network published by Hochreiter & Schmidhuber in 1997. LSTM is often trained by Connectionist Temporal Classification (CTC). At Google, Microsoft and Baidu this approach has revolutionised speech recognition. For example, in 2015, Google's speech recognition experienced a dramatic performance jump of 49% through CTC-trained LSTM, which is now available through Google Voice to billions of smartphone users. Google also used LSTM to improve machine translation, Language Modeling and Multilingual Language Processing. LSTM combined with CNNs also improved automatic image captioning and a plethora of other applications.


=== Languages ===

Early symbolic AI inspired Lisp and Prolog, which dominated early AI programming. Modern AI development often uses mainstream languages such as Python or C++, or niche languages such as Wolfram Language.


=== Evaluating progress ===

In 1950, Alan Turing proposed a general procedure to test the intelligence of an agent now known as the Turing test. This procedure allows almost all the major problems of artificial intelligence to be tested. However, it is a very difficult challenge and at present all agents fail.
Artificial intelligence can also be evaluated on specific problems such as small problems in chemistry, hand-writing recognition and game-playing. Such tests have been termed subject matter expert Turing tests. Smaller problems provide more achievable goals and there are an ever-increasing number of positive results.
For example, performance at draughts (i.e. checkers) is optimal, performance at chess is high-human and nearing super-human (see computer chess: computers versus human) and performance at many everyday tasks (such as recognizing a face or crossing a room without bumping into something) is sub-human.
A quite different approach measures machine intelligence through tests which are developed from mathematical definitions of intelligence. Examples of these kinds of tests start in the late nineties devising intelligence tests using notions from Kolmogorov complexity and data compression. Two major advantages of mathematical definitions are their applicability to nonhuman intelligences and their absence of a requirement for human testers.
A derivative of the Turing test is the Completely Automated Public Turing test to tell Computers and Humans Apart (CAPTCHA). As the name implies, this helps to determine that a user is an actual person and not a computer posing as a human. In contrast to the standard Turing test, CAPTCHA is administered by a machine and targeted to a human as opposed to being administered by a human and targeted to a machine. A computer asks a user to complete a simple test then generates a grade for that test. Computers are unable to solve the problem, so correct solutions are deemed to be the result of a person taking the test. A common type of CAPTCHA is the test that requires the typing of distorted letters, numbers or symbols that appear in an image undecipherable by a computer.


== Applications ==

AI is relevant to any intellectual task. Modern artificial intelligence techniques are pervasive and are too numerous to list here. Frequently, when a technique reaches mainstream use, it is no longer considered artificial intelligence; this phenomenon is described as the AI effect.
High-profile examples of AI include autonomous vehicles (such as drones and self-driving cars), medical diagnosis, creating art (such as poetry), proving mathematical theorems, playing games (such as Chess or Go), search engines (such as Google search), online assistants (such as Siri), image recognition in photographs, spam filtering, prediction of judicial decisions and targeting online advertisements.
With social media sites overtaking TV as a source for news for young people and news organisations increasingly reliant on social media platforms for generating distribution, major publishers now use artificial intelligence (AI) technology to post stories more effectively and generate higher volumes of traffic.


=== Competitions and prizes ===

There are a number of competitions and prizes to promote research in artificial intelligence. The main areas promoted are: general machine intelligence, conversational behavior, data-mining, robotic cars, robot soccer and games.


=== Healthcare ===

Artificial intelligence is breaking into the healthcare industry by assisting doctors. According to Bloomberg Technology, Microsoft has developed AI to help doctors find the right treatments for cancer.  There is a great amount of research and drugs developed relating to cancer. In detail, there are more than 800 medicines and vaccines to treat cancer. This negatively affects the doctors, because there are too many options to choose from, making it more difficult to choose the right drugs for the patients. Microsoft is working on a project to develop a machine called ""Hanover"". Its goal is to memorize all the papers necessary to cancer and help predict which combinations of drugs will be most effective for each patient. One project that is being worked on at the moment is fighting myeloid leukemia, a fatal cancer where the treatment has not improved in decades. Another study was reported to have found that artificial intelligence was as good as trained doctors in identifying skin cancers. Another study is using artificial intelligence to try and monitor multiple high-risk patients, and this is done by asking each patient numerous questions based on data acquired from live doctor to patient interactions.
According to CNN, there was a recent study by surgeons at the Children's National Medical Center in Washington which successfully demonstrated surgery with an autonomous robot. The team supervised the robot while it performed soft-tissue surgery, stitching together a pig's bowel during open surgery, and doing so better than a human surgeon, the team claimed. IBM has created its own artificial intelligence computer, the IBM Watson, which has beaten human intelligence (at some levels). Watson not only won at the game show Jeopardy! against former champions, but, was declared a hero after successfully diagnosing a women who was suffering from leukemia.


=== Automotive ===
Advancements in AI have contributed to the growth of the automotive industry through the creation and evolution of self-driving vehicles. As of 2016, there are over 30 companies utilizing AI into the creation of driverless cars. A few companies involved with AI include Tesla, Google, and Apple.
Many components contribute to the functioning of self-driving cars. These vehicles incorporate systems such as braking, lane changing, collision prevention, navigation and mapping. Together, these systems, as well as high performance computers, are integrated into one complex vehicle.
Recent developments in autonomous automobiles have made the innovation of self-driving trucks possible, though they are still in the testing phase. The UK government has passed legislation to begin testing of self-driving truck platoons in 2018. Self-driving truck platoons are a fleet of self-driving trucks following the lead of one non-self-driving truck, so the truck platoons aren't entirely autonomous yet. Meanwhile, the Daimler, a German automobile corporation, is testing the Freightliner Inspiration which is a semi-autonomous truck that will only be used on the highway.
One main factor that influences the ability for a driver-less automobile to function is mapping. In general, the vehicle would be pre-programmed with a map of the area being driven. This map would include data on the approximations of street light and curb heights in order for the vehicle to be aware of its surroundings. However, Google has been working on an algorithm with the purpose of eliminating the need for pre-programmed maps and instead, creating a device that would be able to adjust to a variety of new surroundings. Some self-driving cars are not equipped with steering wheels or brake pedals, so there has also been research focused on creating an algorithm that is capable of maintaining a safe environment for the passengers in the vehicle through awareness of speed and driving conditions.
Another factor that is influencing the ability for a driver-less automobile is the safety of the passenger. To make a driver-less automobile, engineers must program it to handle high risk situations. These situations could include a head on collision with pedestrians. The car's main goal should be to make a decision that would avoid hitting the pedestrians and saving the passengers in the car. But there is a possibility the car would need to make a decision that would put someone in danger. In other words, the car would need to decide to save the pedestrians or the passengers. The programing of the car in these situations is crucial to a successful driver-less automobile.


=== Finance and economics ===
Financial institutions have long used artificial neural network systems to detect charges or claims outside of the norm, flagging these for human investigation. The use of AI in banking can be traced back to 1987 when Security Pacific National Bank in US set-up a Fraud Prevention Task force to counter the unauthorised use of debit cards. Programs like Kasisto and Moneystream are using AI in financial services.
Banks use artificial intelligence systems today to organize operations, maintain book-keeping, invest in stocks, and manage properties. AI can react to changes overnight or when business is not taking place. In August 2001, robots beat humans in a simulated financial trading competition. AI has also reduced fraud and financial crimes by monitoring behavioral patterns of users for any abnormal changes or anomalies.
The use of AI machines in the market in applications such as online trading and decision making has changed major economic theories. For example, AI based buying and selling platforms have changed the law of supply and demand in that it is now possible to easily estimate individualized demand and supply curves and thus individualized pricing. Furthermore, AI machines reduce information asymmetry in the market and thus making markets more efficient while reducing the volume of trades. Furthermore, AI in the markets limits the consequences of behavior in the markets again making markets more efficient. Other theories where AI has had impact include in rational choice, rational expectations, game theory, Lewis turning point, portfolio optimization and counterfactual thinking.


=== Video games ===

In video games, artificial intelligence is routinely used to generate dynamic purposeful behavior in non-player characters (NPCs). In addition, well-understood AI techniques are routinely used for pathfinding. Some researchers consider NPC AI in games to be a ""solved problem"" for most production tasks. Games with more atypical AI include the AI director of Left 4 Dead (2008) and the neuroevolutionary training of platoons in Supreme Commander 2 (2010).


== Platforms ==
A platform (or ""computing platform"") is defined as ""some sort of hardware architecture or software framework (including application frameworks), that allows software to run"". As Rodney Brooks pointed out many years ago, it is not just the artificial intelligence software that defines the AI features of the platform, but rather the actual platform itself that affects the AI that results, i.e., there needs to be work in AI problems on real-world platforms rather than in isolation.
A wide variety of platforms has allowed different aspects of AI to develop, ranging from expert systems such as Cyc to deep-learning frameworks to robot platforms such as the Roomba with open interface. Recent advances in deep artificial neural networks and distributed computing have led to a proliferation of software libraries, including Deeplearning4j, TensorFlow, Theano and Torch.
Collective AI is a platform architecture that combines individual AI into a collective entity, in order to achieve global results from individual behaviors. With its collective structure, developers can crowdsource information and extend the functionality of existing AI domains on the platform for their own use, as well as continue to create and share new domains and capabilities for the wider community and greater good. As developers continue to contribute, the overall platform grows more intelligent and is able to perform more requests, providing a scalable model for greater communal benefit. Organizations like SoundHound Inc. and the Harvard John A. Paulson School of Engineering and Applied Sciences have used this collaborative AI model.


=== Education in AI ===
A McKinsey Global Institute study found a shortage of 1.5 million highly trained data and AI professionals and managers and a number of private bootcamps have developed programs to meet that demand, including free programs like The Data Incubator or paid programs like General Assembly.


=== Partnership on AI ===
Amazon, Google, Facebook, IBM, and Microsoft have established a non-profit partnership to formulate best practices on artificial intelligence technologies, advance the public's understanding, and to serve as a platform about artificial intelligence. They stated: ""This partnership on AI will conduct research, organize discussions, provide thought leadership, consult with relevant third parties, respond to questions from the public and media, and create educational material that advance the understanding of AI technologies including machine perception, learning, and automated reasoning."" Apple joined other tech companies as a founding member of the Partnership on AI in January 2017. The corporate members will make financial and research contributions to the group, while engaging with the scientific community to bring academics onto the board.


== Philosophy and ethics ==

There are three philosophical questions related to AI:
Is artificial general intelligence possible? Can a machine solve any problem that a human being can solve using intelligence? Or are there hard limits to what a machine can accomplish?
Are intelligent machines dangerous? How can we ensure that machines behave ethically and that they are used ethically?
Can a machine have a mind, consciousness and mental states in exactly the same sense that human beings do? Can a machine be sentient, and thus deserve certain rights? Can a machine intentionally cause harm?


=== The limits of artificial general intelligence ===

Can a machine be intelligent? Can it ""think""?
Alan Turing's ""polite convention""
We need not decide if a machine can ""think""; we need only decide if a machine can act as intelligently as a human being. This approach to the philosophical problems associated with artificial intelligence forms the basis of the Turing test.
The Dartmouth proposal
""Every aspect of learning or any other feature of intelligence can be so precisely described that a machine can be made to simulate it."" This conjecture was printed in the proposal for the Dartmouth Conference of 1956, and represents the position of most working AI researchers.
Newell and Simon's physical symbol system hypothesis
""A physical symbol system has the necessary and sufficient means of general intelligent action."" Newell and Simon argue that intelligence consists of formal operations on symbols. Hubert Dreyfus argued that, on the contrary, human expertise depends on unconscious instinct rather than conscious symbol manipulation and on having a ""feel"" for the situation rather than explicit symbolic knowledge. (See Dreyfus' critique of AI.)
Gödelian arguments
Gödel himself, John Lucas (in 1961) and Roger Penrose (in a more detailed argument from 1989 onwards) made highly technical arguments that human mathematicians can consistently see the truth of their own ""Gödel statements"" and therefore have computational abilities beyond that of mechanical Turing machines. However, the modern consensus in the scientific and mathematical community is that these ""Gödelian arguments"" fail.
The artificial brain argument
The brain can be simulated by machines and because brains are intelligent, simulated brains must also be intelligent; thus machines can be intelligent. Hans Moravec, Ray Kurzweil and others have argued that it is technologically feasible to copy the brain directly into hardware and software, and that such a simulation will be essentially identical to the original.
The AI effect
Machines are already intelligent, but observers have failed to recognize it. When Deep Blue beat Garry Kasparov in chess, the machine was acting intelligently. However, onlookers commonly discount the behavior of an artificial intelligence program by arguing that it is not ""real"" intelligence after all; thus ""real"" intelligence is whatever intelligent behavior people can do that machines still cannot. This is known as the AI Effect: ""AI is whatever hasn't been done yet.""


=== Potential risks and moral reasoning ===
Widespread use of artificial intelligence could have unintended consequences that are dangerous or undesirable. Scientists from the Future of Life Institute, among others, described some short-term research goals to see how AI influences the economy, the laws and ethics that are involved with AI and how to minimize AI security risks. In the long-term, the scientists have proposed to continue optimizing function while minimizing possible security risks that come along with new technologies.
Machines with intelligence have the potential to use their intelligence to make ethical decisions. Research in this area includes ""machine ethics"", ""artificial moral agents"", and the study of ""malevolent vs. friendly AI"".


==== Existential risk ====

The development of full artificial intelligence could spell the end of the human race. Once humans develop artificial intelligence, it will take off on its own and redesign itself at an ever-increasing rate. Humans, who are limited by slow biological evolution, couldn't compete and would be superseded.

A common concern about the development of artificial intelligence is the potential threat it could pose to humanity. This concern has recently gained attention after mentions by celebrities including the late Stephen Hawking, Bill Gates, and Elon Musk. A group of prominent tech titans including Peter Thiel, Amazon Web Services and Musk have committed $1billion to OpenAI a nonprofit company aimed at championing responsible AI development. The opinion of experts within the field of artificial intelligence is mixed, with sizable fractions both concerned and unconcerned by risk from eventual superhumanly-capable AI.
In his book Superintelligence, Nick Bostrom provides an argument that artificial intelligence will pose a threat to mankind. He argues that sufficiently intelligent AI, if it chooses actions based on achieving some goal, will exhibit convergent behavior such as acquiring resources or protecting itself from being shut down. If this AI's goals do not reflect humanity's – one example is an AI told to compute as many digits of pi as possible – it might harm humanity in order to acquire more resources or prevent itself from being shut down, ultimately to better achieve its goal.
For this danger to be realized, the hypothetical AI would have to overpower or out-think all of humanity, which a minority of experts argue is a possibility far enough in the future to not be worth researching. Other counterarguments revolve around humans being either intrinsically or convergently valuable from the perspective of an artificial intelligence.
Concern over risk from artificial intelligence has led to some high-profile donations and investments. In January 2015, Elon Musk donated ten million dollars to the Future of Life Institute to fund research on understanding AI decision making. The goal of the institute is to ""grow wisdom with which we manage"" the growing power of technology. Musk also funds companies developing artificial intelligence such as Google DeepMind and Vicarious to ""just keep an eye on what's going on with artificial intelligence. I think there is potentially a dangerous outcome there.""
Development of militarized artificial intelligence is a related concern. Currently, 50+ countries are researching battlefield robots, including the United States, China, Russia, and the United Kingdom. Many people concerned about risk from superintelligent AI also want to limit the use of artificial soldiers.


==== Devaluation of humanity ====

Joseph Weizenbaum wrote that AI applications cannot, by definition, successfully simulate genuine human empathy and that the use of AI technology in fields such as customer service or psychotherapy was deeply misguided. Weizenbaum was also bothered that AI researchers (and some philosophers) were willing to view the human mind as nothing more than a computer program (a position now known as computationalism). To Weizenbaum these points suggest that AI research devalues human life.


==== Decrease in demand for human labor ====

The relationship between automation and employment is complicated. While automation eliminates old jobs, it also creates new jobs through micro-economic and macro-economic effects. Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist states that ""the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution"" is ""worth taking seriously"". Subjective estimates of the risk vary widely; for example, Michael Osborne and Carl Benedikt Frey estimate 47% of U.S. jobs are at ""high risk"" of potential automation, while an OECD report classifies only 9% of U.S. jobs as ""high risk"". Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy. Author Martin Ford and others go further and argue that a large number of jobs are routine, repetitive and (to an AI) predictable; Ford warns that these jobs may be automated in the next couple of decades, and that many of the new jobs may not be ""accessible to people with average capability"", even with retraining. Economists point out that in the past technology has tended to increase rather than reduce total employment, but acknowledge that ""we're in uncharted territory"" with AI.


==== Artificial moral agents ====
This raises the issue of how ethically the machine should behave towards both humans and other AI agents. This issue was addressed by Wendell Wallach in his book titled Moral Machines in which he introduced the concept of artificial moral agents (AMA). For Wallach, AMAs have become a part of the research landscape of artificial intelligence as guided by its two central questions which he identifies as ""Does Humanity Want Computers Making Moral Decisions"" and ""Can (Ro)bots Really Be Moral"". For Wallach the question is not centered on the issue of whether machines can demonstrate the equivalent of moral behavior in contrast to the constraints which society may place on the development of AMAs.


==== Machine ethics ====

The field of machine ethics is concerned with giving machines ethical principles, or a procedure for discovering a way to resolve the ethical dilemmas they might encounter, enabling them to function in an ethically responsible manner through their own ethical decision making. The field was delineated in the AAAI Fall 2005 Symposium on Machine Ethics: ""Past research concerning the relationship between technology and ethics has largely focused on responsible and irresponsible use of technology by human beings, with a few people being interested in how human beings ought to treat machines. In all cases, only human beings have engaged in ethical reasoning. The time has come for adding an ethical dimension to at least some machines. Recognition of the ethical ramifications of behavior involving machines, as well as recent and potential developments in machine autonomy, necessitate this. In contrast to computer hacking, software property issues, privacy issues and other topics normally ascribed to computer ethics, machine ethics is concerned with the behavior of machines towards human users and other machines. Research in machine ethics is key to alleviating concerns with autonomous systems—it could be argued that the notion of autonomous machines without such a dimension is at the root of all fear concerning machine intelligence. Further, investigation of machine ethics could enable the discovery of problems with current ethical theories, advancing our thinking about Ethics."" Machine ethics is sometimes referred to as machine morality, computational ethics or computational morality. A variety of perspectives of this nascent field can be found in the collected edition ""Machine Ethics"" that stems from the AAAI Fall 2005 Symposium on Machine Ethics.


==== Malevolent and friendly AI ====

Political scientist Charles T. Rubin believes that AI can be neither designed nor guaranteed to be benevolent. He argues that ""any sufficiently advanced benevolence may be indistinguishable from malevolence."" Humans should not assume machines or robots would treat us favorably, because there is no a priori reason to believe that they would be sympathetic to our system of morality, which has evolved along with our particular biology (which AIs would not share). Hyper-intelligent software may not necessarily decide to support the continued existence of humanity, and would be extremely difficult to stop. This topic has also recently begun to be discussed in academic publications as a real source of risks to civilization, humans, and planet Earth.
Physicist Stephen Hawking, Microsoft founder Bill Gates, and SpaceX founder Elon Musk have expressed concerns about the possibility that AI could evolve to the point that humans could not control it, with Hawking theorizing that this could ""spell the end of the human race"".
One proposal to deal with this is to ensure that the first generally intelligent AI is 'Friendly AI', and will then be able to control subsequently developed AIs. Some question whether this kind of check could really remain in place.
Leading AI researcher Rodney Brooks writes, ""I think it is a mistake to be worrying about us developing malevolent AI anytime in the next few hundred years. I think the worry stems from a fundamental error in not distinguishing the difference between the very real recent advances in a particular aspect of AI, and the enormity and complexity of building sentient volitional intelligence.""


=== Machine consciousness, sentience and mind ===

If an AI system replicates all key aspects of human intelligence, will that system also be sentient – will it have a mind which has conscious experiences? This question is closely related to the philosophical problem as to the nature of human consciousness, generally referred to as the hard problem of consciousness.


==== Consciousness ====


==== Computationalism and functionalism ====

Computationalism is the position in the philosophy of mind that the human mind or the human brain (or both) is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind-body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.


==== Strong AI hypothesis ====

The philosophical position that John Searle has named ""strong AI"" states: ""The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds."" Searle counters this assertion with his Chinese room argument, which asks us to look inside the computer and try to find where the ""mind"" might be.


==== Robot rights ====

Mary Shelley's Frankenstein considers a key issue in the ethics of artificial intelligence: if a machine can be created that has intelligence, could it also feel? If it can feel, does it have the same rights as a human? The idea also appears in modern science fiction, such as the film A.I.: Artificial Intelligence, in which humanoid machines have the ability to feel emotions. This issue, now known as ""robot rights"", is currently being considered by, for example, California's Institute for the Future, although many critics believe that the discussion is premature. Some critics of transhumanism argue that any hypothetical robot rights would lie on a spectrum with animal rights and human rights. The subject is profoundly discussed in the 2010 documentary film Plug & Pray.


=== Superintelligence ===

Are there limits to how intelligent machines – or human-machine hybrids – can be? A superintelligence, hyperintelligence, or superhuman intelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind. ‘’Superintelligence’’ may also refer to the form or degree of intelligence possessed by such an agent.


==== Technological singularity ====

If research into Strong AI produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to recursive self-improvement. The new intelligence could thus increase exponentially and dramatically surpass humans. Science fiction writer Vernor Vinge named this scenario ""singularity"". Technological singularity is when accelerating progress in technologies will cause a runaway effect wherein artificial intelligence will exceed human intellectual capacity and control, thus radically changing or even ending civilization. Because the capabilities of such an intelligence may be impossible to comprehend, the technological singularity is an occurrence beyond which events are unpredictable or even unfathomable.
Ray Kurzweil has used Moore's law (which describes the relentless exponential improvement in digital technology) to calculate that desktop computers will have the same processing power as human brains by the year 2029, and predicts that the singularity will occur in 2045.


==== Transhumanism ====

You awake one morning to find your brain has another lobe functioning. Invisible, this auxiliary lobe answers your questions with information beyond the realm of your own memory, suggests plausible courses of action, and asks questions that help bring out relevant facts. You quickly come to rely on the new lobe so much that you stop wondering how it works. You just use it. This is the dream of artificial intelligence.

Robot designer Hans Moravec, cyberneticist Kevin Warwick and inventor Ray Kurzweil have predicted that humans and machines will merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, which has roots in Aldous Huxley and Robert Ettinger, has been illustrated in fiction as well, for example in the manga Ghost in the Shell and the science-fiction series Dune.
In the 1980s artist Hajime Sorayama's Sexy Robots series were painted and published in Japan depicting the actual organic human form with lifelike muscular metallic skins and later ""the Gynoids"" book followed that was used by or influenced movie makers including George Lucas and other creatives. Sorayama never considered these organic robots to be real part of nature but always unnatural product of the human mind, a fantasy existing in the mind even when realized in actual form.
Edward Fredkin argues that ""artificial intelligence is the next stage in evolution"", an idea first proposed by Samuel Butler's ""Darwin among the Machines"" (1863), and expanded upon by George Dyson in his book of the same name in 1998.


== In fiction ==

Thought-capable artificial beings appeared as storytelling devices since antiquity.

The implications of a constructed machine exhibiting artificial intelligence have been a persistent theme in science fiction since the twentieth century. Early stories typically revolved around intelligent robots. The word ""robot"" itself was coined by Karel Čapek in his 1921 play R.U.R., the title standing for ""Rossum's Universal Robots"". Later, the SF writer Isaac Asimov developed the Three Laws of Robotics. He subsequently explored these in his many books, most notably the ""Multivac"" series about a super-intelligent computer of the same name. Asimov's laws are often brought up during layman discussions of machine ethics; while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.
The novel Do Androids Dream of Electric Sheep?, by Philip K. Dick, tells a science fiction story about Androids and humans clashing in a futuristic world. Elements of artificial intelligence include the empathy box, mood organ, and the androids themselves. Throughout the novel, Dick portrays the idea that human subjectivity is altered by technology created with artificial intelligence.
Nowadays AI is firmly rooted in popular culture; intelligent robots appear in innumerable works. HAL 9000, the murderous computer in charge of the Discovery One spaceship in Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), is an example of the common ""robotic rampage"" archetype in science fiction movies. The Terminator (1984) and The Matrix (1999) provide additional widely familiar examples. In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.


== See also ==


== Explanatory notes ==


== References ==


=== AI textbooks ===


=== History of AI ===


=== Other sources ===


== Further reading ==
DH Autor, ‘Why Are There Still So Many Jobs? The History and Future of Workplace Automation’ (2015) 29(3) Journal of Economic Perspectives 3.
TechCast Article Series, John Sagi, ""Framing Consciousness""
Boden, Margaret, Mind As Machine, Oxford University Press, 2006
Gopnik, Alison, ""Making AI More Human: Artificial intelligence has staged a revival by starting to incorporate what we know about how children learn"", Scientific American, vol. 316, no. 6 (June 2017), pp. 60–65.
Johnston, John (2008) The Allure of Machinic Life: Cybernetics, Artificial Life, and the New AI, MIT Press
Marcus, Gary, ""Am I Human?: Researchers need new ways to distinguish artificial intelligence from the natural kind"", Scientific American, vol. 316, no. 3 (March 2017), pp. 58–63. Multiple tests of artificial-intelligence efficacy are needed because, ""just as there is no single test of athletic prowess, there cannot be one ultimate test of intelligence."" One such test, a ""Construction Challenge"", would test perception and physical action—""two important elements of intelligent behavior that were entirely absent from the original Turing test."" Another proposal has been to give machines the same standardized tests of science and other disciplines that schoolchildren take. A so far insuperable stumbling block to artificial intelligence is an incapacity for reliable disambiguation. ""[V]irtually every sentence [that people generate] is ambiguous, often in multiple ways."" A prominent example is known as the ""pronoun disambiguation problem"": a machine has no way of determining to whom or what a pronoun in a sentence—such as ""he"", ""she"" or ""it""—refers.
E McGaughey, 'Will Robots Automate Your Job Away? Full Employment, Basic Income, and Economic Democracy' (2018) SSRN, part 2(3).
Myers, Courtney Boyd ed. (2009). ""The AI Report"". Forbes June 2009
Raphael, Bertram (1976). The Thinking Computer. W.H.Freeman and Company. ISBN 0-7167-0723-3. 
Serenko, Alexander (2010). ""The development of an AI journal ranking based on the revealed preference approach"" (PDF). Journal of Informetrics. 4 (4): 447–459. doi:10.1016/j.joi.2010.04.001. 
Serenko, Alexander; Michael Dohan (2011). ""Comparing the expert survey and citation impact journal ranking methods: Example from the field of Artificial Intelligence"" (PDF). Journal of Informetrics. 5 (4): 629–649. doi:10.1016/j.joi.2011.06.002. 
Sun, R. & Bookman, L. (eds.), Computational Architectures: Integrating Neural and Symbolic Processes. Kluwer Academic Publishers, Needham, MA. 1994.
Tom Simonite (29 December 2014). ""2014 in Computing: Breakthroughs in Artificial Intelligence"". MIT Technology Review. 


== External links ==

What Is AI? – An introduction to artificial intelligence by John McCarthy—a co-founder of the field, and the person who coined the term.
The Handbook of Artificial Intelligence Volume Ⅰ by Avron Barr and Edward A. Feigenbaum (Stanford University)
""Artificial Intelligence"". Internet Encyclopedia of Philosophy. 
Thomason, Richmond. ""Logic and Artificial Intelligence"". In Zalta, Edward N. Stanford Encyclopedia of Philosophy. 
AI at Curlie (based on DMOZ)
AITopics – A large directory of links and other resources maintained by the Association for the Advancement of Artificial Intelligence, the leading organization of academic AI researchers.
List of AI Conferences – A list of 225 AI conferences taking place all over the world."
1,Comparison of programming languages (string functions),3681422,109570,"String functions are used in computer programming languages to manipulate a string or query information about a string (some do both).
Most programming languages that have a string datatype will have some string functions although there may be other low-level ways within each language to handle strings directly. In object-oriented languages, string functions are often implemented as properties and methods of string objects. In functional and list-based languages a string is represented as a list (of character codes), therefore all list-manipulation procedures could be considered string functions. However such languages may implement a subset of explicit string-specific functions as well.
For function that manipulate strings, modern object-oriented languages, like C# and Java have immutable strings and return a copy (in newly allocated dynamic memory), while others, like C manipulate the original string unless the programmer copies data to a new string. See for example Concatenation below.
The most basic example of a string function is the length(string) function. This function returns the length of a string literal.
e.g. length(""hello world"") would return 11.
Other languages may have string functions with similar or exactly the same syntax or parameters or outcomes. For example, in many languages the length function is usually represented as len(string). The below list of common functions aims to help limit this confusion.


== Common string functions (multi language reference) ==
String functions common to many languages are listed below, including the different names used. The below list of common functions aims to help programmers find the equivalent function in a language. Note, string concatenation and regular expressions are handled in separate pages. Statements in guillemets (« … ») are optional.


=== CharAt ===

# Example in ALGOL 68 #
""Hello, World""[2];         // 'e'


=== Compare (integer result) ===


=== Compare (relational operator-based, Boolean result) ===


=== Concatenation ===


=== Contains ===

¢ Example in ALGOL 68 ¢
string in string(""e"", loc int, ""Hello mate"");      ¢ returns true ¢
string in string(""z"", loc int, ""word"");            ¢ returns false ¢


=== Equality ===
Tests if two strings are equal. See also #Compare and #Compare. Note that doing equality checks via a generic Compare with integer result is not only confusing for the programmer but is often a significantly more expensive operation; this is especially true when using ""C-strings"".


=== Find ===


=== Find character ===

^a Given a set of characters, SCAN returns the position of the first character found, while VERIFY returns the position of the first character that does not belong to the set.


=== Format ===

/* example in PL/I */
put string(some_string) edit('My ', 'pen', ' costs', 19.99)(a,a,a,p'$$$V.99')
/* returns ""My pen costs $19.99"" */


=== Inequality ===
Tests if two strings are not equal. See also #Equality.


=== index ===
see #Find


=== indexof ===
see #Find


=== instr ===
see #Find


=== instrrev ===
see #rfind


=== join ===


=== lastindexof ===
see #rfind


=== left ===


=== len ===
see #length


=== length ===


=== locate ===
see #Find


=== Lowercase ===


=== mid ===
see #substring


=== partition ===


=== replace ===


=== reverse ===


=== rfind ===


=== right ===


=== rpartition ===


=== slice ===
see #substring


=== split ===


=== sprintf ===
see #Format


=== strip ===
see #trim


=== strcmp ===
see #Compare (integer result)


=== substring ===


=== Uppercase ===


=== trim ===

trim or strip is used to remove whitespace from the beginning, end, or both beginning and end, of a string.
Other languages
In languages without a built-in trim function, it is usually simple to create a custom function which accomplishes the same task.


==== AWK ====
In AWK, one can use regular expressions to trim:

or:


==== C/C++ ====
There is no standard trim function in C or C++. Most of the available string libraries for C contain code which implements trimming, or functions that significantly ease an efficient implementation. The function has also often been called EatWhitespace in some non-standard C libraries.
In C, programmers often combine a ltrim and rtrim to implement trim:

The open source C++ library Boost has several trim variants, including a standard one:

Note that with boost's function named simply trim the input sequence is modified in-place, and does not return a result.
Another open source C++ library Qt has several trim variants, including a standard one:

The Linux kernel also includes a strip function, strstrip(), since 2.6.18-rc1, which trims the string ""in place"". Since 2.6.33-rc1, the kernel uses strim() instead of strstrip() to avoid false warnings.


==== Haskell ====
A trim algorithm in Haskell:

may be interpreted as follows: f drops the preceding whitespace, and reverses the string. f is then again applied to its own output. Note that the type signature (the second line) is optional.


==== J ====
The trim algorithm in J is a functional description:

That is: filter (#~) for non-space characters (' '&~:) between leading (+./\) and (*.) trailing (+./\.) spaces.


==== JavaScript ====
There is a built-in trim function in JavaScript 1.8.1 (Firefox 3.5 and later), and the ECMAScript 5 standard. In earlier versions it can be added to the String object's prototype as follows:


==== Perl ====
Perl 5 has no built-in trim function. However, the functionality is commonly achieved using regular expressions.
Example:

or:

These examples modify the value of the original variable $string.
Also available for Perl is StripLTSpace in String::Strip from CPAN.
There are, however, two functions that are commonly used to strip whitespace from the end of strings, chomp and chop:
chop removes the last character from a string and returns it.
chomp removes the trailing newline character(s) from a string if present. (What constitutes a newline is $INPUT_RECORD_SEPARATOR dependent).
In Perl 6, the upcoming major revision of the language, strings have a trim method.
Example:


==== Tcl ====
The Tcl string command has three relevant subcommands: trim, trimright and trimleft. For each of those commands, an additional argument may be specified: a string that represents a set of characters to remove—the default is whitespace (space, tab, newline, carriage return).
Example of trimming vowels:


==== XSLT ====
XSLT includes the function normalize-space(string) which strips leading and trailing whitespace, in addition to replacing any whitespace sequence (including line breaks) with a single space.
Example:

XSLT 2.0 includes regular expressions, providing another mechanism to perform string trimming.
Another XSLT technique for trimming is to utilize the XPath 2.0 substring() function.


== References ==


== External links ==
Perl String Functions
Python 2 String Methods
Python 3 String Methods
Scheme String Procedures
Erlang String Functions
.NET String Methods and Properties
Ruby String Class
PHP String functions
java.lang.String members
Haskell Hierarchical Libraries
Arrays in D (in D strings are regular arrays)
std.string from Phobos (D standard library)
Lua String Functions
OCaml String module
Common Lisp String manipulation
FreeBASIC String Functions
Reference.wolfram.com
Tcl reference for string commands"
2,Geographic information system,12398,77692,"A geographic information system (GIS) is a system designed to capture, store, manipulate, analyze, manage, and present spatial or geographic data. The acronym GIS is sometimes used for geographic information science (GIScience) to refer to the academic discipline that studies geographic information systems and is a large domain within the broader academic discipline of geoinformatics. What goes beyond a GIS is a spatial data infrastructure, a concept that has no such restrictive boundaries.
In general, the term describes any information system that integrates, stores, edits, analyzes, shares, and displays geographic information. GIS applications are tools that allow users to create interactive queries (user-created searches), analyze spatial information, edit data in maps, and present the results of all these operations. Geographic information science is the science underlying geographic concepts, applications, and systems.
GIS can refer to a number of different technologies, processes, and methods. It is attached to many operations and has many applications related to engineering, planning, management, transport/logistics, insurance, telecommunications, and business. For that reason, GIS and location intelligence applications can be the foundation for many location-enabled services that rely on analysis and visualization.
GIS can relate unrelated information by using location as the key index variable. Locations or extents in the Earth space–time may be recorded as dates/times of occurrence, and x, y, and z coordinates representing, longitude, latitude, and elevation, respectively. All Earth-based spatial–temporal location and extent references should be relatable to one another and ultimately to a ""real"" physical location or extent. This key characteristic of GIS has begun to open new avenues of scientific inquiry.


== History of development ==
The first known use of the term ""geographic information system"" was by Roger Tomlinson in the year 1968 in his paper ""A Geographic Information System for Regional Planning"". Tomlinson is also acknowledged as the ""father of GIS"".

Previously, one of the first applications of spatial analysis in epidemiology is the 1832 ""Rapport sur la marche et les effets du choléra dans Paris et le département de la Seine"". The French geographer Charles Picquet represented the 48 districts of the city of Paris by halftone color gradient according to the number of deaths by cholera per 1,000 inhabitants. In 1854 John Snow determined the source of a cholera outbreak in London by marking points on a map depicting where the cholera victims lived, and connecting the cluster that he found with a nearby water source. This was one of the earliest successful uses of a geographic methodology in epidemiology. While the basic elements of topography and theme existed previously in cartography, the John Snow map was unique, using cartographic methods not only to depict but also to analyze clusters of geographically dependent phenomena.
The early 20th century saw the development of photozincography, which allowed maps to be split into layers, for example one layer for vegetation and another for water. This was particularly used for printing contours – drawing these was a labour-intensive task but having them on a separate layer meant they could be worked on without the other layers to confuse the draughtsman. This work was originally drawn on glass plates but later plastic film was introduced, with the advantages of being lighter, using less storage space and being less brittle, among others. When all the layers were finished, they were combined into one image using a large process camera. Once color printing came in, the layers idea was also used for creating separate printing plates for each color. While the use of layers much later became one of the main typical features of a contemporary GIS, the photographic process just described is not considered to be a GIS in itself – as the maps were just images with no database to link them to.
Computer hardware development spurred by nuclear weapon research led to general-purpose computer ""mapping"" applications by the early 1960s.
The year 1960 saw the development of the world's first true operational GIS in Ottawa, Ontario, Canada, by the federal Department of Forestry and Rural Development. Developed by Dr. Roger Tomlinson, it was called the Canada Geographic Information System (CGIS) and was used to store, analyze, and manipulate data collected for the Canada Land Inventory – an effort to determine the land capability for rural Canada by mapping information about soils, agriculture, recreation, wildlife, waterfowl, forestry and land use at a scale of 1:50,000. A rating classification factor was also added to permit analysis.
CGIS was an improvement over ""computer mapping"" applications as it provided capabilities for overlay, measurement, and digitizing/scanning. It supported a national coordinate system that spanned the continent, coded lines as arcs having a true embedded topology and it stored the attribute and locational information in separate files. As a result of this, Tomlinson has become known as the ""father of GIS"", particularly for his use of overlays in promoting the spatial analysis of convergent geographic data.
CGIS lasted into the 1990s and built a large digital land resource database in Canada. It was developed as a mainframe-based system in support of federal and provincial resource planning and management. Its strength was continent-wide analysis of complex datasets. The CGIS was never available commercially.
In 1964 Howard T. Fisher formed the Laboratory for Computer Graphics and Spatial Analysis at the Harvard Graduate School of Design (LCGSA 1965–1991), where a number of important theoretical concepts in spatial data handling were developed, and which by the 1970s had distributed seminal software code and systems, such as SYMAP, GRID, and ODYSSEY – that served as sources for subsequent commercial development—to universities, research centers and corporations worldwide.
By the late 1970s two public domain GIS systems (MOSS and GRASS GIS) were in development, and by the early 1980s, M&S Computing (later Intergraph) along with Bentley Systems Incorporated for the CAD platform, Environmental Systems Research Institute (ESRI), CARIS (Computer Aided Resource Information System), MapInfo Corporation and ERDAS (Earth Resource Data Analysis System) emerged as commercial vendors of GIS software, successfully incorporating many of the CGIS features, combining the first generation approach to separation of spatial and attribute information with a second generation approach to organizing attribute data into database structures.
In 1986, Mapping Display and Analysis System (MIDAS), the first desktop GIS product was released for the DOS operating system. This was renamed in 1990 to MapInfo for Windows when it was ported to the Microsoft Windows platform. This began the process of moving GIS from the research department into the business environment.
By the end of the 20th century, the rapid growth in various systems had been consolidated and standardized on relatively few platforms and users were beginning to explore viewing GIS data over the Internet, requiring data format and transfer standards. More recently, a growing number of free, open-source GIS packages run on a range of operating systems and can be customized to perform specific tasks. Increasingly geospatial data and mapping applications are being made available via the World Wide Web (see List of GIS software § GIS as a service).
Several articles on the history of GIS have been published.


== GIS techniques and technology ==
Modern GIS technologies use digital information, for which various digitized data creation methods are used. The most common method of data creation is digitization, where a hard copy map or survey plan is transferred into a digital medium through the use of a CAD program, and geo-referencing capabilities. With the wide availability of ortho-rectified imagery (from satellites, aircraft, Helikites and UAVs), heads-up digitizing is becoming the main avenue through which geographic data is extracted. Heads-up digitizing involves the tracing of geographic data directly on top of the aerial imagery instead of by the traditional method of tracing the geographic form on a separate digitizing tablet (heads-down digitizing).


=== Relating information from different sources ===
GIS uses spatio-temporal (space-time) location as the key index variable for all other information. Just as a relational database containing text or numbers can relate many different tables using common key index variables, GIS can relate otherwise unrelated information by using location as the key index variable. The key is the location and/or extent in space-time.
Any variable that can be located spatially, and increasingly also temporally, can be referenced using a GIS. Locations or extents in Earth space–time may be recorded as dates/times of occurrence, and x, y, and z coordinates representing, longitude, latitude, and elevation, respectively. These GIS coordinates may represent other quantified systems of temporo-spatial reference (for example, film frame number, stream gage station, highway mile-marker, surveyor benchmark, building address, street intersection, entrance gate, water depth sounding, POS or CAD drawing origin/units). Units applied to recorded temporal-spatial data can vary widely (even when using exactly the same data, see map projections), but all Earth-based spatial–temporal location and extent references should, ideally, be relatable to one another and ultimately to a ""real"" physical location or extent in space–time.
Related by accurate spatial information, an incredible variety of real-world and projected past or future data can be analyzed, interpreted and represented. This key characteristic of GIS has begun to open new avenues of scientific inquiry into behaviors and patterns of real-world information that previously had not been systematically correlated.


=== GIS uncertainties ===
GIS accuracy depends upon source data, and how it is encoded to be data referenced. Land surveyors have been able to provide a high level of positional accuracy utilizing the GPS-derived positions. High-resolution digital terrain and aerial imagery, powerful computers and Web technology are changing the quality, utility, and expectations of GIS to serve society on a grand scale, but nevertheless there are other source data that affect overall GIS accuracy like paper maps, though these may be of limited use in achieving the desired accuracy.
In developing a digital topographic database for a GIS, topographical maps are the main source, and aerial photography and satellite imagery are extra sources for collecting data and identifying attributes which can be mapped in layers over a location facsimile of scale. The scale of a map and geographical rendering area representation type are very important aspects since the information content depends mainly on the scale set and resulting locatability of the map's representations. In order to digitize a map, the map has to be checked within theoretical dimensions, then scanned into a raster format, and resulting raster data has to be given a theoretical dimension by a rubber sheeting/warping technology process.
A quantitative analysis of maps brings accuracy issues into focus. The electronic and other equipment used to make measurements for GIS is far more precise than the machines of conventional map analysis. All geographical data are inherently inaccurate, and these inaccuracies will propagate through GIS operations in ways that are difficult to predict.


=== Data representation ===

GIS data represents real objects (such as roads, land use, elevation, trees, waterways, etc.) with digital data determining the mix. Real objects can be divided into two abstractions: discrete objects (e.g., a house) and continuous fields (such as rainfall amount, or elevations). Traditionally, there are two broad methods used to store data in a GIS for both kinds of abstractions mapping references: raster images and vector. Points, lines, and polygons are the stuff of mapped location attribute references. A new hybrid method of storing data is that of identifying point clouds, which combine three-dimensional points with RGB information at each point, returning a ""3D color image"". GIS thematic maps then are becoming more and more realistically visually descriptive of what they set out to show or determine.
For a list of popular GIS file formats, such as shapefiles, see GIS file formats § Popular GIS file formats.


=== Data capture ===

Data capture—entering information into the system—consumes much of the time of GIS practitioners. There are a variety of methods used to enter data into a GIS where it is stored in a digital format.
Existing data printed on paper or PET film maps can be digitized or scanned to produce digital data. A digitizer produces vector data as an operator traces points, lines, and polygon boundaries from a map. Scanning a map results in raster data that could be further processed to produce vector data.
Survey data can be directly entered into a GIS from digital data collection systems on survey instruments using a technique called coordinate geometry (COGO). Positions from a global navigation satellite system (GNSS) like Global Positioning System can also be collected and then imported into a GIS. A current trend in data collection gives users the ability to utilize field computers with the ability to edit live data using wireless connections or disconnected editing sessions. This has been enhanced by the availability of low-cost mapping-grade GPS units with decimeter accuracy in real time. This eliminates the need to post process, import, and update the data in the office after fieldwork has been collected. This includes the ability to incorporate positions collected using a laser rangefinder. New technologies also allow users to create maps as well as analysis directly in the field, making projects more efficient and mapping more accurate.
Remotely sensed data also plays an important role in data collection and consist of sensors attached to a platform. Sensors include cameras, digital scanners and lidar, while platforms usually consist of aircraft and satellites. In England in the mid 1990s, hybrid kite/balloons called helikites first pioneered the use of compact airborne digital cameras as airborne geo-information systems. Aircraft measurement software, accurate to 0.4 mm was used to link the photographs and measure the ground. Helikites are inexpensive and gather more accurate data than aircraft. Helikites can be used over roads, railways and towns where unmanned aerial vehicles (UAVs) are banned.
Recently aerial data collection is becoming possible with miniature UAVs. For example, the Aeryon Scout was used to map a 50-acre area with a ground sample distance of 1 inch (2.54 cm) in only 12 minutes.
The majority of digital data currently comes from photo interpretation of aerial photographs. Soft-copy workstations are used to digitize features directly from stereo pairs of digital photographs. These systems allow data to be captured in two and three dimensions, with elevations measured directly from a stereo pair using principles of photogrammetry. Analog aerial photos must be scanned before being entered into a soft-copy system, for high-quality digital cameras this step is skipped.
Satellite remote sensing provides another important source of spatial data. Here satellites use different sensor packages to passively measure the reflectance from parts of the electromagnetic spectrum or radio waves that were sent out from an active sensor such as radar. Remote sensing collects raster data that can be further processed using different bands to identify objects and classes of interest, such as land cover.
When data is captured, the user should consider if the data should be captured with either a relative accuracy or absolute accuracy, since this could not only influence how information will be interpreted but also the cost of data capture.
After entering data into a GIS, the data usually requires editing, to remove errors, or further processing. For vector data it must be made ""topologically correct"" before it can be used for some advanced analysis. For example, in a road network, lines must connect with nodes at an intersection. Errors such as undershoots and overshoots must also be removed. For scanned maps, blemishes on the source map may need to be removed from the resulting raster. For example, a fleck of dirt might connect two lines that should not be connected.


=== Raster-to-vector translation ===
Data restructuring can be performed by a GIS to convert data into different formats. For example, a GIS may be used to convert a satellite image map to a vector structure by generating lines around all cells with the same classification, while determining the cell spatial relationships, such as adjacency or inclusion.
More advanced data processing can occur with image processing, a technique developed in the late 1960s by NASA and the private sector to provide contrast enhancement, false color rendering and a variety of other techniques including use of two dimensional Fourier transforms. Since digital data is collected and stored in various ways, the two data sources may not be entirely compatible. So a GIS must be able to convert geographic data from one structure to another. In so doing, the implicit assumptions behind different ontologies and classifications require analysis. Object ontologies have gained increasing prominence as a consequence of object-oriented programming and sustained work by Barry Smith and co-workers.


=== Projections, coordinate systems, and registration ===

The earth can be represented by various models, each of which may provide a different set of coordinates (e.g., latitude, longitude, elevation) for any given point on the Earth's surface. The simplest model is to assume the earth is a perfect sphere. As more measurements of the earth have accumulated, the models of the earth have become more sophisticated and more accurate. In fact, there are models called datums that apply to different areas of the earth to provide increased accuracy, like NAD83 for U.S. measurements, and the World Geodetic System for worldwide measurements.


== Spatial analysis with geographical information system (GIS) ==

GIS spatial analysis is a rapidly changing field, and GIS packages are increasingly including analytical tools as standard built-in facilities, as optional toolsets, as add-ins or 'analysts'. In many instances these are provided by the original software suppliers (commercial vendors or collaborative non commercial development teams), while in other cases facilities have been developed and are provided by third parties. Furthermore, many products offer software development kits (SDKs), programming languages and language support, scripting facilities and/or special interfaces for developing one's own analytical tools or variants. The website ""Geospatial Analysis"" and associated book/ebook attempt to provide a reasonably comprehensive guide to the subject. The increased availability has created a new dimension to business intelligence termed ""spatial intelligence"" which, when openly delivered via intranet, democratizes access to geographic and social network data. Geospatial intelligence, based on GIS spatial analysis, has also become a key element for security. GIS as a whole can be described as conversion to a vectorial representation or to any other digitisation process.


=== Slope and aspect ===
Slope can be defined as the steepness or gradient of a unit of terrain, usually measured as an angle in degrees or as a percentage. Aspect can be defined as the direction in which a unit of terrain faces. Aspect is usually expressed in degrees from north. Slope, aspect, and surface curvature in terrain analysis are all derived from neighborhood operations using elevation values of a cell's adjacent neighbours. Slope is a function of resolution, and the spatial resolution used to calculate slope and aspect should always be specified. Various authors have compared techniques for calculating slope and aspect.
The following method can be used to derive slope and aspect:
The elevation at a point or unit of terrain will have perpendicular tangents (slope) passing through the point, in an east-west and north-south direction. These two tangents give two components, ∂z/∂x and ∂z/∂y, which then be used to determine the overall direction of slope, and the aspect of the slope. The gradient is defined as a vector quantity with components equal to the partial derivatives of the surface in the x and y directions.
The calculation of the overall 3x3 grid slope S and aspect A for methods that determine east-west and north-south component use the following formulas respectively:

  
    
      
        tan
        ⁡
        S
        =
        
          
            
              
                (
                
                  
                    
                      ∂
                      z
                    
                    
                      ∂
                      x
                    
                  
                
                )
              
              
                2
              
            
            +
            
              
                (
                
                  
                    
                      ∂
                      z
                    
                    
                      ∂
                      y
                    
                  
                
                )
              
              
                2
              
            
          
        
      
    
    {\displaystyle \tan S={\sqrt {\left({\frac {\partial z}{\partial x}}\right)^{2}+\left({\frac {\partial z}{\partial y}}\right)^{2}}}}
  

  
    
      
        tan
        ⁡
        A
        =
        
          (
          
            
              
                (
                
                  
                    
                      −
                      ∂
                      z
                    
                    
                      ∂
                      y
                    
                  
                
                )
              
              
                (
                
                  
                    
                      ∂
                      z
                    
                    
                      ∂
                      x
                    
                  
                
                )
              
            
          
          )
        
      
    
    {\displaystyle \tan A=\left({\frac {\left({\frac {-\partial z}{\partial y}}\right)}{\left({\frac {\partial z}{\partial x}}\right)}}\right)}
  
Zhou and Liu describe another formula for calculating aspect, as follows:

  
    
      
        A
        =
        
          270
          
            ∘
          
        
        +
        arctan
        ⁡
        
          (
          
            
              
                (
                
                  
                    
                      ∂
                      z
                    
                    
                      ∂
                      x
                    
                  
                
                )
              
              
                (
                
                  
                    
                      ∂
                      z
                    
                    
                      ∂
                      y
                    
                  
                
                )
              
            
          
          )
        
        −
        
          90
          
            ∘
          
        
        
          (
          
            
              
                (
                
                  
                    
                      ∂
                      z
                    
                    
                      ∂
                      y
                    
                  
                
                )
              
              
                |
                
                  
                    
                      ∂
                      z
                    
                    
                      ∂
                      y
                    
                  
                
                |
              
            
          
          )
        
      
    
    {\displaystyle A=270^{\circ }+\arctan \left({\frac {\left({\frac {\partial z}{\partial x}}\right)}{\left({\frac {\partial z}{\partial y}}\right)}}\right)-90^{\circ }\left({\frac {\left({\frac {\partial z}{\partial y}}\right)}{\left|{\frac {\partial z}{\partial y}}\right|}}\right)}
  


=== Data analysis ===
It is difficult to relate wetlands maps to rainfall amounts recorded at different points such as airports, television stations, and schools. A GIS, however, can be used to depict two- and three-dimensional characteristics of the Earth's surface, subsurface, and atmosphere from information points. For example, a GIS can quickly generate a map with isopleth or contour lines that indicate differing amounts of rainfall. Such a map can be thought of as a rainfall contour map. Many sophisticated methods can estimate the characteristics of surfaces from a limited number of point measurements. A two-dimensional contour map created from the surface modeling of rainfall point measurements may be overlaid and analyzed with any other map in a GIS covering the same area. This GIS derived map can then provide additional information - such as the viability of water power potential as a renewable energy source. Similarly, GIS can be used to compare other renewable energy resources to find the best geographic potential for a region.
Additionally, from a series of three-dimensional points, or digital elevation model, isopleth lines representing elevation contours can be generated, along with slope analysis, shaded relief, and other elevation products. Watersheds can be easily defined for any given reach, by computing all of the areas contiguous and uphill from any given point of interest. Similarly, an expected thalweg of where surface water would want to travel in intermittent and permanent streams can be computed from elevation data in the GIS.


=== Topological modeling ===
A GIS can recognize and analyze the spatial relationships that exist within digitally stored spatial data. These topological relationships allow complex spatial modelling and analysis to be performed. Topological relationships between geometric entities traditionally include adjacency (what adjoins what), containment (what encloses what), and proximity (how close something is to something else).


=== Geometric networks ===
Geometric networks are linear networks of objects that can be used to represent interconnected features, and to perform special spatial analysis on them. A geometric network is composed of edges, which are connected at junction points, similar to graphs in mathematics and computer science. Just like graphs, networks can have weight and flow assigned to its edges, which can be used to represent various interconnected features more accurately. Geometric networks are often used to model road networks and public utility networks, such as electric, gas, and water networks. Network modeling is also commonly employed in transportation planning, hydrology modeling, and infrastructure modeling.


=== Hydrological modeling ===
GIS hydrological models can provide a spatial element that other hydrological models lack, with the analysis of variables such as slope, aspect and watershed or catchment area. Terrain analysis is fundamental to hydrology, since water always flows down a slope. As basic terrain analysis of a digital elevation model (DEM) involves calculation of slope and aspect, DEMs are very useful for hydrological analysis. Slope and aspect can then be used to determine direction of surface runoff, and hence flow accumulation for the formation of streams, rivers and lakes. Areas of divergent flow can also give a clear indication of the boundaries of a catchment. Once a flow direction and accumulation matrix has been created, queries can be performed that show contributing or dispersal areas at a certain point. More detail can be added to the model, such as terrain roughness, vegetation types and soil types, which can influence infiltration and evapotranspiration rates, and hence influencing surface flow. One of the main uses of hydrological modeling is in environmental contamination research. Other applications of hydrological modeling include groundwater and surface water mapping, as well as flood risk maps.


=== Cartographic modeling ===

Dana Tomlin probably coined the term ""cartographic modeling"" in his PhD dissertation (1983); he later used it in the title of his book, Geographic Information Systems and Cartographic Modeling (1990). Cartographic modeling refers to a process where several thematic layers of the same area are produced, processed, and analyzed. Tomlin used raster layers, but the overlay method (see below) can be used more generally. Operations on map layers can be combined into algorithms, and eventually into simulation or optimization models.


=== Map overlay ===
The combination of several spatial datasets (points, lines, or polygons) creates a new output vector dataset, visually similar to stacking several maps of the same region. These overlays are similar to mathematical Venn diagram overlays. A union overlay combines the geographic features and attribute tables of both inputs into a single new output. An intersect overlay defines the area where both inputs overlap and retains a set of attribute fields for each. A symmetric difference overlay defines an output area that includes the total area of both inputs except for the overlapping area.
Data extraction is a GIS process similar to vector overlay, though it can be used in either vector or raster data analysis. Rather than combining the properties and features of both datasets, data extraction involves using a ""clip"" or ""mask"" to extract the features of one data set that fall within the spatial extent of another dataset.
In raster data analysis, the overlay of datasets is accomplished through a process known as ""local operation on multiple rasters"" or ""map algebra"", through a function that combines the values of each raster's matrix. This function may weigh some inputs more than others through use of an ""index model"" that reflects the influence of various factors upon a geographic phenomenon.


=== Geostatistics ===

Geostatistics is a branch of statistics that deals with field data, spatial data with a continuous index. It provides methods to model spatial correlation, and predict values at arbitrary locations (interpolation).
When phenomena are measured, the observation methods dictate the accuracy of any subsequent analysis. Due to the nature of the data (e.g. traffic patterns in an urban environment; weather patterns over the Pacific Ocean), a constant or dynamic degree of precision is always lost in the measurement. This loss of precision is determined from the scale and distribution of the data collection.
To determine the statistical relevance of the analysis, an average is determined so that points (gradients) outside of any immediate measurement can be included to determine their predicted behavior. This is due to the limitations of the applied statistic and data collection methods, and interpolation is required to predict the behavior of particles, points, and locations that are not directly measurable.

Interpolation is the process by which a surface is created, usually a raster dataset, through the input of data collected at a number of sample points. There are several forms of interpolation, each which treats the data differently, depending on the properties of the data set. In comparing interpolation methods, the first consideration should be whether or not the source data will change (exact or approximate). Next is whether the method is subjective, a human interpretation, or objective. Then there is the nature of transitions between points: are they abrupt or gradual. Finally, there is whether a method is global (it uses the entire data set to form the model), or local where an algorithm is repeated for a small section of terrain.
Interpolation is a justified measurement because of a spatial autocorrelation principle that recognizes that data collected at any position will have a great similarity to, or influence of those locations within its immediate vicinity.
Digital elevation models, triangulated irregular networks, edge-finding algorithms, Thiessen polygons, Fourier analysis, (weighted) moving averages, inverse distance weighting, kriging, spline, and trend surface analysis are all mathematical methods to produce interpolative data.


=== Address geocoding ===

Geocoding is interpolating spatial locations (X,Y coordinates) from street addresses or any other spatially referenced data such as ZIP Codes, parcel lots and address locations. A reference theme is required to geocode individual addresses, such as a road centerline file with address ranges. The individual address locations have historically been interpolated, or estimated, by examining address ranges along a road segment. These are usually provided in the form of a table or database. The software will then place a dot approximately where that address belongs along the segment of centerline. For example, an address point of 500 will be at the midpoint of a line segment that starts with address 1 and ends with address 1,000. Geocoding can also be applied against actual parcel data, typically from municipal tax maps. In this case, the result of the geocoding will be an actually positioned space as opposed to an interpolated point. This approach is being increasingly used to provide more precise location information.


=== Reverse geocoding ===
Reverse geocoding is the process of returning an estimated street address number as it relates to a given coordinate. For example, a user can click on a road centerline theme (thus providing a coordinate) and have information returned that reflects the estimated house number. This house number is interpolated from a range assigned to that road segment. If the user clicks at the midpoint of a segment that starts with address 1 and ends with 100, the returned value will be somewhere near 50. Note that reverse geocoding does not return actual addresses, only estimates of what should be there based on the predetermined range.


=== Multi-criteria decision analysis ===
Coupled with GIS, multi-criteria decision analysis methods support decision-makers in analysing a set of alternative spatial solutions, such as the most likely ecological habitat for restoration, against multiple criteria, such as vegetation cover or roads. MCDA uses decision rules to aggregate the criteria, which allows the alternative solutions to be ranked or prioritised. GIS MCDA may reduce costs and time involved in identifying potential restoration sites.


=== Data output and cartography ===
Cartography is the design and production of maps, or visual representations of spatial data. The vast majority of modern cartography is done with the help of computers, usually using GIS but production of quality cartography is also achieved by importing layers into a design program to refine it. Most GIS software gives the user substantial control over the appearance of the data.
Cartographic work serves two major functions:
First, it produces graphics on the screen or on paper that convey the results of analysis to the people who make decisions about resources. Wall maps and other graphics can be generated, allowing the viewer to visualize and thereby understand the results of analyses or simulations of potential events. Web Map Servers facilitate distribution of generated maps through web browsers using various implementations of web-based application programming interfaces (AJAX, Java, Flash, etc.).
Second, other database information can be generated for further analysis or use. An example would be a list of all addresses within one mile (1.6 km) of a toxic spill.


=== Graphic display techniques ===
Traditional maps are abstractions of the real world, a sampling of important elements portrayed on a sheet of paper with symbols to represent physical objects. People who use maps must interpret these symbols. Topographic maps show the shape of land surface with contour lines or with shaded relief.
Today, graphic display techniques such as shading based on altitude in a GIS can make relationships among map elements visible, heightening one's ability to extract and analyze information. For example, two types of data were combined in a GIS to produce a perspective view of a portion of San Mateo County, California.
The digital elevation model, consisting of surface elevations recorded on a 30-meter horizontal grid, shows high elevations as white and low elevation as black.
The accompanying Landsat Thematic Mapper image shows a false-color infrared image looking down at the same area in 30-meter pixels, or picture elements, for the same coordinate points, pixel by pixel, as the elevation information.
A GIS was used to register and combine the two images to render the three-dimensional perspective view looking down the San Andreas Fault, using the Thematic Mapper image pixels, but shaded using the elevation of the landforms. The GIS display depends on the viewing point of the observer and time of day of the display, to properly render the shadows created by the sun's rays at that latitude, longitude, and time of day.
An archeochrome is a new way of displaying spatial data. It is a thematic on a 3D map that is applied to a specific building or a part of a building. It is suited to the visual display of heat-loss data.


=== Spatial ETL ===
Spatial ETL tools provide the data processing functionality of traditional extract, transform, load (ETL) software, but with a primary focus on the ability to manage spatial data. They provide GIS users with the ability to translate data between different standards and proprietary formats, whilst geometrically transforming the data en route. These tools can come in the form of add-ins to existing wider-purpose software such as spreadsheets.


=== GIS data mining ===
GIS or spatial data mining is the application of data mining methods to spatial data. Data mining, which is the partially automated search for hidden patterns in large databases, offers great potential benefits for applied GIS-based decision making. Typical applications include environmental monitoring. A characteristic of such applications is that spatial correlation between data measurements require the use of specialized algorithms for more efficient data analysis.


== Applications ==
The implementation of a GIS is often driven by jurisdictional (such as a city), purpose, or application requirements. Generally, a GIS implementation may be custom-designed for an organization. Hence, a GIS deployment developed for an application, jurisdiction, enterprise, or purpose may not be necessarily interoperable or compatible with a GIS that has been developed for some other application, jurisdiction, enterprise, or purpose.
GIS provides, for every kind of location-based organization, a platform to update geographical data without wasting time to visit the field and update a database manually. GIS when integrated with other powerful enterprise solutions like SAP and the Wolfram Language helps creating powerful decision support system at enterprise level.

Many disciplines can benefit from GIS technology. An active GIS market has resulted in lower costs and continual improvements in the hardware and software components of GIS, and usage in the fields of science, government, business, and industry, with applications including real estate, public health, crime mapping, national defense, sustainable development, natural resources, climatology, landscape architecture, archaeology, regional and community planning, transportation and logistics. GIS is also diverging into location-based services, which allows GPS-enabled mobile devices to display their location in relation to fixed objects (nearest restaurant, gas station, fire hydrant) or mobile objects (friends, children, police car), or to relay their position back to a central server for display or other processing.


=== Open Geospatial Consortium standards ===

The Open Geospatial Consortium (OGC) is an international industry consortium of 384 companies, government agencies, universities, and individuals participating in a consensus process to develop publicly available geoprocessing specifications. Open interfaces and protocols defined by OpenGIS Specifications support interoperable solutions that ""geo-enable"" the Web, wireless and location-based services, and mainstream IT, and empower technology developers to make complex spatial information and services accessible and useful with all kinds of applications. Open Geospatial Consortium protocols include Web Map Service, and Web Feature Service.
GIS products are broken down by the OGC into two categories, based on how completely and accurately the software follows the OGC specifications.

Compliant Products are software products that comply to OGC's OpenGIS Specifications. When a product has been tested and certified as compliant through the OGC Testing Program, the product is automatically registered as ""compliant"" on this site.
Implementing Products are software products that implement OpenGIS Specifications but have not yet passed a compliance test. Compliance tests are not available for all specifications. Developers can register their products as implementing draft or approved specifications, though OGC reserves the right to review and verify each entry.


=== Web mapping ===

In recent years there has been a proliferation of free-to-use and easily accessible mapping software such as the proprietary web applications Google Maps and Bing Maps, as well as the free and open-source alternative OpenStreetMap. These services give the public access to huge amounts of geographic data; perceived by many users to be as trustworthy and usable as professional information.
Some of them, like Google Maps and OpenLayers, expose an application programming interface (API) that enable users to create custom applications. These toolkits commonly offer street maps, aerial/satellite imagery, geocoding, searches, and routing functionality. Web mapping has also uncovered the potential of crowdsourcing geodata in projects like OpenStreetMap, which is a collaborative project to create a free editable map of the world. These mashup projects have been proven to provide a high level of value and benefit to end users outside that possible through traditional geographic information.


=== Adding the dimension of time ===

The condition of the Earth's surface, atmosphere, and subsurface can be examined by feeding satellite data into a GIS. GIS technology gives researchers the ability to examine the variations in Earth processes over days, months, and years. As an example, the changes in vegetation vigor through a growing season can be animated to determine when drought was most extensive in a particular region. The resulting graphic represents a rough measure of plant health. Working with two variables over time would then allow researchers to detect regional differences in the lag between a decline in rainfall and its effect on vegetation.
GIS technology and the availability of digital data on regional and global scales enable such analyses. The satellite sensor output used to generate a vegetation graphic is produced for example by the advanced very-high-resolution radiometer (AVHRR). This sensor system detects the amounts of energy reflected from the Earth's surface across various bands of the spectrum for surface areas of about 1 square kilometer. The satellite sensor produces images of a particular location on the Earth twice a day. AVHRR and more recently the moderate-resolution imaging spectroradiometer (MODIS) are only two of many sensor systems used for Earth surface analysis.
In addition to the integration of time in environmental studies, GIS is also being explored for its ability to track and model the progress of humans throughout their daily routines. A concrete example of progress in this area is the recent release of time-specific population data by the U.S. Census. In this data set, the populations of cities are shown for daytime and evening hours highlighting the pattern of concentration and dispersion generated by North American commuting patterns. The manipulation and generation of data required to produce this data would not have been possible without GIS.
Using models to project the data held by a GIS forward in time have enabled planners to test policy decisions using spatial decision support systems.


== Semantics ==
Tools and technologies emerging from the World Wide Web Consortium's Semantic Web are proving useful for data integration problems in information systems. Correspondingly, such technologies have been proposed as a means to facilitate interoperability and data reuse among GIS applications. and also to enable new analysis mechanisms.
Ontologies are a key component of this semantic approach as they allow a formal, machine-readable specification of the concepts and relationships in a given domain. This in turn allows a GIS to focus on the intended meaning of data rather than its syntax or structure. For example, reasoning that a land cover type classified as deciduous needleleaf trees in one dataset is a specialization or subset of land cover type forest in another more roughly classified dataset can help a GIS automatically merge the two datasets under the more general land cover classification. Tentative ontologies have been developed in areas related to GIS applications, for example the hydrology ontology developed by the Ordnance Survey in the United Kingdom and the SWEET ontologies developed by NASA's Jet Propulsion Laboratory. Also, simpler ontologies and semantic metadata standards are being proposed by the W3C Geo Incubator Group to represent geospatial data on the web. GeoSPARQL is a standard developed by the Ordnance Survey, United States Geological Survey, Natural Resources Canada, Australia's Commonwealth Scientific and Industrial Research Organisation and others to support ontology creation and reasoning using well-understood OGC literals (GML, WKT), topological relationships (Simple Features, RCC8, DE-9IM), RDF and the SPARQL database query protocols.
Recent research results in this area can be seen in the International Conference on Geospatial Semantics and the Terra Cognita – Directions to the Geospatial Semantic Web workshop at the International Semantic Web Conference.


== Implications of GIS in society ==

With the popularization of GIS in decision making, scholars have begun to scrutinize the social and political implications of GIS. GIS can also be misused to distort reality for individual and political gain. It has been argued that the production, distribution, utilization, and representation of geographic information are largely related with the social context and has the potential to increase citizen trust in government. Other related topics include discussion on copyright, privacy, and censorship. A more optimistic social approach to GIS adoption is to use it as a tool for public participation.


=== GIS in education ===

At the end of the 20th century, GIS began to be recognized as tools that could be used in the classroom. The benefits of GIS in education seem focused on developing spatial thinking, but there is not enough bibliography or statistical data to show the concrete scope of the use of GIS in education around the world, although the expansion has been faster in those countries where the curriculum mentions them.
GIS seem to provide many advantages in teaching geography because they allow for analyses based on real geographic data and also help raise many research questions from teachers and students in classrooms, as well as they contribute to improvement in learning by developing spatial and geographical thinking and, in many cases, student motivation.


=== GIS in local government ===
GIS is proven as an organization-wide, enterprise and enduring technology that continues to change how local government operates. Government agencies have adopted GIS technology as a method to better manage the following areas of government organization:
Public Safety operations such as Emergency Operations Centers, Fire Prevention, Police and Sheriff mobile technology and dispatch, and mapping weather risks.
Parks and Recreation departments and their functions in asset inventory, land conservation, land management, and cemetery management.
Public Works and Utilities, tracking water and stormwater drainage, electrical assets, engineering projects, and public transportation assets and trends.
Fiber Network Management for interdepartmental network assets
School analytical and demographic data, asset management, and improvement/expansion planning
Public Administration for election data, property records, and zoning/management.
The Open Data initiative is pushing local government to take advantage of technology such as GIS technology, as it encompasses the requirements to fit the Open Data/Open Government model of transparency. With Open Data, local government organizations can implement Citizen Engagement applications and online portals, allowing citizens to see land information, report potholes and signage issues, view and sort parks by assets, view real-time crime rates and utility repairs, and much more. The push for open data within government organizations is driving the growth in local government GIS technology spending, and database management.


== See also ==


== References ==


== Further reading ==
Berry, J. K. (1993) Beyond Mapping: Concepts, Algorithms and Issues in GIS. Fort Collins, CO: GIS World Books.
Bolstad, P. (2005) GIS Fundamentals: A first text on Geographic Information Systems, Second Edition. White Bear Lake, MN: Eider Press, 543 pp.
Burrough, P. A. and McDonnell, R. A. (1998). Principles of geographical information systems. Oxford University Press, Oxford, 327 pp.
Buzai, G.D.; Robinson, D. (2010). ""Geographical Information Systems in Latin America, 1987-2010. A Preliminary Overview"". Journal of Latin American Geography. 9 (3): 9–31. doi:10.1353/lag.2010.0027. 
Chang, K. (2007) Introduction to Geographic Information System, 4th Edition. McGraw Hill, ISBN 978-0071267588
de Smith MJ, Goodchild MF, Longley PA (2007). Geospatial analysis: A comprehensive guide to principles, techniques and software tools (2nd ed.). Troubador, UK. ISBN 978-1848761582. 
Elangovan,K (2006)""GIS: Fundamentals, Applications and Implementations"", New India Publishing Agency, New Delhi""208 pp.
Fu, P., and J. Sun. 2010. Web GIS: Principles and Applications. ESRI Press. Redlands, CA. ISBN 1-58948-245-X.
Harvey, Francis(2008) A Primer of GIS, Fundamental geographic and cartographic concepts. The Guilford Press, 31 pp.
Heywood, I., Cornelius, S., and Carver, S. (2006) An Introduction to Geographical Information Systems. Prentice Hall. 3rd edition.
Longley, P.A., Goodchild, M.F., Maguire, D.J. and Rhind, D.W. (2005) Geographic Information Systems and Science. Chichester: Wiley. 2nd edition.
Maguire, D.J., Goodchild M.F., Rhind D.W. (1997) ""Geographic Information Systems: principles, and applications"" Longman Scientific and Technical, Harlow.
Maliene V, Grigonis V, Palevičius V, Griffiths S (2011). ""Geographic information system: Old principles with new capabilities"". Urban Design International. 16 (1): 1–6. doi:10.1057/udi.2010.25. 
Mennecke, Brian E.; Lawrence, A. West Jr. (October 2001). ""Geographic Information Systems in Developing Countries: Issues in Data Collection, Implementation and Management"". Journal of Global Information Management. 9 (4): 45–55. 
Ott, T. and Swiaczny, F. (2001) Time-integrative GIS. Management and analysis of spatio-temporal data, Berlin / Heidelberg / New York: Springer.
Sajeevan G (March 2008). ""Latitude and longitude – A misunderstanding"" (PDF). Current Science. 94 (5): 568. 
Sajeevan G (2006). ""Customise and empower"". Geospatial Today. 4 (7): 40–43. 
Thurston, J., Poiker, T.K. and J. Patrick Moore. (2003) Integrated Geospatial Technologies: A Guide to GPS, GIS, and Data Logging. Hoboken, New Jersey: Wiley.
Tomlinson, R.F., (2005) Thinking About GIS: Geographic Information System Planning for Managers. ESRI Press. 328 pp.
Wise, S. (2002) GIS Basics. London: Taylor & Francis.
Worboys, Michael; Duckham, Matt (2004). GIS: a computing perspective. Boca Raton: CRC Press. ISBN 0415283752. 
Wheatley, David and Gillings, Mark (2002) Spatial Technology and Archaeology. The Archaeological Application of GIS. London, New York, Taylor & Francis.
Holdstock, David (2017). Strategic GIS Planning and Management in Local Government. Boca Raton, FL: CRC Press. ISBN 9781466556508. 


== External links ==
 Media related to Geographic information systems at Wikimedia Commons"
3,Computational creativity,16300571,61153,"Computational creativity (also known as artificial creativity, mechanical creativity, creative computing or creative computation) is a multidisciplinary endeavour that is located at the intersection of the fields of artificial intelligence, cognitive psychology, philosophy, and the arts.
The goal of computational creativity is to model, simulate or replicate creativity using a computer, to achieve one of several ends:
To construct a program or computer capable of human-level creativity.
To better understand human creativity and to formulate an algorithmic perspective on creative behavior in humans.
To design programs that can enhance human creativity without necessarily being creative themselves.
The field of computational creativity concerns itself with theoretical and practical issues in the study of creativity. Theoretical work on the nature and proper definition of creativity is performed in parallel with practical work on the implementation of systems that exhibit creativity, with one strand of work informing the other.


== Theoretical issues ==
As measured by the amount of activity in the field (e.g., publications, conferences and workshops), computational creativity is a growing area of research. But the field is still hampered by a number of fundamental problems. Creativity is very difficult, perhaps even impossible, to define in objective terms. Is it a state of mind, a talent or ability, or a process? Creativity takes many forms in human activity, some eminent (sometimes referred to as ""Creativity"" with a capital C) and some mundane.
These are problems that complicate the study of creativity in general, but certain problems attach themselves specifically to computational creativity:
Can creativity be hard-wired? In existing systems to which creativity is attributed, is the creativity that of the system or that of the system's programmer or designer?
How do we evaluate computational creativity? What counts as creativity in a computational system? Are natural language generation systems creative? Are machine translation systems creative? What distinguishes research in computational creativity from research in artificial intelligence generally?
If eminent creativity is about rule-breaking or the disavowal of convention, how is it possible for an algorithmic system to be creative? In essence, this is a variant of Ada Lovelace's objection to machine intelligence, as recapitulated by modern theorists such as Teresa Amabile. If a machine can do only what it was programmed to do, how can its behavior ever be called creative?
Indeed, not all computer theorists would agree with the premise that computers can only do what they are programmed to do—a key point in favor of computational creativity.


== Defining creativity in computational terms ==
Because no single perspective or definition seems to offer a complete picture of creativity, the AI researchers Newell, Shaw and Simon developed the combination of novelty and usefulness into the cornerstone of a multi-pronged view of creativity, one that uses the following four criteria to categorize a given answer or solution as creative:
The answer is novel and useful (either for the individual or for society)
The answer demands that we reject ideas we had previously accepted
The answer results from intense motivation and persistence
The answer comes from clarifying a problem that was originally vague
Whereas the above reflects a ""top-down"" approach to computational creativity, an alternative thread has developed among ""bottom-up"" computational psychologists involved in artificial neural network research. During the late 1980s and early 1990s, for example, such generative neural systems were driven by genetic algorithms. Experiments involving recurrent nets were successful in hybridizing simple musical melodies and predicting listener expectations.
Concurrent with such research, a number of computational psychologists took the perspective, popularized by Stephen Wolfram, that system behaviors perceived as complex, including the mind's creative output, could arise from what would be considered simple algorithms. As neuro-philosophical thinking matured, it also became evident that language actually presented an obstacle to producing a scientific model of cognition, creative or not, since it carried with it so many unscientific aggrandizements that were more uplifting than accurate. Thus questions naturally arose as to how ""rich,"" ""complex,"" and ""wonderful"" creative cognition actually was.


== Artificial neural networks ==
Before 1989, artificial neural networks have been used to model certain aspects of creativity. Peter Todd (1989) first trained a neural network to reproduce musical melodies from a training set of musical pieces. Then he used a change algorithm to modify the network's input parameters. The network was able to randomly generate new music in a highly uncontrolled manner. In 1992, Todd extended this work, using the so-called distal teacher approach that had been developed by Paul Munro, Paul Werbos, D. Nguyen and Bernard Widrow, Michael I. Jordan and David Rumelhart. In the new approach there are two neural networks, one of which is supplying training patterns to another. In later efforts by Todd, a composer would select a set of melodies that define the melody space, position them on a 2-d plane with a mouse-based graphic interface, and train a connectionist network to produce those melodies, and listen to the new ""interpolated"" melodies that the network generates corresponding to intermediate points in the 2-d plane.
More recently a neurodynamical model of semantic networks has been developed to study how the connectivity structure of these networks relates to the richness of the semantic constructs, or ideas, they can generate. It was demonstrated that semantic neural networks that have richer semantic dynamics than those with other connectivity structures may provide insight into the important issue of how the physical structure of the brain determines one of the most profound features of the human mind – its capacity for creative thought.


== Key concepts from the literature ==
Some high-level and philosophical themes recur throughout the field of computational creativity.


=== Important categories of creativity ===
Margaret Boden refers to creativity that is novel merely to the agent that produces it as ""P-creativity"" (or ""psychological creativity""), and refers to creativity that is recognized as novel by society at large as ""H-creativity"" (or ""historical creativity""). Stephen Thaler has suggested a new category he calls ""V-"" or ""Visceral creativity"" wherein significance is invented to raw sensory inputs to a Creativity Machine architecture, with the ""gateway"" nets perturbed to produce alternative interpretations, and downstream nets shifting such interpretations to fit the overarching context. An important variety of such V-creativity is consciousness itself, wherein meaning is reflexively invented to activation turnover within the brain.


=== Exploratory and transformational creativity ===
Boden also distinguishes between the creativity that arises from an exploration within an established conceptual space, and the creativity that arises from a deliberate transformation or transcendence of this space. She labels the former as exploratory creativity and the latter as transformational creativity, seeing the latter as a form of creativity far more radical, challenging, and rarer than the former. Following the criteria from Newell and Simon elaborated above, we can see that both forms of creativity should produce results that are appreciably novel and useful (criterion 1), but exploratory creativity is more likely to arise from a thorough and persistent search of a well-understood space (criterion 3) -- while transformational creativity should involve the rejection of some of the constraints that define this space (criterion 2) or some of the assumptions that define the problem itself (criterion 4). Boden's insights have guided work in computational creativity at a very general level, providing more an inspirational touchstone for development work than a technical framework of algorithmic substance. However, Boden's insights are more recently also the subject of formalization, most notably in the work by Geraint Wiggins.


=== Generation and evaluation ===
The criterion that creative products should be novel and useful means that creative computational systems are typically structured into two phases, generation and evaluation. In the first phase, novel (to the system itself, thus P-Creative) constructs are generated; unoriginal constructs that are already known to the system are filtered at this stage. This body of potentially creative constructs are then evaluated, to determine which are meaningful and useful and which are not. This two-phase structure conforms to the Geneplore model of Finke, Ward and Smith, which is a psychological model of creative generation based on empirical observation of human creativity.


=== Combinatorial creativity ===
A great deal, perhaps all, of human creativity can be understood as a novel combination of pre-existing ideas or objects. Common strategies for combinatorial creativity include:
Placing a familiar object in an unfamiliar setting (e.g., Marcel Duchamp's Fountain) or an unfamiliar object in a familiar setting (e.g., a fish-out-of-water story such as The Beverly Hillbillies)
Blending two superficially different objects or genres (e.g., a sci-fi story set in the Wild West, with robot cowboys, as in Westworld, or the reverse, as in Firefly; Japanese haiku poems, etc.)
Comparing a familiar object to a superficially unrelated and semantically distant concept (e.g., ""Makeup is the Western burka""; ""A zoo is a gallery with living exhibits"")
Adding a new and unexpected feature to an existing concept (e.g., adding a scalpel to a Swiss Army knife; adding a camera to a mobile phone)
Compressing two incongruous scenarios into the same narrative to get a joke (e.g., the Emo Philips joke ""Women are always using me to advance their careers. Damned anthropologists!"")
Using an iconic image from one domain in a domain for an unrelated or incongruous idea or product (e.g., using the Marlboro Man image to sell cars, or to advertise the dangers of smoking-related impotence).
The combinatorial perspective allows us to model creativity as a search process through the space of possible combinations. The combinations can arise from composition or concatenation of different representations, or through a rule-based or stochastic transformation of initial and intermediate representations. Genetic algorithms and neural networks can be used to generate blended or crossover representations that capture a combination of different inputs.


==== Conceptual blending ====

Mark Turner and Gilles Fauconnier propose a model called Conceptual Integration Networks that elaborates upon Arthur Koestler's ideas about creativity as well as more recent work by Lakoff and Johnson, by synthesizing ideas from Cognitive Linguistic research into mental spaces and conceptual metaphors. Their basic model defines an integration network as four connected spaces:
A first input space (contains one conceptual structure or mental space)
A second input space (to be blended with the first input)
A generic space of stock conventions and image-schemas that allow the input spaces to be understood from an integrated perspective
A blend space in which a selected projection of elements from both input spaces are combined; inferences arising from this combination also reside here, sometimes leading to emergent structures that conflict with the inputs.
Fauconnier and Turner describe a collection of optimality principles that are claimed to guide the construction of a well-formed integration network. In essence, they see blending as a compression mechanism in which two or more input structures are compressed into a single blend structure. This compression operates on the level of conceptual relations. For example, a series of similarity relations between the input spaces can be compressed into a single identity relationship in the blend.
Some computational success has been achieved with the blending model by extending pre-existing computational models of analogical mapping that are compatible by virtue of their emphasis on connected semantic structures. More recently, Francisco Câmara Pereira presented an implementation of blending theory that employs ideas both from GOFAI and genetic algorithms to realize some aspects of blending theory in a practical form; his example domains range from the linguistic to the visual, and the latter most notably includes the creation of mythical monsters by combining 3-D graphical models.


== Linguistic creativity ==
Language provides continuous opportunity for creativity, evident in the generation of novel sentences, phrasings, puns, neologisms, rhymes, allusions, sarcasm, irony, similes, metaphors, analogies, witticisms, and jokes. Native speakers of morphologically rich languages frequently create new word-forms that are easily understood, although they will never find their way to the dictionary. The area of natural language generation has been well studied, but these creative aspects of everyday language have yet to be incorporated with any robustness or scale.


=== Story generation ===
Substantial work has been conducted in this area of linguistic creation since the 1970s, with the development of James Meehan's TALE-SPIN  system. TALE-SPIN viewed stories as narrative descriptions of a problem-solving effort, and created stories by first establishing a goal for the story's characters so that their search for a solution could be tracked and recorded. The MINSTREL system represents a complex elaboration of this basis approach, distinguishing a range of character-level goals in the story from a range of author-level goals for the story. Systems like Bringsjord's BRUTUS elaborate these ideas further to create stories with complex inter-personal themes like betrayal. Nonetheless, MINSTREL explicitly models the creative process with a set of Transform Recall Adapt Methods (TRAMs) to create novel scenes from old. The MEXICA model of Rafael Pérez y Pérez and Mike Sharples is more explicitly interested in the creative process of storytelling, and implements a version of the engagement-reflection cognitive model of creative writing.
The company Narrative Science makes computer generated news and reports commercially available, including summarizing team sporting events based on statistical data from the game. It also creates financial reports and real estate analyses.


=== Metaphor and simile ===
Example of a metaphor: ""She was an ape.""
Example of a simile: ""Felt like a tiger-fur blanket."" The computational study of these phenomena has mainly focused on interpretation as a knowledge-based process. Computationalists such as Yorick Wilks, James Martin, Dan Fass, John Barnden, and Mark Lee have developed knowledge-based approaches to the processing of metaphors, either at a linguistic level or a logical level. Tony Veale and Yanfen Hao have developed a system, called Sardonicus, that acquires a comprehensive database of explicit similes from the web; these similes are then tagged as bona-fide (e.g., ""as hard as steel"") or ironic (e.g., ""as hairy as a bowling ball"", ""as pleasant as a root canal""); similes of either type can be retrieved on demand for any given adjective. They use these similes as the basis of an on-line metaphor generation system called Aristotle that can suggest lexical metaphors for a given descriptive goal (e.g., to describe a supermodel as skinny, the source terms ""pencil"", ""whip"", ""whippet"", ""rope"", ""stick-insect"" and ""snake"" are suggested).


=== Analogy ===
The process of analogical reasoning has been studied from both a mapping and a retrieval perspective, the latter being key to the generation of novel analogies. The dominant school of research, as advanced by Dedre Gentner, views analogy as a structure-preserving process; this view has been implemented in the structure mapping engine or SME, the MAC/FAC retrieval engine (Many Are Called, Few Are Chosen), ACME (Analogical Constraint Mapping Engine) and ARCS (Analogical Retrieval Constraint System). Other mapping-based approaches include Sapper, which situates the mapping process in a semantic-network model of memory. Analogy is a very active sub-area of creative computation and creative cognition; active figures in this sub-area include Douglas Hofstadter, Paul Thagard, and Keith Holyoak. Also worthy of note here is Peter Turney and Michael Littman's machine learning approach to the solving of SAT-style analogy problems; their approach achieves a score that compares well with average scores achieved by humans on these tests.


=== Joke generation ===

Humour is an especially knowledge-hungry process, and the most successful joke-generation systems to date have focussed on pun-generation, as exemplified by the work of Kim Binsted and Graeme Ritchie. This work includes the JAPE system, which can generate a wide range of puns that are consistently evaluated as novel and humorous by young children. An improved version of JAPE has been developed in the guise of the STANDUP system, which has been experimentally deployed as a means of enhancing linguistic interaction with children with communication disabilities. Some limited progress has been made in generating humour that involves other aspects of natural language, such as the deliberate misunderstanding of pronominal reference (in the work of Hans Wim Tinholt and Anton Nijholt), as well as in the generation of humorous acronyms in the HAHAcronym system of Oliviero Stock and Carlo Strapparava.


=== Neologism ===
The blending of multiple word forms is a dominant force for new word creation in language; these new words are commonly called ""blends"" or ""portmanteau words"" (after Lewis Carroll). Tony Veale has developed a system called ZeitGeist that harvests neological headwords from Wikipedia and interprets them relative to their local context in Wikipedia and relative to specific word senses in WordNet. ZeitGeist has been extended to generate neologisms of its own; the approach combines elements from an inventory of word parts that are harvested from WordNet, and simultaneously determines likely glosses for these new words (e.g., ""food traveller"" for ""gastronaut"" and ""time traveller"" for ""chrononaut""). It then uses Web search to determine which glosses are meaningful and which neologisms have not been used before; this search identifies the subset of generated words that are both novel (""H-creative"") and useful. Neurolinguistic inspirations have been used to analyze the process of novel word creation in the brain, understand neurocognitive processes responsible for intuition, insight, imagination and creativity and to create a server that invents novel names for products, based on their description. Further, the system Nehovah blends two source words into a neologism that blends the meanings of the two source words. Nehovah searches WordNet for synonyms and TheTopTens.com for pop culture hyponyms. The synonyms and hyponyms are blended together to create a set of candidate neologisms. The neologisms are then scored based on their word structure, how unique the word is, how apparent the concepts are conveyed, and if the neologism has a pop culture reference. Nehovah loosely follows conceptual blending.


=== Poetry ===

Like jokes, poems involve a complex interaction of different constraints, and no general-purpose poem generator adequately combines the meaning, phrasing, structure and rhyme aspects of poetry. Nonetheless, Pablo Gervás has developed a noteworthy system called ASPERA that employs a case-based reasoning (CBR) approach to generating poetic formulations of a given input text via a composition of poetic fragments that are retrieved from a case-base of existing poems. Each poem fragment in the ASPERA case-base is annotated with a prose string that expresses the meaning of the fragment, and this prose string is used as the retrieval key for each fragment. Metrical rules are then used to combine these fragments into a well-formed poetic structure. Racter is an example of such a software project.


== Musical creativity ==
Computational creativity in the music domain has focused both on the generation of musical scores for use by human musicians, and on the generation of music for performance by computers. The domain of generation has included classical music (with software that generates music in the style of Mozart and Bach) and jazz. Most notably, David Cope has written a software system called ""Experiments in Musical Intelligence"" (or ""EMI"") that is capable of analyzing and generalizing from existing music by a human composer to generate novel musical compositions in the same style. EMI's output is convincing enough to persuade human listeners that its music is human-generated to a high level of competence.
In the field of contemporary classical music, Iamus is the first computer that composes from scratch, and produces final scores that professional interpreters can play. The London Symphony Orchestra played a piece for full orchestra, included in Iamus' debut CD, which New Scientist described as ""The first major work composed by a computer and performed by a full orchestra"". Melomics, the technology behind Iamus, is able to generate pieces in different styles of music with a similar level of quality.
Creativity research in jazz has focused on the process of improvisation and the cognitive demands that this places on a musical agent: reasoning about time, remembering and conceptualizing what has already been played, and planning ahead for what might be played next. The robot Shimon, developed by Gil Weinberg of Georgia Tech, has demonstrated jazz improvisation. Virtual improvisation software based on machine learning models of musical style include OMax, SoMax and PyOracle, are used to create improvisations in real-time by re-injecting variable length sequences learned on the fly from live performer.
In 1994, a Creativity Machine architecture (see above) was able to generate 11,000 musical hooks by training a synaptically perturbed neural net on 100 melodies that had appeared on the top ten list over the last 30 years. In 1996, a self-bootstrapping Creativity Machine observed audience facial expressions through an advanced machine vision system and perfected its musical talents to generate an album entitled ""Song of the Neurons""
In the field of musical composition, the patented works by René-Louis Baron allowed to make a robot that can create and play a multitude of orchestrated melodies so-called ""coherent"" in any musical style. All outdoor physical parameter associated with one or more specific musical parameters, can influence and develop each of these songs (in real time while listening to the song). The patented invention Medal-Composer raises problems of copyright.


== Visual and artistic creativity ==
Computational creativity in the generation of visual art has had some notable successes in the creation of both abstract art and representational art. The most famous program in this domain is Harold Cohen's AARON, which has been continuously developed and augmented since 1973. Though formulaic, Aaron exhibits a range of outputs, generating black-and-white drawings or colour paintings that incorporate human figures (such as dancers), potted plants, rocks, and other elements of background imagery. These images are of a sufficiently high quality to be displayed in reputable galleries.
Other software artists of note include the NEvAr system (for ""Neuro-Evolutionary Art"") of Penousal Machado. NEvAr uses a genetic algorithm to derive a mathematical function that is then used to generate a coloured three-dimensional surface. A human user is allowed to select the best pictures after each phase of the genetic algorithm, and these preferences are used to guide successive phases, thereby pushing NEvAr's search into pockets of the search space that are considered most appealing to the user.
The Painting Fool, developed by Simon Colton originated as a system for overpainting digital images of a given scene in a choice of different painting styles, colour palettes and brush types. Given its dependence on an input source image to work with, the earliest iterations of the Painting Fool raised questions about the extent of, or lack of, creativity in a computational art system. Nonetheless, in more recent work, The Painting Fool has been extended to create novel images, much as AARON does, from its own limited imagination. Images in this vein include cityscapes and forests, which are generated by a process of constraint satisfaction from some basic scenarios provided by the user (e.g., these scenarios allow the system to infer that objects closer to the viewing plane should be larger and more color-saturated, while those further away should be less saturated and appear smaller). Artistically, the images now created by the Painting Fool appear on a par with those created by Aaron, though the extensible mechanisms employed by the former (constraint satisfaction, etc.) may well allow it to develop into a more elaborate and sophisticated painter.
The artist Krasimira Dimtchevska and the software developer Svillen Ranev have created a computational system combining a rule-based generator of English sentences and a visual composition builder that converts sentences generated by the system into abstract art. The software generates automatically indefinite number of different images using different color, shape and size palettes. The software also allows the user to select the subject of the generated sentences or/and the one or more of the palettes used by the visual composition builder.
An emerging area of computational creativity is that of video games. ANGELINA is a system for creatively developing video games in Java by Michael Cook. One important aspect is Mechanic Miner, a system which can generate short segments of code which act as simple game mechanics. ANGELINA can evaluate these mechanics for usefulness by playing simple unsolvable game levels and testing to see if the new mechanic makes the level solvable. Sometimes Mechanic Miner discovers bugs in the code and exploits these to make new mechanics for the player to solve problems with.
In July 2015 Google released DeepDream – an open source computer vision program, created to detect faces and other patterns in images with the aim of automatically classifying images, which uses a convolutional neural network to find and enhance patterns in images via algorithmic pareidolia, thus creating a dreamlike psychedelic appearance in the deliberately over-processed images.
In August 2015 researchers from Tübingen, Germany created a convolutional neural network that uses neural representations to separate and recombine content and style of arbitrary images which is able to turn images into stylistic imitations of works of art by artists such as a Picasso or Van Gogh in about an hour. Their algorithm is put into use in the website DeepArt that allows users to create unique artistic images by their algorithm.
In early 2016, a global team of researchers explained how a new computational creativity approach known as the Digital Synaptic Neural Substrate (DSNS) could be used to generate original chess puzzles that were not derived from endgame databases. The DSNS is able to combine features of different objects (e.g. chess problems, paintings, music) using stochastic methods in order to derive new feature specifications which can be used to generate objects in any of the original domains. The generated chess puzzles have also been featured on YouTube.


== Creativity in problem solving ==
Creativity is also useful in allowing for unusual solutions in problem solving. In psychology and cognitive science, this research area is called creative problem solving. The Explicit-Implicit Interaction (EII) theory of creativity has recently been implemented using a CLARION-based computational model that allows for the simulation of incubation and insight in problem solving. The emphasis of this computational creativity project is not on performance per se (as in artificial intelligence projects) but rather on the explanation of the psychological processes leading to human creativity and the reproduction of data collected in psychology experiments. So far, this project has been successful in providing an explanation for incubation effects in simple memory experiments, insight in problem solving, and reproducing the overshadowing effect in problem solving.


== Debate about ""general"" theories of creativity ==
Some researchers feel that creativity is a complex phenomenon whose study is further complicated by the plasticity of the language we use to describe it. We can describe not just the agent of creativity as ""creative"" but also the product and the method. Consequently, it could be claimed that it is unrealistic to speak of a general theory of creativity. Nonetheless, some generative principles are more general than others, leading some advocates to claim that certain computational approaches are ""general theories"". Stephen Thaler, for instance, proposes that certain modalities of neural networks are generative enough, and general enough, to manifest a high degree of creative capabilities. Likewise, the Formal Theory of Creativity is based on a simple computational principle published by Jürgen Schmidhuber in 1991. The theory postulates that creativity and curiosity and selective attention in general are by-products of a simple algorithmic principle for measuring and optimizing learning progress.


=== Unified model of creativity ===
A unifying model of creativity was proposed by S. L. Thaler through a series of international patents in computational creativity, beginning in 1997 with the issuance of U.S. Patent 5,659,666. Based upon theoretical studies of traumatized neural networks and inspired by studies of damage-induced vibrational modes in simulated crystal lattices, this extensive intellectual property suite taught the application of a broad range of noise, damage, and disordering effects to a trained neural network so as to drive the formation of novel or confabulatory patterns that could potentially qualify as ideas and/or plans of action.
Thaler's scientific and philosophical papers both preceding and following the issuance of these patents described:
The aspects of creativity accompanying a broad gamut of cognitive functions (e.g., waking to dreaming to near-death trauma),
A shorthand notation for describing creative neural architectures and their function,
Quantitative modeling of the rhythm with which creative cognition occurs, and,
A prescription for critical perturbation regimes leading to the most efficient generation of useful information by a creative neural system.
A bottom-up model that links creativity and a wide range of psychopathologies. 
Thaler has also recruited his generative neural architectures into a theory of consciousness that closely models the temporal evolution of thought, creative or not, while also accounting for the subjective feel associated with this hotly debated mental phenomenon.
In 1989, in one of the most controversial reductions to practice of this general theory of creativity, one neural net termed the ""grim reaper,"" governed the synaptic damage (i.e., rule-changes) applied to another net that had learned a series of traditional Christmas carol lyrics. The former net, on the lookout for both novel and grammatical lyrics, seized upon the chilling sentence, ""In the end all men go to good earth in one eternal silent night,"" thereafter ceasing the synaptic degradation process. In subsequent projects, these systems produced more useful results across many fields of human endeavor, oftentimes bootstrapping their learning from a blank slate based upon the success or failure of self-conceived concepts and strategies seeded upon such internal network damage.


== Criticism of Computational Creativity ==
Traditional computers, as mainly used in the computational creativity application, do not support creativity, as they fundamentally transform a set of discrete, limited domain of input parameters into a set of discrete, limited domain of output parameters using a limited set of computational functions. As such, a computer cannot be creative, as everything in the output must have been already present in the input data or the algorithms. For some related discussions and references to related work are captured in some recent work on philosophical foundations of simulation.
Mathematically, the same set of arguments against creativity has been made by Chaitin. Similar observations come from a Model Theory perspective. All this criticism emphasizes that computational creativity is useful and may look like creativity, but it is not real creativity, as nothing new is created, just transformed in well defined algorithms.


== Events ==
The International Conference on Computational Creativity (ICCC) occurs annually, organized by The Association for Computational Creativity. Events in the series include:
ICCC 2017, Atlanta, Georgia, USA
ICCC 2016, Paris, France
ICCC 2015, Park City, Utah, USA. Keynote: Emily Short
ICCC 2014, Ljubljana, Slovenia. Keynote: Oliver Deussen
ICCC 2013, Sydney, Australia. Keynote: Arne Dietrich
ICCC 2012, Dublin, Ireland. Keynote: Steven Smith
ICCC 2011, Mexico City, Mexico. Keynote: George E Lewis
ICCC 2010, Lisbon, Portugal. Keynote/Inivited Talks: Nancy J Nersessian and Mary Lou Maher
Previously, the community of computational creativity has held a dedicated workshop, the International Joint Workshop on Computational Creativity, every year since 1999. Previous events in this series include:
IJWCC 2003, Acapulco, Mexico, as part of IJCAI'2003
IJWCC 2004, Madrid, Spain, as part of ECCBR'2004
IJWCC 2005, Edinburgh, UK, as part of IJCAI'2005
IJWCC 2006, Riva del Garda, Italy, as part of ECAI'2006
IJWCC 2007, London, UK, a stand-alone event
IJWCC 2008, Madrid, Spain, a stand-alone event
The 1st Conference on Computer Simulation of Musical Creativity will be held
CCSMC 2016, 17–19 June, University of Huddersfield, UK. Keynotes: Geraint Wiggins and Graeme Bailey.


== Publications and forums ==
Design Computing and Cognition is one conference that addresses computational creativity. The ACM Creativity and Cognition conference is another forum for issues related to computational creativity. Journées d'Informatique Musicale 2016 keynote by Shlomo Dubnov was on Information Theoretic Creativity.
A number of recent books provide either a good introduction or a good overview of the field of Computational Creativity. These include:
Pereira, F. C. (2007). ""Creativity and Artificial Intelligence: A Conceptual Blending Approach"". Applications of Cognitive Linguistics series, Mouton de Gruyter.
Veale, T. (2012). ""Exploding the Creativity Myth: The Computational Foundations of Linguistic Creativity"". Bloomsbury Academic, London.
McCormack, J. and d'Inverno, M. (eds.) (2012). ""Computers and Creativity"". Springer, Berlin.
Veale, T., Feyaerts, K. and Forceville, C. (2013, forthcoming). ""Creativity and the Agile Mind: A Multidisciplinary study of a Multifaceted phenomenon"". Mouton de Gruyter.
In addition to the proceedings of conferences and workshops, the computational creativity community has thus far produced these special journal issues dedicated to the topic:
New Generation Computing, volume 24, issue 3, 2006
Journal of Knowledge-Based Systems, volume 19, issue 7, November 2006
AI Magazine, volume 30, number 3, Fall 2009
Minds and Machines, volume 20, number 4, November 2010
Cognitive Computation, volume 4, issue 3, September 2012
AIEDAM, volume 27, number 4, Fall 2013
Computers in Entertainment, two special issues on Music Meta-Creation (MuMe), Fall 2016 (forthcoming)
In addition to these, a new journal has started which focuses on computational creativity within the field of music.
JCMS 2016, Journal of Creative Music Systems


== See also ==
Algorithmic art
Algorithmic composition
Applications of artificial intelligence
Computer art
Computer-generated music
Creative computing
Digital morphogenesis
Digital poetry
Generative systems
Musikalisches Würfelspiel (Musical dice game)
Procedural generation
Lists
List of emerging technologies
Outline of artificial intelligence


== References ==


== Further reading ==
An Overview of Artificial Creativity on Think Artificial
Cohen, H., ""the further exploits of AARON, Painter"", SEHR, volume 4, issue 2: Constructions of the Mind, 1995
Máquinas de computación, creatividad artificial y cine digital
Plotkin, R. ""The Genie in the Machine""


=== Documentaries ===
Noorderlicht: Margaret Boden and Stephen Thaler on Creative Computers
In Its Image"
4,Computational phylogenetics,3986130,58742,"Computational phylogenetics is the application of computational algorithms, methods, and programs to phylogenetic analyses. The goal is to assemble a phylogenetic tree representing a hypothesis about the evolutionary ancestry of a set of genes, species, or other taxa. For example, these techniques have been used to explore the family tree of hominid species and the relationships between specific genes shared by many types of organisms. Traditional phylogenetics relies on morphological data obtained by measuring and quantifying the phenotypic properties of representative organisms, while the more recent field of molecular phylogenetics uses nucleotide sequences encoding genes or amino acid sequences encoding proteins as the basis for classification. Many forms of molecular phylogenetics are closely related to and make extensive use of sequence alignment in constructing and refining phylogenetic trees, which are used to classify the evolutionary relationships between homologous genes represented in the genomes of divergent species. The phylogenetic trees constructed by computational methods are unlikely to perfectly reproduce the evolutionary tree that represents the historical relationships between the species being analyzed. The historical species tree may also differ from the historical tree of an individual homologous gene shared by those species.
Producing a phylogenetic tree requires a measure of homology among the characteristics shared by the taxa being compared. In morphological studies, this requires explicit decisions about which physical characteristics to measure and how to use them to encode distinct states corresponding to the input taxa. In molecular studies, a primary problem is in producing a multiple sequence alignment (MSA) between the genes or amino acid sequences of interest. Progressive sequence alignment methods produce a phylogenetic tree by necessity because they incorporate new sequences into the calculated alignment in order of genetic distance.


== Types of phylogenetic trees and networks ==
Phylogenetic trees generated by computational phylogenetics can be either rooted or unrooted depending on the input data and the algorithm used. A rooted tree is a directed graph that explicitly identifies a most recent common ancestor (MRCA), usually an imputed sequence that is not represented in the input. Genetic distance measures can be used to plot a tree with the input sequences as leaf nodes and their distances from the root proportional to their genetic distance from the hypothesized MRCA. Identification of a root usually requires the inclusion in the input data of at least one ""outgroup"" known to be only distantly related to the sequences of interest.
By contrast, unrooted trees plot the distances and relationships between input sequences without making assumptions regarding their descent. An unrooted tree can always be produced from a rooted tree, but a root cannot usually be placed on an unrooted tree without additional data on divergence rates, such as the assumption of the molecular clock hypothesis.
The set of all possible phylogenetic trees for a given group of input sequences can be conceptualized as a discretely defined multidimensional ""tree space"" through which search paths can be traced by optimization algorithms. Although counting the total number of trees for a nontrivial number of input sequences can be complicated by variations in the definition of a tree topology, it is always true that there are more rooted than unrooted trees for a given number of inputs and choice of parameters.
Both rooted and unrooted phylogenetic trees can be further generalized to rooted or unrooted phylogenetic networks, which allow for the modeling of evolutionary phenomena such as hybridization or horizontal gene transfer.


== Coding characters and defining homology ==


=== Morphological analysis ===
The basic problem in morphological phylogenetics is the assembly of a matrix representing a mapping from each of the taxa being compared to representative measurements for each of the phenotypic characteristics being used as a classifier. The types of phenotypic data used to construct this matrix depend on the taxa being compared; for individual species, they may involve measurements of average body size, lengths or sizes of particular bones or other physical features, or even behavioral manifestations. Of course, since not every possible phenotypic characteristic could be measured and encoded for analysis, the selection of which features to measure is a major inherent obstacle to the method. The decision of which traits to use as a basis for the matrix necessarily represents a hypothesis about which traits of a species or higher taxon are evolutionarily relevant. Morphological studies can be confounded by examples of convergent evolution of phenotypes. A major challenge in constructing useful classes is the high likelihood of inter-taxon overlap in the distribution of the phenotype's variation. The inclusion of extinct taxa in morphological analysis is often difficult due to absence of or incomplete fossil records, but has been shown to have a significant effect on the trees produced; in one study only the inclusion of extinct species of apes produced a morphologically derived tree that was consistent with that produced from molecular data.
Some phenotypic classifications, particularly those used when analyzing very diverse groups of taxa, are discrete and unambiguous; classifying organisms as possessing or lacking a tail, for example, is straightforward in the majority of cases, as is counting features such as eyes or vertebrae. However, the most appropriate representation of continuously varying phenotypic measurements is a controversial problem without a general solution. A common method is simply to sort the measurements of interest into two or more classes, rendering continuous observed variation as discretely classifiable (e.g., all examples with humerus bones longer than a given cutoff are scored as members of one state, and all members whose humerus bones are shorter than the cutoff are scored as members of a second state). This results in an easily manipulated data set but has been criticized for poor reporting of the basis for the class definitions and for sacrificing information compared to methods that use a continuous weighted distribution of measurements.
Because morphological data is extremely labor-intensive to collect, whether from literature sources or from field observations, reuse of previously compiled data matrices is not uncommon, although this may propagate flaws in the original matrix into multiple derivative analyses.


=== Molecular analysis ===
The problem of character coding is very different in molecular analyses, as the characters in biological sequence data are immediate and discretely defined - distinct nucleotides in DNA or RNA sequences and distinct amino acids in protein sequences. However, defining homology can be challenging due to the inherent difficulties of multiple sequence alignment. For a given gapped MSA, several rooted phylogenetic trees can be constructed that vary in their interpretations of which changes are ""mutations"" versus ancestral characters, and which events are insertion mutations or deletion mutations. For example, given only a pairwise alignment with a gap region, it is impossible to determine whether one sequence bears an insertion mutation or the other carries a deletion. The problem is magnified in MSAs with unaligned and nonoverlapping gaps. In practice, sizable regions of a calculated alignment may be discounted in phylogenetic tree construction to avoid integrating noisy data into the tree calculation.


== Distance-matrix methods ==

Distance-matrix methods of phylogenetic analysis explicitly rely on a measure of ""genetic distance"" between the sequences being classified, and therefore they require an MSA as an input. Distance is often defined as the fraction of mismatches at aligned positions, with gaps either ignored or counted as mismatches. Distance methods attempt to construct an all-to-all matrix from the sequence query set describing the distance between each sequence pair. From this is constructed a phylogenetic tree that places closely related sequences under the same interior node and whose branch lengths closely reproduce the observed distances between sequences. Distance-matrix methods may produce either rooted or unrooted trees, depending on the algorithm used to calculate them. They are frequently used as the basis for progressive and iterative types of multiple sequence alignments. The main disadvantage of distance-matrix methods is their inability to efficiently use information about local high-variation regions that appear across multiple subtrees.


=== UPGMA and WPGMA ===

The UPGMA (Unweighted Pair Group Method with Arithmetic mean) and WPGMA (Weighted Pair Group Method with Arithmetic mean) methods produce rooted trees and require a constant-rate assumption - that is, it assumes an ultrametric tree in which the distances from the root to every branch tip are equal.


=== Neighbor-joining ===

Neighbor-joining methods apply general cluster analysis techniques to sequence analysis using genetic distance as a clustering metric. The simple neighbor-joining method produces unrooted trees, but it does not assume a constant rate of evolution (i.e., a molecular clock) across lineages.


=== Fitch-Margoliash method ===
The Fitch-Margoliash method uses a weighted least squares method for clustering based on genetic distance. Closely related sequences are given more weight in the tree construction process to correct for the increased inaccuracy in measuring distances between distantly related sequences. The distances used as input to the algorithm must be normalized to prevent large artifacts in computing relationships between closely related and distantly related groups. The distances calculated by this method must be linear; the linearity criterion for distances requires that the expected values of the branch lengths for two individual branches must equal the expected value of the sum of the two branch distances - a property that applies to biological sequences only when they have been corrected for the possibility of back mutations at individual sites. This correction is done through the use of a substitution matrix such as that derived from the Jukes-Cantor model of DNA evolution. The distance correction is only necessary in practice when the evolution rates differ among branches. Another modification of the algorithm can be helpful, especially in case of concentrated distances (please report to concentration of measure phenomenon and curse of dimensionality): that modification, described in, has been shown to improve the efficiency of the algorithm and its robustness.
The least-squares criterion applied to these distances is more accurate but less efficient than the neighbor-joining methods. An additional improvement that corrects for correlations between distances that arise from many closely related sequences in the data set can also be applied at increased computational cost. Finding the optimal least-squares tree with any correction factor is NP-complete, so heuristic search methods like those used in maximum-parsimony analysis are applied to the search through tree space.


=== Using outgroups ===
Independent information about the relationship between sequences or groups can be used to help reduce the tree search space and root unrooted trees. Standard usage of distance-matrix methods involves the inclusion of at least one outgroup sequence known to be only distantly related to the sequences of interest in the query set. This usage can be seen as a type of experimental control. If the outgroup has been appropriately chosen, it will have a much greater genetic distance and thus a longer branch length than any other sequence, and it will appear near the root of a rooted tree. Choosing an appropriate outgroup requires the selection of a sequence that is moderately related to the sequences of interest; too close a relationship defeats the purpose of the outgroup and too distant adds noise to the analysis. Care should also be taken to avoid situations in which the species from which the sequences were taken are distantly related, but the gene encoded by the sequences is highly conserved across lineages. Horizontal gene transfer, especially between otherwise divergent bacteria, can also confound outgroup usage.


== Maximum parsimony ==
Maximum parsimony (MP) is a method of identifying the potential phylogenetic tree that requires the smallest total number of evolutionary events to explain the observed sequence data. Some ways of scoring trees also include a ""cost"" associated with particular types of evolutionary events and attempt to locate the tree with the smallest total cost. This is a useful approach in cases where not every possible type of event is equally likely - for example, when particular nucleotides or amino acids are known to be more mutable than others.
The most naive way of identifying the most parsimonious tree is simple enumeration - considering each possible tree in succession and searching for the tree with the smallest score. However, this is only possible for a relatively small number of sequences or species because the problem of identifying the most parsimonious tree is known to be NP-hard; consequently a number of heuristic search methods for optimization have been developed to locate a highly parsimonious tree, if not the best in the set. Most such methods involve a steepest descent-style minimization mechanism operating on a tree rearrangement criterion.


=== Branch and bound ===
The branch and bound algorithm is a general method used to increase the efficiency of searches for near-optimal solutions of NP-hard problems first applied to phylogenetics in the early 1980s. Branch and bound is particularly well suited to phylogenetic tree construction because it inherently requires dividing a problem into a tree structure as it subdivides the problem space into smaller regions. As its name implies, it requires as input both a branching rule (in the case of phylogenetics, the addition of the next species or sequence to the tree) and a bound (a rule that excludes certain regions of the search space from consideration, thereby assuming that the optimal solution cannot occupy that region). Identifying a good bound is the most challenging aspect of the algorithm's application to phylogenetics. A simple way of defining the bound is a maximum number of assumed evolutionary changes allowed per tree. A set of criteria known as Zharkikh's rules severely limit the search space by defining characteristics shared by all candidate ""most parsimonious"" trees. The two most basic rules require the elimination of all but one redundant sequence (for cases where multiple observations have produced identical data) and the elimination of character sites at which two or more states do not occur in at least two species. Under ideal conditions these rules and their associated algorithm would completely define a tree.


=== Sankoff-Morel-Cedergren algorithm ===
The Sankoff-Morel-Cedergren algorithm was among the first published methods to simultaneously produce an MSA and a phylogenetic tree for nucleotide sequences. The method uses a maximum parsimony calculation in conjunction with a scoring function that penalizes gaps and mismatches, thereby favoring the tree that introduces a minimal number of such events (an alternative view holds that the trees to be favored are those that maximize the amount of sequence similarity that can be interpreted as homology, a point of view that may lead to different optimal trees ). The imputed sequences at the interior nodes of the tree are scored and summed over all the nodes in each possible tree. The lowest-scoring tree sum provides both an optimal tree and an optimal MSA given the scoring function. Because the method is highly computationally intensive, an approximate method in which initial guesses for the interior alignments are refined one node at a time. Both the full and the approximate version are in practice calculated by dynamic programming.


=== MALIGN and POY ===
More recent phylogenetic tree/MSA methods use heuristics to isolate high-scoring, but not necessarily optimal, trees. The MALIGN method uses a maximum-parsimony technique to compute a multiple alignment by maximizing a cladogram score, and its companion POY uses an iterative method that couples the optimization of the phylogenetic tree with improvements in the corresponding MSA. However, the use of these methods in constructing evolutionary hypotheses has been criticized as biased due to the deliberate construction of trees reflecting minimal evolutionary events. This, in turn, has been countered by the view that such methods should be seen as heuristic approaches to find the trees that maximize the amount of sequence similarity that can be interpreted as homology.


== Maximum likelihood ==
The maximum likelihood method uses standard statistical techniques for inferring probability distributions to assign probabilities to particular possible phylogenetic trees. The method requires a substitution model to assess the probability of particular mutations; roughly, a tree that requires more mutations at interior nodes to explain the observed phylogeny will be assessed as having a lower probability. This is broadly similar to the maximum-parsimony method, but maximum likelihood allows additional statistical flexibility by permitting varying rates of evolution across both lineages and sites. In fact, the method requires that evolution at different sites and along different lineages must be statistically independent. Maximum likelihood is thus well suited to the analysis of distantly related sequences, but it is believed to be computationally intractable to compute due to its NP-hardness.
The ""pruning"" algorithm, a variant of dynamic programming, is often used to reduce the search space by efficiently calculating the likelihood of subtrees. The method calculates the likelihood for each site in a ""linear"" manner, starting at a node whose only descendants are leaves (that is, the tips of the tree) and working backwards toward the ""bottom"" node in nested sets. However, the trees produced by the method are only rooted if the substitution model is irreversible, which is not generally true of biological systems. The search for the maximum-likelihood tree also includes a branch length optimization component that is difficult to improve upon algorithmically; general global optimization tools such as the Newton-Raphson method are often used.


== Bayesian inference ==

Bayesian inference can be used to produce phylogenetic trees in a manner closely related to the maximum likelihood methods. Bayesian methods assume a prior probability distribution of the possible trees, which may simply be the probability of any one tree among all the possible trees that could be generated from the data, or may be a more sophisticated estimate derived from the assumption that divergence events such as speciation occur as stochastic processes. The choice of prior distribution is a point of contention among users of Bayesian-inference phylogenetics methods.
Implementations of Bayesian methods generally use Markov chain Monte Carlo sampling algorithms, although the choice of move set varies; selections used in Bayesian phylogenetics include circularly permuting leaf nodes of a proposed tree at each step and swapping descendant subtrees of a random internal node between two related trees. The use of Bayesian methods in phylogenetics has been controversial, largely due to incomplete specification of the choice of move set, acceptance criterion, and prior distribution in published work. Bayesian methods are generally held to be superior to parsimony-based methods; they can be more prone to long-branch attraction than maximum likelihood techniques, although they are better able to accommodate missing data.
Whereas likelihood methods find the tree that maximizes the probability of the data, a Bayesian approach recovers a tree that represents the most likely clades, by drawing on the posterior distribution. However, estimates of the posterior probability of clades (measuring their 'support') can be quite wide of the mark, especially in clades that aren't overwhelmingly likely. As such, other methods have been put forwards to estimate posterior probability.


== Model selection ==
Molecular phylogenetics methods rely on a defined substitution model that encodes a hypothesis about the relative rates of mutation at various sites along the gene or amino acid sequences being studied. At their simplest, substitution models aim to correct for differences in the rates of transitions and transversions in nucleotide sequences. The use of substitution models is necessitated by the fact that the genetic distance between two sequences increases linearly only for a short time after the two sequences diverge from each other (alternatively, the distance is linear only shortly before coalescence). The longer the amount of time after divergence, the more likely it becomes that two mutations occur at the same nucleotide site. Simple genetic distance calculations will thus undercount the number of mutation events that have occurred in evolutionary history. The extent of this undercount increases with increasing time since divergence, which can lead to the phenomenon of long branch attraction, or the misassignment of two distantly related but convergently evolving sequences as closely related. The maximum parsimony method is particularly susceptible to this problem due to its explicit search for a tree representing a minimum number of distinct evolutionary events.


=== Types of models ===

All substitution models assign a set of weights to each possible change of state represented in the sequence. The most common model types are implicitly reversible because they assign the same weight to, for example, a G>C nucleotide mutation as to a C>G mutation. The simplest possible model, the Jukes-Cantor model, assigns an equal probability to every possible change of state for a given nucleotide base. The rate of change between any two distinct nucleotides will be one-third of the overall substitution rate. More advanced models distinguish between transitions and transversions. The most general possible time-reversible model, called the GTR model, has six mutation rate parameters. An even more generalized model known as the general 12-parameter model breaks time-reversibility, at the cost of much additional complexity in calculating genetic distances that are consistent among multiple lineages. One possible variation on this theme adjusts the rates so that overall GC content - an important measure of DNA double helix stability - varies over time.
Models may also allow for the variation of rates with positions in the input sequence. The most obvious example of such variation follows from the arrangement of nucleotides in protein-coding genes into three-base codons. If the location of the open reading frame (ORF) is known, rates of mutation can be adjusted for position of a given site within a codon, since it is known that wobble base pairing can allow for higher mutation rates in the third nucleotide of a given codon without affecting the codon's meaning in the genetic code. A less hypothesis-driven example that does not rely on ORF identification simply assigns to each site a rate randomly drawn from a predetermined distribution, often the gamma distribution or log-normal distribution. Finally, a more conservative estimate of rate variations known as the covarion method allows autocorrelated variations in rates, so that the mutation rate of a given site is correlated across sites and lineages.


=== Choosing the best model ===
The selection of an appropriate model is critical for the production of good phylogenetic analyses, both because underparameterized or overly restrictive models may produce aberrant behavior when their underlying assumptions are violated, and because overly complex or overparameterized models are computationally expensive and the parameters may be overfit. The most common method of model selection is the likelihood ratio test (LRT), which produces a likelihood estimate that can be interpreted as a measure of ""goodness of fit"" between the model and the input data. However, care must be taken in using these results, since a more complex model with more parameters will always have a higher likelihood than a simplified version of the same model, which can lead to the naive selection of models that are overly complex. For this reason model selection computer programs will choose the simplest model that is not significantly worse than more complex substitution models. A significant disadvantage of the LRT is the necessity of making a series of pairwise comparisons between models; it has been shown that the order in which the models are compared has a major effect on the one that is eventually selected.
An alternative model selection method is the Akaike information criterion (AIC), formally an estimate of the Kullback–Leibler divergence between the true model and the model being tested. It can be interpreted as a likelihood estimate with a correction factor to penalize overparameterized models. The AIC is calculated on an individual model rather than a pair, so it is independent of the order in which models are assessed. A related alternative, the Bayesian information criterion (BIC), has a similar basic interpretation but penalizes complex models more heavily.
A comprehensive step-by-step protocol on constructing phylogenetic tree, including DNA/Amino Acid contiguous sequence assembly, multiple sequence alignment, model-test (testing best-fitting substitution models) and phylogeny reconstruction using Maximum Likelihood and Bayesian Inference, is available at Nature Protocol
A non traditional way of evaluating the phylogenetic tree is to compare it with clustering result. One can use a Multidimensional Scaling technique, so called Interpolative Joining to do dimensionality reduction to visualize the clustering result for the sequences in 3D, and then map the phylogenetic tree onto the clustering result. A better tree usually has a higher correlation with the clustering result.


== Evaluating tree support ==
As with all statistical analysis, the estimation of phylogenies from character data requires an evaluation of confidence. A number of methods exist to test the amount of support for a phylogenetic tree, either by evaluating the support for each sub-tree in the phylogeny (nodal support) or evaluating whether the phylogeny is significantly different from other possible trees (alternative tree hypothesis tests).


=== Nodal support ===
The most common method for assessing tree support is to evaluate the statistical support for each node on the tree. Typically, a node with very low support is not considered valid in further analysis, and visually may be collapsed into a polytomy to indicate that relationships within a clade are unresolved.


==== Consensus tree ====
Many methods for assessing nodal support involve consideration of multiple phylogenies. The consensus tree summarizes the nodes that are shared among a set of trees. In a *strict consensus,* only nodes found in every tree are shown, and the rest are collapsed into an unresolved polytomy. Less conservative methods, such as the *majority-rule consensus* tree, consider nodes that are supported by a given percentage of trees under consideration (such as at least 50%).
For example, in maximum parsimony analysis, there may be many trees with the same parsimony score. A strict consensus tree would show which nodes are found in all equally parsimonious trees, and which nodes differ. Consensus trees are also used to evaluate support on phylogenies reconstructed with Bayesian inference (see below).


==== Bootstrapping and jackknifing ====
In statistics, the bootstrap is a method for inferring the variability of data that has an unknown distribution using pseudoreplications of the original data. For example, given a set of 100 data points, a pseudoreplicate is a data set of the same size (100 points) randomly sampled from the original data, with replacement. That is, each original data point may be represented more than once in the pseudoreplicate, or not at all. Statistical support involves evaluation of whether the original data has similar properties to a large set of pseudoreplicates.
In phylogenetics, bootstrapping is conducted using the columns of the character matrix. Each pseudoreplicate contains the same number of species (rows) and characters (columns) randomly sampled from the original matrix, with replacement. A phylogeny is reconstructed from each pseudoreplicate, with the same methods used to reconstruct the phylogeny from the original data. For each node on the phylogeny, the nodal support is the percentage of pseudoreplicates containing that node.
The statistical rigor of the bootstrap test has been empirically evaluated using viral populations with known evolutionary histories, finding that 70% bootstrap support corresponds to a 95% probability that the clade exists. However, this was tested under ideal conditions (e.g. no change in evolutionary rates, symmetric phylogenies). In practice, values above 70% are generally supported and left to the researcher or reader to evaluate confidence. Nodes with support lower than 70% are typically considered unresolved.
Jackknifing in phylogenetics is a similar procedure, except the columns of the matrix are sampled without replacement. Pseudoreplicates are generated by randomly subsampling the data—for example, a ""10% jackknife"" would involve randomly sampling 10% of the matrix many times to evaluate nodal support.


==== Posterior probability ====
Reconstruction of phylogenies using Bayesian inference generates a posterior distribution of highly probable trees given the data and evolutionary model, rather than a single ""best"" tree. The trees in the posterior distribution generally have many different topologies. Most Bayesian inference methods utilize a Markov-chain Monte Carlo iteration, and the initial steps of this chain are not considered reliable reconstructions of the phylogeny. Trees generated early in the chain are usually discarded as burn-in. The most common method of evaluating nodal support in a Bayesian phylogenetic analysis is to calculate the percentage of trees in the posterior distribution (post-burn-in) which contain the node.
The statistical support for a node in Bayesian inference is expected to reflect the probability that a clade really exists given the data and evolutionary model. Therefore, the threshold for accepting a node as supported is generally higher than for bootstrapping.


==== Step counting methods ====
Bremer support counts the number of extra steps needed to contradict a clade.


=== Shortcomings ===
These measures each have their weaknesses. For example, smaller or larger clades tend to attract larger support values than mid-sized clades, simply as a result of the number of taxa in them.
Bootstrap support can provide high estimates of node support as a result of noise in the data rather than the true existence of a clade.


== Limitations and workarounds ==
Ultimately, there is no way to measure whether a particular phylogenetic hypothesis is accurate or not, unless the true relationships among the taxa being examined are already known (which may happen with bacteria or viruses under laboratory conditions). The best result an empirical phylogeneticist can hope to attain is a tree with branches that are well supported by the available evidence. Several potential pitfalls have been identified:


=== Homoplasy ===

Certain characters are more likely to evolve convergently than others; logically, such characters should be given less weight in the reconstruction of a tree. Weights in the form of a model of evolution can be inferred from sets of molecular data, so that maximum likelihood or Bayesian methods can be used to analyze them. For molecular sequences, this problem is exacerbated when the taxa under study have diverged substantially. As time since the divergence of two taxa increase, so does the probability of multiple substitutions on the same site, or back mutations, all of which result in homoplasies. For morphological data, unfortunately, the only objective way to determine convergence is by the construction of a tree – a somewhat circular method. Even so, weighting homoplasious characters does indeed lead to better-supported trees. Further refinement can be brought by weighting changes in one direction higher than changes in another; for instance, the presence of thoracic wings almost guarantees placement among the pterygote insects because, although wings are often lost secondarily, there is no evidence that they have been gained more than once.


=== Horizontal gene transfer ===
In general, organisms can inherit genes in two ways: vertical gene transfer and horizontal gene transfer. Vertical gene transfer is the passage of genes from parent to offspring, and horizontal (also called lateral) gene transfer occurs when genes jump between unrelated organisms, a common phenomenon especially in prokaryotes; a good example of this is the acquired antibiotic resistance as a result of gene exchange between various bacteria leading to multi-drug-resistant bacterial species. There have also been well-documented cases of horizontal gene transfer between eukaryotes.
Horizontal gene transfer has complicated the determination of phylogenies of organisms, and inconsistencies in phylogeny have been reported among specific groups of organisms depending on the genes used to construct evolutionary trees. The only way to determine which genes have been acquired vertically and which horizontally is to parsimoniously assume that the largest set of genes that have been inherited together have been inherited vertically; this requires analyzing a large number of genes.


=== Hybrids, speciation, introgressions and incomplete lineage sorting ===
The basic assumption underlying the mathematical model of cladistics is a situation where species split neatly in bifurcating fashion. While such an assumption may hold on a larger scale (bar horizontal gene transfer, see above), speciation is often much less orderly. Research since the cladistic method was introduced has shown that hybrid speciation, once thought rare, is in fact quite common, particularly in plants. Also paraphyletic speciation is common, making the assumption of a bifurcating pattern unsuitable, leading to phylogenetic networks rather than trees. Introgression can also move genes between otherwise distinct species and sometimes even genera, complicating phylogenetic analysis based on genes. This phenomenon can contribute to ""incomplete lineage sorting"" and is thought to be a common phenomenon across a number of groups. In species level analysis this can be dealt with by larger sampling or better whole genome analysis. Often the problem is avoided by restricting the analysis to fewer, not closely related specimens.


=== Taxon sampling ===
Owing to the development of advanced sequencing techniques in molecular biology, it has become feasible to gather large amounts of data (DNA or amino acid sequences) to infer phylogenetic hypotheses. For example, it is not rare to find studies with character matrices based on whole mitochondrial genomes (~16,000 nucleotides, in many animals). However, simulations have shown that it is more important to increase the number of taxa in the matrix than to increase the number of characters, because the more taxa there are, the more accurate and more robust is the resulting phylogenetic tree. This may be partly due to the breaking up of long branches.


=== Phylogenetic signal ===
Another important factor that affects the accuracy of tree reconstruction is whether the data analyzed actually contain a useful phylogenetic signal, a term that is used generally to denote whether a character evolves slowly enough to have the same state in closely related taxa as opposed to varying randomly. Tests for phylogenetic signal exist.


=== Continuous characters ===
Morphological characters that sample a continuum may contain phylogenetic signal, but are hard to code as discrete characters. Several methods have been used, one of which is gap coding, and there are variations on gap coding. In the original form of gap coding:

group means for a character are first ordered by size. The pooled within-group standard deviation is calculated ... and differences between adjacent means ... are compared relative to this standard deviation. Any pair of adjacent means is considered different and given different integer scores ... if the means are separated by a ""gap"" greater than the within-group standard deviation ... times some arbitrary constant.

If more taxa are added to the analysis, the gaps between taxa may become so small that all information is lost. Generalized gap coding works around that problem by comparing individual pairs of taxa rather than considering one set that contains all of the taxa.


=== Missing data ===
In general, the more data that are available when constructing a tree, the more accurate and reliable the resulting tree will be. Missing data are no more detrimental than simply having fewer data, although the impact is greatest when most of the missing data are in a small number of taxa. Concentrating the missing data across a small number of characters produces a more robust tree.


== The role of fossils ==
Because many characters involve embryological, or soft-tissue or molecular characters that (at best) hardly ever fossilize, and the interpretation of fossils is more ambiguous than that of living taxa, extinct taxa almost invariably have higher proportions of missing data than living ones. However, despite these limitations, the inclusion of fossils is invaluable, as they can provide information in sparse areas of trees, breaking up long branches and constraining intermediate character states; thus, fossil taxa contribute as much to tree resolution as modern taxa. Fossils can also constrain the age of lineages and thus demonstrate how consistent a tree is with the stratigraphic record; stratocladistics incorporates age information into data matrices for phylogenetic analyses.


== See also ==
List of phylogenetics software


== References ==


== Further reading ==
Charles Semple and Mike Steel (2003), Phylogenetics, Oxford University Press, ISBN 978-0-19-850942-4
Barry A. Cipra (2007), Algebraic Geometers See Ideal Approach to Biology, SIAM News, Volume 40, Number 6
Press, WH; Teukolsky, SA; Vetterling, WT; Flannery, BP (2007). ""Section 16.4. Hierarchical Clustering by Phylogenetic Trees"". Numerical Recipes: The Art of Scientific Computing (3rd ed.). New York: Cambridge University Press. ISBN 978-0-521-88068-8. 
Daniel H. Huson and Regula Rupp and Celine Scornavacca (2010) Phylogenetic Networks: Concepts, Algorithms and Applications, Cambridge University Press"
5,Computational immunology,3112875,56783,"In academia, computational immunology is a field of science that encompasses high-throughput genomic and bioinformatics approaches to immunology. The field's main aim is to convert immunological data into computational problems, solve these problems using mathematical and computational approaches and then convert these results into immunologically meaningful interpretations.


== Introduction ==
The immune system is a complex system of the human body and understanding it is one of the most challenging topics in biology. Immunology research is important for understanding the mechanisms underlying the defense of human body and to develop drugs for immunological diseases and maintain health. Recent findings in genomic and proteomic technologies have transformed the immunology research drastically. Sequencing of the human and other model organism genomes has produced increasingly large volumes of data relevant to immunology research and at the same time huge amounts of functional and clinical data are being reported in the scientific literature and stored in clinical records. Recent advances in bioinformatics or computational biology were helpful to understand and organize these large scale data and gave rise to new area that is called Computational immunology or immunoinformatics.
Computational immunology is a branch of bioinformatics and it is based on similar concepts and tools, such as sequence alignment and protein structure prediction tools. Immunomics is a discipline like genomics and proteomics. It is a science, which specifically combines Immunology with computer science, mathematics, chemistry, and biochemistry for large-scale analysis of immune system functions. It aims to study the complex protein–protein interactions and networks and allows a better understanding of immune responses and their role during normal, diseased and reconstitution states. Computational immunology is a part of immunomics, which is focused on analyzing large scale experimental data.


== History ==
Computational immunology began over 90 years ago with the theoretic modeling of malaria epidemiology. At that time, the emphasis was on the use of mathematics to guide the study of disease transmission. Since then, the field has expanded to cover all other aspects of immune system processes and diseases.


== Immunological database ==
After the recent advances in sequencing and proteomics technology, there have been many fold increase in generation of molecular and immunological data. The data are so diverse that they can be categorized in different databases according to their use in the research. Until now there are total 31 different immunological databases noted in the Nucleic Acids Research (NAR) Database Collection, which are given in the following table, together with some more immune related databases. The information given in the table is taken from the database descriptions in NAR Database Collection.
Online resources for allergy information are also available on http://www.allergen.org. Such data is valuable for investigation of cross-reactivity between known allergens and analysis of potential allergenicity in proteins. The Structural Database of Allergen Proteins (SDAP) stores information of allergenic proteins. The Food Allergy Research and Resource Program (FARRP) Protein Allergen-Online Database contains sequences of known and putative allergens derived from scientific literature and public databases. Allergome emphasizes the annotation of allergens that result in an IgE-mediated disease.


== Tools ==
A variety of computational, mathematical and statistical methods are available and reported. These tools are helpful for collection, analysis, and interpretation of immunological data. They include text mining, information management, sequence analysis, analysis of molecular interactions, and mathematical models that enable advanced simulations of immune system and immunological processes. Attempts are being made for the extraction of interesting and complex patterns from non-structured text documents in the immunological domain. Such as categorization of allergen cross-reactivity information, identification of cancer-associated gene variants and the classification of immune epitopes.
Immunoinformatics is using the basic bioinformatics tools such as ClustalW, BLAST, and TreeView, as well as specialized immunoinformatics tools, such as EpiMatrix, IMGT/V-QUEST for IG and TR sequence analysis, IMGT/ Collier-de-Perles and IMGT/StructuralQuery for IG variable domain structure analysis. Methods that rely on sequence comparison are diverse and have been applied to analyze HLA sequence conservation, help verify the origins of human immunodeficiency virus (HIV) sequences, and construct homology models for the analysis of hepatitis B virus polymerase resistance to lamivudine and emtricitabine.
There are also some computational models which focus on protein–protein interactions and networks. There are also tools which are used for T and B cell epitope mapping, proteasomal cleavage site prediction, and TAP– peptide prediction. The experimental data is very much important to design and justify the models to predict various molecular targets. Computational immunology tools is the game between experimental data and mathematically designed computational tools.


== Applications ==


=== Allergies ===
Allergies, while a critical subject of immunology, also vary considerably among individuals and sometimes even among genetically similar individuals. The assessment of protein allergenic potential focuses on three main aspects: (i) immunogenicity; (ii) cross-reactivity; and (iii) clinical symptoms. Immunogenicity is due to responses of an IgE antibody-producing B cell and/or of a T cell to a particular allergen. Therefore, immunogenicity studies focus mainly on identifying recognition sites of B-cells and T-cells for allergens. The three-dimensional structural properties of allergens control their allergenicity.
The use of immunoinformatics tools can be useful to predict protein allergenicity and will become increasingly important in the screening of novel foods before their wide-scale release for human use. Thus, there are major efforts under way to make reliable broad based allergy databases and combine these with well validated prediction tools in order to enable the identification of potential allergens in genetically modified drugs and foods. Though the developments are on primary stage, the World Health organization and Food and Agriculture Organization have proposed guidelines for evaluating allergenicity of genetically modified foods. According to the Codex alimentarius, a protein is potentially allergenic if it possesses an identity of ≥6 contiguous amino acids or ≥35% sequence similarity over an 80 amino acid window with a known allergen. Though there are rules, their inherent limitations have started to become apparent and exceptions to the rules have been well reported 


=== Infectious diseases and host responses ===
In the study of infectious diseases and host responses, the mathematical and computer models are a great help. These models were very useful in characterizing the behavior and spread of infectious disease, by understanding the dynamics of the pathogen in the host and the mechanisms of host factors which aid pathogen persistence. Examples include Plasmodium falciparum and nematode infection in ruminants.
Much has been done in understanding immune responses to various pathogens by integrating genomics and proteomics with bioinformatics strategies. Many exciting developments in large-scale screening of pathogens are currently taking place. National Institute of Allergy and Infectious Diseases (NIAID) has initiated an endeavor for systematic mapping of B and T cell epitopes of category A-C pathogens. These pathogens include Bacillus anthracis (anthrax), Clostridium botulinum toxin (botulism), Variola major (smallpox), Francisella tularensis (tularemia), viral hemorrhagic fevers, Burkholderia pseudomallei, Staphylococcus enterotoxin B, yellow fever, influenza, rabies, Chikungunya virus etc. Rule-based systems have been reported for the automated extraction and curation of influenza A records.
This development would lead to the development of an algorithm which would help to identify the conserved regions of pathogen sequences and in turn would be useful for vaccine development. This would be helpful in limiting the spread of infectious disease. Examples include a method for identification of vaccine targets from protein regions of conserved HLA binding and computational assessment of cross-reactivity of broadly neutralizing antibodies against viral pathogens. These examples illustrate the power of immunoinformatics applications to help solve complex problems in public health. Immunoinformatics could accelerate the discovery process dramatically and potentially shorten the time required for vaccine development. Immunoinformatics tools have been used to design the vaccine against Dengue virus  and Leishmania 


=== Immune system function ===
Using this technology it is possible to know the model behind immune system. It has been used to model T-cell-mediated suppression, peripheral lymphocyte migration, T-cell memory, tolerance, thymic function, and antibody networks. Models are helpful to predicts dynamics of pathogen toxicity and T-cell memory in response to different stimuli. There are also several models which are helpful in understanding the nature of specificity in immune network and immunogenicity.
For example, it was useful to examine the functional relationship between TAP peptide transport and HLA class I antigen presentation. TAP is a transmembrane protein responsible for the transport of antigenic peptides into the endoplasmic reticulum, where MHC them class I molecules can bind them and presented to T cells. As TAP does not bind all peptides equally, TAP-binding affinity could influence the ability of a particular peptide to gain access to the MHC class I pathway. Artificial neural network (ANN), a computer model was used to study peptide binding to human TAP and its relationship with MHC class I binding. The affinity of HLA-binding peptides for TAP was found to differ according to the HLA supertype concerned using this method. This research could have important implications for the design of peptide based immuno-therapeutic drugs and vaccines. It shows the power of the modeling approach to understand complex immune interactions.
There exist also methods which integrate peptide prediction tools with computer simulations that can provide detailed information on the immune response dynamics specific to the given pathogen's peptides .


=== Cancer Informatics ===
Cancer is the result of somatic mutations which provide cancer cells with a selective growth advantage. Recently it has been very important to determine the novel mutations. Genomics and proteomics techniques are used worldwide to identify mutations related to each specific cancer and their treatments. Computational tools are used to predict growth and surface antigens on cancerous cells. There are publications explaining a targeted approach for assessing mutations and cancer risk. Algorithm CanPredict was used to indicate how closely a specific gene resembles known cancer-causing genes. Cancer immunology has been given so much importance that the data related to it is growing rapidly. Protein–protein interaction networks provide valuable information on tumorigenesis in humans. Cancer proteins exhibit a network topology that is different from normal proteins in the human interactome. Immunoinformatics have been useful in increasing success of tumour vaccination. Recently, pioneering works have been conducted to analyse the host immune system dynamics in response to artificial immunity induced by vaccination strategies.. Other simulation tools use predicted cancer peptides to forecast immune specific anticancer responses that is dependent on the specified HLA. These resources are likely to grow significantly in the near future and immunoinformatics will be a major growth area in this domain.


== See also ==
Computational biology
Immunology
Genetics
Cancer
Immunity


== References ==


== External links ==
Boston University Center for Computational Immunology
York Computational Immunology Lab
Immunoinformatics Immunological Software and Web Services from Gajendra Pal Singh Raghava group"
6,List of important publications in computer science,454351,53953,"This is a list of important publications in computer science, organized by field.
Some reasons why a particular publication might be regarded as important:
Topic creator – A publication that created a new topic
Breakthrough – A publication that changed scientific knowledge significantly
Influence – A publication which has significantly influenced the world or has had a massive impact on the teaching of computer science.


== Artificial intelligence ==


=== Computing Machinery and Intelligence ===
Alan Turing
Mind, 59:433–460, 1950.
Online copy
Description: This paper discusses whether machines can think and suggested the Turing test as a method for checking it.


=== A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence ===
John McCarthy
Marvin Minsky
N. Rochester
C.E. Shannon
Online copy
Description: This summer research proposal inaugurated and defined the field. It contains the first use of the term artificial intelligence and this succinct description of the philosophical foundation of the field: ""every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it."" (See philosophy of AI) The proposal invited researchers to the Dartmouth conference, which is widely considered the ""birth of AI"". (See history of AI.)


=== Fuzzy sets ===
Lotfi Zadeh
Information and Control, Vol. 8, pp. 338–353. (1965).
Description: The seminal paper published in 1965 provides details on the mathematics of fuzzy set theory.


=== Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference ===
Judea Pearl
ISBN 1-55860-479-0 Publisher: Morgan Kaufmann Pub, 1988
Description: This book introduced Bayesian methods to AI.


=== Artificial Intelligence: A Modern Approach ===
Stuart J. Russell and Peter Norvig
Prentice Hall, Englewood Cliffs, New Jersey, 1995, ISBN 0-13-080302-2
Textbook's website
Description: The standard textbook in Artificial Intelligence. The book web site lists over 1100 colleges.


=== Machine learning ===


==== An Inductive Inference Machine ====
Ray Solomonoff
IRE Convention Record, Section on Information Theory, Part 2, pp. 56–62, 1957
(A longer version of this, a privately circulated report, 1956, is online).
Description: The first paper written on machine learning. Emphasized the importance of training sequences, and the use of parts of previous solutions to problems in constructing trial solutions to new problems.


==== Language identification in the limit ====
E. Mark Gold
Information and Control, 10(5):447–474, 1967
Online version: (HTML) (PDF)
Description: This paper created Algorithmic learning theory.


==== On the uniform convergence of relative frequencies of events to their probabilities ====
V. Vapnik, A. Chervonenkis
Theory of Probability and its Applications, 16(2):264—280, 1971
Description: Computational learning theory, VC theory, statistical uniform convergence and the VC dimension.


==== A theory of the learnable ====
Leslie Valiant
Communications of the ACM, 27(11):1134–1142 (1984)
Description: The Probably approximately correct learning (PAC learning) framework.


==== Learning representations by back-propagating errors ====
David E. Rumelhart, Geoffrey E. Hinton and Ronald J. Williams
Nature, 323, 533—536, 1986
Description: Development of Backpropagation algorithm for artificial neural networks. Note that the algorithm was first described by Paul Werbos in 1974.


==== Induction of Decision Trees ====
J.R. Quinlan
Machine Learning, 1. 81—106, 1986.
Description: Decision Trees are a common learning algorithm and a decision representation tool. Development of decision trees was done by many researchers in many areas, even before this paper. Though this paper is one of the most influential in the field.


==== Learning Quickly When Irrelevant Attributes Abound: A New Linear-threshold Algorithm ====
Nick Littlestone
Machine Learning 2: 285–318, 1988
Online version(PDF)
Description: One of the papers that started the field of on-line learning. In this learning setting, a learner receives a sequence of examples, making predictions after each one, and receiving feedback after each prediction. Research in this area is remarkable because (1) the algorithms and proofs tend to be very simple and beautiful, and (2) the model makes no statistical assumptions about the data. In other words, the data need not be random (as in nearly all other learning models), but can be chosen arbitrarily by ""nature"" or even an adversary. Specifically, this paper introduced the winnow algorithm.


==== Learning to predict by the method of Temporal difference ====
Richard S. Sutton
Machine Learning 3(1): 9–44
Online copy
Description: The Temporal difference method for reinforcement learning.


==== Learnability and the Vapnik–Chervonenkis dimension ====
A. Blumer
A. Ehrenfeucht
D. Haussler
M. K. Warmuth
Journal of the ACM, 36(4):929–965, 1989.
Description: The complete characterization of PAC learnability using the VC dimension.


==== Cryptographic limitations on learning boolean formulae and finite automata ====
M. Kearns
L. G. Valiant
In Proceedings of the 21st Annual ACM Symposium on Theory of Computing, pages 433–444, New York. ACM.
Online version(HTML)
Description: Proving negative results for PAC learning.


==== The strength of weak learnability ====
Robert E. Schapire
Machine Learning, 5(2):197–227, 1990.
Online version(HTML)
Description: Proving that weak and strong learnability are equivalent in the noise free PAC framework. The proof was done by introducing the boosting method.


==== A training algorithm for optimum margin classifiers ====
Bernhard E. Boser
Isabelle M. Guyon
Vladimir N. Vapnik
Proceedings of the Fifth Annual Workshop on Computational Learning Theory 5 144–152, Pittsburgh (1992).
Online version(HTML)
Description: This paper presented support vector machines, a practical and popular machine learning algorithm. Support vector machines often use the kernel trick.


==== A fast learning algorithm for deep belief nets ====
Geoffrey E. Hinton
Simon Osindero
Yee-Whye Teh
Neural Computation (2006)
Online PDF
Description: This paper presented a tractable greedy layer-wise learning algorithm for deep belief networks which led to great advancement in the field of deep learning.


==== Knowledge-based analysis of microarray gene expression data by using support vector machines ====
MP Brown
WN Grundy
D Lin
Nello Cristianini
CW Sugnet
TS Furey
M Ares Jr,
David Haussler
PNAS, 2000 January 4;97(1):262–7 <http://www.pnas.org/cgi/content/abstract/97/1/262>
Description: The first application of supervised learning to gene expression data, in particular Support Vector Machines. The method is now standard, and the paper one of the most cited in the area.


== Collaborative networks ==
Camarinha-Matos, L. M.; Afsarmanesh, H. (2005). ""Collaborative networks: A new scientific discipline, J"". Intelligent Manufacturing. 16 (4–5): 439–452. doi:10.1007/s10845-005-1656-3. 
Camarinha-Matos, L. M.; Afsarmanesh, H. (2008). Collaborative Networks: Reference Modeling, Springer: New York.


== Compilers ==


=== On the translation of languages from left to right ===
Knuth, D. E. (July 1965). ""On the translation of languages from left to right"" (PDF). Information and Control. 8 (6): 607–639. doi:10.1016/S0019-9958(65)90426-2. Retrieved 29 May 2011. 
Description: LR parser, which does bottom up parsing for deterministic context-free languages. Later derived parsers, such as the LALR parser, have been and continue to be standard practice, such as in Yacc and descendents.


=== Semantics of Context-Free Languages. ===
Donald Knuth
Math. Systems Theory 2:2 (1968), 127–145.
Description: About grammar attribution, the base for yacc's s-attributed and zyacc's LR-attributed approach.


=== A program data flow analysis procedure ===
Frances E. Allen, J. Cocke
Commun. ACM, 19, 137—147.
Description: From the abstract: ""The global data relationships in a program can be exposed and codified by the static analysis methods described in this paper. A procedure is given which determines all the definitions which can possibly reach each node of the control flow graph of the program and all the definitions that are live on each edge of the graph.""


=== A Unified Approach to Global Program Optimization ===
Gary Kildall
Proceedings of ACM SIGACT-SIGPLAN 1973 Symposium on Principles of Programming Languages.
pdf
Description: Formalized the concept of data-flow analysis as fixpoint computation over lattices, and showed that most static analyses used for program optimization can be uniformly expressed within this framework.


=== YACC: Yet another compiler-compiler ===
Stephen C. Johnson
Unix Programmer's Manual Vol 2b, 1979
Online copy (HTML)
Description: Yacc is a tool that made compiler writing much easier.


=== gprof: A Call Graph Execution Profiler ===
Susan L. Graham, Peter B. Kessler, Marshall Kirk McKusick
Proceedings of the ACM SIGPLAN 1982 Symposium on Compiler Construction, SIGPLAN Notices 17, 6, Boston, MA. June 1982.
Online copy; pdf
Description: The gprof profiler


=== Compilers: Principles, Techniques and Tools ===
Alfred V. Aho
Ravi Sethi
Jeffrey D. Ullman
Monica Lam
Addison-Wesley, 1986. ISBN 0-201-10088-6
Description: This book became a classic in compiler writing. It is also known as the Dragon book, after the (red) dragon that appears on its cover.


== Computer architecture ==


=== Colossus computer ===
T. H. Flowers
Annals of the History of Computing, Vol. 5 (No. 3), 1983, pp. 239–252.
The Design of Colossus
Description: The Colossus machines were early computing devices used by British codebreakers to break German messages encrypted with the Lorenz Cipher during World War II. Colossus was an early binary electronic digital computer. The design of Colossus was later described in the referenced paper.


=== First Draft of a Report on the EDVAC ===
John von Neumann
June 30, 1945, the ENIAC project.
First Draft of a report on the EDVAC (PDF)
Description: It contains the first published description of the logical design of a computer using the stored-program concept, which has come to be known as the von Neumann architecture.


=== Architecture of the IBM System/360 ===
Gene Amdahl, Fred Brooks, G. A. Blaauw
IBM Journal of Research and Development, 1964.
Architecture of the IBM System/360
Description: The IBM System/360 (S/360) is a mainframe computer system family announced by IBM on April 7, 1964. It was the first family of computers making a clear distinction between architecture and implementation.


=== The case for the reduced instruction set computer ===
DA Patterson, DR Ditzel
Computer ArchitectureNews, vol. 8, no. 6, October 1980, pp 25–33.
Online version(PDF)
Description: The reduced instruction set computer( RISC) CPU design philosophy. The RISC is a CPU design philosophy that favors a reduced set of simpler instructions.


=== Comments on ""the Case for the Reduced Instruction Set Computer"" ===
DW Clark, WD Strecker
Computer Architecture News, 1980.
Online version(PDF)
Description:


=== The CRAY-1 Computer System ===
DW Clark, WD Strecker
Communications of the ACM, January 1978, volume 21, number 1, pages 63–72.
Online version(PDF)
Description: The Cray-1 was a supercomputer designed by a team including Seymour Cray for Cray Research. The first Cray-1 system was installed at Los Alamos National Laboratory in 1976, and it went on to become one of the best known and most successful supercomputers in history.


=== Validity of the Single Processor Approach to Achieving Large Scale Computing Capabilities ===
Gene Amdahl
AFIPS 1967 Spring Joint Computer Conference, Atlantic City, N.J.
Online version(PDF)
Description: The Amdahl's Law.


=== A Case for Redundant Arrays of Inexpensive Disks (RAID) ===
David A. Patterson, Garth Gibson, Randy H. Katz
In International Conference on Management of Data, pages 109—116, 1988.
Online version(PDF)
Description: This paper discusses the concept of RAID disks, outlines the different levels of RAID, and the benefits of each level. It is a good paper for discussing issues of reliability and fault tolerance of computer systems, and the cost of providing such fault-tolerance.


=== The case for a single-chip multiprocessor ===
Kunle Olukotun, Basem Nayfeh, Lance Hammond, Ken Wilson, Kunyung Chang
In SIGOPS Oper. Syst. Rev. 30, pages 2–11, 1996.
Online version(PDF)
Description: This paper argues that the approach taken to improving the performance of processors by adding multiple instruction issue and out-of-order execution cannot continue to provide speedups indefinitely. It lays out the case for making single chip processors that contain multiple ""cores"". With the mainstream introduction of multicore processors by Intel in 2005, and their subsequent domination of the market, this paper was shown to be prescient.


== Computer graphics ==


=== The Rendering Equation ===
J. Kajiya
SIGGRAPH: ACM Special Interest Group on Computer Graphics and Interactive Techniques pages 143—150


=== Elastically deformable models ===
Demetri Terzopoulos, John Platt, Alan Barr, Kurt Fleischer
Computer Graphics, 21(4), 1987, 205–214, Proc. ACM SIGGRAPH'87 Conference, Anaheim, CA, July 1987.
Online version(PDF)
Description: The Academy of Motion Picture Arts and Sciences cited this paper as a ""milestone in computer graphics"".


== Computer vision ==


=== The Phase Correlation Image Alignment Method ===
C.D. Kuglin and D.C. Hines
IEEE 1975 Conference on Cybernetics and Society, 1975, New York, pp. 163–165, September
Description: A correlation method based upon the inverse Fourier transform


=== Determining Optical Flow ===
B.K.P. Horn and B.G. Schunck
Artificial Intelligence, Volume 17, 185–203, 1981
OA article here: doi:10.1016/0004-3702(81)90024-2
Description: A method for estimating the image motion of world points between 2 frames of a video sequence.


=== An Iterative Image Registration Technique with an Application to Stereo Vision ===
Lucas, B.D. and Kanade, T.
Proceedings of the 7th International Joint Conference on Artificial Intelligence, 674–679, Vancouver, Canada, 1981
Online version
Description: This paper provides efficient technique for image registration


=== The Laplacian Pyramid as a compact image code ===
Peter J. Burt and Edward H. Adelson
IEEE Transactions on Communications, volume = ""COM-31,4"", pp. 532–540, 1983.
Online version
Description: A technique for image encoding using local operators of many scales.


=== Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images ===
Stuart Geman and Donald Geman
IEEE Transactions on Pattern Analysis and Machine Intelligence, 1984
Description: introduced 1) MRFs for image analysis 2) the Gibbs sampling which revolutionized computational Bayesian statistics and thus had paramount impact in many other fields in addition to Computer Vision.


=== Snakes: Active contour models ===
Michael Kass, Andrew Witkin, and Demetri Terzopoulos
Description: An interactive variational technique for image segmentation and visual tracking.


=== Condensation – conditional density propagation for visual tracking ===
M. Isard and A. Blake
International Journal of Computer Vision, 29(1):5–28, 1998.
Online version
Description: A technique for visual tracking


=== Object recognition from local scale-invariant features ===
David Lowe
International Conference on Computer Vision, pp. 1150–1157, 1999
[1]
Description: A technique (scale-invariant feature transform) for robust feature description


== Concurrent, parallel, and distributed computing ==

Topics covered: concurrent computing, parallel computing, and distributed computing.


== Databases ==


=== A relational model for large shared data banks ===
E. F. Codd
Communications of the ACM, 13(6):377–387, June 1970
Description: This paper introduced the relational model for databases. This model became the number one model.


=== Binary B-Trees for Virtual Memory ===
Rudolf Bayer
ACM-SIGFIDET Workshop 1971, San Diego, California, Session 5B, p. 219–235.
Description: This paper introduced the B-Trees data structure. This model became the number one model.


=== Relational Completeness of Data Base Sublanguages ===
E. F. Codd
In: R. Rustin (ed.): Database Systems: 65-98, Prentice Hall and IBM Research Report RJ 987, San Jose, California : (1972)
Online version (PDF)
Description: Completeness of Data Base Sublanguages


=== The Entity Relationship Model – Towards a Unified View of Data ===
Peter Chen
ACM Transactions on Database Systems, Vol. 1, No. 1, March 1976, pp. 9–36
Description: This paper introduced the entity-relationship diagram(ERD) method of database design.


=== SEQUEL: A structured English query language ===
Donald D. Chamberlin, Raymond F. Boyce
International Conference on Management of Data, Proceedings of the 1974 ACM SIGFIDET (now SIGMOD) workshop on Data description, access and control, Ann Arbor, Michigan, pp. 249–264
Description: This paper introduced the SQL language.


=== The notions of consistency and predicate locks in a database system ===
K.P. Eswaran, Jim Gray, R.A. Lorie, I.L. Traiger
Communications of the ACM 19, 1976, 624—633
Description: This paper defined the concepts of transaction, consistency and schedule. It also argued that a transaction needs to lock a logical rather than a physical subset of the database.


=== Federated database systems for managing distributed, heterogeneous, and autonomous databases ===
Amit Sheth, J.A. Larson,""
ACM Computing Surveys - Special issue on heterogeneous databases Surveys, Volume 22 Issue 3, Pages 183 - 236, Sept. 1990
ACM source
Description: Introduced federated database systems concept leading huge impact on data interoperability and integration of hetereogenous data sources.


=== Mining association rules between sets of items in large databases ===
Rakesh Agrawal, Tomasz Imielinski, Arun Swami
Proc. of the ACM SIGMOD Conference on Management of Data, pages 207–216, Washington, D.C., May 1993
Online copy (HTML)
Description: Association rules, a very common method for data mining.


== History of computation ==


=== The Computer from Pascal to von Neumann ===
Goldstine, Herman H. (1972). The Computer from Pascal to von Neumann. Princeton University Press. ISBN 0-691-08104-2. 
Description: Perhaps the first book on the history of computation.


=== A History of Computing in the Twentieth Century ===
edited by:
Nicholas Metropolis
J. Howlett
Gian-Carlo Rota
Academic Press, 1980, ISBN 0-12-491650-3
Description: Several chapters by pioneers of computing.


== Information retrieval ==


=== A Vector Space Model for Automatic Indexing ===
Gerard Salton, A. Wong, C. S. Yang
Commun. ACM 18(11): 613–620 (1975)
Description: Presented the vector space model.


=== Extended Boolean Information Retrieval ===
Gerard Salton, Edward A. Fox, Harry Wu
Commun. ACM 26(11): 1022–1036 (1983)
Description: Presented the inverted index


=== A Statistical Interpretation of Term Specificity and Its Application in Retrieval ===
Karen Spärck Jones
Journal of Documentation 28: 11–21 (1972). doi:10.1108/eb026526.
Description: Conceived a statistical interpretation of term specificity called Inverse document frequency (IDF), which became a cornerstone of term weighting.


== Networking ==


=== Data Communications and Networking ===
Behrouz A. Forouzan. ISBN 0073376221, Copyright year: 2013, Publisher:: McGraw hill education.
Description: This book presents a comprehensive and accessible approach to data communications and networking that has made this book a favorite with students and professionals alike. More than 830 figures and 150 tables accompany the text and provide a visual and intuitive opportunity for understanding the material.


== Operating systems ==


=== An experimental timesharing system. ===
Fernando J. Corbató, M. Merwin-Daggett, and R.C. Daley
Proceedings of the AFIPS FJCC, pages 335–344, 1962.
Online copy (HTML)
Description: This paper discuss time-sharing as a method of sharing computer resource. This idea changed the interaction with computer systems.


=== The Working Set Model for Program Behavior ===
Peter J. Denning
Communications of the ACM, Vol. 11, No. 5, May 1968, pp 323–333
Online version(PDF)
Description: The beginning of cache. For more information see SIGOPS Hall of Fame.


=== Virtual Memory, Processes, and Sharing in MULTICS ===
Robert C. Daley, Jack B. Dennis
Communications of the ACM, Vol. 11, No. 5, May 1968, pp. 306–312.
Online version(PDF)
Description: The classic paper on Multics, the most ambitious operating system in the early history of computing. Difficult reading, but it describes the implications of trying to build a system that takes information sharing to its logical extreme. Most operating systems since Multics have incorporated a subset of its facilities.


=== The nucleus of a multiprogramming system ===
Per Brinch Hansen
Communications of the ACM, Vol. 13, No. 4, April 1970, pp. 238–242
Online version(PDF)
Description: Classic paper on the extensible nucleus architecture of the RC 4000 multiprogramming system, and what became known as the operating system kernel and microkernel architecture.


=== Operating System Principles ===
Per Brinch Hansen
Prentice Hall, Englewood Cliffs, NJ, July 1973
Online version (ACM Digital Library)
Description: The first comprehensive textbook on operating systems. Includes the first monitor notation (Chapter 7).


=== A note on the confinement problem ===
Butler W. Lampson
Communications of the ACM, 16(10):613–615, October 1973.
Online version(PDF)
Description: This paper addresses issues in constraining the flow of information from untrusted programs. It discusses covert channels, but more importantly it addresses the difficulty in obtaining full confinement without making the program itself effectively unusable. The ideas are important when trying to understand containment of malicious code, as well as aspects of trusted computing.


=== The UNIX Time-Sharing System ===
Dennis M. Ritchie and Ken Thompson
Communications of the ACM 17(7), July 1974.
Online copy
Description: The Unix operating system and its principles were described in this paper. The main importance is not of the paper but of the operating system, which had tremendous effect on operating system and computer technology.


=== Weighted voting for replicated data ===
David K. Gifford
Proceedings of the 7th ACM Symposium on Operating Systems Principles, pages 150–159, December 1979. Pacific Grove, California
Online copy (few formats)
Description: This paper describes the consistency mechanism known as quorum consensus. It is a good example of algorithms that provide a continuous set of options between two alternatives (in this case, between the read-one write-all, and the write-one read-all consistency methods). There have been many variations and improvements by researchers in the years that followed, and it is one of the consistency algorithms that should be understood by all. The options available by choosing different size quorums provide a useful structure for discussing of the core requirements for consistency in distributed systems.


=== Experiences with Processes and Monitors in Mesa ===
Butler W. Lampson, David D. Redell
Communications of the ACM, Vol. 23, No. 2, February 1980, pp. 105–117.
Online copy (PDF)
Description: This is the classic paper on synchronization techniques, including both alternate approaches and pitfalls.


=== Scheduling Techniques for Concurrent Systems ===
J. K. Ousterhout
Proceedings of Third International Conference on Distributed Computing Systems, 1982, 22—30.
Description: Algorithms for coscheduling of related processes were given


=== A Fast File System for UNIX ===
Marshall Kirk Mckusick, William N. Joy, Samuel J. Leffler, Robert S. Fabry
IACM Transactions on Computer Systems, Vol. 2, No. 3, August 1984, pp. 181–197.
Online copy (PDF)
Description: The file system of UNIX. One of the first papers discussing how to manage disk storage for high-performance file systems. Most file-system research since this paper has been influenced by it, and most high-performance file systems of the last 20 years incorporate techniques from this paper.


=== The Design of the UNIX Operating System ===
Maurice J. Bach, AT&T Bell Labs
Prentice Hall • 486 pp • Published 05/27/1986
This definitive description principally covered the System V Release 2 kernel, with some new features from Release 3 and BSD.


=== The Design and Implementation of a Log-Structured File System ===
Mendel Rosenblum, J. K. Ousterhout
ACM Transactions on Computer Systems, Vol. 10, No. 1 (February 1992), pp. 26–52.
Online version
Description: Log-structured file system.


=== Microkernel operating system architecture and Mach ===
David L. Black, David B. Golub, Daniel P. Julin, Richard F. Rashid, Richard P. Draves, Randall W. Dean, Alessandro Forin, Joseph Barrera, Hideyuki Tokuda, Gerald Malan, David Bohman
Proceedings of the USENIX Workshop on Microkernels and Other Kernel Architectures, pages 11–30, April 1992.
Description: This is a good paper discussing one particular microkernel architecture and contrasting it with monolithic kernel design. Mach underlies Mac OS X, and its layered architecture had a significant impact on the design of the Windows NT kernel and modern microkernels like L4. In addition, its memory-mapped files feature was added to many monolithic kernels.


=== An Implementation of a Log-Structured File System for UNIX ===
Margo Seltzer, Keith Bostic, Marshall Kirk McKusick, Carl Staelin
Proceedings of the Winter 1993 USENIX Conference, San Diego, CA, January 1993, 307-326
Online version
Description: The paper was the first production-quality implementation of that idea which spawned much additional discussion of the viability and short-comings of log-structured filesystems. While ""The Design and Implementation of a Log-Structured File System"" was certainly the first, this one was important in bringing the research idea to a usable system.


=== Soft Updates: A Solution to the Metadata Update problem in File Systems ===
G. Ganger, M. McKusick, C. Soules, Y. Patt
ACM Transactions on Computer Systems 18, 2. pp 127–153, May 2000
Online version
Description: A new way of maintaining filesystem consistency.


== Programming languages ==


=== The FORTRAN Automatic Coding System ===
John Backus et al.
Proceedings of the WJCC (Western Joint Computer Conference), Los Angeles, California, February 1957.
Online version(PDF)
Description: This paper describes the design and implementation of the first FORTRAN compiler by the IBM team. Fortran is a general-purpose, procedural, imperative programming language that is especially suited to numeric computation and scientific computing.


=== Recursive functions of symbolic expressions and their computation by machine, part I ===
John McCarthy.
Communications of the ACM, 3(4):184–195, April 1960.
Several online versions
Description: This paper introduced LISP, the first functional programming language, which was used heavily in many areas of computer science, especially in AI. LISP also has powerful features for manipulating LISP programs within the language.


=== ALGOL 60 ===
Revised Report on the Algorithmic Language Algol 60 by Peter Naur, et al. – The very influential ALGOL definition; with the first formally defined syntax.
Brian Randell and L. J. Russell, ALGOL 60 Implementation: The Translation and Use of ALGOL 60 Programs on a Computer. Academic Press, 1964. The design of the Whetstone Compiler. One of the early published descriptions of implementing a compiler. See the related papers: Whetstone Algol Revisited, and The Whetstone KDF9 Algol Translator by Brian Randell
Edsger W. Dijkstra, Algol 60 translation: an Algol 60 translator for the x1 and making a translator for Algol 60, report MR 35/61. Mathematisch Centrum, Amsterdam, 1961.
Description: Algol 60 introduced block structure.


=== The next 700 programming languages ===
Peter Landin
Communications of the ACM 9(3):157–65, March 1966
Description: This seminal paper proposed an ideal language ISWIM, which without being ever implemented influenced the whole later development.


=== Fundamental Concepts in Programming Languages ===
Christopher Strachey
pdf
Description: Fundamental Concepts in Programming Languages introduced much programming language terminology still in use today, including R-values, L-values, parametric polymorphism, and ad hoc polymorphism.


=== Lambda Papers ===
Gerald Jay Sussman and Guy L. Steele, Jr.
AI Memos, 1975–1980
Links to pdf's
Description: This series of papers and reports first defined the influential Scheme programming language and questioned the prevailing practices in programming language design, employing lambda calculus extensively to model programming language concepts and guide efficient implementation without sacrificing expressive power.


=== Structure and Interpretation of Computer Programs ===
Harold Abelson and Gerald Jay Sussman
MIT Press, 1984, 1996
Description: This textbook explains core computer programming concepts, and is widely considered a classic text in computer science.
Online course


=== Comprehending Monads ===
Philip Wadler
Mathematical structures in computer science 2.04 (1992): 461-493.
Online copy
Description: This paper introduced monads to functional programming.


=== Towards a Theory of Type Structure ===
John Reynolds
Programming Symposium. Springer Berlin Heidelberg, 1974.
online copy
Description: This paper introduced System F and created the modern notion of Parametric polymorphism


=== An axiomatic basis for computer programming ===
Tony Hoare
Communications of the ACM, Volume 12 Issue 10, Oct. 1969, Pages 576-580
Description: This paper introduce Hoare logic, which forms the foundation of program verification


== Scientific computing ==

Wilkinson, J. H.; Reinsch, C. (1971). Linear algebra, volume II of Handbook for Automatic Computation. Springer. ISBN 978-0-387-05414-8. 
Golub, Gene H.; van Loan, Charles F. (1996) [1983], Matrix Computations, 3rd edition, Johns Hopkins University Press;, ISBN 978-0-8018-5414-9 


=== Computational linguistics ===
Booth, T. L. (1969). ""Probabilistic representation of formal languages"". IEEE Conference Record of the 1969 Tenth Annual Symposium on Switching and Automata Theory. pp. 74–81. 
Contains the first presentation of stochastic context-free grammars.
Koskenniemi, Kimmo (1983), Two-level morphology: A general computational model of word-form recognition and production (PDF), Department of General Linguistics, University of Helsinki 
The first published description of computational morphology using finite state transducers. (Kaplan and Kay had previously done work in this field and presented this at a conference; the linguist Johnson had remarked the possibility in 1972, but not produced any implementation.)
Rabiner, Lawrence R. (1989). ""A tutorial on hidden Markov models and selected applications in speech recognition"". Proceedings of the IEEE. 77 (2): 257–286. doi:10.1109/5.18626. 
An overview of hidden Markov models geared toward speech recognition and other NLP fields, describing the Viterbi and forward-backward algorithms.
Brill, Eric (1995). ""Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging"". Computational Linguistics. 21 (4): 543–566. 
Describes a now commonly used POS tagger based on transformation-based learning.
Manning, Christopher D.; Schütze, Hinrich (1999), Foundation of Statistical Natural Language Processing, MIT Press 
Textbook on statistical and probabilistic methods in NLP.
Frost, Richard A. (2006). ""Realization of Natural-Language Interfaces Using Lazy Functional Programming"" (PDF). ACM Computing Surveys. 38 (4). 
This survey documents relatively less researched importance of lazy functional programming languages (i.e. Haskell) to construct Natural Language Processors and to accommodated many linguistic theories.


== Software engineering ==


=== Software engineering: Report of a conference sponsored by the NATO Science Committee ===
Peter Naur, Brian Randell (eds.)
Garmisch, Germany, 7–11 October 1968, Brussels, Scientific Affairs Division, NATO (1969) 231pp.
Online copy (PDF)
Description: Conference of leading people in software field c. 1968
The paper defined the field of Software engineering


=== A Description of the Model-View-Controller User Interface Paradigm in the Smalltalk-80 System ===
Krasner, Glenn E.; Pope, Stephen T.
The Journal of Object Technology, Aug-Sep 1988
Online copy (PDF)
Description: A description of the system that originated the (now dominant) GUI programming paradigm of Model–view–controller


=== Go To Statement Considered Harmful ===
Dijkstra, E. W.
Communications of the ACM, 11(3):147–148, March 1968
Online copy
Description: Don't use goto – the beginning of structured programming.


=== On the criteria to be used in decomposing systems into modules ===
David Parnas
Communications of the ACM, Volume 15, Issue 12:1053–1058, December 1972.
Online copy (PDF)
Description: The importance of modularization and information hiding. Note that information hiding was first presented in a different paper of the same author – ""Information Distributions Aspects of Design Methodology"", Proceedings of IFIP Congress '71, 1971, Booklet TA-3, pp. 26–30


=== Hierarchical Program Structures ===
Ole-Johan Dahl, C. A. R. Hoare
in Dahl, Dijkstra and Hoare, Structured Programming, Academic Press, London and New York, pp. 175–220, 1972.
Description: The beginning of Object-oriented programming. This paper argued that programs should be decomposed to independent components with small and simple interfaces. They also argued that objects should have both data and related methods.


=== A technique for software module specification with examples ===
David Parnas
Comm. ACM 15, 5 (May 1972), 330–336.
Online copy (PDF)
Description: software specification.


=== Structured Design ===
Wayne Stevens, Glenford Myers, and Larry Constantine
IBM Systems Journal, 13 (2), 115–139, 1974.
On-line copy (PDF)
Description: Seminal paper on Structured Design, data flow diagram, coupling, and cohesion.


=== The Emperor's Old Clothes ===
C.A.R. Hoare
Communications of the ACM, Vol. 24, No. 2, February 1981, pp. 75–83.
Archived copy (PDF)
Description: Illustrates the ""second-system effect"" and the importance of simplicity.


=== The Mythical Man-Month: Essays on Software Engineering ===
Brooks, Jr., F. P.
Addison Wesley Professional. 2nd edition, 1995.
Description: Throwing more people at the task will not speed its completion...


=== No Silver Bullet: Essence and Accidents of Software Engineering ===
Brooks, Frederick. P., Jr. (April 1987). ""No Silver Bullet: Essence and Accidents of Software Engineering"". Computer. 20 (4): 10–19. doi:10.1109/MC.1987.1663532. 


=== The Cathedral and the Bazaar ===
Raymond, E.S.
First Monday, 3, 3 (March 1998)
Online copy (HTML)
Description: Open source methodology.


=== Design Patterns: Elements of Reusable Object Oriented Software ===
E. Gamma, R. Helm, R. Johnson, J. Vlissides
Addison–Wesley, Reading, Massachusetts, 1995.
Description: This book was the first to define and list design patterns in computer science.


=== Statecharts: A Visual Formalism For Complex Systems ===
David Harel
D. Harel. Statecharts: A visual formalism for complex systems. Science of Computer Programming, 8:231—274, 1987
Online version
Description: Statecharts are a visual modeling method. They are an extension of state machine that might be exponentially more efficient. Therefore, statcharts enable formal modeling of applications that were too complex before. Statecharts are part of the UML diagrams.


== Security ==


=== Anonymity Systems ===
David Chaum. Untraceable electronic mail, return addresses, and digital pseudonyms. Communications of the ACM, 4(2):84–88, February 1981.
Dingledine and Mathewson, Anonymity Loves Company: Usability and the Network Effect, Workshop on the Economics of Information Security (WEIS) 2006


=== Cryptography ===

Whitfield Diffie and Martin E. Hellman, New Directions in Cryptography, IEEE Transactions on Information Theory, November 1976
R. L. Rivest and A. Shamir and L. M. Adelman, A Method For Obtaining Digital Signatures And Public-Key Cryptosystems, MIT/LCS/TM-82, 1977
Merkle, R. Security, Authentication, and Public Key Systems, PhD Thesis, 1979 Stanford University. (Just read chapter 2, pages 11–15, in which Merkle invents cryptographic hash functions.)


=== Passwords ===
Morris, Robert and Thompson, Ken. Password security: a case history, Communications of the ACM CACM Homepage archive Volume 22 Issue 11, Nov. 1979 Pages 594-597. PDF
Mazurek et al., Measuring password guessability for an entire university, CCS '13 Proceedings of the 2013 ACM SIGSAC conference on Computer & communications security, Pages 173-186


=== System Security ===
Saltzer and Schroeder, The Protection of Information in Computer Systems, ACM Symposium on Operating System Principles (October 1973) HTML HTML2
Karger and Schell, Thirty Years later: Lessons from the Multics Security Evaluation, ACSAC 2002
Lamport, Butler. A Note on the Confinement Problem, Communications of the ACM, 16:10 (Oct. 1973), pp. 613–615. PDF
Thompson, Reflections on Trusting Trust, Communications of the ACM, 27:8, Aug 1984
J.E. Forrester and B.P. Miller, An Empirical Study of the Robustness of Windows NT Applications Using Random Testing, 4th USENIX Windows Systems Symposium, Seattle, August 2000.
Zissis D and Lekkas D, Addressing cloud computing security issues, Future Generation Computer Systems, 28/3, pp. 583-592, Elsevier 2012


=== Usable Security ===
Whitten, Alma, Why Johnny Can't Encrypt: A Usability Evaluation of PGP 5.0, Proceedings of the 8th conference on USENIX Security Symposium, Volume 8, Pages 14–28
Garfinkel, Simson and Shelat, Abhi, Remembrance of Data Passed, IEEE Security and Privacy, Volume 1 Issue 1, January 2003, Page 17-27


== Theoretical computer science ==

Topics covered: theoretical computer science, including computability theory, computational complexity theory, algorithms, algorithmic information theory, information theory and formal verification.


== See also ==
DBLP (Digital Bibliography & Library Project in computer science)
List of open problems in computer science
List of computer science journals
List of computer science conferences
The Collection of Computer Science Bibliographies
Paris Kanellakis Award, a prize given to honor specific theoretical accomplishments that have had a significant and demonstrable effect on the practice of computing.


== References ==


== External links ==
ACM Classic Books Series
Most cited articles in Computer Science (CiteSeer Database)
50 most influential papers ACM SIGPLAN papers published in PLDI from 1979 through 1999; organized into a special SIGPLAN proceedings.


=== Academic Search Engines ===
Google Scholar
CiteSeer
Live Academic
Odysci
ISI Web of Science"
7,Church–Turing thesis,6854,52072,"In computability theory, the Church–Turing thesis (also known as computability thesis, the Turing–Church thesis, the Church–Turing conjecture, Church's thesis, Church's conjecture, and Turing's thesis) is a hypothesis about the nature of computable functions. It states that a function on the natural numbers is computable by a human being following an algorithm, ignoring resource limitations, if and only if it is computable by a Turing machine. The thesis is named after American mathematician Alonzo Church and the British mathematician Alan Turing. Before the precise definition of computable function, mathematicians often used the informal term effectively calculable to describe functions that are computable by paper-and-pencil methods. In the 1930s, several independent attempts were made to formalize the notion of computability:
In 1933, Austrian-American mathematician Kurt Gödel, with Jacques Herbrand, created a formal definition of a class called general recursive functions. The class of general recursive functions is the smallest class of functions (possibly with more than one argument) which includes all constant functions, projections, the successor function, and which is closed under function composition, recursion, and minimization.
In 1936, Alonzo Church created a method for defining functions called the λ-calculus. Within λ-calculus, he defined an encoding of the natural numbers called the Church numerals. A function on the natural numbers is called λ-computable if the corresponding function on the Church numerals can be represented by a term of the λ-calculus.
Also in 1936, before learning of Church's work, Alan Turing created a theoretical model for machines, now called Turing machines, that could carry out calculations from inputs by manipulating symbols on a tape. Given a suitable encoding of the natural numbers as sequences of symbols, a function on the natural numbers is called Turing computable if some Turing machine computes the corresponding function on encoded natural numbers.
Church and Turing proved that these three formally defined classes of computable functions coincide: a function is λ-computable if and only if it is Turing computable if and only if it is general recursive. This has led mathematicians and computer scientists to believe that the concept of computability is accurately characterized by these three equivalent processes. Other formal attempts to characterize computability have subsequently strengthened this belief (see below).
On the other hand, the Church–Turing thesis states that the above three formally-defined classes of computable functions coincide with the informal notion of an effectively calculable function. Since, as an informal notion, the concept of effective calculability does not have a formal definition, the thesis, although it has near-universal acceptance, cannot be formally proven.
Since its inception, variations on the original thesis have arisen, including statements about what can physically be realized by a computer in our universe (physical Church-Turing thesis) and what can be efficiently computed (Church–Turing thesis (complexity theory)). These variations are not due to Church or Turing, but arise from later work in complexity theory and digital physics. The thesis also has implications for the philosophy of mind (see below).


== Statement in Church's and Turing's words ==

J.B. Rosser (1939) addresses the notion of ""effective computability"" as follows: ""Clearly the existence of CC and RC (Church's and Rosser's proofs) presupposes a precise definition of 'effective'. 'Effective method' is here used in the rather special sense of a method each step of which is precisely predetermined and which is certain to produce the answer in a finite number of steps"". Thus the adverb-adjective ""effective"" is used in a sense of ""1a: producing a decided, decisive, or desired effect"", and ""capable of producing a result"".
In the following, the words ""effectively calculable"" will mean ""produced by any intuitively 'effective' means whatsoever"" and ""effectively computable"" will mean ""produced by a Turing-machine or equivalent mechanical device"". Turing's ""definitions"" given in a footnote in his 1939 Ph.D. thesis Systems of Logic Based on Ordinals, supervised by Church, are virtually the same:

† We shall use the expression “computable function” to mean a function calculable by a machine, and let “effectively calculable” refer to the intuitive idea without particular identification with any one of these definitions.

The thesis can be stated as: Every effectively calculable function is a computable function.
Turing stated it this way:

It was stated… that “a function is effectively calculable if its values can be found by some purely mechanical process.” We may take this literally, understanding that by a purely mechanical process one which could be carried out by a machine. The development… leads to… an identification of computability† with effective calculability. [† is the footnote quoted above.]


== History ==

One of the important problems for logicians in the 1930s was David Hilbert's Entscheidungsproblem, which asked whether there was a mechanical procedure for separating mathematical truths from mathematical falsehoods. This quest required that the notion of ""algorithm"" or ""effective calculability"" be pinned down, at least well enough for the quest to begin. But from the very outset Alonzo Church's attempts began with a debate that continues to this day. Was the notion of ""effective calculability"" to be (i) an ""axiom or axioms"" in an axiomatic system, or (ii) merely a definition that ""identified"" two or more propositions, or (iii) an empirical hypothesis to be verified by observation of natural events, or (iv) or just a proposal for the sake of argument (i.e. a ""thesis"").


=== Circa 1930–1952 ===
In the course of studying the problem, Church and his student Stephen Kleene introduced the notion of λ-definable functions, and they were able to prove that several large classes of functions frequently encountered in number theory were λ-definable. The debate began when Church proposed to Gödel that one should define the ""effectively computable"" functions as the λ-definable functions. Gödel, however, was not convinced and called the proposal ""thoroughly unsatisfactory"". Rather, in correspondence with Church (ca 1934–5), Gödel proposed axiomatizing the notion of ""effective calculability""; indeed, in a 1935 letter to Kleene, Church reported that:
""His [Gödel's] only idea at the time was that it might be possible, in terms of effective calculability as an undefined notion, to state a set of axioms which would embody the generally accepted properties of this notion, and to do something on that basis"".
But Gödel offered no further guidance. Eventually, he would suggest his recursion, modified by Herbrand's suggestion, that Gödel had detailed in his 1934 lectures in Princeton NJ (Kleene and Rosser transcribed the notes). But he did not think that the two ideas could be satisfactorily identified ""except heuristically"".
Next, it was necessary to identify and prove the equivalence of two notions of effective calculability. Equipped with the λ-calculus and ""general"" recursion, Stephen Kleene with help of Church and J. Barkley Rosser produced proofs (1933, 1935) to show that the two calculi are equivalent. Church subsequently modified his methods to include use of Herbrand–Gödel recursion and then proved (1936) that the Entscheidungsproblem is unsolvable: there is no generalized algorithm that can determine whether a well formed formula has a ""normal form"".
Many years later in a letter to Davis (ca 1965), Gödel said that ""he was, at the time of these [1934] lectures, not at all convinced that his concept of recursion comprised all possible recursions"". By 1963–4 Gödel would disavow Herbrand–Gödel recursion and the λ-calculus in favor of the Turing machine as the definition of ""algorithm"" or ""mechanical procedure"" or ""formal system"".
A hypothesis leading to a natural law?: In late 1936 Alan Turing's paper (also proving that the Entscheidungsproblem is unsolvable) was delivered orally, but had not yet appeared in print. On the other hand, Emil Post's 1936 paper had appeared and was certified independent of Turing's work. Post strongly disagreed with Church's ""identification"" of effective computability with the λ-calculus and recursion, stating:
""Actually the work already done by Church and others carries this identification considerably beyond the working hypothesis stage. But to mask this identification under a definition… blinds us to the need of its continual verification.""
Rather, he regarded the notion of ""effective calculability"" as merely a ""working hypothesis"" that might lead by inductive reasoning to a ""natural law"" rather than by ""a definition or an axiom"". This idea was ""sharply"" criticized by Church.
Thus Post in his 1936 paper was also discounting Kurt Gödel's suggestion to Church in 1934–5 that the thesis might be expressed as an axiom or set of axioms.
Turing adds another definition, Rosser equates all three: Within just a short time, Turing's 1936–37 paper ""On Computable Numbers, with an Application to the Entscheidungsproblem"" appeared. In it he stated another notion of ""effective computability"" with the introduction of his a-machines (now known as the Turing machine abstract computational model). And in a proof-sketch added as an ""Appendix"" to his 1936–37 paper, Turing showed that the classes of functions defined by λ-calculus and Turing machines coincided. Church was quick to recognise how compelling Turing's analysis was. In his review of Turing's paper he made clear that Turing's notion made ""the identification with effectiveness in the ordinary (not explicitly defined) sense evident immediately"".
In a few years (1939) Turing would propose, like Church and Kleene before him, that his formal definition of mechanical computing agent was the correct one. Thus, by 1939, both Church (1934) and Turing (1939) had individually proposed that their ""formal systems"" should be definitions of ""effective calculability""; neither framed their statements as theses.
Rosser (1939) formally identified the three notions-as-definitions:
""All three definitions are equivalent, so it does not matter which one is used.""
Kleene proposes Church's Thesis: This left the overt expression of a ""thesis"" to Kleene. In his 1943 paper Recursive Predicates and Quantifiers Kleene proposed his ""THESIS I"":
""This heuristic fact [general recursive functions are effectively calculable] …led Church to state the following thesis(22). The same thesis is implicit in Turing's description of computing machines(23).
""THESIS I. Every effectively calculable function (effectively decidable predicate) is general recursive [Kleene's italics]

""Since a precise mathematical definition of the term effectively calculable (effectively decidable) has been wanting, we can take this thesis… as a definition of it…""""(22) references Church 1936
""(23) references Turing 1936–7

Kleene goes on to note that:
""…the thesis has the character of an hypothesis—a point emphasized by Post and by Church(24). If we consider the thesis and its converse as definition, then the hypothesis is an hypothesis about the application of the mathematical theory developed from the definition. For the acceptance of the hypothesis, there are, as we have suggested, quite compelling grounds.""
""(24) references Post 1936 of Post and Church's Formal definitions in the theory of ordinal numbers, Fund. Math. vol 28 (1936) pp.11–21 (see ref. #2, Davis 1965:286).

Kleene's Church–Turing Thesis: A few years later (1952) Kleene, who switched from presenting his work in the mathematical terminology of the lambda calculus of his phd advisor Alonzo Church to the theory of general recursive functions of his other teacher Kurt Gödel, would overtly name the Church–Turing thesis in his correction of Turing's paper ""The Word Problem in Semi-Groups with Cancellation"", defend, and express the two ""theses"" and then ""identify"" them (show equivalence) by use of his Theorem XXX:
""Heuristic evidence and other considerations led Church 1936 to propose the following thesis.
Thesis I. Every effectively calculable function (effectively decidable predicate) is general recursive.

Theorem XXX: ""The following classes of partial functions are coextensive, i.e. have the same members: (a) the partial recursive functions, (b) the computable functions…"".
Turing's thesis: ""Turing's thesis that every function which would naturally be regarded as computable is computable under his definition, i.e. by one of his machines, is equivalent to Church's thesis by Theorem XXX.""
Kleene himself never stated that Turing had made a mistake in his paper, important in its own right for helping to establish the unsolvability of problems in group theoretic computations, although corrections to Turing's paper were also made later by Boone who originally pointed out ""points in the proof require clarification, which can be given"" and Turing's only PhD student, Robin Gandy. That Kleene doesn't mention this mistake in the body of his textbook where his presents his work on Turing machines but buried the fact he was correcting Alan Turing in the appendix was appreciated by Turing himself can be surmised from the ending of Turing's last publication ""Solvable and Unsolvable Problems"" which ends not with a bibliography but the words,


=== Later developments ===
An attempt to understand the notion of ""effective computability"" better led Robin Gandy (Turing's student and friend) in 1980 to analyze machine computation (as opposed to human-computation acted out by a Turing machine). Gandy's curiosity about, and analysis of, ""cellular automata"", ""Conway's game of life"", ""parallelism"" and ""crystalline automata"" led him to propose four ""principles (or constraints) ... which it is argued, any machine must satisfy."" His most-important fourth, ""the principle of causality"" is based on the ""finite velocity of propagation of effects and signals; contemporary physics rejects the possibility of instantaneous action at a distance."" From these principles and some additional constraints—(1a) a lower bound on the linear dimensions of any of the parts, (1b) an upper bound on speed of propagation (the velocity of light), (2) discrete progress of the machine, and (3) deterministic behavior—he produces a theorem that ""What can be calculated by a device satisfying principles I–IV is computable.""
In the late 1990s Wilfried Sieg analyzed Turing's and Gandy's notions of ""effective calculability"" with the intent of ""sharpening the informal notion, formulating its general features axiomatically, and investigating the axiomatic framework"". In his 1997 and 2002 work Sieg presents a series of constraints on the behavior of a computor—""a human computing agent who proceeds mechanically"". These constraints reduce to:
""(B.1) (Boundedness) There is a fixed bound on the number of symbolic configurations a computor can immediately recognize.
""(B.2) (Boundedness) There is a fixed bound on the number of internal states a computor can be in.
""(L.1) (Locality) A computor can change only elements of an observed symbolic configuration.
""(L.2) (Locality) A computor can shift attention from one symbolic configuration to another one, but the new observed configurations must be within a bounded distance of the immediately previously observed configuration.
""(D) (Determinacy) The immediately recognizable (sub-)configuration determines uniquely the next computation step (and id [instantaneous description] )""; stated another way: ""A computor's internal state together with the observed configuration fixes uniquely the next computation step and the next internal state.""
The matter remains in active discussion within the academic community.


=== The thesis as a definition ===
The thesis can be viewed as nothing but an ordinary mathematical definition. Comments by Gödel on the subject suggest this view, e.g. “…the correct definition of mechanical computability was established beyond any doubt by Turing.” The case for viewing the thesis as nothing more than a definition is made explicitly by Robert I. Soare, where it is also argued that Turing's definition of computability is no less likely to be correct than the epsilon-delta definition of a continuous function.


== Success of the thesis ==
Other formalisms (besides recursion, the λ-calculus, and the Turing machine) have been proposed for describing effective calculability/computability. Stephen Kleene (1952) adds to the list the functions ""reckonable in the system S1"" of Kurt Gödel 1936, and Emil Post's (1943, 1946) ""canonical [also called normal] systems"". In the 1950s Hao Wang and Martin Davis greatly simplified the one-tape Turing-machine model (see Post–Turing machine). Marvin Minsky expanded the model to two or more tapes and greatly simplified the tapes into ""up-down counters"", which Melzak and Lambek further evolved into what is now known as the counter machine model. In the late 1960s and early 1970s researchers expanded the counter machine model into the register machine, a close cousin to the modern notion of the computer. Other models include combinatory logic and Markov algorithms. Gurevich adds the pointer machine model of Kolmogorov and Uspensky (1953, 1958): ""... they just wanted to ... convince themselves that there is no way to extend the notion of computable function.""
All these contributions involve proofs that the models are computationally equivalent to the Turing machine; such models are said to be Turing complete. Because all these different attempts at formalizing the concept of ""effective calculability/computability"" have yielded equivalent results, it is now generally assumed that the Church–Turing thesis is correct. In fact, Gödel (1936) proposed something stronger than this; he observed that there was something ""absolute"" about the concept of ""reckonable in S1"":
""It may also be shown that a function which is computable ['reckonable'] in one of the systems Si, or even in a system of transfinite type, is already computable [reckonable] in S1. Thus the concept 'computable' ['reckonable'] is in a certain definite sense 'absolute', while practically all other familiar metamathematical concepts (e.g. provable, definable, etc.) depend quite essentially on the system to which they are defined""


== Informal usage in proofs ==
Proofs in computability theory often invoke the Church–Turing thesis in an informal way to establish the computability of functions while avoiding the (often very long) details which would be involved in a rigorous, formal proof. To establish that a function is computable by Turing machine, it is usually considered sufficient to give an informal English description of how the function can be effectively computed, and then conclude ""by the Church–Turing thesis"" that the function is Turing computable (equivalently, partial recursive).
Dirk van Dalen gives the following example for the sake of illustrating this informal use of the Church–Turing thesis:
EXAMPLE: Each infinite RE set contains an infinite recursive set.
Proof: Let A be infinite RE. We list the elements of A effectively, n0, n1, n2, n3, ...
From this list we extract an increasing sublist: put m0=n0, after finitely many steps we find an nk such that nk > m0, put m1=nk. We repeat this procedure to find m2 > m1, etc. this yields an effective listing of the subset B={m0,m1,m2,...} of A, with the property mi < mi+1.
Claim. B is decidable. For, in order to test k in B we must check if k=mi for some i. Since the sequence of mi's is increasing we have to produce at most k+1 elements of the list and compare them with k. If none of them is equal to k, then k not in B. Since this test is effective, B is decidable and, by Church's thesis, recursive.
In order to make the above example completely rigorous, one would have to carefully construct a Turing Machine, or λ-function, or carefully invoke recursion axioms, or at best, cleverly invoke various theorems of computability theory. But because the computability theorist believes that Turing computability correctly captures what can be computed effectively, and because an effective procedure is spelled out in English for deciding the set B, the computability theorist accepts this as proof that the set is indeed recursive.


== Variations ==
The success of the Church–Turing thesis prompted variations of the thesis to be proposed. For example, the Physical Church–Turing thesis (PCTT) states:

All physically computable functions are Turing-computable.

The Church–Turing thesis says nothing about the efficiency with which one model of computation can simulate another. It has been proved for instance that a (multi-tape) universal Turing machine only suffers a logarithmic slowdown factor in simulating any Turing machine. A variation of the Church–Turing thesis addresses whether an arbitrary but ""reasonable"" model of computation can be efficiently simulated. This is called the Feasibility Thesis, also known as the (Classical) Complexity-Theoretic Church–Turing Thesis (SCTT) or the Extended Church–Turing Thesis, which is not due to Church or Turing, but rather was realized gradually in the development of complexity theory. It states:

A probabilistic Turing machine can efficiently simulate any realistic model of computation.

The word 'efficiently' here means up to polynomial-time reductions. This thesis was originally called Computational Complexity-Theoretic Church–Turing Thesis by Ethan Bernstein and Umesh Vazirani (1997). The Complexity-Theoretic Church–Turing Thesis, then, posits that all 'reasonable' models of computation yield the same class of problems that can be computed in polynomial time. Assuming the conjecture that probabilistic polynomial time (BPP) equals deterministic polynomial time (P), the word 'probabilistic' is optional in the Complexity-Theoretic Church–Turing Thesis. A similar thesis, called the Invariance Thesis, was introduced by Cees F. Slot and Peter van Emde Boas. It states: ""Reasonable"" machines can simulate each other within a polynomially bounded overhead in time and a constant-factor overhead in space. The thesis originally appeared in a paper at STOC'84, which was the first paper to show that polynomial-time overhead and constant-space overhead could be simultaneously achieved for a simulation of a Random Access Machine on a Turing machine.
If BQP is shown to be a strict superset of BPP, it would invalidate the Complexity-Theoretic Church–Turing Thesis. In other words, there would be efficient quantum algorithms that perform tasks that do not have efficient probabilistic algorithms. This would not however invalidate the original Church–Turing thesis, since a quantum computer can always be simulated by a Turing machine, but it would invalidate the classical Complexity-Theoretic Church–Turing thesis for efficiency reasons. Consequently, the Quantum Complexity-Theoretic Church–Turing thesis states:

A quantum Turing machine can efficiently simulate any realistic model of computation.

Eugene Eberbach and Peter Wegner claim that the Church–Turing thesis is sometimes interpreted too broadly, stating ""the broader assertion that algorithms precisely capture what can be computed is invalid."" They claim that forms of computation not captured by the thesis are relevant today, terms which they call super-Turing computation.


== Philosophical implications ==
Philosophers have interpreted the Church–Turing thesis as having implications for the philosophy of mind. B. Jack Copeland states that it's an open empirical question whether there are actual deterministic physical processes that, in the long run, elude simulation by a Turing machine; furthermore, he states that it is an open empirical question whether any such processes are involved in the working of the human brain. There are also some important open questions which cover the relationship between the Church–Turing thesis and physics, and the possibility of hypercomputation. When applied to physics, the thesis has several possible meanings:
The universe is equivalent to a Turing machine; thus, computing non-recursive functions is physically impossible. This has been termed the Strong Church–Turing thesis, or Church–Turing–Deutsch principle, and is a foundation of digital physics.
The universe is not equivalent to a Turing machine (i.e., the laws of physics are not Turing-computable), but incomputable physical events are not ""harnessable"" for the construction of a hypercomputer. For example, a universe in which physics involves random real numbers, as opposed to computable reals, would fall into this category.
The universe is a hypercomputer, and it is possible to build physical devices to harness this property and calculate non-recursive functions. For example, it is an open question whether all quantum mechanical events are Turing-computable, although it is known that rigorous models such as quantum Turing machines are equivalent to deterministic Turing machines. (They are not necessarily efficiently equivalent; see above.) John Lucas and Roger Penrose have suggested that the human mind might be the result of some kind of quantum-mechanically enhanced, ""non-algorithmic"" computation.
There are many other technical possibilities which fall outside or between these three categories, but these serve to illustrate the range of the concept.


== Non-computable functions ==
One can formally define functions that are not computable. A well-known example of such a function is the Busy Beaver function. This function takes an input n and returns the largest number of symbols that a Turing machine with n states can print before halting, when run with no input. Finding an upper bound on the busy beaver function is equivalent to solving the halting problem, a problem known to be unsolvable by Turing machines. Since the busy beaver function cannot be computed by Turing machines, the Church–Turing thesis states that this function cannot be effectively computed by any method.
Several computational models allow for the computation of (Church-Turing) non-computable functions. These are known as hypercomputers. Mark Burgin argues that super-recursive algorithms such as inductive Turing machines disprove the Church–Turing thesis. His argument relies on a definition of algorithm broader than the ordinary one, so that non-computable functions obtained from some inductive Turing machines are called computable. This interpretation of the Church–Turing thesis differs from the interpretation commonly accepted in computability theory, discussed above. The argument that super-recursive algorithms are indeed algorithms in the sense of the Church–Turing thesis has not found broad acceptance within the computability research community.


== See also ==
Abstract machine
Church's thesis in constructive mathematics
Church–Turing–Deutsch principle, which states that every physical process can be simulated by a universal computing device
Computability logic
Computability theory
Decidability
Hypercomputer
Model of computation
Oracle (computer science)
Super-recursive algorithm


== Footnotes ==


== References ==


== External links ==
The Church–Turing Thesis entry by B. Jack Copeland in the Stanford Encyclopedia of Philosophy.
Computation in Physical Systems entry by Gualtiero Piccinini in the Stanford Encyclopedia of Philosophy—a comprehensive philosophical treatment of relevant issues.
Kaznatcheev, Artem (September 11, 2014). ""Transcendental idealism and Post's variant of the Church-Turing thesis"". Theory, Evolution, and Games Group."
8,Computational complexity theory,7543,46919,"Computational complexity theory is a branch of the theory of computation in theoretical computer science that focuses on classifying computational problems according to their inherent difficulty, and relating those classes to each other. A computational problem is understood to be a task that is in principle amenable to being solved by a computer, which is equivalent to stating that the problem may be solved by mechanical application of mathematical steps, such as an algorithm.
A problem is regarded as inherently difficult if its solution requires significant resources, whatever the algorithm used. The theory formalizes this intuition, by introducing mathematical models of computation to study these problems and quantifying the amount of resources needed to solve them, such as time and storage. Other complexity measures are also used, such as the amount of communication (used in communication complexity), the number of gates in a circuit (used in circuit complexity) and the number of processors (used in parallel computing). One of the roles of computational complexity theory is to determine the practical limits on what computers can and cannot do.
Closely related fields in theoretical computer science are analysis of algorithms and computability theory. A key distinction between analysis of algorithms and computational complexity theory is that the former is devoted to analyzing the amount of resources needed by a particular algorithm to solve a problem, whereas the latter asks a more general question about all possible algorithms that could be used to solve the same problem. More precisely, computational complexity theory tries to classify problems that can or cannot be solved with appropriately restricted resources. In turn, imposing restrictions on the available resources is what distinguishes computational complexity from computability theory: the latter theory asks what kind of problems can, in principle, be solved algorithmically.


== Computational problems ==


=== Problem instances ===
A computational problem can be viewed as an infinite collection of instances together with a solution for every instance. The input string for a computational problem is referred to as a problem instance, and should not be confused with the problem itself. In computational complexity theory, a problem refers to the abstract question to be solved. In contrast, an instance of this problem is a rather concrete utterance, which can serve as the input for a decision problem. For example, consider the problem of primality testing. The instance is a number (e.g. 15) and the solution is ""yes"" if the number is prime and ""no"" otherwise (in this case ""no""). Stated another way, the instance is a particular input to the problem, and the solution is the output corresponding to the given input.
To further highlight the difference between a problem and an instance, consider the following instance of the decision version of the traveling salesman problem: Is there a route of at most 2000 kilometres passing through all of Germany's 15 largest cities? The quantitative answer to this particular problem instance is of little use for solving other instances of the problem, such as asking for a round trip through all sites in Milan whose total length is at most 10 km. For this reason, complexity theory addresses computational problems and not particular problem instances.


=== Representing problem instances ===
When considering computational problems, a problem instance is a string over an alphabet. Usually, the alphabet is taken to be the binary alphabet (i.e., the set {0,1}), and thus the strings are bitstrings. As in a real-world computer, mathematical objects other than bitstrings must be suitably encoded. For example, integers can be represented in binary notation, and graphs can be encoded directly via their adjacency matrices, or by encoding their adjacency lists in binary.
Even though some proofs of complexity-theoretic theorems regularly assume some concrete choice of input encoding, one tries to keep the discussion abstract enough to be independent of the choice of encoding. This can be achieved by ensuring that different representations can be transformed into each other efficiently.


=== Decision problems as formal languages ===

Decision problems are one of the central objects of study in computational complexity theory. A decision problem is a special type of computational problem whose answer is either yes or no, or alternately either 1 or 0. A decision problem can be viewed as a formal language, where the members of the language are instances whose output is yes, and the non-members are those instances whose output is no. The objective is to decide, with the aid of an algorithm, whether a given input string is a member of the formal language under consideration. If the algorithm deciding this problem returns the answer yes, the algorithm is said to accept the input string, otherwise it is said to reject the input.
An example of a decision problem is the following. The input is an arbitrary graph. The problem consists in deciding whether the given graph is connected, or not. The formal language associated with this decision problem is then the set of all connected graphs — to obtain a precise definition of this language, one has to decide how graphs are encoded as binary strings.


=== Function problems ===
A function problem is a computational problem where a single output (of a total function) is expected for every input, but the output is more complex than that of a decision problem, that is, it isn't just yes or no. Notable examples include the traveling salesman problem and the integer factorization problem.
It is tempting to think that the notion of function problems is much richer than the notion of decision problems. However, this is not really the case, since function problems can be recast as decision problems. For example, the multiplication of two integers can be expressed as the set of triples (a, b, c) such that the relation a × b = c holds. Deciding whether a given triple is a member of this set corresponds to solving the problem of multiplying two numbers.


=== Measuring the size of an instance ===
To measure the difficulty of solving a computational problem, one may wish to see how much time the best algorithm requires to solve the problem. However, the running time may, in general, depend on the instance. In particular, larger instances will require more time to solve. Thus the time required to solve a problem (or the space required, or any measure of complexity) is calculated as a function of the size of the instance. This is usually taken to be the size of the input in bits. Complexity theory is interested in how algorithms scale with an increase in the input size. For instance, in the problem of finding whether a graph is connected, how much more time does it take to solve a problem for a graph with 2n vertices compared to the time taken for a graph with n vertices?
If the input size is n, the time taken can be expressed as a function of n. Since the time taken on different inputs of the same size can be different, the worst-case time complexity T(n) is defined to be the maximum time taken over all inputs of size n. If T(n) is a polynomial in n, then the algorithm is said to be a polynomial time algorithm. Cobham's thesis says that a problem can be solved with a feasible amount of resources if it admits a polynomial time algorithm.


== Machine models and complexity measures ==


=== Turing machine ===

A Turing machine is a mathematical model of a general computing machine. It is a theoretical device that manipulates symbols contained on a strip of tape. Turing machines are not intended as a practical computing technology, but rather as a general model of a computing machine—anything from an advanced supercomputer to a mathematician with a pencil and paper. It is believed that if a problem can be solved by an algorithm, there exists a Turing machine that solves the problem. Indeed, this is the statement of the Church–Turing thesis. Furthermore, it is known that everything that can be computed on other models of computation known to us today, such as a RAM machine, Conway's Game of Life, cellular automata or any programming language can be computed on a Turing machine. Since Turing machines are easy to analyze mathematically, and are believed to be as powerful as any other model of computation, the Turing machine is the most commonly used model in complexity theory.
Many types of Turing machines are used to define complexity classes, such as deterministic Turing machines, probabilistic Turing machines, non-deterministic Turing machines, quantum Turing machines, symmetric Turing machines and alternating Turing machines. They are all equally powerful in principle, but when resources (such as time or space) are bounded, some of these may be more powerful than others.
A deterministic Turing machine is the most basic Turing machine, which uses a fixed set of rules to determine its future actions. A probabilistic Turing machine is a deterministic Turing machine with an extra supply of random bits. The ability to make probabilistic decisions often helps algorithms solve problems more efficiently. Algorithms that use random bits are called randomized algorithms. A non-deterministic Turing machine is a deterministic Turing machine with an added feature of non-determinism, which allows a Turing machine to have multiple possible future actions from a given state. One way to view non-determinism is that the Turing machine branches into many possible computational paths at each step, and if it solves the problem in any of these branches, it is said to have solved the problem. Clearly, this model is not meant to be a physically realizable model, it is just a theoretically interesting abstract machine that gives rise to particularly interesting complexity classes. For examples, see non-deterministic algorithm.


=== Other machine models ===
Many machine models different from the standard multi-tape Turing machines have been proposed in the literature, for example random access machines. Perhaps surprisingly, each of these models can be converted to another without providing any extra computational power. The time and memory consumption of these alternate models may vary. What all these models have in common is that the machines operate deterministically.
However, some computational problems are easier to analyze in terms of more unusual resources. For example, a non-deterministic Turing machine is a computational model that is allowed to branch out to check many different possibilities at once. The non-deterministic Turing machine has very little to do with how we physically want to compute algorithms, but its branching exactly captures many of the mathematical models we want to analyze, so that non-deterministic time is a very important resource in analyzing computational problems.


=== Complexity measures ===
For a precise definition of what it means to solve a problem using a given amount of time and space, a computational model such as the deterministic Turing machine is used. The time required by a deterministic Turing machine M on input x is the total number of state transitions, or steps, the machine makes before it halts and outputs the answer (""yes"" or ""no""). A Turing machine M is said to operate within time f(n), if the time required by M on each input of length n is at most f(n). A decision problem A can be solved in time f(n) if there exists a Turing machine operating in time f(n) that solves the problem. Since complexity theory is interested in classifying problems based on their difficulty, one defines sets of problems based on some criteria. For instance, the set of problems solvable within time f(n) on a deterministic Turing machine is then denoted by DTIME(f(n)).
Analogous definitions can be made for space requirements. Although time and space are the most well-known complexity resources, any complexity measure can be viewed as a computational resource. Complexity measures are very generally defined by the Blum complexity axioms. Other complexity measures used in complexity theory include communication complexity, circuit complexity, and decision tree complexity.
The complexity of an algorithm is often expressed using big O notation.


=== Best, worst and average case complexity ===

The best, worst and average case complexity refer to three different ways of measuring the time complexity (or any other complexity measure) of different inputs of the same size. Since some inputs of size n may be faster to solve than others, we define the following complexities:
Best-case complexity: This is the complexity of solving the problem for the best input of size n.
Worst-case complexity: This is the complexity of solving the problem for the worst input of size n.
Average-case complexity: This is the complexity of solving the problem on an average. This complexity is only defined with respect to a probability distribution over the inputs. For instance, if all inputs of the same size are assumed to be equally likely to appear, the average case complexity can be defined with respect to the uniform distribution over all inputs of size n.
For example, consider the deterministic sorting algorithm quicksort. This solves the problem of sorting a list of integers that is given as the input. The worst-case is when the input is sorted or sorted in reverse order, and the algorithm takes time O(n2) for this case. If we assume that all possible permutations of the input list are equally likely, the average time taken for sorting is O(n log n). The best case occurs when each pivoting divides the list in half, also needing O(n log n) time.


=== Upper and lower bounds on the complexity of problems ===
To classify the computation time (or similar resources, such as space consumption), one is interested in proving upper and lower bounds on the maximum amount of time required by the most efficient algorithm solving a given problem. The complexity of an algorithm is usually taken to be its worst-case complexity, unless specified otherwise. Analyzing a particular algorithm falls under the field of analysis of algorithms. To show an upper bound T(n) on the time complexity of a problem, one needs to show only that there is a particular algorithm with running time at most T(n). However, proving lower bounds is much more difficult, since lower bounds make a statement about all possible algorithms that solve a given problem. The phrase ""all possible algorithms"" includes not just the algorithms known today, but any algorithm that might be discovered in the future. To show a lower bound of T(n) for a problem requires showing that no algorithm can have time complexity lower than T(n).
Upper and lower bounds are usually stated using the big O notation, which hides constant factors and smaller terms. This makes the bounds independent of the specific details of the computational model used. For instance, if T(n) = 7n2 + 15n + 40, in big O notation one would write T(n) = O(n2).


== Complexity classes ==


=== Defining complexity classes ===
A complexity class is a set of problems of related complexity. Simpler complexity classes are defined by the following factors:
The type of computational problem: The most commonly used problems are decision problems. However, complexity classes can be defined based on function problems, counting problems, optimization problems, promise problems, etc.
The model of computation: The most common model of computation is the deterministic Turing machine, but many complexity classes are based on non-deterministic Turing machines, Boolean circuits, quantum Turing machines, monotone circuits, etc.
The resource (or resources) that are being bounded and the bounds: These two properties are usually stated together, such as ""polynomial time"", ""logarithmic space"", ""constant depth"", etc.
Some complexity classes have complicated definitions that do not fit into this framework. Thus, a typical complexity class has a definition like the following:
The set of decision problems solvable by a deterministic Turing machine within time f(n). (This complexity class is known as DTIME(f(n)).)
But bounding the computation time above by some concrete function f(n) often yields complexity classes that depend on the chosen machine model. For instance, the language {xx | x is any binary string} can be solved in linear time on a multi-tape Turing machine, but necessarily requires quadratic time in the model of single-tape Turing machines. If we allow polynomial variations in running time, Cobham-Edmonds thesis states that ""the time complexities in any two reasonable and general models of computation are polynomially related"" (Goldreich 2008, Chapter 1.2). This forms the basis for the complexity class P, which is the set of decision problems solvable by a deterministic Turing machine within polynomial time. The corresponding set of function problems is FP.


=== Important complexity classes ===

Many important complexity classes can be defined by bounding the time or space used by the algorithm. Some important complexity classes of decision problems defined in this manner are the following:
The logarithmic-space classes (necessarily) do not take into account the space needed to represent the problem.
It turns out that PSPACE = NPSPACE and EXPSPACE = NEXPSPACE by Savitch's theorem.
Other important complexity classes include BPP, ZPP and RP, which are defined using probabilistic Turing machines; AC and NC, which are defined using Boolean circuits; and BQP and QMA, which are defined using quantum Turing machines. #P is an important complexity class of counting problems (not decision problems). Classes like IP and AM are defined using Interactive proof systems. ALL is the class of all decision problems.


=== Hierarchy theorems ===

For the complexity classes defined in this way, it is desirable to prove that relaxing the requirements on (say) computation time indeed defines a bigger set of problems. In particular, although DTIME(n) is contained in DTIME(n2), it would be interesting to know if the inclusion is strict. For time and space requirements, the answer to such questions is given by the time and space hierarchy theorems respectively. They are called hierarchy theorems because they induce a proper hierarchy on the classes defined by constraining the respective resources. Thus there are pairs of complexity classes such that one is properly included in the other. Having deduced such proper set inclusions, we can proceed to make quantitative statements about how much more additional time or space is needed in order to increase the number of problems that can be solved.
More precisely, the time hierarchy theorem states that

  
    
      
        
          
            D
            T
            I
            M
            E
          
        
        
          
            (
          
        
        f
        (
        n
        )
        
          
            )
          
        
        ⊊
        
          
            D
            T
            I
            M
            E
          
        
        
          
            (
          
        
        f
        (
        n
        )
        ⋅
        
          log
          
            2
          
        
        ⁡
        (
        f
        (
        n
        )
        )
        
          
            )
          
        
      
    
    {\displaystyle {\mathsf {DTIME}}{\big (}f(n){\big )}\subsetneq {\mathsf {DTIME}}{\big (}f(n)\cdot \log ^{2}(f(n)){\big )}}
  .
The space hierarchy theorem states that

  
    
      
        
          
            D
            S
            P
            A
            C
            E
          
        
        
          
            (
          
        
        f
        (
        n
        )
        
          
            )
          
        
        ⊊
        
          
            D
            S
            P
            A
            C
            E
          
        
        
          
            (
          
        
        f
        (
        n
        )
        ⋅
        log
        ⁡
        (
        f
        (
        n
        )
        )
        
          
            )
          
        
      
    
    {\displaystyle {\mathsf {DSPACE}}{\big (}f(n){\big )}\subsetneq {\mathsf {DSPACE}}{\big (}f(n)\cdot \log(f(n)){\big )}}
  .
The time and space hierarchy theorems form the basis for most separation results of complexity classes. For instance, the time hierarchy theorem tells us that P is strictly contained in EXPTIME, and the space hierarchy theorem tells us that L is strictly contained in PSPACE.


=== Reduction ===

Many complexity classes are defined using the concept of a reduction. A reduction is a transformation of one problem into another problem. It captures the informal notion of a problem being at most as difficult as another problem. For instance, if a problem X can be solved using an algorithm for Y, X is no more difficult than Y, and we say that X reduces to Y. There are many different types of reductions, based on the method of reduction, such as Cook reductions, Karp reductions and Levin reductions, and the bound on the complexity of reductions, such as polynomial-time reductions or log-space reductions.
The most commonly used reduction is a polynomial-time reduction. This means that the reduction process takes polynomial time. For example, the problem of squaring an integer can be reduced to the problem of multiplying two integers. This means an algorithm for multiplying two integers can be used to square an integer. Indeed, this can be done by giving the same input to both inputs of the multiplication algorithm. Thus we see that squaring is not more difficult than multiplication, since squaring can be reduced to multiplication.
This motivates the concept of a problem being hard for a complexity class. A problem X is hard for a class of problems C if every problem in C can be reduced to X. Thus no problem in C is harder than X, since an algorithm for X allows us to solve any problem in C. The notion of hard problems depends on the type of reduction being used. For complexity classes larger than P, polynomial-time reductions are commonly used. In particular, the set of problems that are hard for NP is the set of NP-hard problems.
If a problem X is in C and hard for C, then X is said to be complete for C. This means that X is the hardest problem in C. (Since many problems could be equally hard, one might say that X is one of the hardest problems in C.) Thus the class of NP-complete problems contains the most difficult problems in NP, in the sense that they are the ones most likely not to be in P. Because the problem P = NP is not solved, being able to reduce a known NP-complete problem, Π2, to another problem, Π1, would indicate that there is no known polynomial-time solution for Π1. This is because a polynomial-time solution to Π1 would yield a polynomial-time solution to Π2. Similarly, because all NP problems can be reduced to the set, finding an NP-complete problem that can be solved in polynomial time would mean that P = NP.


== Important open problems ==


=== P versus NP problem ===

The complexity class P is often seen as a mathematical abstraction modeling those computational tasks that admit an efficient algorithm. This hypothesis is called the Cobham–Edmonds thesis. The complexity class NP, on the other hand, contains many problems that people would like to solve efficiently, but for which no efficient algorithm is known, such as the Boolean satisfiability problem, the Hamiltonian path problem and the vertex cover problem. Since deterministic Turing machines are special non-deterministic Turing machines, it is easily observed that each problem in P is also member of the class NP.
The question of whether P equals NP is one of the most important open questions in theoretical computer science because of the wide implications of a solution. If the answer is yes, many important problems can be shown to have more efficient solutions. These include various types of integer programming problems in operations research, many problems in logistics, protein structure prediction in biology, and the ability to find formal proofs of pure mathematics theorems. The P versus NP problem is one of the Millennium Prize Problems proposed by the Clay Mathematics Institute. There is a US$1,000,000 prize for resolving the problem.


=== Problems in NP not known to be in P or NP-complete ===
It was shown by Ladner that if P ≠ NP then there exist problems in NP that are neither in P nor NP-complete. Such problems are called NP-intermediate problems. The graph isomorphism problem, the discrete logarithm problem and the integer factorization problem are examples of problems believed to be NP-intermediate. They are some of the very few NP problems not known to be in P or to be NP-complete.
The graph isomorphism problem is the computational problem of determining whether two finite graphs are isomorphic. An important unsolved problem in complexity theory is whether the graph isomorphism problem is in P, NP-complete, or NP-intermediate. The answer is not known, but it is believed that the problem is at least not NP-complete. If graph isomorphism is NP-complete, the polynomial time hierarchy collapses to its second level. Since it is widely believed that the polynomial hierarchy does not collapse to any finite level, it is believed that graph isomorphism is not NP-complete. The best algorithm for this problem, due to László Babai and Eugene Luks has run time 
  
    
      
        O
        (
        
          2
          
            
              n
              log
              ⁡
              n
            
          
        
        )
      
    
    {\displaystyle O(2^{\sqrt {n\log n}})}
   for graphs with n vertices, although some recent work by Babai offers some potentially new perspectives on this.
The integer factorization problem is the computational problem of determining the prime factorization of a given integer. Phrased as a decision problem, it is the problem of deciding whether the input has a prime factor less than k. No efficient integer factorization algorithm is known, and this fact forms the basis of several modern cryptographic systems, such as the RSA algorithm. The integer factorization problem is in NP and in co-NP (and even in UP and co-UP). If the problem is NP-complete, the polynomial time hierarchy will collapse to its first level (i.e., NP will equal co-NP). The best known algorithm for integer factorization is the general number field sieve, which takes time 
  
    
      
        O
        (
        
          e
          
            
              
                (
                
                  
                    64
                    9
                  
                
                )
              
              
                1
                
                  /
                
                3
              
            
            (
            log
            ⁡
            n
            
              )
              
                1
                
                  /
                
                3
              
            
            (
            log
            ⁡
            log
            ⁡
            n
            
              )
              
                2
                
                  /
                
                3
              
            
          
        
        )
      
    
    {\displaystyle O(e^{\left({\frac {64}{9}}\right)^{1/3}(\log n)^{1/3}(\log \log n)^{2/3}})}
   to factor an integer n. However, the best known quantum algorithm for this problem, Shor's algorithm, does run in polynomial time. Unfortunately, this fact doesn't say much about where the problem lies with respect to non-quantum complexity classes.


=== Separations between other complexity classes ===
Many known complexity classes are suspected to be unequal, but this has not been proved. For instance P ⊆ NP ⊆ PP ⊆ PSPACE, but it is possible that P = PSPACE. If P is not equal to NP, then P is not equal to PSPACE either. Since there are many known complexity classes between P and PSPACE, such as RP, BPP, PP, BQP, MA, PH, etc., it is possible that all these complexity classes collapse to one class. Proving that any of these classes are unequal would be a major breakthrough in complexity theory.
Along the same lines, co-NP is the class containing the complement problems (i.e. problems with the yes/no answers reversed) of NP problems. It is believed that NP is not equal to co-NP; however, it has not yet been proven. It is clear that if these two complexity classes are not equal then P is not equal to NP, since if P=NP we would also have P=co-NP, since problems in NP are dual to those in co-NP.
Similarly, it is not known if L (the set of all problems that can be solved in logarithmic space) is strictly contained in P or equal to P. Again, there are many complexity classes between the two, such as NL and NC, and it is not known if they are distinct or equal classes.
It is suspected that P and BPP are equal. However, it is currently open if BPP = NEXP.


== Intractability ==

A problem that can be solved in theory (e.g. given large but finite resources, especially time), but for which in practice any solution takes too many resources to be useful, is known as an intractable problem. Conversely, a problem that can be solved in practice is called a tractable problem, literally ""a problem that can be handled"". The term infeasible (literally ""cannot be done"") is sometimes used interchangeably with intractable, though this risks confusion with a feasible solution in mathematical optimization.
Tractable problems are frequently identified with problems that have polynomial-time solutions (P, PTIME); this is known as the Cobham–Edmonds thesis. Problems that are known to be intractable in this sense include those that are EXPTIME-hard. If NP is not the same as P, then NP-hard problems are also intractable in this sense.
However, this identification is inexact: a polynomial-time solution with large exponent or large constant term grows quickly, and may be impractical for practical size problems; conversely, an exponential-time solution that grows slowly may be practical on realistic input, or a solution that takes a long time in the worst case may take a short time in most cases or the average case, and thus still be practical. Saying that a problem is not in P does not imply that all large cases of the problem are hard or even that most of them are. For example, the decision problem in Presburger arithmetic has been shown not to be in P, yet algorithms have been written that solve the problem in reasonable times in most cases. Similarly, algorithms can solve the NP-complete knapsack problem over a wide range of sizes in less than quadratic time and SAT solvers routinely handle large instances of the NP-complete Boolean satisfiability problem.
To see why exponential-time algorithms are generally unusable in practice, consider a program that makes 2n operations before halting. For small n, say 100, and assuming for the sake of example that the computer does 1012 operations each second, the program would run for about 4 × 1010 years, which is the same order of magnitude as the age of the universe. Even with a much faster computer, the program would only be useful for very small instances and in that sense the intractability of a problem is somewhat independent of technological progress. However, an exponential-time algorithm that takes 1.0001n operations is practical until n gets relatively large.
Similarly, a polynomial time algorithm is not always practical. If its running time is, say, n15, it is unreasonable to consider it efficient and it is still useless except on small instances. Indeed, in practice even n3 or n2 algorithms are often impractical on realistic sizes of problems.


== History ==
An early example of algorithm complexity analysis is the running time analysis of the Euclidean algorithm done by Gabriel Lamé in 1844.
Before the actual research explicitly devoted to the complexity of algorithmic problems started off, numerous foundations were laid out by various researchers. Most influential among these was the definition of Turing machines by Alan Turing in 1936, which turned out to be a very robust and flexible simplification of a computer.
The beginning of systematic studies in computational complexity is attributed to the seminal 1965 paper ""On the Computational Complexity of Algorithms"" by Juris Hartmanis and Richard E. Stearns, which laid out the definitions of time complexity and space complexity, and proved the hierarchy theorems. In addition, in 1965 Edmonds suggested to consider a ""good"" algorithm to be one with running time bounded by a polynomial of the input size.
Earlier papers studying problems solvable by Turing machines with specific bounded resources include John Myhill's definition of linear bounded automata (Myhill 1960), Raymond Smullyan's study of rudimentary sets (1961), as well as Hisao Yamada's paper on real-time computations (1962). Somewhat earlier, Boris Trakhtenbrot (1956), a pioneer in the field from the USSR, studied another specific complexity measure. As he remembers:

However, [my] initial interest [in automata theory] was increasingly set aside in favor of computational complexity, an exciting fusion of combinatorial methods, inherited from switching theory, with the conceptual arsenal of the theory of algorithms. These ideas had occurred to me earlier in 1955 when I coined the term ""signalizing function"", which is nowadays commonly known as ""complexity measure"".

In 1967, Manuel Blum developed an axiomatic complexity theory based on his axioms and proved an important result, the so-called, speed-up theorem. The field really began to flourish in 1971 when the US researcher Stephen Cook and, working independently, Leonid Levin in the USSR, proved that there exist practically relevant problems that are NP-complete. In 1972, Richard Karp took this idea a leap forward with his landmark paper, ""Reducibility Among Combinatorial Problems"", in which he showed that 21 diverse combinatorial and graph theoretical problems, each infamous for its computational intractability, are NP-complete.


== See also ==


== References ==


=== Citations ===


=== Textbooks ===
Arora, Sanjeev; Barak, Boaz (2009), Computational Complexity: A Modern Approach, Cambridge, ISBN 978-0-521-42426-4, Zbl 1193.68112 
Downey, Rod; Fellows, Michael (1999), Parameterized complexity, Berlin, New York: Springer-Verlag 
Du, Ding-Zhu; Ko, Ker-I (2000), Theory of Computational Complexity, John Wiley & Sons, ISBN 978-0-471-34506-0 
Garey, Michael R.; Johnson, David S. (1979), Computers and Intractability: A Guide to the Theory of NP-Completeness, W. H. Freeman, ISBN 0-7167-1045-5 
Goldreich, Oded (2008), Computational Complexity: A Conceptual Perspective, Cambridge University Press 
van Leeuwen, Jan, ed. (1990), Handbook of theoretical computer science (vol. A): algorithms and complexity, MIT Press, ISBN 978-0-444-88071-0 
Papadimitriou, Christos (1994), Computational Complexity (1st ed.), Addison Wesley, ISBN 0-201-53082-1 
Sipser, Michael (2006), Introduction to the Theory of Computation (2nd ed.), USA: Thomson Course Technology, ISBN 0-534-95097-3 


=== Surveys ===
Khalil, Hatem; Ulery, Dana (1976), ""A Review of Current Studies on Complexity of Algorithms for Partial Differential Equations"", Proceedings of the annual conference on - ACM 76, ACM '76 Proceedings of the 1976 Annual Conference: 197, doi:10.1145/800191.805573 
Cook, Stephen (1983), ""An overview of computational complexity"" (PDF), Commun. ACM, ACM, 26 (6): 400–408, doi:10.1145/358141.358144, ISSN 0001-0782 
Fortnow, Lance; Homer, Steven (2003), ""A Short History of Computational Complexity"" (PDF), Bulletin of the EATCS, 80: 95–133 
Mertens, Stephan (2002), ""Computational Complexity for Physicists"", Computing in Science and Eng., Piscataway, NJ, USA: IEEE Educational Activities Department, 4 (3): 31–47, arXiv:cond-mat/0012185 , doi:10.1109/5992.998639, ISSN 1521-9615 


== External links ==
The Complexity Zoo
Hazewinkel, Michiel, ed. (2001) [1994], ""Computational complexity classes"", Encyclopedia of Mathematics, Springer Science+Business Media B.V. / Kluwer Academic Publishers, ISBN 978-1-55608-010-4 
http://mathoverflow.net/questions/34487/what-are-the-most-important-results-and-papers-in-complexity-theory-that-every/"
9,Timeline of programming languages,23696,45448,"This is a record of historically important programming languages, by decade.


== Pre-1950 ==


== 1950s ==


== 1960s ==


== 1970s ==


== 1980s ==


== 1990s ==


== 2000s ==


== 2010s ==


== See also ==
Programming language
Timeline of computing
History of computing hardware
History of programming languages


== References ==


== External links ==
Online encyclopedia for the history of programming languages
Diagram & history of programming languages
Eric Levenez's timeline diagram of computer languages history"
10,History of compiler construction,21310186,43068,"In computing, a compiler is a computer program that transforms source code written in a programming language or computer language (the source language), into another computer language (the target language, often having a binary form known as object code or machine code). The most common reason for transforming source code is to create an executable program.
Any program written in a high level programming language must be translated to object code before it can be executed, so all programmers using such a language use a compiler or an interpreter. Thus, compilers are very important to programmers. Improvements to a compiler may lead to a large number of improved executable programs.
Compilers are large and complex programs, but systematic analysis and research by computer scientists has led to a clearer understanding of compiler construction and a large body of theory has been developed around them. Research into compiler construction has led to tools that make it much easier to create compilers, so that today computer science students can create their own small language and develop a simple compiler for it in a few weeks.


== First compilers ==
Software for early computers was primarily written in assembly language. It is usually more productive for a programmer to use a high-level language, and programs written in a high-level language can be reused on different kinds of computers. Even so, it took a while for compilers to become established, because they generated code that did not perform as well as hand-written assembler, they were daunting development projects in their own right, and the very limited memory capacity of early computers created many technical problems for practical compiler implementations.
The first compiler was written by Corrado Böhm, in 1951, for his PhD thesis. The term compiler was coined by Grace Hopper., referring to her A-0 system which functioned as a loader or linker, not the modern notion of a compiler. The FORTRAN team led by John W. Backus at IBM introduced the first commercially available compiler, in 1957, which took 18 person-years to create.
The first ALGOL 58 compiler was completed by the end of 1958 by Friedrich L. Bauer, Hermann Bottenbruch, Heinz Rutishauser, and Klaus Samelson for the Z22 computer. Bauer et al. had been working on compiler technology for the Sequentielle Formelübersetzung (i.e. sequential formula translation) in the previous years.
By 1960, an extended Fortran compiler, ALTAC, was available on the Philco 2000, so it is probable that a Fortran program was compiled for both IBM and Philco computer architectures in mid-1960. The first known demonstrated cross-platform high-level language was COBOL. In a demonstration in December 1960, a COBOL program was compiled and executed on both the UNIVAC II and the RCA 501.


== Self-hosting compilers ==

Like any other software, there are benefits from implementing a compiler in a high-level language. In particular, a compiler can be self-hosted – that is, written in the programming language it compiles. Building a self-hosting compiler is a bootstrapping problem, i.e. the first such compiler for a language must be either hand written machine code or compiled by a compiler written in another language, or compiled by running the compiler in an interpreter.


=== Corrado Böhm PhD dissertation ===
Corrado Böhm developed a language, a machine, and a translation method for compiling that language on the machine in his PhD dissertation dated 1951. He not only described a complete compiler, but also defined for the first time that compiler in its own language. The language was interesting in itself, because every statement (including input statements, output statements and control statements) was a special case of an assignment statement.


=== NELIAC ===
The Navy Electronics Laboratory International ALGOL Compiler or NELIAC was a dialect and compiler implementation of the ALGOL 58 programming language developed by the Naval Electronics Laboratory in 1958.
NELIAC was the brainchild of Harry Huskey — then Chairman of the ACM and a well known computer scientist (and later academic supervisor of Niklaus Wirth), and supported by Maury Halstead, the head of the computational center at NEL. The earliest version was implemented on the prototype USQ-17 computer (called the Countess) at the laboratory. It was the world's first self-compiling compiler - the compiler was first coded in simplified form in assembly language (the bootstrap), then re-written in its own language and compiled by the bootstrap, and finally re-compiled by itself, making the bootstrap obsolete.


=== Lisp ===
Another early self-hosting compiler was written for Lisp by Tim Hart and Mike Levin at MIT in 1962. They wrote a Lisp compiler in Lisp, testing it inside an existing Lisp interpreter. Once they had improved the compiler to the point where it could compile its own source code, it was self-hosting.
The compiler as it exists on the standard compiler tape is a machine language program that was obtained by having the S-expression definition of the compiler work on itself through the interpreter. (AI Memo 39)
This technique is only possible when an interpreter already exists for the very same language that is to be compiled. It borrows directly from the notion of running a program on itself as input, which is also used in various proofs in theoretical computer science, such as the proof that the halting problem is undecidable.


=== Forth ===
Forth is an example of a self-hosting compiler. The self compilation and cross compilation features of Forth are commonly confused with metacompilation and metacompilers. Like Lisp, Forth is an extensible programming language. It is the extensible programming language features of Forth and Lisp that enable them to generate new versions of themselves or port themselves to new environments.


== Context-free grammars and parsers ==
A parser is an important component of a compiler. It parses the source code of a computer programming language to create some form of internal representation. Programming languages tend to be specified in terms of a context-free grammar because fast and efficient parsers can be written for them. Parsers can be written by hand or generated by a parser generator. A context-free grammar provides a simple and precise mechanism for describing how programming language constructs are built from smaller blocks. The formalism of context-free grammars was developed in the mid-1950s by Noam Chomsky.
Block structure was introduced into computer programming languages by the ALGOL project (1957–1960), which, as a consequence, also featured a context-free grammar to describe the resulting ALGOL syntax.
Context-free grammars are simple enough to allow the construction of efficient parsing algorithms which, for a given string, determine whether and how it can be generated from the grammar. If a programming language designer is willing to work within some limited subsets of context-free grammars, more efficient parsers are possible.


=== LR parsing ===

The LR parser (left to right) was invented by Donald Knuth in 1965 in a paper, ""On the Translation of Languages from Left to Right"". An LR parser is a parser that reads input from Left to right (as it would appear if visually displayed) and produces a Rightmost derivation. The term LR(k) parser is also used, where k refers to the number of unconsumed lookahead input symbols that are used in making parsing decisions.
Knuth proved that LR(k) grammars can be parsed with an execution time essentially proportional to the length of the program, and that every LR(k) grammar for k > 1 can be mechanically transformed into an LR(1) grammar for the same language. In other words, it is only necessary to have one symbol lookahead to parse any deterministic context-free grammar(DCFG).
Korenjak (1969) was the first to show parsers for programming languages could be produced using these techniques. Frank DeRemer devised the more practical Simple LR (SLR) and Look-ahead LR (LALR) techniques, published in his PhD dissertation at MIT in 1969. This was an important breakthrough, because LR(k) translators, as defined by Donald Knuth, were much too large for implementation on computer systems in the 1960s and 1970s.
In practice, LALR offers a good solution; the added power of LALR(1) parsers over SLR(1) parsers (that is, LALR(1) can parse more complex grammars than SLR(1)) is useful, and, though LALR(1) is not comparable with LL(1) (LALR(1) cannot parse all LL(1) grammars), most LL(1) grammars encountered in practice can be parsed by LALR(1). LR(1) grammars are more powerful again than LALR(1); however, an LR(1) grammar requires a canonical LR parser which would be extremely large in size and is not considered practical. The syntax of many programming languages are defined by grammars that can be parsed with an LALR(1) parser, and for this reason LALR parsers are often used by compilers to perform syntax analysis of source code.
A recursive ascent parser implements an LALR parser using mutually-recursive functions rather than tables. Thus, the parser is directly encoded in the host language similar to recursive descent. Direct encoding usually yields a parser which is faster than its table-driven equivalent for the same reason that compilation is faster than interpretation. It is also (in principle) possible to hand edit a recursive ascent parser, whereas a tabular implementation is nigh unreadable to the average human.
Recursive ascent was first described by Thomas Pennello in his article ""Very fast LR parsing"" in 1986. The technique was later expounded upon by G.H. Roberts in 1988 as well as in an article by Leermakers, Augusteijn, Kruseman Aretz in 1992 in the journal Theoretical Computer Science.


=== LL parsing ===

An LL parser parses the input from Left to right, and constructs a Leftmost derivation of the sentence (hence LL, as opposed to LR). The class of grammars which are parsable in this way is known as the LL grammars. LL grammars are an even more restricted class of context-free grammars than LR grammars. Nevertheless, they are of great interest to compiler writers, because such a parser is simple and efficient to implement.
LL(k) grammars can be parsed by a recursive descent parser which is usually coded by hand, although a notation such as META II might alternatively be used.
The design of ALGOL sparked investigation of recursive descent, since the ALGOL language itself is recursive. The concept of recursive descent parsing was discussed in the January 1961 issue of CACM in separate papers by A.A. Grau and Edgar T. ""Ned"" Irons.   Richard Waychoff and colleagues also implemented recursive descent in the Burroughs ALGOL compiler in March 1961, the two groups used different approaches but were in at least informal contact.
The idea of LL(1) grammars was introduced by Lewis and Stearns (1968).
Recursive descent was popularised by Niklaus Wirth with PL/0, an educational programming language used to teach compiler construction in the 1970s.
LR parsing can handle a larger range of languages than LL parsing, and is also better at error reporting, i.e. it detects syntactic errors when the input does not conform to the grammar as soon as possible.


=== Earley parser ===
In 1970, Jay Earley invented what came to be known as the Earley parser. Earley parsers are appealing because they can parse all context-free languages reasonably efficiently.


== Grammar description languages ==
John Backus proposed ""metalinguistic formulas"" to describe the syntax of the new programming language IAL, known today as ALGOL 58 (1959). Backus's work was based on the Post canonical system devised by Emil Post.
Further development of ALGOL led to ALGOL 60; in its report (1963), Peter Naur named Backus's notation Backus normal form (BNF), and simplified it to minimize the character set used. However, Donald Knuth argued that BNF should rather be read as Backus–Naur form, and that has become the commonly accepted usage.
Niklaus Wirth defined extended Backus–Naur form (EBNF), a refined version of BNF, in the early 1970s for PL/0. Augmented Backus–Naur form (ABNF) is another variant. Both EBNF and ABNF are widely used to specify the grammar of programming languages, as the inputs to parser generators, and in other fields such as defining communication protocols.


== Parser generators ==

A parser generator generates the lexical-analyser portion of a compiler. It is a program that takes a description of a formal grammar of a specific programming language and produces a parser for that language. That parser can be used in a compiler for that specific language. The parser detects and identifies the reserved words and symbols of the specific language from a stream of text and returns these as tokens to the code which implements the syntactic validation and translation into object code. This second part of the compiler can also be created by a compiler-compiler using a formal rules-of-precedence syntax-description as input.
The first compiler-compiler to use that name was written by Tony Brooker in 1960 and was used to create compilers for the Atlas computer at the University of Manchester, including the Atlas Autocode compiler. However it was rather different from modern compiler-compilers, and today would probably be described as being somewhere between a highly customisable generic compiler and an extensible-syntax language. The name 'compiler-compiler' was far more appropriate for Brooker's system than it is for most modern compiler-compilers, which are more accurately described as parser generators. It is almost certain that the ""Compiler Compiler"" name has entered common use due to Yacc rather than Brooker's work being remembered.
In the early 1960s, Robert McClure at Texas Instruments invented a compiler-compiler called TMG, the name taken from ""transmogrification"". In the following years TMG was ported to several UNIVAC and IBM mainframe computers.
The Multics project, a joint venture between MIT and Bell Labs, was one of the first to develop an operating system in a high level language. PL/I was chosen as the language, but an external supplier could not supply a working compiler. The Multics team developed their own subset dialect of PL/I known as Early PL/I (EPL) as their implementation language in 1964. TMG was ported to GE-600 series and used to develop EPL by Douglas McIlroy, Robert Morris, and others.
Not long after Ken Thompson wrote the first version of Unix for the PDP-7 in 1969, Doug McIlroy created the new system's first higher-level language: an implementation of McClure's TMG. TMG was also the compiler definition tool used by Ken Thompson to write the compiler for the B language on his PDP-7 in 1970. B was the immediate ancestor of C.
An early LALR parser generator was called ""TWS"", created by Frank DeRemer and Tom Pennello.


=== XPL ===
XPL is a dialect of the PL/I programming language, used for the development of compilers for computer languages. It was designed and implemented in 1967 by a team with William M. McKeeman, James J. Horning, and David B. Wortman at Stanford University and the University of California, Santa Cruz. It was first announced at the 1968 Fall Joint Computer Conference in San Francisco.
XPL featured a relatively simple translator writing system dubbed ANALYZER, based upon a bottom-up compiler precedence parsing technique called MSP (mixed strategy precedence). XPL was bootstrapped through Burroughs Algol onto the IBM System/360 computer. (Some subsequent versions of XPL used on University of Toronto internal projects utilized an SLR(1) parser, but those implementations have never been distributed).


=== Yacc ===
Yacc is a parser generator (loosely, compiler-compiler), not to be confused with lex, which is a lexical analyzer frequently used as a first stage by Yacc. Yacc was developed by Stephen C. Johnson at AT&T for the Unix operating system. The name is an acronym for ""Yet Another Compiler Compiler."" It generates an LALR(1) compiler based on a grammar written in a notation similar to Backus–Naur form.
Johnson worked on Yacc in the early 1970s at Bell Labs. He was familiar with TMG and its influence can be seen in Yacc and the design of the C programming language. Because Yacc was the default compiler generator on most Unix systems, it was widely distributed and used. Derivatives such as GNU Bison are still in use.
The compiler generated by Yacc requires a lexical analyzer. Lexical analyzer generators, such as lex or flex are widely available. The IEEE POSIX P1003.2 standard defines the functionality and requirements for both Lex and Yacc.


== Metacompilers ==

Metacompilers differ from parser generators, taking as input a program written in a metalanguage. Their input consists grammar analyzing formula and code production transforms that output executable code. Many can be programmed in their own metalanguage enabling them to compile themselves, making them self-hosting extensible language compilers.
Many metacompilers build on the work of Dewey Val Schorre. His META II compiler, first released in 1964, was the first documented metacompiler. Able to define its own language and others, META II accepted syntax formula having imbedded output (code production)s. It also translated to one of the earliest instances of a virtual machine. Lexical analysis was performed by built token recognizing functions: .ID, .STRING, and .NUMBER. Quoted strings in syntax formula recognize lexemes that are not kept.
TREE-META, a second generation Schorre metacompiler, appeared around 1968. It extended the capabilities of META II, adding unparse rules separating code production from the grammar analysis. Tree transform operations in the syntax formula produce abstract syntax trees that the unparse rules operate on. The unparse tree pattern matching provided peephole optimization ability.
CWIC, described in a 1970 ACM publication is a third generation Schorre metacompiler that added lexing rules and backtracking operators to the grammar analysis. LISP 2 was married with the unparse rules of TREEMETA in the CWIC generator language. With LISP 2 processing, CWIC can generate fully optimized code. CWIC also provided binary code generation into named code sections. Single and multipass compiles could be implemented using CWIC.
CWIC compiled to 8 bit byte addressable machine code instructions primarily designed to produce IBM System/360 code.
Later generations are not publicly documented. One important feature would be the abstraction of the target processor instruction set, generating to a pseudo machine instruction set, macros, that could be separately defined or mapped to a real machine's instructions. Optimizations applying to sequential instructions could then be applied to the pseudo instruction before their expansion to target machine code.


== Cross compilation ==
A cross compiler runs in one environment but produces object code for another. Cross compilers are used for embedded development, where the target computer has limited capabilities.
An early example of cross compilation was AIMICO, where a FLOW-MATIC program on a UNIVAC II was used to generate assembly language for the IBM 705, which was then assembled on the IBM computer.
The ALGOL 68C compiler generated ZCODE output, that could then be either compiled into the local machine code by a ZCODE translator or run interpreted. ZCODE is a register-based intermediate language. This ability to interpret or compile ZCODE encouraged the porting of ALGOL 68C to numerous different computer platforms.


== Optimizing compilers ==
Compiler optimization is the process of improving the quality of object code without changing the results it produces.
The developers of the first FORTRAN compiler aimed to generate code that was better than the average hand-coded assembler, so that customers would actually use their product. In one of the first real compilers, they often succeeded.
Later compilers, like IBM's Fortran IV compiler, placed more priority on good diagnostics and executing more quickly, at the expense of object code optimization. It wasn't until the IBM System/360 series that IBM provided two separate compilers: a fast executing code checker, and a slower optimizing one.
Frances E. Allen, working alone and jointly with John Cocke, introduced many of the concepts for optimization. Allen's 1966 paper, Program Optimization, introduced the use of graph data structures to encode program content for optimization. Her 1970 papers, Control Flow Analysis and A Basis for Program Optimization established intervals as the context for efficient and effective data flow analysis and optimization. Her 1971 paper with Cocke, A Catalogue of Optimizing Transformations, provided the first description and systematization of optimizing transformations. Her 1973 and 1974 papers on interprocedural data flow analysis extended the analysis to whole programs. Her 1976 paper with Cocke describes one of the two main analysis strategies used in optimizing compilers today.
Allen developed and implemented her methods as part of compilers for the IBM 7030 Stretch-Harvest and the experimental Advanced Computing System. This work established the feasibility and structure of modern machine- and language-independent optimizers. She went on to establish and lead the PTRAN project on the automatic parallel execution of FORTRAN programs. Her PTRAN team developed new parallelism detection schemes and created the concept of the program dependence graph, the primary structuring method used by most parallelizing compilers.
Programming Languages and their Compilers by John Cocke and Jacob T. Schwartz, published early in 1970, devoted more than 200 pages to optimization algorithms. It included many of the now familiar techniques such as redundant code elimination and strength reduction.


=== Peephole Optimization ===
Peephole optimization is a very simple but effective optimization technique. It was invented by William M. McKeeman and published in 1965 in CACM. It was used in the XPL compiler that McKeeman helped develop.


=== Capex COBOL Optimizer ===
Capex Corporation developed the ""COBOL Optimizer"" in the mid 1970s for COBOL. This type of optimizer depended, in this case, upon knowledge of 'weaknesses' in the standard IBM COBOL compiler, and actually replaced (or patched) sections of the object code with more efficient code. The replacement code might replace a linear table lookup with a binary search for example or sometimes simply replace a relatively 'slow' instruction with a known faster one that was otherwise functionally equivalent within its context. This technique is now known as ""Strength reduction"". For example, on the IBM System/360 hardware the CLI instruction was, depending on the particular model, between twice and 5 times as fast as a CLC instruction for single byte comparisons.
Modern compilers typically provide optimization options, so programmers can choose whether or not to execute an optimization pass.


== Diagnostics ==
When a compiler is given a syntactically incorrect program, a good, clear error message is helpful. From the perspective of the compiler writer, it is often difficult to achieve.
The WATFIV Fortran compiler was developed at the University of Waterloo, Canada in the late 1960s. It was designed to give better error messages than IBM's Fortran compilers of the time. In addition, WATFIV was far more usable, because it combined compiling, linking and execution into one step, whereas IBM's compilers had three separate components to run.


=== PL/C ===
PL/C was a computer programming language developed at Cornell University in the early 1970s. While PL/C was a subset of IBM's PL/I language, it was designed with the specific goal of being used for teaching programming. The two researchers and academic teachers who designed PL/C were Richard W. Conway and Thomas R. Wilcox. They submitted the famous article ""Design and implementation of a diagnostic compiler for PL/I"" published in the Communications of ACM in March 1973.
PL/C eliminated some of the more complex features of PL/I, and added extensive debugging and error recovery facilities. The PL/C compiler had the unusual capability of never failing to compile any program, through the use of extensive automatic correction of many syntax errors and by converting any remaining syntax errors to output statements.


== Just in Time compilation ==

Just in time compilation (JIT) is the generation of executable code on-the-fly or as close as possible to its actual execution, to take advantage of run time metrics or other performance enhancing options.


== Intermediate representation ==

Most modern compilers have a lexer and parser that produce an intermediate representation of the program. The intermediate representation is a simple sequence of operations which can be used by an optimizer and a code generator which produces instructions in the machine language of the target processor. Because the code generator uses an intermediate representation, the same code generator can be used for many different high level languages.
There are many possibilities for the intermediate representation. Three-address code, also known as a quadruple or quad is a common form, where there is an operator, two operands, and a result. Two-address code or triples have a stack to which results are written, in contrast to the explicit variables of three-address code.
Static Single Assignment (SSA) was developed by Ron Cytron, Jeanne Ferrante, Barry K. Rosen, Mark N. Wegman, and F. Kenneth Zadeck, researchers at IBM in the 1980s. In SSA, a variable is given a value only once. A new variable is created rather than modifying an existing one. SSA simplifies optimization and code generation.


== Code Generation ==

A code generator generates machine language instructions for the target processor.


=== Register Allocation ===
Sethi–Ullman algorithm or Sethi-Ullman numbering is a method to minimise the number of registers needed to hold variables.


== Notable compilers ==

Amsterdam Compiler Kit by Andrew Tanenbaum and Ceriel Jacobs
Berkeley Pascal, created by Ken Thompson in 1975. Bill Joy and others at University of California, Berkeley added improvements
GNU Compiler Collection, formerly the GNU C Compiler. First created by Richard Stallman in 1987, GCC is the major compiler used to build Linux.
LLVM, formerly known as the Low Level Virtual Machine
Small-C by Ron Cain and James E Hendrix
Turbo Pascal, created by Anders Hejlsberg, first released in 1983.
WATFOR, created at the University of Waterloo. One of the first popular educational compilers, although now largely obsolete.


== See also ==
History of programming languages
Lex (and Flex lexical analyser), the token parser commonly used in conjunction with yacc (and Bison).
BNF, a metasyntax used to express context-free grammar: that is, a formal way to describe formal languages.
Self-interpreter, an interpreter written in a language it can interpret.


== References ==


== Further reading ==
Backus, John, et al., ""The FORTRAN Automatic Coding System"", Proceedings of the Western Joint Computer Conference, Los Angeles, California, February 1957. Describes the design and implementation of the first FORTRAN compiler by the IBM team.
Knuth, D. E., RUNCIBLE-algebraic translation on a limited computer, Communications of the ACM, Vol. 2, p. 18, (Nov. 1959).
Irons, Edgar T., A syntax directed compiler for ALGOL 60, Communications of the ACM, Vol. 4, p. 51. (Jan. 1961)
Dijkstra, Edsger W. (1961). ""ALGOL 60 Translation: An ALGOL 60 Translator for the X1 and Making a Translator for ALGOL 60 (PDF) (Technical report). Amsterdam: Mathematisch Centrum. 35. 
Conway, Melvin E., Design of a separable transition-diagram compiler, Communications of the ACM, Volume 6, Issue 7 (July 1963)
Floyd, R. W., Syntactic analysis and operator precedence, Journal of the ACM, Vol. 10, p. 316. (July 1963).
Cheatham, T. E., and Sattley, K., Syntax directed compilation, SJCC p. 31. (1964).
Randell, Brian; Russell, Lawford John, ALGOL 60 Implementation: The Translation and Use of ALGOL 60 Programs on a Computer, Academic Press, 1964
Knuth, D. E. (July 1965). ""On the translation of languages from left to right"" (PDF). Information and Control. 8 (6): 607–639. doi:10.1016/S0019-9958(65)90426-2. Retrieved 29 May 2011. 
Cocke, John; Schwartz, Jacob T., Programming Languages and their Compilers: Preliminary Notes, Courant Institute of Mathematical Sciences technical report, New York University, 1969.
Bauer, Friedrich L.; Eickel, Jürgen (Eds.), Compiler Construction, An Advanced Course, 2nd ed. Lecture Notes in Computer Science 21, Springer 1976, ISBN 3-540-07542-9
Gries, David, Compiler Construction for Digital Computers, New York : Wiley, 1971. ISBN 0-471-32776-X


== External links ==
Compiler Construction before 1980 — Annotated literature list by Dick Grune"
11,String literal,199706,42371,"A string literal or anonymous string is a type of literal in programming for the representation of a string value within the source code of a computer program. Most often in modern languages this is a quoted sequence of characters (formally ""bracketed delimiters""), as in x = ""foo"", where ""foo"" is a string literal with value foo – the quotes are not part of the value, and one must use a method such as escape sequences to avoid the problem of delimiter collision and allow the delimiters themselves to be embedded in a string. However, there are numerous alternate notations for specifying string literals, particularly more complicated cases, and the exact notation depends on the individual programming language in question. Nevertheless, there are some general guidelines that most modern programming languages follow.


== Syntax ==


=== Bracketed delimiters ===
Most modern programming languages use bracket delimiters (also balanced delimiters) to specify string literals. Double quotations are the most common quoting delimiters used:

 ""Hi There!""

An empty string is literally written by a pair of quotes with no character at all in between:

 """"

Some languages either allow or mandate the use of single quotations instead of double quotations (the string must begin and end with the same kind of quotation mark and the type of quotation mark may give slightly different semantics):

 'Hi There!'

Note that these quotation marks are unpaired (the same character is used as an opener and a closer), which is a hangover from the typewriter technology which was the precursor of the earliest computer input and output devices.
In terms of regular expressions, a basic quoted string literal is given as:

""[^""]*""

This means that a string literal is written as: a quote, followed by zero, one, or more non-quote characters, followed by a quote. In practice this is often complicated by escaping, other delimiters, and excluding newlines.


==== Paired delimiters ====
A number of languages provide for paired delimiters, where the opening and closing delimiters are different. These also often allow nested strings, so delimiters can be embedded, so long as they are paired, but still result in delimiter collision for embedding an unpaired closing delimiter. Examples include PostScript, which uses parentheses, as in (The quick (brown fox)) and m4, which uses the backtick (`) as the starting delimiter, and the apostrophe (') as the ending delimiter. Tcl allows both quotes (for interpolated strings) and braces (for raw strings), as in ""The quick brown fox"" or {The quick {brown fox}}; this derives from the single quotations in Unix shells and the use of braces in C for compound statements, since blocks of code is in Tcl syntactically the same thing as string literals – that the delimiters are paired is essential for making this feasible.
While the Unicode character set includes paired (separate opening and closing) versions of both single and double quotations, used in text, mostly in other languages than English, these are rarely used in programming languages (because ASCII is preferred, and these are not included in ASCII):

 “Hi There!”
 ‘Hi There!’
 „Hi There!“
 «Hi There!»

The paired double quotations can be used in Visual Basic .NET, but many other programming languages will not accept them. Unpaired marks are preferred for compatibility - many web browsers, text editors, and other tools will not correctly display unicode paired quotes, and so even in languages where they are permitted, many projects forbid their use for source code.


=== Whitespace delimiters ===
String literals might be ended by newlines.
One example is MediaWiki template parameters.

 {{Navbox
 |name=Nulls
 |title=[[wikt:Null|Nulls]] in [[computing]]
 }}

There might be special syntax for multi-line strings.
In YAML, string literals may be specified by the relative positioning of whitespace and indentation.


=== Declarative notation ===
In the original FORTRAN programming language (for example), string literals were written in so-called Hollerith notation, where a decimal count of the number of characters was followed by the letter H, and then the characters of the string:

This declarative notation style is contrasted with bracketed delimiter quoting, because it does not require the use of balanced ""bracketed"" characters on either side of the string.
Advantages:
eliminates text searching (for the delimiter character) and therefore requires significantly less overhead
avoids the problem of delimiter collision
enables the inclusion of metacharacters that might otherwise be mistaken as commands
can be used for quite effective data compression of plain text strings
Drawbacks:
this type of notation is error-prone if used as manual entry by programmers
special care is needed in case of multi byte encodings
This is however not a drawback when the prefix is generated by an algorithm as is most likely the case.


== Delimiter collision ==

When using quoting, if one wishes to represent the delimiter itself in a string literal, one runs into the problem of delimiter collision. For example, if the delimiter is a double quote, one cannot simply represent a double quote itself by the literal """""" as the second quote is interpreted as the end of the string literal, not as the value of the string, and similarly one cannot write ""This is ""in quotes"", but invalid."" as the middle quoted portion is instead interpreted as outside of quotes. There are various solutions, the most general-purpose of which is using escape sequences, such as ""\"""" or ""This is \""in quotes\"" and properly escaped."", but there are many other solutions.
Note that paired quotes, such as braces in Tcl, allow nested string, such as {foo {bar} zork} but do not otherwise solve the problem of delimiter collision, since an unbalanced closing delimiter cannot simply be included, as in {}}.


=== Doubling up ===
A number of languages, including Pascal, BASIC, DCL, Smalltalk, SQL, J, and Fortran, avoid delimiter collision by doubling up on the quotation marks that are intended to be part of the string literal itself:


=== Dual quoting ===
Some languages, such as Fortran, Modula-2, JavaScript, Python, and PHP allow more than one quoting delimiter; in the case of two possible delimiters, this is known as dual quoting. Typically, this consists of allowing the programmer to use either single quotations or double quotations interchangeably – each literal must use one or the other.

This does not allow having a single literal with both delimiters in it, however. This can be worked around by using several literals and using string concatenation:

Note that Python has string literal concatenation, so consecutive string literals are concatenated even without an operator, so this can be reduced to:

D supports a few quoting delimiters, with such strings starting with q""[ and ending with ]"" or similarly for other delimiter character (any of () <> {} or []). D also supports here document-style strings via similar syntax.
In some programming languages, such as sh and Perl, there are different delimiters that are treated differently, such as doing string interpolation or not, and thus care must be taken when choosing which delimiter to use; see different kinds of strings, below.


=== Multiple quoting ===
A further extension is the use of multiple quoting, which allows the author to choose which characters should specify the bounds of a string literal.
For example, in Perl:

all produce the desired result. Although this notation is more flexible, few languages support it; other than Perl, Ruby (influenced by Perl) and C++11 also support these. In C++11, raw strings can have various delimiters, beginning with R""delimiter( and end with )delimiter"". The delimiter can be from zero to 16 characters long and may contain any member of the basic source character set except whitespace characters, parentheses, or backslash. A variant of multiple quoting is the use of here document-style strings.
Lua (as of 5.1) provides a limited form of multiple quoting, particularly to allow nesting of long comments or embedded strings. Normally one uses [[ and ]] to delimit literal strings (initial newline stripped, otherwise raw), but the opening brackets can include any number of equal signs, and only closing brackets with the same number of signs close the string. For example:

Multiple quoting is particularly useful with regular expressions that contain usual delimiters such as quotes, as this avoids needing to escape them. An early example is sed, where in the substitution command s/regex/replacement/ the default slash / delimiters can be replaced by another character, as in s,regex,replacement, .


=== Constructor functions ===
Another option, which is rarely used in modern languages, is to use a function to construct a string, rather than representing it via a literal. This is generally not used in modern languages because the computation is done at run time, rather than at parse time.
For example, early forms of BASIC did not include escape sequences or any other workarounds listed here, and thus one instead was required to use the CHR$ function, which returns a string containing the character corresponding to its argument. In ASCII the quotation mark has the value 34, so to represent a string with quotes on an ASCII system one would write

In C, a similar facility is available via sprintf and the %c ""character"" format specifier, though in the presence of other workarounds this is generally not used:

These constructor functions can also be used to represent nonprinting characters, though escape sequences are generally used instead. A similar technique can be used in C++ with the std::string stringification operator.


== Escape sequences ==

Escape sequences are a general technique for representing characters that are otherwise difficult to represent directly, including delimiters, nonprinting characters (such as backspaces), newlines, and whitespace characters (which are otherwise impossible to distinguish visually), and have a long history. They are accordingly widely used in string literals, and adding an escape sequence (either to a single character or throughout a string) is known as escaping.
One character is chosen as a prefix to give encodings for characters that are difficult or impossible to include directly. Most commonly this is backslash; in addition to other characters, a key point is that backslash itself can be encoded as a double backslash \\ and for delimited strings the delimiter itself can be encoded by escaping, say by \"" for "". A regular expression for such escaped strings can be given as follows, as found in the ANSI C specification:

""(\\.|[^\\""])*""

meaning ""a quote; followed by zero or more of either an escaped character (backslash followed by something, possibly backslash or quote), or a non-escape, non-quote character; ending in a quote"" – the only issue is distinguishing the terminating quote from a quote preceded by a backslash, which may itself be escaped. Note that multiple characters can follow the backslash, such as \uFFFF, depending on the escaping scheme.
An escaped string must then itself be lexically analyzed, converting the escaped string into the unescaped string that it represents. This is done during the evaluation phase of the overall lexing of the computer language: the evaluator of the lexer of the overall language executes its own lexer for escaped string literals.
Among other things, it must be possible to encode the character that normally terminates the string constant, plus there must be some way to specify the escape character itself. Escape sequences are not always pretty or easy to use, so many compilers also offer other means of solving the common problems. Escape sequences, however, solve every delimiter problem and most compilers interpret escape sequences. When an escape character is inside a string literal, it means ""this is the start of the escape sequence"". Every escape sequence specifies one character which is to be placed directly into the string. The actual number of characters required in an escape sequence varies. The escape character is on the top/left of the keyboard, but the editor will translate it, therefore it is not directly tapeable into a string. The backslash is used to represent the escape character in a string literal.
Many languages support the use of metacharacters inside string literals. Metacharacters have varying interpretations depending on the context and language, but are generally a kind of 'processing command' for representing printing or nonprinting characters.
For instance, in a C string literal, if the backslash is followed by a letter such as ""b"", ""n"" or ""t"", then this represents a nonprinting backspace, newline or tab character respectively. Or if the backslash is followed by 1-3 octal digits, then this sequence is interpreted as representing the arbitrary character with the specified ASCII code. This was later extended to allow more modern hexadecimal character code notation:

Note: Not all sequences in the above list are supported by all parsers, and there may be other escape sequences which are not in the above list.


=== Nested escaping ===
When code in one programming language is embedded inside another, embedded strings may require multiple levels of escaping. This is particularly common in regular expressions and SQL query within other languages, or other languages inside shell scripts. This double-escaping is often difficult to read and author.
Incorrect quoting of nested strings can present a security vulnerability. Use of untrusted data, as in data fields of an SQL query, should use prepared statements to prevent a code injection attack. In PHP 2 through 5.3, there was a feature called magic quotes which automatically escaped strings (for convenience and security), but due to problems was removed from version 5.4 onward.


=== Raw strings ===
A few languages provide a method of specifying that a literal is to be processed without any language-specific interpretation. This avoids the need for escaping, and yields more legible strings.
Raw strings are particularly useful when a common character needs to be escaped, notably in regular expressions (nested as string literals), where backslash \ is widely used, and in DOS/Windows paths, where backslash is used as a path separator. The profusion of backslashes is known as leaning toothpick syndrome, and can be reduced by using raw strings. Compare escaped and raw pathnames:

Extreme examples occur when these are combined – Uniform Naming Convention paths begin with \\, and thus an escaped regular expression matching a UNC name begins with 8 backslashes, ""\\\\\\\\"", due to needing to escape the string and the regular expression. Using raw strings reduces this to 4 (escaping in the regular expression), as in C# @""\\\\"".
In XML documents, CDATA sections allows use of characters such as & and < without an XML parser attempting to interpret them as part of the structure of the document itself. This can be useful when including literal text and scripting code, to keep the document well formed.


== Multiline string literals ==
In many languages, string literals can contain literal newlines, spanning several lines. Alternatively, newlines can be escaped, most often as \n. For example:

and

are both valid bash, producing:

foo
bar

Languages that allow literal newlines include bash, Lua, Perl, PHP, R, and Tcl. In some other languages string literals cannot include newlines.
Two issues with multiline string literals are leading and trailing newlines, and indentation. If the initial or final delimiters are on separate lines, there are extra newlines, while if they are not, the delimiter makes the string harder to read, particularly for the first line, which is often indented differently from the rest. Further, the literal must be unindented, as leading whitespace is preserved – this breaks the flow of the code if the literal occurs within indented code.
The most common solution for these problems is here document-style string literals. Formally speaking, a here document is not a string literal, but instead a stream literal or file literal. These originate in shell scripts and allow a literal to be fed as input to an external command. The opening delimiter is <<END where END can be any word, and the closing delimiter is END on a line by itself, serving as a content boundary – the << is due to redirecting stdin from the literal. Due to the delimiter being arbitrary, these also avoid the problem of delimiter collision. These also allow initial tabs to be stripped via the variant syntax <<-END though leading spaces are not stripped. The same syntax has since been adopted for multiline string literals in a number of languages, most notably Perl, and are also referred to as here documents, and retain the syntax, despite being strings and not involving redirection. As with other string literals, these can sometimes have different behavior specified, such as variable interpolation.
Python, whose usual string literals do not allow literal newlines, instead has a special form of string, designed for multiline literals, called triple quoting. These use a tripled delimiter, either ''' or """""". These literals strip leading indentation and the trailing newline (but not the leading newline), and are especially used for inline documentation, known as docstrings.
Tcl allows literal newlines in strings and has no special syntax to assist with multiline strings, though delimiters can be placed on lines by themselves and leading and trailing newlines stripped via string trim, while string map can be used to strip indentation.


== String literal concatenation ==
A few languages provide string literal concatenation, where adjacent string literals are implicitly joined into a single literal at compile time. This is a feature of C, C++, D, Ruby, and Python, which copied it from C. Notably, this concatenation happens at compile time, during lexical analysis (as a phase following initial tokenization), and is contrasted with both run time string concatenation (generally with the + operator) and concatenation during constant folding, which occurs at compile time, but in a later phase (after phrase analysis or ""parsing""). Most languages, such as C#, Java and Perl, do not support implicit string literal concatenation, and instead require explicit concatenation, such as with the + operator (this is also possible in D and Python, but illegal in C/C++ – see below); in this case concatenation may happen at compile time, via constant folding, or may be deferred to run time.


=== Motivation ===
In C, where the concept and term originate, string literal concatenation was introduced for two reasons:
To allow long strings to span multiple lines with proper indentation in contrast to line continuation, which destroys the indentation scheme; and
To allow the construction of string literals by macros (via stringizing).
In practical terms, this allows string concatenation in early phases of compilation (""translation"", specifically as part of lexical analysis), without requiring phrase analysis or constant folding. For example, the following are valid C/C++:

However, the following are invalid:

This is because string literals have pointer type, char * (C) or const char [n] (C++), which cannot be added; this is not a restriction in most other languages.
This is particularly important when used in combination with the C preprocessor, to allow strings to be computed following preprocessing, particularly in macros. As a simple example:

will (if the file is called a.c) expand to:

which is then concatenated, being equivalent to:

A common use case is in constructing printf or scanf format strings, where format specifiers are given by macros.
A more complex example uses stringification of integers (by the preprocessor) to define a macro that expands to a sequence of string literals, which are then concatenated to a single string literal with the file name and line number:

Beyond syntactic requirements of C/C++, implicit concatenation is a form of syntactic sugar, making it simpler to split string literals across several lines, avoiding the need for line continuation (via backslashes) and allowing one to add comments to parts of strings. For example, in Python, one can comment a regular expression in this way:


=== Problems ===
Implicit string concatenation is not required by modern compilers, which implement constant folding, and causes hard-to-spot errors due to unintentional concatenation from omitting a comma, particularly in vertical lists of strings, as in:

Accordingly, it is not used in most languages, and it has been proposed for deprecation from D and Python. However, removing the feature breaks backwards compatibility, and replacing it with a concatenation operator introduces issues of precedence – string literal concatenation occurs during lexing, prior to operator evaluation, but concatenation via an explicit operator occurs at the same time as other operators, hence precedence is an issue, potentially requiring parentheses to ensure desired evaluation order.
A subtler issue is that in C and C++, there are different types of string literals, and concatenation of these has implementation-defined behavior, which poses a potential security risk.


== Different kinds of strings ==
Some languages provide more than one kind of literal, which have different behavior. This is particularly used to indicate raw strings (no escaping), or to disable or enable variable interpolation, but has other uses, such as distinguishing character sets. Most often this is done by changing the quoting character or adding a prefix. This is comparable to prefixes and suffixes to integer literals, such as to indicate hexadecimal numbers or long integers.
One of the oldest examples is in shell scripts, where single quotes indicate a raw string or ""literal string"", while double quotes have escape sequences and variable interpolation.
For example, in Python, raw strings are preceded by an r or R – compare 'C:\\Windows' with r'C:\Windows' (though, a Python raw string cannot end in an odd number of backslashes). Python 2 also distinguishes two types of strings: 8-bit ASCII (""bytes"") strings (the default), explicitly indicated with a b or B prefix, and Unicode strings, indicated with a u or U prefix.
C#'s notation for raw strings is called @-quoting.

While this disables escaping, it allows double-up quotes, which allow one to represent quotes within the string:

C++11 allows raw strings, unicode strings (UTF-8, UTF-16, and UTF-32), and wide character strings, determined by prefixes.
In Tcl, brace-delimited strings are literal, while quote-delimited strings have escaping and interpolation.
Perl has a wide variety of strings, which are more formally considered operators, and are known as quote and quote-like operators. These include both a usual syntax (fixed delimiters) and a generic syntax, which allows a choice of delimiters; these include:

REXX uses suffix characters to specify characters or strings using their hexadecimal or binary code. E.g.,

all yield the space character, avoiding the function call X2C(20).


== Variable interpolation ==

Languages differ on whether and how to interpret string literals as either 'raw' or 'variable interpolated'. Variable interpolation is the process of evaluating an expression containing one or more variables, and returning output where the variables are replaced with their corresponding values in memory. In sh-compatible Unix shells, quotation-delimited ("") strings are interpolated, while apostrophe-delimited (') strings are not. For example, the following Perl code:

produces the output:

Nancy said Hello World to the crowd of people.

The sigil character ($) is interpreted to indicate variable interpolation.
Similarly, the printf function produces the same output using notation such as:

The metacharacters (%s) indicate variable interpolation.
This is contrasted with ""raw"" strings:

which produce output like:

$name said $greeting to the crowd of people.

Here the $ characters are not sigils, and are not interpreted to have any meaning other than plain text.


== Embedding source code in string literals ==
Languages that lack flexibility in specifying string literals make it particularly cumbersome to write programming code that generates other programming code. This is particularly true when the generation language is the same or similar to the output language.
For example:
writing code to produce quines
generating an output language from within a web template;
using XSLT to generate XSLT, or SQL to generate more SQL
generating a PostScript representation of a document for printing purposes, from within a document-processing application written in C or some other language.
writing shaders
Nevertheless, some languages are particularly well-adapted to produce this sort of self-similar output, especially those that support multiple options for avoiding delimiter collision.
Using string literals as code that generates other code may have adverse security implications, especially if the output is based at least partially on untrusted user input. This is particularly acute in the case of Web-based applications, where malicious users can take advantage of such weaknesses to subvert the operation of the application, for example by mounting an SQL injection attack.


== See also ==
Character literal
Sigil (computer programming)


== Notes ==


== References ==


== External links ==
Literals In Programming"
12,C string handling,33691376,41536,"The C programming language has a set of functions implementing operations on strings (character strings and byte strings) in its standard library. Various operations, such as copying, concatenation, tokenization and searching are supported. For character strings, the standard library uses the convention that strings are null-terminated: a string of n characters is represented as an array of n + 1 elements, the last of which is a ""NUL"" character.
The only support for strings in the programming language proper is that the compiler translates quoted string constants into null-terminated strings.


== Definitions ==
A string is a contiguous sequence of code units terminated by the first zero code (\0, corresponding to the null character). In C, there are two types of strings: string, which is sometimes called byte string which uses the type char as code units (one char is at least 8 bits), and wide string which uses the type wchar_t as code units.
A common misconception is that all char arrays are strings, because string literals are converted to arrays during the compilation (or translation) phase. It is important to remember that a string ends at the first zero code unit. An array or string literal that contains a zero before the last byte therefore contains a string, or possibly several strings, but is not itself a string. Conversely, it is possible to create a char array that is not null-terminated and is thus not a string: char is often used as a small integer when needing to save memory.
The term pointer to a string is used in C to describe a pointer to the initial (lowest-addressed) byte of a string. In C, pointers are used to pass strings to functions. Documentation (including this page) will often use the term string to mean pointer to a string.
The term length of a string is used in C to describe the number of bytes preceding the zero byte. strlen is a standardised function commonly used to determine the length of a string. A common mistake is to not realize that a string uses one more unit of memory than this length, in order to store the zero that ends the string.


== Character encodings ==
Each string ends at the first occurrence of the zero code unit of the appropriate kind (char or wchar_t). Consequently, a byte string can contain non-NUL characters in ASCII or any ASCII extension, but not characters in encodings such as UTF-16 (even though a 16-bit code unit might be nonzero, its high or low byte might be zero). The encodings that can be stored in wide strings are defined by the width of wchar_t. In most implementations, wchar_t is at least 16 bits, and so all 16-bit encodings, such as UCS-2, can be stored. If wchar_t is 32-bits, then 32-bit encodings, such as UTF-32, can be stored.
Variable-width encodings can be used in both byte strings and wide strings. String length and offsets are measured in bytes or wchar_t, not in ""characters"", which can be confusing to beginning programmers. UTF-8 and Shift JIS are often used in C byte strings, while UTF-16 is often used in C wide strings when wchar_t is 16 bits. Truncating strings with variable length characters using functions like strncpy can produce invalid sequences at the end of the string. This can be unsafe if the truncated parts are interpreted by code that assumes the input is valid.
Support for Unicode literals such as char foo[512] = ""φωωβαρ"";(UTF-8) or wchar_t foo[512] = L""φωωβαρ""; (UTF-16 or UTF-32) is implementation defined, and may require that the source code be in the same encoding. Some compilers or editors will require entering all non-ASCII characters as \xNN sequences for each byte of UTF-8, and/or \uNNNN for each word of UTF-16.


== Overview of functions ==
Most of the functions that operate on C strings are declared in the string.h header (cstring in C++), while functions that operate on C wide strings are declared in the wchar.h header (cwchar in C++). These headers also contain declarations of functions used for handling memory buffers; the name is thus something of a misnomer.
Functions declared in string.h are extremely popular since, as a part of the C standard library, they are guaranteed to work on any platform which supports C. However, some security issues exist with these functions, such as potential buffer overflows when not used carefully and properly, causing the programmers to prefer safer and possibly less portable variants, out of which some popular ones are listed below. Some of these functions also violate const-correctness by accepting a const string pointer and returning a non-const pointer within the string. To correct this, some have been separated into two overloaded functions in the C++ version of the standard library.
In historical documentation the term ""character"" was often used instead of ""byte"" for C strings, which leads many to believe that these functions somehow do not work for UTF-8. In fact all lengths are defined as being in bytes and this is true in all implementations, and these functions work as well with UTF-8 as with single-byte encodings. The BSD documentation has been fixed to make this clear, but POSIX, Linux, and Windows documentation still uses ""character"" in many places where ""byte"" or ""wchar_t"" is the correct term.
Functions for handling memory buffers can process sequences of bytes that include null-byte as part of the data. Names of these functions typically start with mem, as opposite to the str prefix.


=== Constants and types ===


=== Functions ===


==== Multibyte functions ====
These functions all take a pointer to a mbstate_t object that the caller must maintain. This was originally intended to track shift states in the mb encodings, but modern ones such as UTF-8 do not need this. However these functions were designed on the assumption that the wc encoding is not a variable-width encoding and thus are designed to deal with exactly one wchar_t at a time, passing it by value rather than using a string pointer. As UTF-16 is a variable-width encoding, the mbstate_t has been reused to keep track of surrogate pairs in the wide encoding, though the caller must still detect and call mbtowc twice for a single character.


=== Numeric conversions ===
The C standard library contains several functions for numeric conversions. The functions that deal with byte strings are defined in the stdlib.h header (cstdlib header in C++). The functions that deal with wide strings are defined in the wchar.h header (cwchar header in C++).
The strtoxxx functions are not const-correct, since they accept a const string pointer and return a non-const pointer within the string. Also, since the Normative Amendment 1 (C95), atoxx functions are considered subsumed by strtoxxx functions, for which reason neither C95 nor any later standard provides wide-character versions of these functions.


== Popular extensions ==


== Replacements ==
Despite the well-established need to replace strcat and strcpy with functions that do not allow buffer overflows, no accepted standard has arisen. This is partly due to the mistaken belief by many C programmers that strncat and strncpy have the desired behavior; however, neither function was designed for this (they were intended to manipulate null-padded fixed-size string buffers, a data format less commonly used in modern software), and the behavior and arguments are non-intuitive and often written incorrectly even by expert programmers.
The most popular replacement are the strlcat and strlcpy functions, which appeared in OpenBSD 2.4 in December, 1998. These functions always write one NUL to the destination buffer, truncating the result if necessary, and return the size of buffer that would be needed, which allows detection of the truncation and provides a size for creating a new buffer that will not truncate. They have been criticized on the basis of allegedly being inefficient and encouraging the use of C strings (instead of some superior alternative form of string). Consequently, they have not been included in the GNU C library (used by software on Linux), although they are implemented in the C libraries for OpenBSD, FreeBSD, NetBSD, Solaris, OS X, and QNX, as well as in alternative C libraries for Linux, such as musl. The lack of GNU C library support has not stopped various software authors from using it and bundling a replacement, among other SDL, GLib, ffmpeg, rsync, and even internally in the Linux kernel. Open source implementations for these functions are available.
As part of its 2004 Security Development Lifecycle, Microsoft introduced a family of ""secure"" functions including strcpy_s and strcat_s (along with many others). These functions were standardized with some minor changes as part of the optional C11 (Annex K) proposed by ISO/IEC WDTR 24731. Experience with these functions has shown significant problems with their adoption and errors in usage, so the removal of Annex K is proposed for the next revision of the C standard. These functions perform various checks including whether the string is too long to fit in the buffer. If the checks fail, a user-specified ""runtime-constraint handler"" function is called, which usually aborts the program. Some functions perform destructive operations before calling the runtime-constraint handler; for example, strcat_s sets the destination to the empty string, which can make it difficult to recover from error conditions or debug them. These functions attracted considerable criticism because initially they were implemented only on Windows and at the same time warning messages started to be produced by Microsoft Visual C++ suggesting the programmers to use these functions instead of standard ones. This has been speculated by some to be an attempt by Microsoft to lock developers into its platform. Although open-source implementations of these functions are available, these functions are not present in common Unix C libraries.
If the string length is known, then memcpy or memmove can be more efficient than strcpy, so some programs use them to optimize C string manipulation. They accept a buffer length as a parameter, so they can be employed to prevent buffer overflows in a manner similar to the aforementioned functions.


== See also ==
C syntax § Strings –  source code syntax, including backslash escape sequences
String functions


== Notes ==


== References ==


== External links ==
Fast memcpy in C, multiple C coding examples to target different types of CPU instruction architectures"
13,Academic genealogy of computer scientists,30729888,40768,"The following is an academic genealogy of computer scientists and is constructed by following the pedigree of thesis advisors.
Smaller text indicates advisors or advisees specialized in a field unrelated to computer science.


== Europe ==


=== Denmark ===
Peter Naur
(Olivier Danvy)


=== Finland ===
Arto Salomaa


=== France ===
Many French computer scientists worked at the National Institute for Research in Computer Science and Control (INRIA).
Marcel-Paul Schützenberger
Maurice Nivat
Philippe Flajolet
Gérard Huet
Francois Fages
Thierry Coquand
Hugo Herbelin

Xavier Leroy
Christine Paulin-Mohring
Didier Rémy
François Pottier

Bruno Courcelle

Louis Nolin
Bernard Robinet
Emmanuel Saint-James
Olivier Danvy (Secondary advisor: Emmanuel Saint-James)

Jean-François Perrot
Jacques Sakarovitch
Jean-Eric Pin
Pascal Weil

Gérard Berry
Gilles Kahn
Patrick Cousot
Alain Colmerauer


=== Germany ===
Karl Steinbuch
Franz Baader
Carl Adam Petri
Martin Odersky
Bernhard Steffen


=== Italy ===
Corrado Böhm
Ugo Montanari
Paolo Ciancarini
Roberto Gorrieri
Nadia Busi

Davide Sangiorgi


=== Netherlands ===


==== Van Wijngaarden / Dijkstra ====
Adriaan van Wijngaarden was director of the computer science department at the Centrum Wiskunde & Informatica. It was influential in the development of ALGOL 68.
Cornelis Benjamin Biezeno (1933: honoris causa. Universiteit van Amsterdam)
Adriaan van Wijngaarden (1945: Enige toepassingen van Fourierintegralen op elastische problemen. Technische Universiteit Delft)
Willem van der Poel (1956: The Logical Principles of Some Simple Computers. Universiteit van Amsterdam)
Gerard Holzmann (1979: Coordination Problems in Multiprocessing Systems. Technische Universiteit Delft)

Edsger Dijkstra (1959: Communication with an Automatic Computer. Universiteit van Amsterdam)
Nico Habermann (1967: On the Harmonious Co-operation of Abstract Machines. Technische Universiteit Eindhoven)
Lawrence Snyder (1973: An Analysis of Parameter Evalutation Mechanisms for Recursive Procedures. Carnegie Mellon University)
Tim Teitelbaum (1975: Minimal Distance Analysis of Syntax Errors in Computer Programs. Carnegie Mellon University)
Sten Andler (1979: Predicate Path Expressions: A High-level Synchronization Mechanism. Carnegie Mellon University)
John Ousterhout (1980: Partitioning and Cooperation in a Distributed Multiprocessor Operating System: MEDUSA. Carnegie Mellon University)
Philip Wadler (1984: Listlessness Is Better than Laziness: An Algorithm that Transforms Applicative Programs to Eliminate Intermediate Lists. Carnegie Mellon University) (Secondary advisor: Guy L. Steele, Jr.)
David Notkin (1984: Interactive Structure-Oriented Computing. Carnegie Mellon University)

Martin Rem (1976: Associons and the Closure Statement. Technische Universiteit Eindhoven) (Secondary advisor: Frans Kruseman Aretz)
Jan L. A. van de Snepscheut (1983: Trace Theory and VLSI Design. Technische Universiteit Eindhoven) (Secondary advisor: Edsger Dijkstra)
Peter Hilbers (1989: Mappings of Algorithms on Processor Networks. Rijksuniversiteit Groningen)

Jan Tijmen Udding (1984: Classification and Composition of Delay-Insensitive Circuits. Technische Universiteit Eindhoven) (Secondary advisor: Edsger Dijkstra)
Anne Kaldewaij (1986: A Formalism for Concurrent Processes. Technische Universiteit Eindhoven) (Secondary advisor: Frans Kruseman Aretz)

Guus Zoutendijk (1960: Methods of Feasible Directions : A Study in Lineair and Non-linear Programming. Universiteit van Amsterdam)
Marc Nico Spijker (1968: Stability and Convergence of Finite-Difference Methods. Universiteit Leiden)

Jaco de Bakker (1967: Formal Definition of Programming Languages: with an Application to the Definition of ALGOL 60. Universiteit van Amsterdam)
Willem-Paul de Roever (1974: Recursive Program Schemes: Semantics and Proof Theory. Vrije Universiteit Amsterdam)
Paul Vitanyi (1978: Lindenmayer Systems: Structure, Languages, and Growth Functions. Vrije Universiteit Amsterdam) (Secondary advisor: Arto K. Salomaa)
Ronald Cramer (1997: Modular design of secure yet practical cryptographic protocols. Universiteit van Amsterdam) (Secondary advisor: Ivan Bjerre Damgård)
Peter Grünwald (1998: The minimum description length principle and reasoning under uncertainty. Universiteit van Amsterdam)

Anton Nijholt (1980: Context-Free Grammars : Covers, Normal Forms, and Parsing. Vrije Universiteit Amsterdam)
Giuseppe Scollo (1993: The Engineering of Logics. Universiteit Twente)
Ed Brinksma (1988: On the Design of Extended LOTOS; a Specification Language for Open Distributed Systems. Universiteit Twente) (Primary advisor: Christian Anton Vissers)

John-Jules Meyer (1985: Programming Calculi Based on Fixed Point Transformations: Semantics and Applications. Vrije Universiteit Amsterdam)
Wiebe van der Hoek (1992: Modalities for Reasoning about Knowledge and Quantities. Vrije Universiteit Amsterdam) (Secondary advisor: Johan van Benthem)

Joost Kok (1989: Semantic Models for Parallel Computation in Data Flow, Logic- and Object-Oriented Programming. Vrije Universiteit Amsterdam)
Jan Rutten (1989: A Parallel Object-Oriented Language: Design and Semantic Foundations. Vrije Universiteit Amsterdam)
Frank S. de Boer (1991: Reasoning about Dynamically Evolving Process Structures: A Proof Theory for the Parallel Object-0riented Language POOL. Vrije Universiteit Amsterdam)
Marcello Bonsangue (1996: Topological Dualities in Semantics. Vrije Universiteit Amsterdam) (Secondary advisor: Joost Kok)

Reinder van de Riet (1968: Algol 60 as Formula Manipulation Language. Universiteit van Amsterdam)
Peter Apers (1982: Query Processing and Data Allocation in Distributed Database Systems. Vrije Universiteit Amsterdam)
Arno Siebes (1990: On Complex Objects. Universiteit Twente) (Secondary advisor: Martin L. Kersten)

Martin L. Kersten (1985: A Model for a Secure Programming Environment. Vrije Universiteit Amsterdam) (Secondary advisor: Anthony Ira Wasserman)
Stefan Manegold (2002: Understanding, Modeling, and Improving Main-Memory Database Performance. Universiteit van Amsterdam)

Roel Wieringa (1990: Algebraic Foundations for Dynamic Conceptual Models. Vrije Universiteit Amsterdam)
Frances Brazier (1991: Design and Evaluation of a User Interface for Information Retrieval. Vrije Universiteit Amsterdam) (Primary advisor: Sipke D. Fokkema)

Hugo Brandt Corstius (1970: Exercises in Computational Linguistics. Universiteit van Amsterdam) (Secondary advisor: Frans Kruseman Aretz)
Maarten van Emden (1971: An Analysis of Complexity. Universiteit van Amsterdam)
Jonathan Schaeffer (1986: Experiments in Search and Knowledge. University of Waterloo) (Secondary advisor: Randy G. Goebel)

Peter van Emde Boas (1974: Abstract Resource-Bound Classes. Universiteit van Amsterdam) (Secondary advisor: Pieter Cornelis Baayen)
Arjen Lenstra (1984: Polynomial Time Algorithms for the Factorization of Polynomials. Universiteit van Amsterdam)
Leen Torenvliet (1986: Structural Concepts in Relativised Hierarchies. Universiteit van Amsterdam)
Harry Buhrman (1993: Resource Bounded Reductions. Universiteit van Amsterdam) (Primary advisor: Steven Elliot Homer)

Herman te Riele (1976: A Computational Study of Generalized Aliquot Sequences. Universiteit van Amsterdam)
Dick Grune (1982: On the Design of ALEPH. Universiteit van Amsterdam) (Secondary advisor: Cornelis H. A. Koster)


==== Brouwer / Van Dalen ====
Several of the students of Dirk van Dalen, a descendant of Brouwer, became the first Dutch theoretical computer scientists, which still has a strong focus on lambda calculus, rewrite systems and functional programming.
Luitzen Egbertus Jan Brouwer (1907: Over de grondslagen der wiskunde. Universiteit van Amsterdam)
Arend Heyting (1925: Intuitionistische axiomatiek der projectieve meetkunde. Universiteit van Amsterdam)
Dirk van Dalen (1963: Extension Problems in Intuitionistic Plane Projective Geometry. Universiteit van Amsterdam)
Henk Barendregt (1971: Some Extensional Terms for Combinatory Logics and Lambda-Calculi. Universiteit Utrecht)
Roel de Vrijer (1987: Surjective Pairing and Strong Normalization: Two Themes in Lambda Calculus. Universiteit van Amsterdam)
Pieter Hartel (1988: Performance Analysis of Storage Management in Combinator Graph Reduction. Universiteit van Amsterdam) (Primary advisor: Bob Hertzberger)
Mariangiola Dezani-Ciancaglini (1996: Logical Semantics for Concurrent Lambda-Calculus. Katholieke Universiteit Nijmegen) (Secondary advisor: Corrado Böhm)

Jan van Leeuwen (1972: Rule-Labeled Programs: A Study of a Generalization of Context-Free Grammars and Some Classes of Formal Languages. Universiteit Utrecht)
Mark Overmars (1983: The Design of Dynamic Data Structures. Universiteit Utrecht)
Mark de Berg (1992: Efficient Algorithms for Ray Shooting and Hidden Surface Removal. Universiteit Utrecht)
Marc van Kreveld (1992: New Results on Data Structures in Computational Geometry. Universiteit Utrecht)

Hans Bodlaender (1986: Distributed Computing - Structure and Complexity. Universiteit Utrecht)
Harry Wijshoff (1987: Data Organization in Parallel Computers. Universiteit Utrecht)
Gerard Tel (1989: The Structure of Distributed Algorithms. Universiteit Utrecht)

Jan Bergstra (1976: Computability and Continuity in Finite Types. Universiteit Utrecht)
Frits Vaandrager (1990: Algebraic Techniques for Concurrency and Their Application. Universiteit van Amsterdam)
Linda van der Gaag (1990: Probability-Based Models for Plausible Reasoning. Universiteit van Amsterdam)
Chris Verhoef (1990: Linear unary operators in process algebra. Universiteit van Amsterdam)
Jan Friso Groote (1991: Process Algebra and Structured Operational Semantics. Universiteit van Amsterdam)
Wan Fokkink (1994: Clocks, Trees and Stars in Process Theory. Universiteit van Amsterdam)
Jaco van de Pol (1996: Termination of Higher-Order Rewrite Systems. Universiteit Utrecht) (Secondary advisor: Marc Bezem)

Jan Willem Klop (1980: Combinatory reduction systems. Universiteit Utrecht)
Vincent van Oostrom (1994: Confluence for Abstract and Higher-Order Rewriting. Vrije Universiteit Amsterdam)

Albert Visser (1981: Aspects of Diagonalization & Provability. Universiteit Utrecht)
Wim Ruitenburg (1982: Intuitionistic Algebra, Theory and Sheaf Models. Universiteit Utrecht)
Catholijn Jonker (1994: Constraints and Negations in Logic Programming. Universiteit Utrecht) (Secondary advisor: Jan van Leeuwen)

Anne Sjerp Troelstra (1966: Intuitionistic General Topology. Universiteit van Amsterdam)
Gerard R. Renardel de Lavalette (1985: Theories with Type-free Application and Extended Bar Induction. Universiteit van Amsterdam)
Hans van Ditmarsch (2000: Knowledge Games. Rijksuniversiteit Groningen) (Secondary advisor: Johan van Benthem)

Ieke Moerdijk (1985: Topics in Intuitionism and Topos Theory. Universiteit van Amsterdam)
Marc Bezem (1986: Bar recursion and functionals of finite type. Universiteit Utrecht) (Secondary advisor: Dirk van Dalen)


=== Norway ===
Ole-Johan Dahl
Kristen Nygaard
Trygve Reenskaug


=== Poland ===
Grzegorz Rozenberg
Antoni W. Mazurkiewicz


=== Sweden ===
Bengt Nordström
Lennart Augustsson


=== United Kingdom ===
James H. Wilkinson


==== Edinburgh ====
Rod Burstall was one of the founders of the Laboratory for Foundations of Computer Science at the University of Edinburgh.
Rod Burstall (1956: Heuristic and Decision Tree Methods on Computers: Some Operational Research Applications. University of Birmingham)
Gordon Plotkin (1972: (dissertation title unknown). University of Edinburgh)
Glynn Winskel (1980: Events in Computation. University of Edinburgh)
Thomas Hildebrandt

Luca Cardelli (1982: An Algebraic Approach to Hardware Description and Verification. University of Edinburgh)
Eugenio Moggi (1988: The Partial Lambda Calculus. University of Edinburgh)
Philippa Gardner
Alex Simpson (computer scientist)

J Strother Moore (1973: Computational Logic: Structure Sharing and Proof of Program Properties. University of Edinburgh)
Panagiotis Manolios

Michael J. C. Gordon
Jeffrey Joyce

Don Sannella (1982: Semantics, Implementation and Pragmatics of Clear, a Program Specification Language. University of Edinburgh)
David Aspinall
Martin Hofmann (Secondary advisor: Gordon Plotkin)

Thorsten Altenkirch
Michael Mendler (Secondary advisor: Michael P. Fourman)
Masahito Hasegawa

Robin Popplestone
Alan Mycroft
Chistopher Longuet-Higgins, Richard Gregory, and Donald Mitchie founded the Department of Machine Intelligence and Perception at the University of Edinburgh
Christopher Longuet-Higgins (1947: Some problems in theoretical chemistry by the method of molecular orbitals. University of Oxford)
Geoffrey Hinton (1977: Relaxation and its role in vision. University of Edinburgh)
Mark Steedman (1973: The formal description of musical perception. University of Edinburgh)

Richard Gregory
Donald Mitchie
Gordon Plotkin (1972: Automatic methods of inductive inference. University of Edinburgh)
Austin Tate (1975: Using goal structure to direct search in a problem solver. University of Edinburgh)
Andrew Blake (scientist) (1983: Parallel computation in low level vision. University of Edinburgh)
Stephen Muggleton (1986: Inductive acquisition of expert knowledge. University of Edinburgh)


==== Cambridge ====
Maurice Wilkes was the first head of the University of Cambridge Computer Laboratory
Maurice Wilkes
Peter Wegner
Clement McGowan (Secondary advisor: Juris Hartmanis)
Daniel M. Berry (Secondary advisor: Clement McGowan)
Nancy Leveson (Secondary advisor: Anthony Ira Wasserman)

David Wheeler
Mathai Joseph
Roger Needham
Ross J. Anderson
David L. Tennenhouse
Peter G. Gyarmati

Robin Milner never did a Ph.D.
Robin Milner
Mads Tofte
Faron Moller
Chris Tofts


==== Oxford ====
Christopher Strachey was the first Professor of Computation at Oxford.
Christopher Strachey
Peter Landin (worked as the assistant of Strachey, did not do a PhD.)
Chris Wadsworth
Peter Mosses
Jens Palsberg

David Turner (Secondary advisor: Dana Scott)

Tony Hoare established the undergraduate computer science course and led the Oxford University Computing Laboratory for many years.
Tony Hoare
Cliff Jones (computer scientist)
Tobias Nipkow

Bill Roscoe
Peter Lauer (computer scientist)
Eike Best
Javier Esparza


==== Warwick ====
Mathai Joseph
Zhiming Liu
Paritosh Pandya

Mike Paterson
Leslie Valiant


== North America ==


=== Church ===
Siméon Poisson (1800: (dissertation title unknown). École Polytechnique)
Michel Chasles (1814: (dissertation title unknown). École Polytechnique)
H. A. Newton (1850: (dissertation title unknown). Yale University)
E. H. Moore (1885: Extensions of Certain Theorems of Clifford and Cayley in the Geometry of n Dimensions. Yale University)
Oswald Veblen (1903: A System of Axioms for Geometry. The University of Chicago)
Philip Franklin
Alan Perlis
Gary Lindstrom
David Parnas
Richard J. Lipton
Dan Boneh
Avi Wigderson

Alonzo Church (1927: Alternatives to Zermelo's Assumption. Princeton University)
Stephen Kleene (1934: A Theory of Positive Integers in Formal Logic. Princeton University)
Robert Lee Constable (1968: Extending and Refining Hierarchies of Computable Functions. University of Wisconsin-Madison)
Steven Muchnick
Uwe Frederik Pleban
Peter Lee

Kurt Mehlhorn
Edmund M. Clarke
Robert Harper (computer scientist) (1985: Aspects of the Implementation of Type Theory. Cornell University)
Benjamin C. Pierce (1991: Programming with Intersection Types and Bounded Polymorphism. Carnegie Mellon University) (Secondary advisor: John C. Reynolds)
Gregory Morrisett

John Rosser (1934: A Mathematical Logic without Variables. Princeton University)
Theodore Hailperin
Steven Orey
Elliott Mendelson
George Collins (logician)
Gerald Sacks

Alan Turing (1938: Systems of Logic Based on Ordinals. Princeton University)
Robin Gandy (1953: On Axiomatic Systems in Mathematics and Theories in Physics. University of Cambridge)

Hartley Rogers, Jr. (1952: Some Results on Definability and Decidability in Elementary Theories, (Parts I-V). Princeton University)
Patrick C. Fischer (1962: Theory of Provable Recursive Functions. Massachusetts Institute of Technology)
Arnold L. Rosenberg (1965: Nonwriting Extendsions of Finite Automata. Harvard University)
Dennis Ritchie (1968: Program Structure and Computational Complexity. Harvard University)
Albert R. Meyer (1972: On Complex Recursive Functions. Harvard University)
Nancy Lynch
Leonid Levin
Jeanne Ferrante
Charles Rackoff
Larry Stockmeyer
David Harel
Joseph Halpern
Daphne Koller

Robert L. Probert (1973: On the Complexity of Matrix Multiplication. Waterloo University)
Lawrence V. Saxton (1973: Input-Output Conventions and the Complexity of Transductions. Waterloo University)
Stan J. Thomas (1983: A Non-First-Normal-Form Relational Database Model. Vanderbilt University)
Dirk Van Gucht (1985: Theory of Unnormalized Relational Structures. Vanderbilt University)

David Park (1964: Set-Theoretic Constructions in Model Theory. Massachusetts Institute of Technology)
Mike Paterson
Ian Parberry
Leslie Valiant

John C. Mitchell (1984: Lambda Calculus Models of Typed Programming Languages. Massachusetts Institute of Technology)

Michael O. Rabin (1957: Recursive Unsolvability of Group Theoretic Problems. Princeton University)
Dana Scott (1958: Convergent Sequences of Complete Theories. Princeton University)
Jack Copeland
Angus Macintyre
Marko Petkovšek
Fred S. Roberts
Ketan Mulmuley
Michael Fourman (1974: Connections between category theory and logic. University of Oxford)

Peter B. Andrews (mathematician)
Frank Pfenning
Hongwei Xi Boston University

George David Birkhoff (1907: Asymptotic Properties of Certain Ordinary Differential Equations with Applications to Boundary Value and Expansion Problems. The University of Chicago)
Clarence Raymond Adams (1922: The General Theory of the Linear Partial q-Difference Equation and of the Linear Partial Difference Equation of the Intermediate Type. Harvard University)
Anthony Morse (1937: Convergence in Variation and Related Topics. Brown University)
Woody Bledsoe (1953: Separative Measures for Topological Spaces. University of California, Berkeley)
Robert S. Boyer (1971: Locking: A Restriction of Resolution. University of Texas at Austin)


=== Harvard ===
Alfred North Whitehead (1884: (dissertation title unknown). University of Cambridge)
Willard Van Orman Quine (1932: The Logic of Sequences: A Generalization of Principia Mathematica. Harvard University)
Hao Wang (academic) (1948: An Economical Ontology for Classical Arithmetic. Harvard University)
Stephen Cook (1966: On the Minimum Computation Time of Functions. Harvard University)


=== Hopcroft / Lefschetz ===
Felix Klein (1868: Über die Transformation der allgemeinen Gleichung des zweiten Grades zwischen Linien-Koordinaten auf eine kanonische Form. Rheinische Friedrich-Wilhelms-Universität Bonn)
Ferdinand von Lindemann (1873: Über unendlich kleine Bewegungen und über Kraftsysteme bei allgemeiner projektivischer Maßbestimmung. Friedrich-Alexander-Universität Erlangen-Nürnberg)
Arnold Sommerfeld (1891: Die willkürlichen Functionen in der mathematischen Physik. Universität Königsberg)
Ernst Guillemin (1926: Theorie der Frequenzvervielfachung durch Eisenkernkoppelung. Ludwig-Maximilians-Universität München)
Robert Fano (1947: Theoretical Limitations on the Broadband Matching of Arbitrary Impedances. Massachusetts Institute of Technology)
William Linvill (1949: Analysis and Design of Sampled-Data Control Systems. Massachusetts Institute of Technology)
Bernard Widrow (1956: A Study of Rough Amplitude Quantization by Means of Nyquist Sampling Theory. Massachusetts Institute of Technology)
Richard Mattson (1962: The Analysis and Synthesis of Adaptive Systems Which Use Networks of Threshold Elements. Stanford University)
John Hopcroft (1964: Synthesis of Threshold Logic Networks. Stanford University)
Alfred Aho (1967: Indexed Grammars: An Extension of Context Free Grammars. Princeton University)
Zvi Galil (1975: The Complexity of Resolution Procedures for Theorem Proving in the Propositional Calculus. Cornell University)
David Eppstein (1989: Efficient Algorithms for Sequence Analysis with Concave and Convex Gap Costs. Columbia University)

Merrick L. Furst (1980: A Subexponenial Algorithm for Trivalent Isomorphism. Cornell University)
Andrew Appel (1985: Compile-Time Evaluation and Code Generation in Semantics-Directed Compilers. Carnegie Mellon University) (Primary advisor: Ravi Sethi)

Oskar Bolza (1886: Über die Reduction hyperelliptischer Integrale erster Ordnung und erster Gattung auf elliptische, insbesondere über die Reduction durch eine Transformation vierten Grades. Georg-August-Universität Göttingen)
John Hector McDonald (1900: Concerning the System of the Binary Cubic and Quadratic with Application to the Reduction of Hyperelliptic Integrals to Elliptic Integrals by a Transformation of Order Four. The University of Chicago)
Waldemar Trjitzinsky (1926: The Elliptic Cylinder Differential Equation. University of California, Berkeley)
Richard Hamming (1942: Some Problems in the Boundary Value Theory of Linear Differential Equations. University of Illinois at Urbana-Champaign)

William Edward Story (1875: On the Algebraic Relations Existing Between the Polars of a Binary Quantic. Universität Leipzig)
Solomon Lefschetz (1911: On the Existence of Loci with Given Singularities. Clark University)
Albert W. Tucker (1932: An Abstract Approach to Manifolds. Princeton University)
Marvin Minsky (1954: Theory of Neural-Analog Reinforcement Systems and Its Application to the Brain Model Problem. Princeton University)
Manuel Blum (1964: A Machine-Independent Theory of the Complexity of Recursive Functions. Massachusetts Institute of Technology)
John Gill, III (1972: Probabilistic Turing Machines and Complexity of Computation. University of California, Berkeley)
Gary Miller (computer scientist) (1975: Riemann's Hypothesis and Tests for Primality. University of California, Berkeley)
F. Thomson Leighton (1981: Layouts for the Shuffle-Exchange Graph and Lower Bound Techniques for VLSI. Massachusetts Institute of Technology)
Peter Shor (1985: Random Planar Matching and Bin Packing. Massachusetts Institute of Technology)

Dana Angluin (1976: An Application of the Theory of Computational Complexity to the Study of Inductive Inference. University of California, Berkeley)
Leonard Adleman (1976: Number-Theoretic Aspects of Computational Complexity. University of California, Berkeley)
Michael Sipser (1980: Nondeterminism and the Size of Two-Way Finite Automata. University of California, Berkeley)
Lance Fortnow (1989: Complexity-Theoretic Aspects of Interactive Proof Systems. Massachusetts Institute of Technology)
Daniel Spielman (1995: Computationally Efficient Error-Correcting Codes and Holographic Proofs. Massachusetts Institute of Technology)

Jeffrey Shallit (1983: Metric Theory of Pierce Expansions. University of California, Berkeley)
Silvio Micali (1983: Randomness Versus Hardness. University of California, Berkeley)
Eric Bach (1984: Analytic Methods in the Analysis and Design of Number-Theoretic Algorithms. University of California, Berkeley)
Shafrira Goldwasser (1984: Probabilitstic Encryption: Theory and Applications. University of California, Berkeley)
Johan Håstad

Vijay Vazirani (1984: Maximum Matchings without Blossoms. University of California, Berkeley)
Umesh Vazirani (1986: Randomness, Adversaries and Computation. University of California, Berkeley)
Madhu Sudan (1992: Efficient Checking of Polynomials and Proofs and the Hardness of Approximation Problems. University of California, Berkeley)
Sanjeev Arora (1994: Probabilistic Checking of Proofs and Hardness of Approximation Problems. University of California, Berkeley)
Andris Ambainis (2001: Quantum Entanglement, Quantum Communication and the Limits of Quantum Computing. University of California, Berkeley)
Scott Aaronson (2004: Limits on Efficient Computation in the Physical World. University of California, Berkeley)

Steven Rudich (1989: Limits on the Provable Consequences of One-Way Functions. University of California, Berkeley)
Moni Naor (1989: Implicit Storage Schemes for Quick Retrieval. University of California, Berkeley)
Ronitt Rubinfeld (1990: A Mathematical Theory of Self-Checking, Self-Testing and Self-Correcting Programs. University of California, Berkeley)
Sampath Kannan (1990: Program Checkers for Algebraic Problems. University of California, Berkeley)
Russell Impagliazzo (1992: Pseudo-Random Generators for Probabilistic Algorithms and for Cryptography. University of California, Berkeley)
Mor Harchol-Balter (1996: Network Analysis without Exponentiality Assumptions. University of California, Berkeley)
Luis von Ahn (2005: Human Computation. Carnegie Mellon University)
Ryan Williams (computer scientist) (2007: Algorithms and Resource Requirements for Fundamental Problems. Carnegie Mellon University)

Gerald Sussman (1973: A Computational Model of Skill Acquisition. Massachusetts Institute of Technology)
Drew McDermott (1976: Flexibility and Efficiency in a Computer Program for Designing Circuits. Massachusetts Institute of Technology)
Guy Steele, Jr. (1980: The Definition and Implementation of a Computer Programming Language Based on Constraints. Massachusetts Institute of Technology)
Philip Wadler (1984: Listlessness Is Better than Laziness: An Algorithm that Transforms Applicative Programs to Eliminate Intermediate Lists. Carnegie Mellon University) (Primary advisor: Nico Habermann)

Ken Forbus (1984: Qualitative Process Theory. Massachusetts Institute of Technology)

Scott Fahlman (1977: A System for Representing and Using Real-World Knowledge. Massachusetts Institute of Technology) (Secondary advisor: Gerald Sussman)

John McCarthy (computer scientist) (1951: Projection Operators and Partial Differential Equations. Princeton University)
Barbara Huberman Liskov (1968: A Program to Play Chess End Games. Stanford University)


=== California Institute of Technology ===


==== Knuth ====
Søren Rasmusen ((year unknown): (dissertation title unknown). )
Bernt Michael Holmboe ((year unknown): (dissertation title unknown). )
Carl Anton Bjerknes ((year unknown): (dissertation title unknown). )
Marius Sophus Lie ((year unknown): (dissertation title unknown). )
Elling Bolt Holst ((year unknown): (dissertation title unknown). )
Axel Thue (1889: (dissertation title unknown). University of Christiania)
Thoralf Skolem (1926: Einige Sätze über ganzzahlige Lösungen gewisser Gleichungen und Ungleichungen. Universitetet i Oslo)
Øystein Ore (1924: (dissertation title unknown). Universitetet i Oslo)
Grace Hopper (1934: New Types of Irreducibility Criteria. Yale University)
Marshall Hall, Jr. (1936: An Isomorphism Between Linear Recurring Sequences and Algebraic Rings. Yale University)
Donald Knuth (1963: Finite Semifields and Projective Planes. California Institute of Technology)
Vaughan Pratt (1972: Shellsort and Sorting Networks. Stanford University)
David Harel (1978: Logics of Programs: Axiomatics and Descriptive Power. Massachusetts Institute of Technology)

Jeffrey Scott Vitter (1980: Analysis of Coalesced Hashing. Stanford University)


==== Hartmanis ====
Eric Temple Bell
Morgan Ward
Robert P. Dilworth
Juris Hartmanis
Edward Reingold
Dexter Kozen
Hubie Chen

Neil Immerman
Allan Borodin
David G. Kirkpatrick
Ian Munro (computer scientist)


=== Floyd ===
Bob Floyd never received a PhD, although he worked closely with Donald Knuth on The Art of Computer Programming.
Bob Floyd ((year unknown): (dissertation title unknown). The University of Chicago)
Zohar Manna (1968: Termination of Algorithms. Carnegie Mellon University)

Adi Shamir (1977: The Fixed Points Of Recursive Definitions. Weizmann Institute of Science)
Martín Abadi (1987: Temporal Theorem Proving. Stanford University)
Shmuel Katz (computer scientist) (1977: Invariants And The Logical Analysis Of Programs. Weizmann Institute of Science)
Nachum Dershowitz (1979: The Evolution of Programs. Weizmann Institute of Science)

Jay Earley (1968: An Efficient Context-Free Parsing Algorithm. Carnegie Mellon University)
Robert Tarjan (1972: An Efficient Planarity Algorithm. Stanford University)
Daniel Sleator (1981: An 
  
    
      
        O
        (
        n
        m
        log
        ⁡
        n
        )
      
    
    {\displaystyle O(nm\log n)}
   Algorithm for Maximum Network Flow. Princeton University)
John R. Gilbert (1981: Graph Separator Theorems and Sparse Gaussian Elimination. Stanford University)
Raimund Seidel (1987: Output-Size Sensitive Algorithms for Constructive Problems in Computational Geometry. Cornell University)
Nina Amenta (1994: Helly Theorems and Generalized Linear Programming. University of California, Berkeley)

Monika Henzinger (1993: Fully Dynamic Graph Algorithms and Their Data Structures. Princeton University)

Ronald Rivest (1974: Analysis of Associative Retrieval Algorithms. Stanford University)
Ron Pinter (1982: The Impact of Layer Assignment Methods on Layout Algorithms for Integrated Circuits. Massachusetts Institute of Technology)
Avrim Blum (1991: Algorithms for Approximate Graph Coloring. Massachusetts Institute of Technology)
Robert Schapire (1991: The Design and Analysis of Efficient Learning Algorithms. Massachusetts Institute of Technology)

David Plaisted (1976: Theorem Proving and Semantic Trees. Stanford University)


=== Ullman ===


=== Hilbert ===
David Hilbert (1885, University of Königsberg)
Hugo Steinhaus
Mark Kac
Harry Kesten
Ed Granirer
Tony Lau
Maria Klawe

Hermann Weyl
Saunders MacLane
Roger Conant Lyndon
Calvin Creston Elgot

Anil Nerode
Bob Soare
Richard Tenney

Micael Morley
Terry Millar
Mark Manasse

Kurt Schutte
Wolfgang Maass

Wilhelm Ackermann
Richard Courant
Haskell Curry
Hellmuth Kneser
Reinhold Baer
Earl J. Schweppe
Elizabeth A. Unger

Erich Hecke (1910, University of Göttingen)
Heinrich Behnke (1923, University of Hamburg)
Hans Langmaack (1960, University of Münster)
Ernst-Rüdiger Olderog (1981, University of Kiel)


=== Aiken ===
Emory Leon Chaffee ((year unknown): (dissertation title unknown). )
Howard Aiken ((year unknown): (dissertation title unknown). )
Gerrit Blaauw ((year unknown): (dissertation title unknown). )
Christian Vissers ((year unknown): (dissertation title unknown). )
Hendrik Brinksma ((year unknown): (dissertation title unknown). )

Fred Brooks (1956: The Analytic Design of Automatic Data Processing Systems. )
Anthony Oettinger (1954: A Study for the Design of an Automatic Dictionary. )
William Hines Bossert ((year unknown): (dissertation title unknown). )
Gerald J. Popek ((year unknown): (dissertation title unknown). )
John Heidemann ((year unknown): (dissertation title unknown). )

Sheila Greibach (1963: Inverses of Phrase Structure Generators. Harvard University)
Ronald Book ( : Grammars with Time Functions. )
Michael J. Fischer ((year unknown): (dissertation title unknown). )
Mitchell Wand ((year unknown): (dissertation title unknown). )
Michael Martin Hammer ((year unknown): (dissertation title unknown). )
Dennis McLeod ((year unknown): (dissertation title unknown). )

Jean Gallier (1978: Semantics and Correctness of Classes of Deterministic and Nondeterministic Recursive Programs. University of California, Los Angeles)
Wayne Snyder (1988: Complete Sets of Transformations for General Unification. University of Pennsylvania)

Richard Karp (1959: Some Applications of Logical Syntax to Digital Computer Programming. Harvard University)
Robert Keller (computer scientist) (1970: Closures of Parallel Program Schemata. University of California, Berkeley)
Paul Hudak (1982: Object and Task Reclamation in Distributed Applicative Processing Systems. University of Utah)
Kai Li ((year unknown): (dissertation title unknown). )

Kellogg Booth ((year unknown): (dissertation title unknown). )
Ron Shamir ((year unknown): (dissertation title unknown). )
Rajeev Motwani (1988: Probabilistic Analysis of Matching and network flow Algorithms. )

Eugene Lawler (1963: Some Aspects of Discrete Mathematical Programming. )
David Shmoys (1984: Approximation Algorithms for Problems in Sequencing, Scheduling, and Communication Network Design. )
Philip N. Klein ((year unknown): (dissertation title unknown). )
Ramamurthy Ravi ((year unknown): (dissertation title unknown). )

Clifford Stein ((year unknown): (dissertation title unknown). )

Lee J. White (1967: A Parametric Study of Matchings and Coverings in Weighted Graphs. University of Michigan)
Sargur Srihari (1976: Comparative Evaluation of Stored Pattern Classifiers. The Ohio State University)
Venu Govindaraju (1992: Locating Faces in Newspaper Photographs. University at Buffalo)


=== Stanford ===
George Forsythe
Ramon E. Moore
Cleve Moler
Jack Dongarra
Charles F. Van Loan

William M. McKeeman
Eric Hehner (Primary advisor: David Barkley Wortman)

Richard P. Brent (Primary advisor: Gene Howard Golub)
J. Alan George
Gaston Gonnet
Ricardo Baeza-Yates

Michael Alexander Malcolm
David Cheriton
Willy Zwaenepoel
John Carter
Lixin Zhang

Mootaz Elnozahy
Cliff Mercer

David S. Johnson
Peter Keleher


=== Other ===
Harold Stone (computer scientist)
Harold N. (Hal) Gabow
Matthias Stallmann
Manfred K. Warmuth
Yoav Freund

Franco P. Preparata
Roberto Tamassia

Georg Kreisel
Richard Statman

Herbert A. Simon
Allen Newell
Robert Kendall Lindsay
Terrence Wendall Pratt
Daniel Paul Friedman
Matthias Felleisen
Shriram Krishnamurthi

Charles Bachman
Edwin Boring
Cooper Harold Langford
Arthur Burks
John Henry Holland
Kenneth A De Jong
Edgar F. Codd
Stephen T. Hedetniemi (Primary advisor: Frank Harary)
Donald F. Stanat
Jon Bentley
Charles Leiserson (Primary advisor: Hsiang-Tsung Kung)
Guy Blelloch
Thomas H. Cormen

Gul Agha (Secondary advisor: Carl Hewitt)

Robert ""Bob"" Allen Paige
Friedrich ""Fritz"" Henglein

Carl Gustav Hempel
John Alan Robinson


== See also ==
List of computer scientists


== References ==


== Further reading ==
Johnson, David S. (Summer 1984). ""The genealogy of theoretical computer science: a preliminary report"". ACM SIGACT News. 16 (2): 36–49. doi:10.1145/1008959.1008960. 
Parberry, Ian; Johnson, David S. (June 1995). James Ford; Fillia Makedon; Samuel Rebelsky, eds. ""The SIGACT Theoretical Computer Science Genealogy: Preliminary Report"" (PDF). Proceedings of DAGS 95, ""Electronic Publishing and the Information Superhighway"". Boston, MA: Birkhauser: 197–205. 
Coonce, Harry B. (December 2004). ""Computer science and the mathematics genealogy project"". ACM SIGACT News. 35 (4). 


== External links ==
Software Engineering Academic Genealogy
SIGACT Theoretical Computer Science Genealogy (archived on 13 October 2007)
Mathematics Genealogy Project
AI Genealogy Project
Computer Engineering Academic Genealogy by Yuan Xie, Pennsylvania State University"
14,List of computer scientists,6834,39023,"This is a list of computer scientists, people who do work in computer science, in particular researchers and authors.
Some persons notable as programmers are included here because they work in research as well as program. A few of these people pre-date the invention of the digital computer; they are now regarded as computer scientists because their work can be seen as leading to the invention of the computer. Others are mathematicians whose work falls within what would now be called theoretical computer science, such as complexity theory and algorithmic information theory.


== A ==
Wil van der Aalst – business process management, process mining, Petri nets
Scott Aaronson – quantum computing and complexity theory
Hal Abelson – intersection of computing and teaching
Serge Abiteboul – database theory
Samson Abramsky – game semantics
Leonard Adleman – RSA, DNA computing
Manindra Agrawal – polynomial-time primality testing
Luis von Ahn – human-based computation
Alfred Aho – compilers book, the 'a' in AWK
Frances E. Allen – compiler optimization
Gene Amdahl – supercomputer developer, founder of Amdahl Corporation
David P. Anderson – volunteer computing
Andrew Appel – compiler of text books
Cecilia R. Aragon – inventor of the treap, human-centered data science
Bruce Arden – programming language compilers (GAT, MAD), virtual memory architecture, MTS
Sanjeev Arora – PCP theorem
Winifred ""Tim"" Alice Asprey – established the computer science curriculum at Vassar College
John Vincent Atanasoff – computer pioneer, creator of ABC or Atanasoff Berry Computer


== B ==
Charles Babbage (1791–1871) – invented first mechanical computer called the supreme mathematician
Charles Bachman – American computer scientist, known for Integrated Data Store
Roland Carl Backhouse – mathematics of program construction
John Backus – FORTRAN, Backus–Naur form, first complete compiler
David F. Bacon – Programming languages, garbage collection
David A. Bader
Victor Bahl
Anthony James Barr – SAS System
Jean Bartik (1924–2011) – one of the first computer programmers, on ENIAC (1946), one of the first Vacuum tube computers, back when ""programming"" involved using cables, dials, and switches to physically rewire the machine; worked with John Mauchly toward BINAC (1949), EDVAC (1949), UNIVAC (1951) to develop early ""stored program"" computers
Andrew Barto
Rudolf Bayer – B-tree
James C. Beatty (1934–1978) – compiler optimization, super-computing
Gordon Bell (born 1934) – computer designer DEC VAX, author: Computer Structures
Steven M. Bellovin – network security
Tim Berners-Lee – World Wide Web
Daniel J. Bernstein – qmail, software as protected speech
Peter Bernus
Abhay Bhushan
Dines Bjørner – Vienna Development Method (VDM), RAISE
Gerrit Blaauw – one of the principal designers of the IBM System 360 line of computers
Sue Black
David Blei
Dorothy Blum – National Security Agency
Lenore Blum – complexity
Manuel Blum – cryptography
Barry Boehm – software engineering economics, spiral development
Corrado Bohm – author of the structured program theorem
Kurt Bollacker
Jeff Bonwick – inventor of slab allocation and ZFS
Grady Booch – Unified Modeling Language, Object Management Group
George Boole – Boolean logic
Andrew Booth – developed the first rotating drum storage device
Kathleen Booth – developed the first assembly language
Anita Borg (1949–2003) – American computer scientist, founder of Anita Borg Institute for Women and Technology
Bert Bos – Cascading Style Sheets
Mikhail Botvinnik – World Chess Champion, computer scientist and electrical engineer, pioneer of early expert system AI,inventor of Computer chess
Jonathan Bowen – Z notation, formal methods
Stephen R. Bourne – Bourne shell, portable ALGOL 68C compiler
Harry Bouwman (born 1953) – Dutch Information systems researcher, and Professor at the Åbo Akademi University
Robert S. Boyer – string searching, ACL2 theorem prover
Karlheinz Brandenburg – Main mp3 contributor
Jack E. Bresenham – early computer-graphics contributions, including Bresenham's algorithm
Sergey Brin – co-founder of Google
David J. Brown – unified memory architecture, binary compatibility
Per Brinch Hansen (surname ""Brinch Hansen"") – concurrency
Sjaak Brinkkemper – methodology of product software development
Fred Brooks – System 360, OS/360, The Mythical Man-Month, No Silver Bullet
Rod Brooks
Michael Butler – Event-B


== C ==
Lee Calcote – cloud computing
Tracy Camp – wireless computing
Martin Campbell-Kelly – history of computing
Rosemary Candlin
Bryan Cantrill – inventor of DTrace
Luca Cardelli – objects
Edwin Catmull – computer graphics
Vinton Cerf – Internet, TCP/IP
Gregory Chaitin
Zhou Chaochen – duration calculus
Peter Chen – entity-relationship model, data modeling, conceptual model
Leonardo Chiariglione, founder of MPEG
Alonzo Church – mathematics of combinators, lambda calculus
Alberto Ciaramella - speech recognition, patent informatics
Edmund M. Clarke – model checking
John Cocke – RISC
Edgar F. Codd (1923–2003) – formulated the database relational model
Jacques Cohen – computer science professor
Simon Colton – computational creativity
Alain Colmerauer – Prolog
Paul Justin Compton – Ripple Down Rules
Gordon Cormack – co-inventor of dynamic Markov compression
Stephen Cook – NP-completeness
James Cooley – Fast Fourier transform (FFT)
Danese Cooper – Open Source Software
Fernando J. Corbató – Compatible Time-Sharing System (CTSS), Multics
Kit Cosper - Open Source Software
Patrick Cousot – abstract interpretation
Ingemar Cox – digital watermarking
Seymour Cray – Cray Research, supercomputer
Nello Cristianini – machine learning, pattern analysis, artificial intelligence
Jon Crowcroft – networking
W. Bruce Croft
Glen Culler – interactive computing, computer graphics, high performance computing
Haskell Curry


== D ==
Luigi Dadda – designer of the Dadda multiplier
Ole-Johan Dahl – Simula
Ryan Dahl – founder of node.js project
Andries van Dam – computer graphics, hypertext
Samir Das – Wireless Networks, Mobile Computing, Vehicular ad hoc network, Sensor Networks, Mesh networking, Wireless ad hoc network
Christopher J. Date – proponent of database relational model
Jeff Dean – Bigtable, MapReduce, Spanner of Google
Erik Demaine – computational origami
Tom DeMarco
Richard DeMillo – computer security, software engineering, educational technology
Dorothy E. Denning – computer security
Peter J. Denning – identified the use of an operating system's working set and balance set, President of ACM
Michael Dertouzos – Director of Massachusetts Institute of Technology (MIT) Laboratory for Computer Science (LCS) from 1974 to 2001
Alexander Dewdney
Vinod Dham – P5 Pentium processor
Jan Dietz (born 1945) (decay constant) – information systems theory and Design & Engineering Methodology for Organizations
Whitfield Diffie (born 1944) (linear response function) – public key cryptography, Diffie–Hellman key exchange
Edsger Dijkstra – algorithms, Goto considered harmful, semaphore (programming)
Alan Dix – literally wrote the book on human–computer interaction
Jack Dongarra – linear algebra high performance computing (HCI)
Marco Dorigo – ant colony optimization
Paul Dourish – human computer interaction
Charles Stark Draper (1901–1987) – designer of Apollo Guidance Computer, ""father of inertial navigation"", MIT professor
Susan Dumais – information retrieval


== E ==
Peter Eades – graph drawing
Annie J. Easley
Wim Ebbinkhuijsen – COBOL
John Presper Eckert – ENIAC
Brendan Eich – JavaScript, Mozilla
Philip Emeagwali – supercomputing
E. Allen Emerson – model checking
Douglas Engelbart – tiled windows, hypertext, computer mouse
David Eppstein
Andrey Ershov
Don Estridge (1937–1985) – led development of original IBM Personal Computer (PC); known as ""father of the IBM PC""
Oren Etzioni – MetaCrawler, Netbot
Christopher Riche Evans
David C. Evans – computer graphics
Shimon Even


== F ==
Scott Fahlman
Edward Feigenbaum – intelligence
Edward Felten – computer security
Tim Finin
Raphael Finkel
Donald Firesmith
Gary William Flake
Tommy Flowers – Colossus computer
Robert Floyd – NP-completeness
Sally Floyd – Internet congestion control
Lawrence J. Fogel – Evolutionary programming
James D. Foley
Ken Forbus
Lance Fortnow
Martin Fowler
Herbert W. Franke
Edward Fredkin
Yoav Freund
Daniel P. Friedman
Ping Fu


== G ==
Richard Gabriel
V. K. Govindan
Zvi Galil
Bernard Galler – MAD (programming language)
Hector Garcia-Molina
Michael Garey – NP-completeness
Hugo de Garis
Bill Gates – co-founder of Microsoft
David Gelernter
Charles Geschke
Zoubin Ghahramani
Sanjay Ghemawat
Juan E. Gilbert – Human-Centered Computing
Lee Giles – CiteSeer
Seymour Ginsburg – formal languages, automata theory, AFL theory, database theory
Robert L. Glass
Kurt Gödel – computability – not a computer scientist per se, but his work was invaluable in the field
Joseph Goguen
Adele Goldberg – Smalltalk
Andrew V. Goldberg -- algorithms, algorithm engineering
Ian Goldberg – cryptographer, off-the-record messaging
Oded Goldreich – cryptography, computational complexity theory
Shafi Goldwasser – cryptography, computational complexity theory
Gene Golub – Matrix computation
Martin Charles Golumbic – algorithmic graph theory
Gastón Gonnet – co-founder of Waterloo Maple Inc.
James Gosling – NeWS, Java
Paul Graham – Viaweb, On Lisp, Arc
Robert M. Graham – programming language compilers (GAT, MAD), virtual memory architecture, Multics
Susan L. Graham – compilers, programming environments
Jim Gray – database
Sheila Greibach – Greibach normal form, AFL theory
Ralph Griswold – SNOBOL
Bill Gropp – Message Passing Interface, PETSc
Tom Gruber
Ramanathan V. Guha – RDF, Netscape, RSS, Epinions
Neil J. Gunther – computer performance analysis, capacity planning
Peter G. Gyarmati – adaptivity in operating systems and networking


== H ==
Philipp Matthäus Hahn – mechanical calculator
Eldon C. Hall – Apollo Guidance Computer
Wendy Hall
Joseph Halpern
Margaret Hamilton – ultra-reliable software design
Richard Hamming – Hamming code, founder of the Association for Computing Machinery
Jiawei Han – data mining
Juris Hartmanis – computational complexity theory
Johan Håstad – computational complexity theory
Les Hatton – software failure and vulnerabilities
Igor Hawryszkiewycz, (born 1948), American computer scientist and organizational theorist
He Jifeng – provably correct systems
Eric Hehner – predicative programming, formal methods, quote notation
Martin Hellman – encryption
Gernot Heiser – development of L4 and founder of OK Labs
James Hendler – Semantic Web
John L. Hennessy – computer architecture
Andrew Herbert
Carl Hewitt
Danny Hillis – Connection Machine
Geoffrey Hinton
Julia Hirschberg
C. A. R. Hoare – logic, rigor, Communicating sequential processes (CSP)
Betty Holberton – ENIAC programmer, developed the first Sort Merge Generator
John Henry Holland – genetic algorithms
Herman Hollerith (1860–1929) – invented recording of data on a machine readable medium, using punched cards
Gerard Holzmann – software verification, logic model checking (SPIN)
John Hopcroft – compilers
Admiral Grace Hopper (1906–1992) – developed early compilers: FLOW-Matic, COBOL; worked on UNIVAC; gave speeches on computer history, where she gave out nano-seconds
Eric Horvitz – artificial intelligence
Alston Householder
Paul Hudak (1952–2015) – Haskell programming language design
David A. Huffman (1925–1999) – Huffman coding, used in data compression
John Hughes – structuring computations with arrows; QuickCheck randomized program testing framework; Haskell programming language design.
Watts Humphrey (1927–2010) – Personal Software Process (PSP), Software quality, Team Software Process (TSP)


== I ==
Jean Ichbiah – Ada
Dan Ingalls – Smalltalk, BitBlt, Lively Kernel
Mary Jane Irwin
Kenneth E. Iverson – APL, J


== J ==
Ivar Jacobson – Unified Modeling Language, Object Management Group
Anil K. Jain (born 1948)
Ramesh Jain
Jonathan James
David S. Johnson
Stephen C. Johnson
Cliff Jones – Vienna Development Method (VDM)
Michael I. Jordan
Mathai Joseph
Aravind K. Joshi
Bill Joy (born 1954) – Sun Microsystems, BSD UNIX, vi, csh
Dan Jurafsky – Natural language processing


== K ==
William Kahan – numerical analysis
Robert E. Kahn – TCP/IP
Avinash Kak – digital image processing
Poul-Henning Kamp – inventor of GBDE, FreeBSD Jails, Varnish cache
David Karger
Richard Karp – NP-completeness
Narendra Karmarkar – Karmarkar's algorithm
Marek Karpinski – NP optimization problems
Alan Kay – Dynabook, Smalltalk, overlapping windows
Neeraj Kayal – AKS primality test
John George Kemeny – BASIC
Ken Kennedy – compiling for parallel and vector machines
Brian Kernighan (born 1942) – Unix, the 'k' in AWK
Carl Kesselman – grid computing
Gregor Kiczales – CLOS, reflection, aspect-oriented programming
Peter T. Kirstein – Internet
Stephen Cole Kleene – Kleene closure, recursion theory
Dan Klein – Natural language processing, Machine translation
Leonard Kleinrock – ARPANET, queueing theory, packet switching, hierarchical routing
Donald Knuth – The Art of Computer Programming, MIX/MMIX, TeX, literate programming
Andrew Koenig – C++
Daphne Koller – Artificial intelligence, bayesian network
Michael Kölling – BlueJ
Andrey Nikolaevich Kolmogorov – algorithmic complexity theory
Janet L. Kolodner – case-based reasoning
David Korn – Korn shell
Kees Koster – ALGOL 68
Robert Kowalski – logic programming
John Koza – genetic programming
John Krogstie – SEQUAL framework
Joseph Kruskal – Kruskal's algorithm
Thomas E. Kurtz (born 1928) – BASIC programming language; Dartmouth College computer professor


== L ==
Monica S. Lam
Leslie Lamport – algorithms for distributed computing, LaTeX
Butler W. Lampson
Peter J. Landin
Tom Lane
Börje Langefors
Chris Lattner – creator of Swift (programming language) and LLVM compiler infrastructure
Steve Lawrence
Edward D. Lazowska
Joshua Lederberg
Manny M Lehman
Charles E. Leiserson – cache-oblivious algorithms, provably good work-stealing, coauthor of Introduction to Algorithms
Douglas Lenat – artificial intelligence, Cyc
Yann LeCun
Rasmus Lerdorf – PHP
Max Levchin – Gausebeck-Levchin test and PayPal
Leonid Levin – computational complexity theory
Kevin Leyton-Brown – artificial intelligence
J.C.R. Licklider
David Liddle
John Lions – Lions Book
Richard J. Lipton – computational complexity theory
Barbara Liskov – programming languages
Darrell Long – Computer data storage
Patricia D. Lopez – broadening participation in computing
Gillian Lovegrove
Ada Lovelace – first programmer
Eugene Luks
Nancy Lynch


== M ==
Nadia Magnenat Thalmann – computer graphics, virtual actor
Tom Maibaum
Zohar Manna – fuzzy logic
James Martin – information engineering
Robert C. Martin (Uncle Bob) – software craftsmanship
John Mashey
Yuri Matiyasevich – solving Hilbert's tenth problem
Yukihiro Matsumoto – Ruby (programming language)
John Mauchly (1907–1980) – designed ENIAC, first general-purpose electronic digital computer, as well as EDVAC, BINAC and UNIVAC I, the first commercial computer; worked with Jean Bartik on ENIAC and Grace Murray Hopper on UNIVAC
Derek McAuley – ubiquitous computing, computer architecture, networking
John McCarthy – Lisp (programming language), artificial intelligence
Andrew McCallum
Douglas McIlroy – pipes
Chris McKinstry – artificial intelligence, Mindpixel
Marshall Kirk McKusick – BSD, Berkeley Fast File System
Lambert Meertens – ALGOL 68, ABC (programming language)
Bertrand Meyer – Eiffel (programming language)
Silvio Micali – cryptography
Robin Milner – ML (programming language)
Jack Minker – database logic
Marvin Minsky – artificial intelligence, perceptrons, Society of Mind
Tom M. Mitchell
Paul Mockapetris – Domain Name System (DNS)
Cleve Moler – numerical analysis, MATLAB
John P. Moon – inventor, Apple Inc.
Charles H. Moore – Forth programming language
Edward F. Moore – Moore machine
Gordon Moore – Moore's law
J Strother Moore – string searching, ACL2 theorem prover
Hans Moravec – robotics
Carroll Morgan
Robert Tappan Morris – Morris worm
Joel Moses – Macsyma
Rajeev Motwani – randomized algorithm
Stephen Muggleton – Inductive Logic Programming
Alan Mycroft – programming languages


== N ==
Mihai Nadin – anticipation research
Makoto Nagao – machine translation, natural language processing, digital library
Frieder Nake – pioneered computer arts
Peter Naur – BNF, ALGOL 60
Roger Needham – computer security
James G. Nell – GERAM
Bernard de Neumann – massively parallel autonomous cellular processor, software engineering research
Klara Dan von Neumann (1911-1963) – early computers, ENIAC programmer and control designer
John von Neumann (1903–1957) – early computers, von Neumann machine, set theory, functional analysis, mathematics pioneer, linear programming, quantum mechanics
Allen Newell – artificial intelligence, Computer Structures
Max Newman – Colossus, MADM
Andrew Ng – artificial intelligence, machine learning, robotics
Nils Nilsson – artificial intelligence
G.M. Nijssen – NIAM
Tobias Nipkow – proof assistance
Jerre Noe – computerized banking
Peter Nordin – artificial intelligence, genetic programming, evolutionary robotics
Donald Norman – user interfaces, usability
Peter Norvig – artificial intelligence, Director of Research at Google
George Novacky – Assistant Department Chair and Senior Lecturer in Computer Science, Assistant Dean of CAS for Undergraduate Studies at University of Pittsburgh
Kristen Nygaard – Simula


== O ==
T. William Olle – Ferranti Mercury
Steve Omohundro
John Ousterhout – Tcl programming Language
Mark Overmars – game programming
Martin Odersky – Scala programming Language
Severo Ornstein
John O'Sullivan- wifi


== P ==
Larry Page – co-founder of Google
Sankar Pal
Paritosh Pandya
Christos Papadimitriou
David Parnas – information hiding, modular programming
Yale Patt – Instruction-level parallelism, speculative architectures
David A. Patterson
Mihai Pătraşcu – data structures
Lawrence Paulson – ML
Randy Pausch (1960–2008) – Human-Computer interaction, Carnegie professor, ""Last Lecture""
Juan Pavón – software agents
Judea Pearl – artificial intelligence, search algorithms
David Pearson – CADES, computer graphics
Alan Perlis – Programming Pearls
Radia Perlman – spanning tree protocol
Pier Giorgio Perotto – designer of Programma 101, arguably the first personal computer
Rózsa Péter – recursive function theory
Simon Peyton Jones – functional programming
Roberto Pieraccini – Speech technologist, technical director at Jibo Inc.
Gordon Plotkin
Amir Pnueli – temporal logic
Willem van der Poel – computer graphics, robotics, geographic information systems, imaging, multimedia, virtual environments, games
Emil Post – mathematics
Jon Postel – Internet
Franco Preparata – computer engineering, computational geometry, parallel algorithms, computational biology
William H. Press – numerical algorithms


== R ==
Rapelang Rabana
Roberto Ierusalimschy – Lua (programming language)
Michael O. Rabin – nondeterministic machine
Dragomir R. Radev – Natural language processing, Information Retrieval
T. V. Raman – accessibility, Emacspeak
Brian Randell – dependability
Raj Reddy – AI
David P. Reed
Trygve Reenskaug – Model-view-controller (MVC) software architecture pattern
John C. Reynolds
Joyce K. Reynolds – Internet
Bernard Richards – medical informatics
Martin Richards – BCPL
Adam Riese
C. J. van Rijsbergen
Dennis Ritchie – C (programming language), UNIX
Ron Rivest – RSA, MD5, RC4
Colette Rolland – REMORA methodology, meta modelling
Azriel Rosenfeld
Douglas T. Ross – Structured Analysis and Design Technique
Guido van Rossum – Python (programming language)
Winston W. Royce – Waterfall model
Rudy Rucker – mathematician, writer, educator
Steven Rudich – complexity theory, cryptography
Jeff Rulifson
James Rumbaugh – Unified Modeling Language, Object Management Group
Peter Ružička – Slovak computer scientist and mathematician


== S ==
George Sadowsky
Gerard Salton – information retrieval
Jean E. Sammet – programming languages
Claude Sammut – artificial-intelligence researcher
Carl Sassenrath – operating systems, programming languages, Amiga, REBOL
Mahadev Satyanarayanan – file systems, distributed systems, mobile computing, pervasive computing
Walter Savitch – discovery of complexity class NL, Savitch's theorem, natural language processing, mathematical linguistics
Jonathan Schaeffer
Wilhelm Schickard – one of the first calculating machines
Steve Schneider – formal methods, security
Bruce Schneier – cryptography, security
Fred B. Schneider – concurrent and distributed computing
Dana Scott – domain theory
Michael L. Scott – programming languages, algorithms, distributed computing
Ravi Sethi – compilers, 2nd Dragon Book
Nigel Shadbolt
Adi Shamir – RSA, cryptanalysis
Claude Shannon – information theory
David E. Shaw – computational finance, computational biochemistry, parallel architectures
Cliff Shaw – systems programmer, artificial intelligence
Scott Shenker – networking
Ben Shneiderman – human-computer interaction, information visualization
Edward H. Shortliffe – MYCIN (medical diagnostic expert system)
Joseph Sifakis – model checking
Herbert A. Simon – artificial intelligence
Munindar P. Singh – multiagent systems, software engineering, artificial intelligence, social networks
Ramesh Sitaraman – helped build Akamai's high performance network
Daniel Sleator – splay tree, amortized analysis
Aaron Sloman – artificial intelligence and cognitive science
Arne Sølvberg – information modelling
Brian Cantwell Smith – reflection (computer science), 3lisp
Steven Spewak – Enterprise architecture planning
Carol Spradling
Robert Sproull
Rohini Kesavan Srihari – Information Retrieval, Text Analytics, Multilingual Text Mining
Sargur Srihari – Pattern Recognition, Machine learning, Computational criminology, CEDAR-FOX
Maciej Stachowiak – GNOME, Safari, WebKit
Richard Stallman (born 1953) – GNU Project
Ronald Stamper
Richard E. Stearns – computational complexity theory
Guy L. Steele, Jr. – Scheme, Common Lisp
Thomas Sterling – creator of Beowulf clusters
W. Richard Stevens (1951–1999) – author of books, including TCP/IP Illustrated and Advanced Programming in the Unix Environment
Larry Stockmeyer – computational complexity, distributed computing
Salvatore Stolfo - computer security, machine learning
Michael Stonebraker – relational database practice and theory
Olaf Storaasli – finite element machine, linear algebra, high performance computing
Christopher Strachey – denotational semantics
Bjarne Stroustrup – C++
Madhu Sudan – computational complexity theory, coding theory
Gerald Jay Sussman – Scheme
Bert Sutherland – graphics, Internet
Ivan Sutherland – graphics
Mario Szegedy – complexity theory, quantum computing


== T ==
Roberto Tamassia – computational geometry, computer security
Andrew S. Tanenbaum – operating systems, MINIX
Bernhard Thalheim – conceptual modelling foundation
Éva Tardos
Gábor Tardos
Robert Tarjan – splay tree
Valerie Taylor
Mario Tchou – italian engineer, of Chinese descent, leader of Olivetti Elea project
Jaime Teevan
Shang-Hua Teng – analysis of algorithms
Larry Tesler – human-computer interaction, graphical user interface, Apple Macintosh
Avie Tevanian – Mach kernel team, NeXT, Mac OS X
Charles P. Thacker – Xerox Alto, Microsoft Research
Daniel Thalmann – computer graphics, virtual actor
Ken Thompson – Unix
Sebastian Thrun – AI researcher and inventor of autonomous driving
Walter F. Tichy – RCS
Seinosuke Toda – computation complexity, recipient of 1998 Gödel Prize
Linus Torvalds – Linux kernel, Git
Godfried Toussaint – computational geometry – computational music theory
Gloria Townsend
Edwin E. Tozer – business information systems
Joseph F Traub – computational complexity of scientific problems
John Tukey – founder of FFT algorithm, Box plot, Exploratory Data Analysis and Coining the term 'bit'
Murray Turoff – computer-mediated communication
Alan Turing (1912–1954) – British computing pioneer, Turing machine, algorithms, cryptology, computer architecture


== U ==
Jeffrey D. Ullman – compilers, databases, complexity theory
Umar Saif


== V ==
Leslie Valiant – computational complexity theory, computational learning theory
Vladimir Vapnik – pattern recognition, computational learning theory
Moshe Vardi – professor of computer science at Rice University
Dorothy Vaughan
Umesh Vazirani
Vijay Vazirani
Manuela M. Veloso
François Vernadat – enterprise modeling
Richard Veryard – enterprise modeling
Paul Vitanyi – Kolmogorov complexity, Information distance, Normalized compression distance, Normalized Google distance
Andrew Viterbi – Viterbi algorithm
Jeffrey Scott Vitter – external memory algorithms, compressed data structures, data compression, databases
Paul Vixie – DNS, BIND, PAIX, Internet Software Consortium, MAPS, DNSBL


== W ==
David Wagner – security, cryptography
Larry Wall – Perl programming language
David Waltz
James Z. Wang
Steve Ward
Manfred K. Warmuth – computational learning theory
David H. D. Warren – AI, logic programming, Prolog, the 'w' in WAM
Kevin Warwick – artificial intelligence
Jan Weglarz
Peter Wegner – object-oriented programming, interaction (computer science)
Peter J. Weinberger – programming language design, the 'w' in AWK
Mark Weiser – ubiquitous computing
Joseph Weizenbaum – artificial intelligence, ELIZA
David Wheeler – EDSAC, subroutines
Franklin H. Westervelt – use of computers in engineering education, conversational use of computers, MTS, ARPANET, distance learning
Steve Whittaker – human computer interaction, computer support for cooperative work, social media
Jennifer Widom – nontraditional data management
Gio Wiederhold – database management systems
Norbert Wiener – Cybernetics
Adriaan van Wijngaarden – Dutch pioneer; ARRA, ALGOL
Mary Allen Wilkes – LINC developer, assembler-linker designer
Maurice Vincent Wilkes – microprogramming, EDSAC
Yorick Wilks – computational linguistics, artificial intelligence
James H. Wilkinson – numerical analysis
Sophie Wilson – ARM architecture
Shmuel Winograd – Coppersmith–Winograd algorithm
Terry Winograd – artificial intelligence, SHRDLU
Patrick Winston – artificial intelligence
Niklaus Wirth – Pascal, Modula, Oberon (programming language)
Neil Wiseman – computer graphics
Dennis E. Wisnosky – Integrated Computer-Aided Manufacturing (ICAM), IDEF
Stephen Wolfram – Mathematica
Mike Woodger – Pilot ACE, ALGOL 60, Ada (programming language)
Beatrice Helen Worsley – wrote the first PhD dissertation involving modern computers; was one of the people who wrote Transcode
Steve Wozniak – engineered first generation personal computers at Apple Computer
Jie Wu – computer networks
William Wulf – compilers


== Y ==
Mihalis Yannakakis
Andrew Chi-Chih Yao
John Yen
Edward Yourdon – Structured Systems Analysis and Design Method
Moti Yung
Yash Khalkar


== Z ==
Lotfi Zadeh – fuzzy logic
Hans Zantema – termination analysis
Arif Zaman – pseudo-random number generator
Shlomo Zilberstein – artificial intelligence, anytime algorithms, automated planning, and decentralized POMDPs
Jill Zimmerman – James M. Beall Professor of Mathematics and Computer Science at Goucher College
Konrad Zuse – German pioneer of hardware and software
Mark Zuckerberg – Founder of Facebook


== See also ==


== References ==


== External links ==
CiteSeer list of the most cited authors in computer science
Computer scientists with h-index >= 40"
15,Computational linguistics,5561,38820,"Computational linguistics is an interdisciplinary field concerned with the statistical or rule-based modeling of natural language from a computational perspective, as well as the study of appropriate computational approaches to linguistic questions.
Traditionally, computational linguistics was performed by computer scientists who had specialized in the application of computers to the processing of a natural language. Today, computational linguists often work as members of interdisciplinary teams, which can include regular linguists, experts in the target language, and computer scientists. In general, computational linguistics draws upon the involvement of linguists, computer scientists, experts in artificial intelligence, mathematicians, logicians, philosophers, cognitive scientists, cognitive psychologists, psycholinguists, anthropologists and neuroscientists, among others.
Computational linguistics has theoretical and applied components. Theoretical computational linguistics focuses on issues in theoretical linguistics and cognitive science, and applied computational linguistics focuses on the practical outcome of modeling human language use.
The Association for Computational Linguistics defines computational linguistics as:
...the scientific study of language from a computational perspective. Computational linguists are interested in providing computational models of various kinds of linguistic phenomena.


== Origins ==
Computational linguistics is often grouped within the field of artificial intelligence, but actually was present before the development of artificial intelligence. Computational linguistics originated with efforts in the United States in the 1950s to use computers to automatically translate texts from foreign languages, particularly Russian scientific journals, into English. Since computers can make arithmetic calculations much faster and more accurately than humans, it was thought to be only a short matter of time before they could also begin to process language. Computational and quantitative methods are also used historically in attempted reconstruction of earlier forms of modern languages and subgrouping modern languages into language families. Earlier methods such as lexicostatistics and glottochronology have been proven to be premature and inaccurate. However, recent interdisciplinary studies which borrow concepts from biological studies, especially gene mapping, have proved to produce more sophisticated analytical tools and more trustful results.
When machine translation (also known as mechanical translation) failed to yield accurate translations right away, automated processing of human languages was recognized as far more complex than had originally been assumed. Computational linguistics was born as the name of the new field of study devoted to developing algorithms and software for intelligently processing language data. The term ""computational linguistics"" itself was first coined by David Hays, founding member of both the Association for Computational Linguistics and the International Committee on Computational Linguistics. When artificial intelligence came into existence in the 1960s, the field of computational linguistics became that sub-division of artificial intelligence dealing with human-level comprehension and production of natural languages.
In order to translate one language into another, it was observed that one had to understand the grammar of both languages, including both morphology (the grammar of word forms) and syntax (the grammar of sentence structure). In order to understand syntax, one had to also understand the semantics and the lexicon (or 'vocabulary'), and even something of the pragmatics of language use. Thus, what started as an effort to translate between languages evolved into an entire discipline devoted to understanding how to represent and process natural languages using computers.
Nowadays research within the scope of computational linguistics is done at computational linguistics departments, computational linguistics laboratories, computer science departments, and linguistics departments. Some research in the field of computational linguistics aims to create working speech or text processing systems while others aim to create a system allowing human-machine interaction. Programs meant for human-machine communication are called conversational agents.


== Approaches ==
Just as computational linguistics can be performed by experts in a variety of fields and through a wide assortment of departments, so too can the research fields broach a diverse range of topics. The following sections discuss some of the literature available across the entire field broken into four main area of discourse: developmental linguistics, structural linguistics, linguistic production, and linguistic comprehension.


=== Developmental approaches ===
Language is a cognitive skill which develops throughout the life of an individual. This developmental process has been examined using a number of techniques, and a computational approach is one of them. Human language development does provide some constraints which make it harder to apply a computational method to understanding it. For instance, during language acquisition, human children are largely only exposed to positive evidence. This means that during the linguistic development of an individual, only evidence for what is a correct form is provided, and not evidence for what is not correct. This is insufficient information for a simple hypothesis testing procedure for information as complex as language, and so provides certain boundaries for a computational approach to modeling language development and acquisition in an individual.
Attempts have been made to model the developmental process of language acquisition in children from a computational angle, leading to both statistical grammars and connectionist models. Work in this realm has also been proposed as a method to explain the evolution of language through history. Using models, it has been shown that languages can be learned with a combination of simple input presented incrementally as the child develops better memory and longer attention span. This was simultaneously posed as a reason for the long developmental period of human children. Both conclusions were drawn because of the strength of the neural network which the project created.
The ability of infants to develop language has also been modeled using robots in order to test linguistic theories. Enabled to learn as children might, a model was created based on an affordance model in which mappings between actions, perceptions, and effects were created and linked to spoken words. Crucially, these robots were able to acquire functioning word-to-meaning mappings without needing grammatical structure, vastly simplifying the learning process and shedding light on information which furthers the current understanding of linguistic development. It is important to note that this information could only have been empirically tested using a computational approach.
As our understanding of the linguistic development of an individual within a lifetime is continually improved using neural networks and learning robotic systems, it is also important to keep in mind that languages themselves change and develop through time. Computational approaches to understanding this phenomenon have unearthed very interesting information. Using the Price Equation and Pólya urn dynamics, researchers have created a system which not only predicts future linguistic evolution, but also gives insight into the evolutionary history of modern-day languages. This modeling effort achieved, through computational linguistics, what would otherwise have been impossible.
It is clear that the understanding of linguistic development in humans as well as throughout evolutionary time has been fantastically improved because of advances in computational linguistics. The ability to model and modify systems at will affords science an ethical method of testing hypotheses that would otherwise be intractable.


=== Structural approaches ===
In order to create better computational models of language, an understanding of language’s structure is crucial. To this end, the English language has been meticulously studied using computational approaches to better understand how the language works on a structural level. One of the most important pieces of being able to study linguistic structure is the availability of large linguistic corpora, or samples. This grants computational linguists the raw data necessary to run their models and gain a better understanding of the underlying structures present in the vast amount of data which is contained in any single language. One of the most cited English linguistic corpora is the Penn Treebank. Derived from widely-different sources, such as IBM computer manuals and transcribed telephone conversations, this corpus contains over 4.5 million words of American English. This corpus has been primarily annotated using part-of-speech tagging and syntactic bracketing and has yielded substantial empirical observations related to language structure.
Theoretical approaches to the structure of languages have also been developed. These works allow computational linguistics to have a framework within which to work out hypotheses that will further the understanding of the language in a myriad of ways. One of the original theoretical theses on internalization of grammar and structure of language proposed two types of models. In these models, rules or patterns learned increase in strength with the frequency of their encounter. The work also created a question for computational linguists to answer: how does an infant learn a specific and non-normal grammar (Chomsky Normal Form) without learning an overgeneralized version and getting stuck? Theoretical efforts like these set the direction for research to go early in the lifetime of a field of study, and are crucial to the growth of the field.
Structural information about languages allows for the discovery and implementation of similarity recognition between pairs of text utterances. For instance, it has recently been proven that based on the structural information present in patterns of human discourse, conceptual recurrence plots can be used to model and visualize trends in data and create reliable measures of similarity between natural textual utterances. This technique is a strong tool for further probing the structure of human discourse. Without the computational approach to this question, the vastly complex information present in discourse data would have remained inaccessible to scientists.
Information regarding the structural data of a language is available for English as well as other languages, such as Japanese. Using computational methods, Japanese sentence corpora were analyzed and a pattern of log-normality was found in relation to sentence length. Though the exact cause of this lognormality remains unknown, it is precisely this sort of intriguing information which computational linguistics is designed to uncover. This information could lead to further important discoveries regarding the underlying structure of Japanese, and could have any number of effects on the understanding of Japanese as a language. Computational linguistics allows for very exciting additions to the scientific knowledge base to happen quickly and with very little room for doubt.
Without a computational approach to the structure of linguistic data, much of the information that is available now would still be hidden under the vastness of data within any single language. Computational linguistics allows scientists to parse huge amounts of data reliably and efficiently, creating the possibility for discoveries unlike any seen in most other approaches.


=== Production approaches ===
The production of language is equally as complex in the information it provides and the necessary skills which a fluent producer must have. That is to say, comprehension is only half the problem of communication. The other half is how a system produces language, and computational linguistics has made some very interesting discoveries in this area.

In a now famous paper published in 1950 Alan Turing proposed the possibility that machines might one day have the ability to ""think"". As a thought experiment for what might define the concept of thought in machines, he proposed an ""imitation test"" in which a human subject has two text-only conversations, one with a fellow human and another with a machine attempting to respond like a human. Turing proposes that if the subject cannot tell the difference between the human and the machine, it may be concluded that the machine is capable of thought. Today this test is known as the Turing test and it remains an influential idea in the area of artificial intelligence.

One of the earliest and best known examples of a computer program designed to converse naturally with humans is the ELIZA program developed by Joseph Weizenbaum at MIT in 1966. The program emulated a Rogerian psychotherapist when responding to written statements and questions posed by a user. It appeared capable of understanding what was said to it and responding intelligently, but in truth it simply followed a pattern matching routine that relied on only understanding a few keywords in each sentence. Its responses were generated by recombining the unknown parts of the sentence around properly translated versions of the known words. For example, in the phrase ""It seems that you hate me"" ELIZA understands ""you"" and ""me"" which matches the general pattern ""you [some words] me"", allowing ELIZA to update the words ""you"" and ""me"" to ""I"" and ""you"" and replying ""What makes you think I hate you?"". In this example ELIZA has no understanding of the word ""hate"", but it is not required for a logical response in the context of this type of psychotherapy.
Some projects are still trying to solve the problem which first started computational linguistics off as its own field in the first place. However, the methods have become more refined and clever, and consequently the results generated by computational linguists have become more enlightening. In an effort to improve computer translation, several models have been compared, including hidden Markov models, smoothing techniques, and the specific refinements of those to apply them to verb translation. The model which was found to produce the most natural translations of German and French words was a refined alignment model with a first-order dependence and a fertility model[16]. They also provide efficient training algorithms for the models presented, which can give other scientists the ability to improve further on their results. This type of work is specific to computational linguistics, and has applications which could vastly improve understanding of how language is produced and comprehended by computers.
Work has also been done in making computers produce language in a more naturalistic manner. Using linguistic input from humans, algorithms have been constructed which are able to modify a system's style of production based on a factor such as linguistic input from a human, or more abstract factors like politeness or any of the five main dimensions of personality. This work takes a computational approach via parameter estimation models to categorize the vast array of linguistic styles we see across individuals and simplify it for a computer to work in the same way, making human-computer interaction much more natural.


==== Text-based interactive approach ====
Many of the earliest and simplest models of human-computer interaction, such as ELIZA for example, involve a text-based input from the user to generate a response from the computer. By this method, words typed by a user trigger the computer to recognize specific patterns and reply accordingly, through a process known as keyword spotting.


==== Speech-based interactive approach ====
Recent technologies have placed more of an emphasis on speech-based interactive systems. These systems, such as Siri of the iOS operating system, operate on a similar pattern-recognizing technique as that of text-based systems, but with the former, the user input is conducted through speech recognition. This branch of linguistics involves the processing of the user's speech as sound waves and the interpreting of the acoustics and language patterns in order for the computer to recognize the input.


=== Comprehension approaches ===
Much of the focus of modern computational linguistics is on comprehension. With the proliferation of the internet and the abundance of easily accessible written human language, the ability to create a program capable of understanding human language would have many broad and exciting possibilities, including improved search engines, automated customer service, and online education.
Early work in comprehension included applying Bayesian statistics to the task of optical character recognition, as illustrated by Bledsoe and Browing in 1959 in which a large dictionary of possible letters were generated by ""learning"" from example letters and then the probability that any one of those learned examples matched the new input was combined to make a final decision. Other attempts at applying Bayesian statistics to language analysis included the work of Mosteller and Wallace (1963) in which an analysis of the words used in The Federalist Papers was used to attempt to determine their authorship (concluding that Madison most likely authored the majority of the papers).
In 1971 Terry Winograd developed an early natural language processing engine capable of interpreting naturally written commands within a simple rule governed environment. The primary language parsing program in this project was called SHRDLU, which was capable of carrying out a somewhat natural conversation with the user giving it commands, but only within the scope of the toy environment designed for the task. This environment consisted of different shaped and colored blocks, and SHRDLU was capable of interpreting commands such as ""Find a block which is taller than the one you are holding and put it into the box."" and asking questions such as ""I don't understand which pyramid you mean."" in response to the user's input. While impressive, this kind of natural language processing has proven much more difficult outside the limited scope of the toy environment. Similarly a project developed by NASA called LUNAR was designed to provide answers to naturally written questions about the geological analysis of lunar rocks returned by the Apollo missions. These kinds of problems are referred to as question answering.
Initial attempts at understanding spoken language were based on work done in the 1960s and 1970s in signal modeling where an unknown signal is analyzed to look for patterns and to make predictions based on its history. An initial and somewhat successful approach to applying this kind of signal modeling to language was achieved with the use of hidden Markov models as detailed by Rabiner in 1989. This approach attempts to determine probabilities for the arbitrary number of models that could be being used in generating speech as well as modeling the probabilities for various words generated from each of these possible models. Similar approaches were employed in early speech recognition attempts starting in the late 70s at IBM using word/part-of-speech pair probabilities.
More recently these kinds of statistical approaches have been applied to more difficult tasks such as topic identification using Bayesian parameter estimation to infer topic probabilities in text documents.


== Applications ==
Modern computational linguistics is often a combination of studies in computer science and programming, math, particularly statistics, language structures, and natural language processing. Combined, these fields most often lead to the development of systems that can recognize speech and perform some task based on that speech. Examples include speech recognition software, such as Apple's Siri feature, spellcheck tools, speech synthesis programs, which are often used to demonstrate pronunciation or help the disabled, and machine translation programs and websites, such as Google Translate and Word Reference.
Computational linguistics can be especially helpful in situations involving social media and the Internet. For example, filters in chatrooms or on website searches require computational linguistics. Chat operators often use filters to identify certain words or phrases and deem them inappropriate so that users cannot submit them. Another example of using filters is on websites. Schools use filters so that websites with certain keywords are blocked from children to view. There are also many programs in which parents use Parental controls to put content filters in place. Computational linguists can also develop programs that group and organize content through Social media mining. An example of this is Twitter, in which programs can group tweets by subject or keywords. Computational linguistics is also used for document retrieval and clustering. When you do an online search, documents and websites are retrieved based on the frequency of unique labels related to what you typed into a search engine. For instance, if you search ""red, large, four-wheeled vehicle,"" with the intention of finding pictures of a red truck, the search engine will still find the information desired by matching words such as ""four-wheeled"" with ""car"".


== Subfields ==
Computational linguistics can be divided into major areas depending upon the medium of the language being processed, whether spoken or textual; and upon the task being performed, whether analyzing language (recognition) or synthesizing language (generation).
Speech recognition and speech synthesis deal with how spoken language can be understood or created using computers. Parsing and generation are sub-divisions of computational linguistics dealing respectively with taking language apart and putting it together. Machine translation remains the sub-division of computational linguistics dealing with having computers translate between languages. The possibility of automatic language translation, however, has yet to be realized and remains a notoriously hard branch of computational linguistics.
Some of the areas of research that are studied by computational linguistics include:
Computational complexity of natural language, largely modeled on automata theory, with the application of context-sensitive grammar and linearly bounded Turing machines.
Computational semantics comprises defining suitable logics for linguistic meaning representation, automatically constructing them and reasoning with them
Computer-aided corpus linguistics, which has been used since the 1970s as a way to make detailed advances in the field of discourse analysis
Design of parsers or chunkers for natural languages
Design of taggers like POS-taggers (part-of-speech taggers)
Machine translation as one of the earliest and most difficult applications of computational linguistics draws on many subfields.
Simulation and study of language evolution in historical linguistics/glottochronology.


== Legacy ==
The subject of computational linguistics has had a recurring impact on popular culture:
The 1983 film WarGames features a young computer hacker who interacts with an artificially intelligent supercomputer.
A 1997 film, Conceiving Ada, focuses on Ada Lovelace, considered one of the first computer scientists, as well as themes of computational linguistics.
Her, a 2013 film, depicts a man's interactions with the ""world's first artificially intelligent operating system.""
The 2014 film The Imitation Game follows the life of computer scientist Alan Turing, developer of the Turing Test.
The 2015 film Ex Machina centers around human interaction with artificial intelligence.


== See also ==


== References ==


== Further reading ==


== External links ==
Association for Computational Linguistics (ACL)
ACL Anthology of research papers
ACL Wiki for Computational Linguistics

CICLing annual conferences on Computational Linguistics
Computational Linguistics – Applications workshop
Free online introductory book on Computational Linguistics at the Wayback Machine (archived January 25, 2008)
Language Technology World
Resources for Text, Speech and Language Processing
The Research Group in Computational Linguistics"
16,Community informatics,816023,38542,"Community informatics (CI) is an interdisciplinary field that is concerned with using information and communication technology (ICT) to empower members of communities and support their social, cultural, and economic development.  Community informatics may contribute to enhancing democracy, supporting the development of social capital, and building well connected communities; moreover, it is probable that such similar actions may let people experience new positive social change. In community informatics, there are several considerations which are the social context, shared values, distinct processes that are taken by members in a community, and social and technical systems. It is formally located as an academic discipline within a variety of academic faculties including information science, information systems, computer science, planning, development studies, and library science among others and draws on insights on community development from a range of backgrounds and disciplines. It is an interdisciplinary approach interested in using ICTs for different forms of community action, as distinct from pure academic study about ICT effects.


== Background ==
Most humans live in communities. In some urban areas, community and neighborhood are conflated but this may be a limited definition. Communities are defined as people coming together in pursuit of common aims or shared practices through any means, including physical, electronic, and social networks. They proliferate even while the ability to define them is amorphous.
Cultures ensure their growth and survival by continuing the norms and mores that are the bases of their way of life. Communities can use the infrastructure of ICTs as a method of continuing cultures within the context of the Internet and the World Wide Web. Once a cultural identity is defined within the context of these technologies, it can be replicated and disseminated through various means, including the sharing of information through websites, applications, databases, and file sharing. In this manner, a group that defines its cultural identity within the construct of technology infrastructure is empowered to hold valuable exchanges within the spheres of economics, political power, high and popular culture, education, and entertainment.
Since the inception of the Internet and the World Wide Web, we have seen the exponential growth of enterprises ranging from electronic commerce, social networking, entertainment and education, as well as a myriad of other contrivances and file exchanges that allow for an ongoing cultural enrichment through technology. However, there has been a general lag as to which populations can benefit through these services through impediments such as geographic location, a lack of funds, gaps in technology and the expertise and skills that are required to operate these systems.
To date there has been very considerable investment in supporting the electronic development of business communities, one-to-many social tools (for example, corporate intranets, or purpose-built exchange and social networking services such as eBay, or Myspace), or in developing applications for individual use. There is far less understanding, or investment in human-technical networks and processes that are intended to deliberately result in social change or community change, particularly in communities for whom electronic communication is secondary to having an adequate income or social survival.
The communal dimension (and focus of Community Informatics) results in a strong interest in studying and developing strategies for how ICTs can enable and empower those living in physical communities. This is particularly the case in those communities where ICT access is done communally, through Telecentres, information kiosks, community multimedia centres, and other technologies. This latter set of approaches has become of very considerable interest as Information and Communications Technology for Development (ICT4D) has emerged as significant element in strategic (and funding) approaches to social and economic development in Less Developed Countries. ICT4D initiatives have been undertaken by public, NGO and private sector agencies concerned with development such as the United Nations Development Program, the World Bank, the Swiss Agency for Development and Cooperation (SDC), the MS Swaminathan Research Foundation; have emerged as a key element in the poverty alleviation component of the UN's Millennium Development Goals; and as important directions for private sector investment both from a market perspective (cf. the ""Bottom of the Pyramid"") and from companies concerned with finding a delivery channel for goods and services into rural and low income communities.
There is thus growing interest in Community Informatics as an approach to understanding of how different ICTs can enable and empower marginalized communities to achieve their collective goals.


== Understanding communities ==
It is crucial to know how communities are formed and evolved and how the participation to a community occurs and differs while formation process. Understanding the nature of communities and the participation process will surely ensure designing and implemenameting a successful ICT solution that benefits members of community while communicating with each other or performing certain tasks.  The following points include a brief description of the nature of each potential community formation.


=== Community as a place ===
A group of people may form a community according to the place in which they live, enjoy staying, and work. They usually participate in communities within these three places since they gather together on consistent basis so that it is highly expected that such community is formed. Beside the home and the work gathering, people usually like to spend their time at informal places called third places in where they meet their new or old friends or have a chance to meet new people.


=== Community as a socio-spatial entity ===
A group of people may form a community as they have frequent direct interactions or live in close proximity to each other. The members of such community may have strong bond and focused common goals which give them a higher status over other communities. Moreover, as the number of the members increases, the community may become reputable and has a higher status over other communities.


=== Community as links between people ===
A group of people may form a community as they have common shared identity. People may form such community to support and advocate common shared values, morals or norms in which they believe. Such a community may have a set of symbols and be associated with a status over other communities. The inclusion and the exclusion to such community depend on whether or not a member share the same identity with others in the community. For instance, people who descend from one origin may form a community in which only people from that origin can join the community even though they do not know each other in advance.


=== Community of interests ===
A group of people may form a community as they have similar affinity for a particular activity, experience, or subject. The geographical location is not necessary while forming such community, and the inclusion and the exclusion to such community depends on whether a new member has that affinity or not.


=== Communities linked to life stage ===
A group of people may form a community if they share a similar experience in a distinct life stage. The experience could be related to the members themselves or to their relatives, such as their children. For instance, parents of elementary school children may form a community in which they care about their children while in school. As it is mentioned in the previous type of community formation, the members of such community have a common interest which is caring about their children while in school. This type of community may persist over time, but the inclusion and the exclusion to it may happen consistently as people are no longer in that distinct life stage.


=== Communities of practice ===
A group of people who share a similar profession may form a community in which they work to attain their goals and advance in their profession. Three important concepts are considered while forming community of practice which are mutual engagement, joint enterprise, and shared repertoire. In a community of practice, the members have to be mutually engaged with each other by establishing collaborative relationships that will allow them to willingly work on certain joint activities. In the second concept which is joint enterprise, the members of a community of practice are supposed to discuss and agree upon the work responsibilities so that they can work in harmony, and each member knows his responsibility and his expected contributions to the community. In addition to these two concepts, the members of the community of practice have a shared repertoire of procedures or ways to perform certain tasks. They usually agree upon these procedures and practices that they establish and develop over time.


== Conceptual approaches ==
As an academic discipline, CI can be seen as a field of practice in applied information and communications technology. Community informatics is a technique for looking at economic and social development within the construct of technology—online health communities, social networking websites, cultural awareness and enhancement through online connections and networks, electronic commerce, information exchanges, as well as a myriad of other aspects that contributes to creating a personal and group identity. The term was brought to prominence by Michael Gurstein. Michael Gurstein says that community informatics is a technology strategy or discipline that connects at the community level economic and social development with the emergence of community and civic networks, electronic commerce, online participation, self-help, virtual health communities, ""Tele-centres"", as well as other types of online institutions and corporations. He brought out the first representative collection of academic papers, although others, such as Brian Loader and his colleagues at the University of Teesside used the term in the mid-1990s.
CI brings together the practices of community development and organization, and insights from fields such as sociology, planning, computer science, critical theory, women's studies, library and information sciences, management information systems, and management studies. Its outcomes—community networks and community-based ICT-enabled service applications—are of increasing interest to grassroots organizations, NGOs and civil society, governments, the private sector, and multilateral agencies among others. Self-organized community initiatives of all varieties, from different countries, are concerned with ways to harness ICT for social capital, poverty alleviation and for the empowerment of the ""local"" in relation to its larger economic, political and social environments. Some claim it is potentially a form of 'radical practice'.
Community informatics may in fact, not gel as a single field within the academy, but remain a convenient locale for interdisciplinary activity, drawing upon many fields of social practice and endeavour, as well as knowledge of community applications of technology. However, one can begin to see the emergence of a postmodern ""trans-discipline"" presenting a challenge to existing disciplinary ""stove-pipes"" from the perspectives of the rapidly evolving fields of technology practice, technology change, public policy and commercial interest. Whether or not such a ""trans-discipline"" can maintain its momentum remains to be seen given the incertitude about the boundaries of such disciplines as community development.
Furthermore, there is a continuing disconnect between those coming from an Information Science perspective for whom social theories, including general theories of organisation are unfamiliar or seemingly irrelevant to solving complex 'technical' problems, and those whose focus is upon the theoretical and practical issues around working with communities for democratic and social change 
Given that many of those most actively involved in early efforts were academics, it is only inevitable that a process of ""sense-making"" with respect to these efforts would follow from ""tool-making"" efforts. These academics, and some community activists connected globally through the medium.
A first formal meeting of researchers with an academic interest in these initiatives was held in conjunction with the 1999 Global Community Networking Conference in Buenos Aires, Argentina. This meeting began the process of linking community-based ICT initiatives in developed countries with initiatives undertaken in developing countries, which were often part of larger economic and social development programmes funded by agencies such as the UN Development Programme, World Bank, or the International Development Research Centre. Academics and researchers interested in ICT efforts in developed countries began to see common and overlapping interests with those interested in similar work in less developed countries. For example, the issue of sustainability as a technical, cultural, and economic problem for community informatics has resulted in a special issue of the Journal of Community Informatics  as well as the subject of ongoing conferences in Prato, Italy and other conferences in South Africa.
In Canada, the beginnings of CI can be recognized from various trials in community networking in the 1970s (Clement 1981). An essential development occurred in the 1990s, due to the change of cost of computers and modems. Moreover, examples of using computer networking to initiate and enhance social activities was acknowledged by women's groups (Balka 1992) and by the labor movement (Mazepa 1997). 


=== Social informatics beyond an immediate concern for a community ===

Social informatics refers to the body of research and study that examines social aspects of computerization—including the roles of information technology in social and organizational change, the uses of information technologies in social contexts, and the ways that the social organization of information technologies is influenced by social forces and social practices. Historically, social informatics research has been strong in the Scandinavian countries, the UK and Northern Europe. In Europe some researchers have pointed out that in order to create awareness of the importance of social issues of computing, one has to focus on didactics of social informatics. Within North America, the field is represented largely through independent research efforts at a number of diverse institutions. Social informatics research diverges from earlier, deterministic (both social and technological) models for measuring the social impacts of technology. Such technological deterministic models characterized information technologies as tools to be installed and used with a pre-determined set of impacts on society dictated by the technology's stated capabilities. Similarly, the socially deterministic theory represented by some proponents of the social construction of technology (SCOT) or social shaping of technology theory see technology as the product of human social forces.


== Criticisms ==
There is a tension between the practice and research ends of the field. To some extent this reflects the gap, familiar from other disciplines such as community development, community organizing and community based research. In addition, the difficulty that Information Systems has in recognising the qualitative dimension of technology research means that the kind of approach taken by supporters of community informatics is difficult to justify to a positive field oriented towards solutions of technical, rather than social problems. This is a difficulty also seen in the relationship between strict technology research and management research. Problems in conceptualising and evaluating complex social interventions relying on a technical base are familiar from community health and community education. There are long-standing debates about the desire for accountable - especially quantifiable and outcome-focused social development, typically practised by government or supported by foundations, and the more participatory, qualitatively rich, process-driven priorities of grass-roots community activists, familiar from theorists such as Paulo Freire, or Deweyan pragmatism.
Some of the theoretical and practical tensions are also familiar from such disciplines as program evaluation and social policy, and perhaps paradoxically, Management Information Systems, where there is continual debate over the relative virtue and values of different forms of research and action, spread around different understandings of the virtues or otherwise of allegedly ""scientific"" or ""value-free"" activity (frequently associated with ""responsible"" and deterministic public policy philosophies), and contrasted with more interpretive and process driven viewpoints in bottom-up or practice driven activity. Community informatics would in fact probably benefit from closer knowledge of, and relationship to, theorists, practitioners, and evaluators of rigorous qualitative research and practice.
A further concern is the potential for practice to be ""hijacked"" by policy or academic agendas, rather than being driven by community goals, both in developed and developing countries. The ethics of technology intervention in indigenous or other communities has not been sufficiently explored, even though ICTs are increasingly looked upon as an important tool for social and economic development in such communities. Moreover, neither explicit theoretical positions nor ideological positioning has yet emerged. Many projects appear to have developed with no particular disciplinary affiliation, arising more directly from policy or practice imperatives to 'do something' with technology as funding opportunities arise or as those at the grassroots (or working with the grassroots) identify ICT as possible resources to respond to local issues, problems or opportunities. The papers and documented outcomes (as questions or issues for further research or elaboration) on the wiki of the October 2006 Prato conference demonstrate that many of the social, rather than technical issues are key questions of concern to any practitioner in community settings: how to bring about change; the nature of authentic or manufactured community; ethical frameworks; or the politics of community research.
A different strain of critique has emerged from gender studies. Some theorists have argued that feminist contributions to the field have yet to be fully acknowledged and Community Informatics as a research area has yet to welcome feminist interventions. This exists despite the presence of several gender-oriented studies and leadership roles played by women in community informatics initiatives.


== Research and practice interests ==
Research and practice ranges from concerns with purely virtual communities; to situations in which virtual or online communication are used to enhance existing communities in urban, rural, or remote geographic locations in developed or developing countries; to applications of ICTs for the range of areas of interest for communities including social and economic development, environmental management, media and ""content"" production, public management and e-governance among others. A central concern, although one not always realized in practice is with ""enabling"" or ""empowering"" communities with ICT that is, ensuring that the technology is available for the community. This further implies an approach to development which is rather more ""bottom up"" than ""top down"".
Areas of concern range from small-scale projects in particular communities or organizations which might involve only a handful of people, such as telecentres; an on online community of disabled people; civic networks and to large national, government sponsored networking projects in countries such as Australia and Canada or local community projects such as working with Maori families in New Zealand. The Gates Foundation has been active in supporting public libraries in countries such as Chile. An area of rapidly developing interest is in the use of ICT as a means to enhance citizen engagement as an ""e-Governance"" counterpart (or counterweight) to transaction oriented initiatives.
A key conceptual element and framing concept for Community Informatics is that of ""effective use"" introduced initially by Michael Gurstein in a critique of a research pre-occupation with the Digital Divide as ICT ""access"". CI is concerned with how ICTs are used in practice and not simply facilitating ""access"" to them and the notion of ""effective use"" is a bridge between CI research (research and analysis of the constituent elements of effective use), CI policy (developing enabling structures and programmes supportive of ""effective use"") and practice (implementing applications and services in support of local communities).
Another way to understand CI is Clement and Shade's ""access rainbow"" (Clement and Shade 2000). Clement and Shade have contended that accomplishing insignificant specialized connectedness to the Internet is no assurance that an individual or group will prevail with regards to appropriating new ICTs in ways that advance their improvement, independence, or empowerment. It is an approach which has multi-layered socio-specialized model for universal access to ICTs. It is displayed as seven layers, starting with the fundamental technical components of connectedness and moving upward through layers that inexorably push the essential social framework of access. The seven layers are: 1. Carriage 2. Devices 3. Software tools Internet 4. Content/services 5. Service/access 6. Literacy / social facilitation 7. Governance.Even though that all elements are important, the most important one is the content /service layer in the middle, since this is where the actual utility is most direct. The upper layers focus on social dimensions and the lower layers focus on technical aspects.
Many practitioners would dispute any necessary connection to university research, regarding academic theorising and interventions as constraining or irrelevant to grassroots activity which should be beyond the control of traditional institutions, or simply irrelevant to practical local goals.
Some of the commonalities and differences may be in fact be due to national and cultural differences. For example, the capacity of many North American (and particularly US) universities to engage in service learning as part of progressive charters in communities large and small is part of a long-standing tradition absent elsewhere. The tradition of service learning is almost entirely absent in the UK, Australia, or New Zealand, (and of limited significance in Canada) where the State has traditionally played a much stronger role in the delivery of community services and information.
In some countries such as the UK, there is a tradition of locally based grassroots community technology, for example in Manchester, or in Hebden Bridge. In Italy and the Netherlands, there also appears to have been a strong connection between the development of local civic networks based around a tradition of civic oppositionism, connected into the work of progressive academics.
In Latin America, Africa and many parts of Asia these efforts have been driven by external funding agencies as part of larger programs and initiatives in support of broader economic and social development goals. However, these efforts have now become significantly ""indigenized"" (and particularly in Latin America) and ""bottom-up"" ICT efforts are increasingly playing a leading role in defining the future use of ICT within local communities.
In Canada, The Canadian Research Alliance for Community Innovation and Networking (CRACIN) was established in 2003. Their goal is to explore and archive the status and achievements of CI activities in Canada. It is a research partnership between scholastics, specialists, and public sector delegates. 


== Networks ==
There are emerging online and personal networks of researchers and practitioners in community informatics and community networking in many countries as well as international groupings. The past decade has also seen conferences in many countries, and there is an emerging literature for theoreticians and practitioners including the on-line Journal of Community Informatics.
It is surprising in fact, how much in common is found when people from developed and non-developed countries meet. A common theme is the struggle to convince policy makers of the legitimacy of this approach to developing electronically literate societies, instead of a top-down or trickle-down approach, or an approach dominated by technical, rather than social solutions which in the end, tend to help vendors rather than communities. A common criticism that is frequently raised amongst participants at events such as the Prato conferences is that a focus on technical solutions evades the social changes that communities need to achieve in their values, activities and other people-oriented outcomes in order to make better use of technology.
The field tends to have a progressive bent, being concerned about the use of technology for social and cultural development connected to a desire for capacity building or expanding social capital, and in a number of countries, governments and foundations have funded a variety of community informatics projects and initiatives, particularly from a more tightly controlled, though not well-articulated social planning perspective, though knowledge about long-term effects of such forms of social intervention on use of technology is still in its early stages.


=== Public libraries and community networks ===
Even though that community networks and public libraries have similitudes in various ways, there are some obstacles that upset the probability of cooperation in the future between them. Albeit both CNs and libraries are concerned with giving information services to the society, an exchange is by all accounts lacking between the two communities. The mission of libraries is frequently rather barely engaged and, with regards to managing people and different institutes, their methodology can be to some degree unbending. Thusly, CN specialists, while institutionally more adaptable, rush to expel the part of public libraries in the community, tending to see the library essentially as a store of books upheld by public subsidizing. Public libraries have a long-standing custom of association with their communities, yet their conditions and concerns contrast from those of community networks (CNs).


== See also ==


== References ==


== External links ==
Center for Community Informatics - Loyola University, Maryland
Center for Community Informatics Research, Development & Training
Community Informatics - University of Illinois at Urbana-Champaign
Community Informatics - Penn State
Community Informatics - University of Michigan
Community Informatics Research Group, University of Pittsburgh
Journal of Community Informatics
Community Informatics Research Network
Association for Community Networking"
17,List of important publications in theoretical computer science,24095830,38158,"This is a list of important publications in theoretical computer science, organized by field.
Some reasons why a particular publication might be regarded as important:
Topic creator – A publication that created a new topic
Breakthrough – A publication that changed scientific knowledge significantly
Influence – A publication which has significantly influenced the world or has had a massive impact on the teaching of theoretical computer science.


== Computability ==


=== Cutland's Computability: An Introduction to Recursive Function Theory (Cambridge) ===
Cutland, Nigel J. (1980). Computability: An Introduction to Recursive Function Theory. Cambridge University Press. ISBN 0-521-29465-7. 
The review of this early text by Carl Smith of Purdue University (in the Society for Industrial and Applied Mathematics Reviews), reports that this a text with an ""appropriate blend of intuition and rigor… in the exposition of proofs"" that presents ""the fundamental results of classical recursion theory [RT]... in a style... accessible to undergraduates with minimal mathematical background"". While he states that it ""would make an excellent introductory text for an introductory course in [RT] for mathematics students"", he suggests that an ""instructor must be prepared to substantially augment the material… "" when it used with computer science students (given a dearth of material on RT applications to this area).


=== Decidability of second order theories and automata on infinite trees ===
Michael O. Rabin
Transactions of the American Mathematical Society, vol. 141, pp. 1–35, 1969
Description: The paper presented the tree automaton, an extension of the automata. The tree automaton had numerous applications to proofs of correctness of programs.


=== Finite automata and their decision problems ===
Michael O. Rabin and Dana S. Scott
IBM Journal of Research and Development, vol. 3, pp. 114–125, 1959
Online version (Not Free)
Description: Mathematical treatment of automata, proof of core properties, and definition of non-deterministic finite automaton.


=== Introduction to Automata Theory, Languages, and Computation ===

John E. Hopcroft, Jeffrey D. Ullman, and Rajeev Motwani
Addison-Wesley, 2001, ISBN 0-201-02988-X
Description: A popular textbook.


=== On certain formal properties of grammars ===
Chomsky, N. (1959). ""On certain formal properties of grammars"". Information and Control. 2 (2): 137–167. doi:10.1016/S0019-9958(59)90362-6. 
Description: This article introduced what is now known as the Chomsky hierarchy, a containment hierarchy of classes of formal grammars that generate formal languages.


=== On computable numbers, with an application to the Entscheidungsproblem ===
Alan Turing
Proceedings of the London Mathematical Society, Series 2, vol. 42, pp. 230–265, 1937, doi:10.1112/plms/s2-42.1.230.
Errata appeared in vol. 43, pp. 544–546, 1938, doi:10.1112/plms/s2-43.6.544.
HTML version, PDF version
Description: This article set the limits of computer science. It defined the Turing Machine, a model for all computations. On the other hand, it proved the undecidability of the halting problem and Entscheidungsproblem and by doing so found the limits of possible computation.


=== Rekursive Funktionen ===
Péter, Rózsa (1951). Rekursive Funktionen. Academic Press. ISBN 9780125526500. 
The first textbook on the theory of recursive functions. The book went through many editions and earned Péter the Kossuth Prize from the Hungarian government. Reviews by Raphael M. Robinson and Stephen Kleene praised the book for providing an effective elementary introduction for students.


== Computational complexity theory ==


=== Arora & Barak's Computational Complexity and Goldreich's Computational Complexity (both Cambridge) ===
Sanjeev Arora and Boaz Barak, ""Computational Complexity: A Modern Approach,"" Cambridge University Press, 2009, 579 pages, Hardcover
Oded Goldreich, ""Computational Complexity: A Conceptual Perspective, Cambridge University Press, 2008, 606 pages, Hardcover
Besides the estimable press bringing these recent texts forward, they are very positively reviewed in ACM's SIGACT News by Daniel Apon of the University of Arkansas, who identifies them as ""textbooks for a course in complexity theory, aimed at early graduate… or... advanced undergraduate students… [with] numerous, unique strengths and very few weaknesses,"" and states that both are:

""excellent texts that thoroughly cover both the breadth and depth of computational complexity theory… [by] authors... each [who] are giants in theory of computing [where each will be] ...an exceptional reference text for experts in the field… [and that] ...theorists, researchers and instructors of any school of thought will find either book useful.""

The reviewer notes that there is ""a definite attempt in [Arora and Barak] to include very up-to-date material, while Goldreich focuses more on developing a contextual and historical foundation for each concept presented,"" and that he ""applaud[s] all… authors for their outstanding contributions.""


=== A machine-independent theory of the complexity of recursive functions ===
Blum, Manuel (1967). ""A Machine-Independent Theory of the Complexity of Recursive Functions"" (PDF). Journal of the ACM. 14 (2): 322–336. doi:10.1145/321386.321395. 
Description: The Blum axioms.


=== Algebraic methods for interactive proof systems ===
Lund, C.; Fortnow, L.; Karloff, H.; Nisan, N. (1992). ""Algebraic methods for interactive proof systems"". Journal of the ACM. 39 (4): 859–868. doi:10.1145/146585.146605. 
Description: This paper showed that PH is contained in IP.


=== The complexity of theorem proving procedures ===
Cook, Stephen A. (1971). ""The Complexity of Theorem-Proving Procedures"" (PDF). Proceedings of the 3rd Annual ACM Symposium on Theory of Computing: 151–158. doi:10.1145/800157.805047. 
Description: This paper introduced the concept of NP-Completeness and proved that Boolean satisfiability problem (SAT) is NP-Complete. Note that similar ideas were developed independently slightly later by Leonid Levin at ""Levin, Universal Search Problems. Problemy Peredachi Informatsii 9(3):265-266, 1973"".


=== Computers and Intractability: A Guide to the Theory of NP-Completeness ===
Garey, Michael R.; Johnson, David S. (1979). Computers and Intractability: A Guide to the Theory of NP-Completeness. New York: Freeman. ISBN 0-7167-1045-5. 
Description: The main importance of this book is due to its extensive list of more than 300 NP-Complete problems. This list became a common reference and definition. Though the book was published only few years after the concept was defined such an extensive list was found.


=== Degree of difficulty of computing a function and a partial ordering of recursive sets ===
Rabin, Michael O. (1960). ""Degree of difficulty of computing a function and a partial ordering of recursive sets"" (PDF). Technical Report No. 2. Jerusalem: Hebrew University. 
Description: This technical report was the first publication talking about what later was renamed computational complexity


=== How good is the simplex method? ===
Victor Klee and George J. Minty
Klee, Victor; Minty, George J. (1972). ""How good is the simplex algorithm?"". In Shisha, Oved. Inequalities III (Proceedings of the Third Symposium on Inequalities held at the University of California, Los Angeles, Calif., September 1–9, 1969, dedicated to the memory of Theodore S. Motzkin). New York-London: Academic Press. pp. 159–175. MR 0332165. 
Description: Constructed the ""Klee–Minty cube"" in dimension D, whose 2D corners are each visited by Dantzig's simplex algorithm for linear optimization.


=== How to construct random functions ===
Goldreich, O.; Goldwasser, S.; Micali, S. (1986). ""How to construct random functions"" (PDF). Journal of the ACM. 33 (4): 792–807. doi:10.1145/6490.6503. 
Description: This paper showed that the existence of one way functions leads to computational randomness.


=== IP = PSPACE ===
Shamir, A. (1992). ""IP = PSPACE"". Journal of the ACM. 39 (4): 869–877. doi:10.1145/146585.146609. 
Description: IP is a complexity class whose characterization (based on interactive proof systems) is quite different from the usual time/space bounded computational classes. In this paper, Shamir extended the technique of the previous paper by Lund, et al., to show that PSPACE is contained in IP, and hence IP = PSPACE, so that each problem in one complexity class is solvable in the other.


=== Reducibility among combinatorial problems ===

R. M. Karp
In R. E. Miller and J. W. Thatcher, editors, Complexity of Computer Computations, Plenum Press, New York, NY, 1972, pp. 85–103
Description: This paper showed that 21 different problems are NP-Complete and showed the importance of the concept.


=== The Knowledge Complexity of Interactive Proof Systems ===
Goldwasser, S.; Micali, S.; Rackoff, C. (1989). ""The Knowledge Complexity of Interactive Proof Systems"" (PDF). SIAM J. Comput. 18 (1): 186–208. doi:10.1137/0218012. 
Description: This paper introduced the concept of zero knowledge.


=== A letter from Gödel to von Neumann ===
Kurt Gödel
A Letter from Gödel to John von Neumann, March 20, 1956
Online version
Description: Gödel discusses the idea of efficient universal theorem prover.


=== On the computational complexity of algorithms ===
Hartmanis, Juris; Stearns, Richard (1965). ""On the computational complexity of algorithms"". Transactions of the American Mathematical Society. 117: 285–306. doi:10.1090/s0002-9947-1965-0170805-7. 
Description: This paper gave computational complexity its name and seed.


=== Paths, trees, and flowers ===
Edmonds, J. (1965). ""Paths, trees, and flowers"". Canadian Journal of Mathematics. 17: 449–467. doi:10.4153/CJM-1965-045-4. 
Description: There is a polynomial time algorithm to find a maximum matching in a graph that is not bipartite and another step toward the idea of computational complexity. For more information see [3].


=== Theory and applications of trapdoor functions ===
Yao, A. C. (1982). ""Theory and application of trapdoor functions"". 23rd Annual Symposium on Foundations of Computer Science (SFCS 1982). pp. 80–91. doi:10.1109/SFCS.1982.45. 
Description: This paper creates a theoretical framework for trapdoor functions and described some of their applications, like in cryptography. Note that the concept of trapdoor functions was brought at ""New directions in cryptography"" six years earlier (See section V ""Problem Interrelationships and Trap Doors."").


=== Computational Complexity ===
C.H. Papadimitriou
Addison-Wesley, 1994, ISBN 0-201-53082-1
Description: An introduction to computational complexity theory, the book explains its author's characterization of P-SPACE and other results.


=== Interactive proofs and the hardness of approximating cliques ===
Feige, U.; Goldwasser, S.; Lovász, L.; Safra, S.; Szegedy, M. (1996). ""Interactive proofs and the hardness of approximating cliques"". Journal of the ACM. 43 (2): 268–292. doi:10.1145/226643.226652. 


=== Probabilistic checking of proofs: a new characterization of NP ===
Arora, S.; Safra, S. (1998). ""Probabilistic checking of proofs: A new characterization of NP"". Journal of the ACM. 45: 70–122. doi:10.1145/273865.273901. 


=== Proof verification and the hardness of approximation problems ===
Arora, S.; Lund, C.; Motwani, R.; Sudan, M.; Szegedy, M. (1998). ""Proof verification and the hardness of approximation problems"". Journal of the ACM. 45 (3): 501–555. doi:10.1145/278298.278306. 
Description: These three papers established the surprising fact that certain problems in NP remain hard even when only an approximative solution is required. See PCP theorem.


=== The Instrinsic Computational Difficulty of Functions ===
Cobham, Alan (1964). ""The Instrinsic Computational Difficulty of Functions"" (PDF). Proc. of the 1964 International Congress for Logic, Methodology, and the Philosophy of Science: 24–30. 
Description: First definition of the complexity class P. One of the founding papers of complexity theory.


== Algorithms ==


=== ""A machine program for theorem proving"" ===
Davis, M.; Logemann, G.; Loveland, D. (1962). ""A machine program for theorem-proving"" (PDF). Communications of the ACM. 5 (7): 394–397. doi:10.1145/368273.368557. 
Description: The DPLL algorithm. The basic algorithm for SAT and other NP-Complete problems.


=== ""A machine-oriented logic based on the resolution principle"" ===
Robinson, J. A. (1965). ""A Machine-Oriented Logic Based on the Resolution Principle"". Journal of the ACM. 12: 23–41. doi:10.1145/321250.321253. 
Description: First description of resolution and unification used in automated theorem proving; used in Prolog and logic programming.


=== ""The traveling-salesman problem and minimum spanning trees"" ===
Held, M.; Karp, R. M. (1970). ""The Traveling-Salesman Problem and Minimum Spanning Trees"". Operations Research. 18 (6): 1138–1162. doi:10.1287/opre.18.6.1138. 
Description: The use of an algorithm for minimum spanning tree as an approximation algorithm for the NP-Complete travelling salesman problem. Approximation algorithms became a common method for coping with NP-Complete problems.


=== ""A polynomial algorithm in linear programming"" ===
L. G. Khachiyan
Soviet Mathematics - Doklady, vol. 20, pp. 191–194, 1979
Description: For long, there was no provably polynomial time algorithm for the linear programming problem. Khachiyan was the first to provide an algorithm that was polynomial (and not just was fast enough most of the time as previous algorithms). Later, Narendra Karmarkar presented a faster algorithm at: Narendra Karmarkar, ""A new polynomial time algorithm for linear programming"", Combinatorica, vol 4, no. 4, p. 373–395, 1984.


=== ""Probabilistic algorithm for testing primality"" ===
Rabin, M. (1980). ""Probabilistic algorithm for testing primality"". Journal of Number Theory. 12 (1): 128–138. doi:10.1016/0022-314X(80)90084-0. 
Description: The paper presented the Miller-Rabin primality test and outlined the program of randomized algorithms.


=== ""Optimization by simulated annealing"" ===
Kirkpatrick, S.; Gelatt, C. D.; Vecchi, M. P. (1983). ""Optimization by Simulated Annealing"". Science. 220 (4598): 671–680. Bibcode:1983Sci...220..671K. doi:10.1126/science.220.4598.671. PMID 17813860. 
Description: This article described simulated annealing which is now a very common heuristic for NP-Complete problems.


=== The Art of Computer Programming ===

Donald Knuth
Description: This monograph has three popular algorithms books and a number of fascicles. The algorithms are written in both English and MIX assembly language (or MMIX assembly language in more recent fascicles). This makes algorithms both understandable and precise. However, the use of a low-level programming language frustrates some programmers more familiar with modern structured programming languages.


=== Algorithms + Data Structures = Programs ===

Niklaus Wirth
Prentice Hall, 1976, ISBN 0-13-022418-9
Description: An early, influential book on algorithms and data structures, with implementations in Pascal.


=== The Design and Analysis of Computer Algorithms ===
Alfred V. Aho, John E. Hopcroft, and Jeffrey D. Ullman
Addison-Wesley, 1974, ISBN 0-201-00029-6
Description: One of the standard texts on algorithms for the period of approximately 1975–1985.


=== How to Solve It By Computer ===
Dromey, R. G. (1982). How to Solve it by Computer. Prentice-Hall International. ISBN 978-0-13-434001-2. 
Description: Explains the Whys of algorithms and data-structures. Explains the Creative Process, the Line of Reasoning, the Design Factors behind innovative solutions.


=== Algorithms ===
Robert Sedgewick
Addison-Wesley, 1983, ISBN 0-201-06672-6
Description: A very popular text on algorithms in the late 1980s. It was more accessible and readable (but more elementary) than Aho, Hopcroft, and Ullman. There are more recent editions.


=== Introduction to Algorithms ===

Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein
3rd Edition, MIT Press, 2009, ISBN 978-0-262-03384-8.
Description: This textbook has become so popular that it is almost the de facto standard for teaching basic algorithms. The 1st edition (with first three authors) was published in 1990, the 2nd edition in 2001, and the 3rd in 2009.


== Algorithmic information theory ==


=== ""On Tables of Random Numbers"" ===
Kolmogorov, Andrei N. (1963). ""On Tables of Random Numbers"". Sankhyā Ser. A. 25: 369–375. MR 0178484. 
Kolmogorov, Andrei N. (1963). ""On Tables of Random Numbers"". Theoretical Computer Science. 207 (2): 387–395. doi:10.1016/S0304-3975(98)00075-9. MR 1643414. 
Description: Proposed a computational and combinatorial approach to probability.


=== ""A formal theory of inductive inference"" ===
Ray Solomonoff
Information and Control, vol. 7, pp. 1–22 and 224–254, 1964
Online copy: part I, part II
Description: This was the beginning of algorithmic information theory and Kolmogorov complexity. Note that though Kolmogorov complexity is named after Andrey Kolmogorov, he said that the seeds of that idea are due to Ray Solomonoff. Andrey Kolmogorov contributed a lot to this area but in later articles.


=== ""Algorithmic information theory"" ===
Chaitin, Gregory (1977). ""Algorithmic information theory"" (PDF). IBM Journal of Research and Development. IBM. 21 (4): 350–359. doi:10.1147/rd.214.0350. Archived from the original (PDF) on 2009-05-30. 
Description: An introduction to algorithmic information theory by one of the important people in the area.


== Information theory ==


=== ""A mathematical theory of communication"" ===
Shannon, C.E. (1948). ""A mathematical theory of communication"". Bell System Technical Journal. 27: 379–423, 623–656. 
Description: This paper created the field of information theory.


=== ""Error detecting and error correcting codes"" ===
Hamming, Richard (1950). ""Error detecting and error correcting codes"". Bell System Technical Journal. 29: 147–160. doi:10.1002/j.1538-7305.1950.tb00463.x. 
Description: In this paper, Hamming introduced the idea of error-correcting code. He created the Hamming code and the Hamming distance and developed methods for code optimality proofs.


=== ""A method for the construction of minimum redundancy codes"" ===
Huffman, D. (1952). ""A Method for the Construction of Minimum-Redundancy Codes"" (PDF). Proceedings of the IRE. 40 (9): 1098–1101. doi:10.1109/JRPROC.1952.273898. 
Description: The Huffman coding.


=== ""A universal algorithm for sequential data compression"" ===
Ziv, J.; Lempel, A. (1977). ""A universal algorithm for sequential data compression"". IEEE Transactions on Information Theory. 23 (3): 337–343. doi:10.1109/TIT.1977.1055714. Archived from the original on 2003-12-04. 
Description: The LZ77 compression algorithm.


=== Elements of Information Theory ===
Cover, Thomas M.; Thomas, Joy A. (1991). Elements of Information Theory. Wiley. 
Description: A popular introduction to information theory.


== Formal verification ==


=== Assigning Meaning to Programs ===
Floyd, Robert (1967). ""Assigning Meaning to Programs"" (PDF). Mathematical Aspects of Computer Science. Proceedings of Symposia in Applied Mathematics. 19: 19–32. doi:10.1090/psapm/019/0235771. ISBN 9780821813195. 
Description: Robert Floyd's landmark paper Assigning Meanings to Programs introduces the method of inductive assertions and describes how a program annotated with first-order assertions may be shown to satisfy a pre- and post-condition specification - the paper also introduces the concepts of loop invariant and verification condition.


=== An Axiomatic Basis for Computer Programming ===
Hoare, C. A. R. (October 1969). ""An axiomatic basis for computer programming"" (PDF). Communications of the ACM. 12 (10): 576–580. doi:10.1145/363235.363259. Archived from the original (PDF) on 2016-03-04. 
Description: Tony Hoare's paper An Axiomatic Basis for Computer Programming describes a set of inference (i.e. formal proof) rules for fragments of an Algol-like programming language described in terms of (what are now called) Hoare-triples.


=== Guarded Commands, Nondeterminacy and Formal Derivation of Programs ===
Dijkstra, E. W. (1975). ""Guarded commands, nondeterminacy and formal derivation of programs"". Communications of the ACM. 18 (8): 453–457. doi:10.1145/360933.360975. 
Description: Edsger Dijkstra's paper Guarded Commands, Nondeterminacy and Formal Derivation of Programs (expanded by his 1976 postgraduate-level textbook A Discipline of Programming) proposes that, instead of formally verifying a program after it has been written (i.e. post facto), programs and their formal proofs should be developed hand-in-hand (using predicate transformers to progressively refine weakest pre-conditions), a method known as program (or formal) refinement (or derivation), or sometimes ""correctness-by-construction"".


=== Proving Assertions about Parallel Programs ===
Edward A. Ashcroft
J. Comput. Syst. Sci. 10(1): 110-135 (1975)
Description: The paper that introduced invariance proofs of concurrent programs.


=== An Axiomatic Proof Technique for Parallel Programs I ===
Susan S. Owicki, David Gries
Acta Inf. 6: 319-340 (1976)
Description: In this paper, along with the same authors paper ""Verifying Properties of Parallel Programs: An Axiomatic Approach. Commun. ACM 19(5): 279-285 (1976)"", the axiomatic approach to parallel programs verification was presented.


=== A Discipline of Programming ===
Edsger W. Dijkstra
1976
Description: Edsger Dijkstra's classic postgraduate-level textbook A Discipline of Programming extends his earlier paper Guarded Commands, Nondeterminacy and Formal Derivation of Programs and firmly establishes the principle of formally deriving programs (and their proofs) from their specification.


=== Denotational Semantics ===
Joe Stoy
1977
Description: Joe Stoy's Denotational Semantics is the first (postgraduate level) book-length exposition of the mathematical (or functional) approach to the formal semantics of programming languages (in contrast to the operational and algebraic approaches).


=== The Temporal Logic of Programs ===
Pnueli, A. (1977). ""The temporal logic of programs"". 18th Annual Symposium on Foundations of Computer Science (SFCS 1977). IEEE. pp. 46–57. doi:10.1109/SFCS.1977.32. 
Description: The use of temporal logic was suggested as a method for formal verification.


=== Characterizing correctness properties of parallel programs using fixpoints (1980) ===
E. Allen Emerson, Edmund M. Clarke
In Proc. 7th International Colloquium on Automata Languages and Programming, pages 169-181, 1980
Description: Model checking was introduced as a procedure to check correctness of concurrent programs.


=== Communicating Sequential Processes (1978) ===
C.A.R. Hoare
1978
Description: Tony Hoare's (original) communicating sequential processes (CSP) paper introduces the idea of concurrent processes (i.e. programs) that do not share variables but instead cooperate solely by exchanging synchronous messages.


=== A Calculus of Communicating Systems ===
Robin Milner
1980
Description: Robin Milner's A Calculus of Communicating Systems (CCS) paper describes a process algebra permitting systems of concurrent processes to be reasoned about formally, something which has not been possible for earlier models of concurrency (semaphores, critical sections, original CSP).


=== Software Development: A Rigorous Approach ===
Cliff Jones
1980
Description: Cliff Jones' textbook Software Development: A Rigorous Approach is the first full-length exposition of the Vienna Development Method (VDM), which had evolved (principally) at IBM's Vienna research lab over the previous decade and which combines the idea of program refinement as per Dijkstra with that of data refinement (or reification) whereby algebraically-defined abstract data types are formally transformed into progressively more ""concrete"" representations.


=== The Science of Programming ===
David Gries
1981
Description: David Gries' textbook The Science of Programming describes Dijkstra's weakest precondition method of formal program derivation, except in a very much more accessible manner than Dijkstra's earlier A Discipline of Programming.
It shows how to construct programs that work correctly (without bugs, other than from typing errors). It does this by showing how to use precondition and postcondition predicate expressions and program proving techniques to guide the way programs are created.
The examples in the book are all small-scale, and clearly academic (as opposed to real-world). They emphasize basic algorithms, such as sorting and merging, and string manipulation. Subroutines (functions) are included, but object-oriented and functional programming environments are not addressed.


=== Communicating Sequential Processes (1985) ===
C.A.R. Hoare
1985
Description: Tony Hoare's Communicating Sequential Processes (CSP) textbook (currently the third most cited computer science reference of all time) presents an updated CSP model in which cooperating processes do not even have program variables and which, like CCS, permits systems of processes to be reasoned about formally.


=== Linear logic (1987) ===
Girard, J.-Y (1987). ""Linear Logic"" (PDF). Theoretical Computer Science. London Mathematical Society. 50 (1): 1–102. doi:10.1016/0304-3975(87)90045-4. Archived from the original (PDF) on 2006-11-29. 
Description: Girard's linear logic was a breakthrough in designing typing systems for sequential and concurrent computation, especially for resource conscious typing systems.


=== A Calculus of Mobile Processes (1989) ===
R. Milner, J. Parrow, D. Walker
1989
Online version: Part 1 and Part 2
Description: This paper introduces the Pi-Calculus, a generalisation of CCS which allows process mobility. The calculus is extremely simple and has become the dominant paradigm in the theoretical study of programming languages, typing systems and program logics.


=== The Z Notation: A Reference Manual ===
Spivey, J. M. (1992). The Z Notation: A Reference Manual (2nd ed.). Prentice Hall International. ISBN 0-13-978529-9. 
Description: Mike Spivey's classic textbook The Z Notation: A Reference Manual summarises the formal specification language Z notation which, although originated by Jean-Raymond Abrial, had evolved (principally) at Oxford University over the previous decade.


=== Communication and Concurrency ===
Robin Milner
Prentice-Hall International, 1989
Description: Robin Milner's textbook Communication and Concurrency is a more accessible, although still technically advanced, exposition of his earlier CCS work.


=== a Practical Theory of Programming ===
Eric Hehner
Springer, 1993, current edition online here
Description: the up-to-date version of Predicative programming. The basis for C.A.R. Hoare's UTP. The simplest and most comprehensive formal methods.


== References =="
18,LP-type problem,34676009,37751,"In the study of algorithms, an LP-type problem (also called a generalized linear program) is an optimization problem that shares certain properties with low-dimensional linear programs and that may be solved by similar algorithms. LP-type problems include many important optimization problems that are not themselves linear programs, such as the problem of finding the smallest circle containing a given set of planar points. They may be solved by a combination of randomized algorithms in an amount of time that is linear in the number of elements defining the problem, and subexponential in the dimension of the problem.


== Definition ==
LP-type problems were defined by Sharir & Welzl (1992) as problems in which one is given as input a finite set S of elements, and a function f that maps subsets of S to values from a totally ordered set. The function is required to satisfy two key properties:
Monotonicity: for every two sets A ⊆ B ⊆ S, f(A) ≤ f(B) ≤ f(S).
Locality: for every two sets A ⊆ B ⊆ S and every element x in S, if f(A) = f(B) = f(A ∪ {x}), then f(A) = f(B ∪ {x}).
A basis of an LP-type problem is a set B ⊆ S with the property that every proper subset of B has a smaller value of f than B itself, and the dimension (or combinatorial dimension) of an LP-type problem is defined to be the maximum cardinality of a basis.
It is assumed that an optimization algorithm may evaluate the function f only on sets that are themselves bases or that are formed by adding a single element to a basis. Alternatively, the algorithm may be restricted to two primitive operations: a violation test that determines, for a basis B and an element x whether f(B) = f(B ∪ {x}), and a basis computation that (with the same inputs) finds a basis of B ∪ {x}. The task for the algorithm to perform is to evaluate f(S) by only using these restricted evaluations or primitives.


== Examples and applications ==
A linear program may be defined by a system of d non-negative real variables, subject to n linear inequality constraints, together with a non-negative linear objective function to be minimized. This may be placed into the framework of LP-type problems by letting S be the set of constraints, and defining f(A) (for a subset A of the constraints) to be the minimum objective function value of the smaller linear program defined by A. With suitable general position assumptions (in order to prevent multiple solution points having the same optimal objective function value), this satisfies the monotonicity and locality requirements of an LP-type problem, and has combinatorial dimension equal to the number d of variables. Similarly, an integer program (consisting of a collection of linear constraints and a linear objective function, as in a linear program, but with the additional restriction that the variables must take on only integer values) satisfies both the monotonicity and locality properties of an LP-type problem, with the same general position assumptions as for linear programs. Theorems of Bell (1977) and Scarf (1977) show that, for an integer program with d variables, the combinatorial dimension is at most 2d.
Many natural optimization problems in computational geometry are LP-type:

The smallest circle problem is the problem of finding the minimum radius of a circle containing a given set of n points in the plane. It satisfies monotonicity (adding more points can only make the circle larger) and locality (if the smallest circle for set A contains B and x, then the same circle also contains B ∪ {x}). Because the smallest circle is always determined by some three points, the smallest circle problem has combinatorial dimension three, even though it is defined using two-dimensional Euclidean geometry. More generally, the smallest enclosing ball of points in d dimensions forms an LP-type problem of combinatorial dimension d + 1. The smallest circle problem can be generalized to the smallest ball enclosing a set of balls, to the smallest ball that touches or surrounds each of a set of balls, to the weighted 1-center problem, or to similar smaller enclosing ball problems in non-Euclidean spaces such as the space with distances defined by Bregman divergence. The related problem of finding the smallest enclosing ellipsoid is also an LP-type problem, but with a larger combinatorial dimension, d(d + 3)/2.
Let K0, K1, ... be a sequence of n convex sets in d-dimensional Euclidean space, and suppose that we wish to find the longest prefix of this sequence that has a common intersection point. This may be expressed as an LP-type problem in which f(A) = −i where Ki is the first member of A that does not belong to an intersecting prefix of A, and where f(A) = −n if there is no such member. The combinatorial dimension of this system is d + 1.
Suppose we are given a collection of axis-aligned rectangular boxes in three-dimensional space, and wish to find a line directed into the positive octant of space that cuts through all the boxes. This may be expressed as an LP-type problem with combinatorial dimension 4.
The problem of finding the closest distance between two convex polytopes, specified by their sets of vertices, may be represented as an LP-type problem. In this formulation, the set S is the set of all vertices in both polytopes, and the function value f(A) is the negation of the smallest distance between the convex hulls of the two subsets A of vertices in the two polytopes. The combinatorial dimension of the problem is d + 1 if the two polytopes are disjoint, or d + 2 if they have a nonempty intersection.
Let S = {f0, f1, ...} be a set of quasiconvex functions. Then the pointwise maximum maxi fi is itself quasiconvex, and the problem of finding the minimum value of maxi fi is an LP-type problem. It has combinatorial dimension at most 2d + 1, where d is the dimension of the domain of the functions, but for sufficiently smooth functions the combinatorial dimension is smaller, at most d + 1. Many other LP-type problems can also be expressed using quasiconvex functions in this way; for instance, the smallest enclosing circle problem is the problem of minimizing maxi fi where each of the functions fi measures the Euclidean distance from one of the given points.
LP-type problems have also been used to determine the optimal outcomes of certain games in algorithmic game theory, improve vertex placement in finite element method meshes, solve facility location problems, analyze the time complexity of certain exponential-time search algorithms, and reconstruct the three-dimensional positions of objects from their two-dimensional images.


== Algorithms ==


=== Seidel ===
Seidel (1991) gave an algorithm for low-dimensional linear programming that may be adapted to the LP-type problem framework. Seidel's algorithm takes as input the set S and a separate set X (initially empty) of elements known to belong to the optimal basis. It then considers the remaining elements one-by-one in a random order, performing violation tests for each one and, depending on the result, performing a recursive call to the same algorithm with a larger set of known basis elements. It may be expressed with the following pseudocode:

In a problem with combinatorial dimension d, the violation test in the ith iteration of the algorithm fails only when x is one of the d − |X| remaining basis elements, which happens with probability at most (d − |X|)/i. Based on this calculation, it can be shown that overall the expected number of violation tests performed by the algorithm is O(d! n), linear in n but worse than exponential in d.


=== Clarkson ===
Clarkson (1995) defines two algorithms, a recursive algorithm and an iterative algorithm, for linear programming based on random sampling techniques, and suggests a combination of the two that calls the iterative algorithm from the recursive algorithm. The recursive algorithm repeatedly chooses random samples whose size is approximately the square root of the input size, solves the sampled problem recursively, and then uses violation tests to find a subset of the remaining elements that must include at least one basis element:

In each iteration, the expected size of V is O(√n), and whenever V is nonempty it includes at least one new element of the eventual basis of S. Therefore, the algorithm performs at most d iterations, each of which performs n violation tests and makes a single recursive call to a subproblem of size O(d√n).
Clarkson's iterative algorithm assigns weights to each element of S, initially all of them equal. It then chooses a set R of 9d2 elements from S at random, and computes the sets B and V as in the previous algorithm. If the total weight of V is at most 2/(9d − 1) times the total weight of S (as happens with constant probability) then the algorithm doubles the weights of every element of V, and as before it repeats this process until V becomes empty. In each iteration, the weight of the optimal basis can be shown to increase at a greater rate than the total weight of S, from which it follows that the algorithm must terminate within O(log n) iterations.
By using the recursive algorithm to solve a given problem, switching to the iterative algorithm for its recursive calls, and then switching again to Seidel's algorithm for the calls made by the iterative algorithm, it is possible solve a given LP-type problem using O(dn + d! dO(1) log n) violation tests.
When applied to a linear program, this algorithm can be interpreted as being a dual simplex method. With certain additional computational primitives beyond the violation test and basis computation primitives, this method can be made deterministic.


=== Matoušek, Sharir, and Welzl ===
Matoušek, Sharir & Welzl (1996) describe an algorithm that uses an additional property of linear programs that is not always held by other LP-type problems, that all bases have the same cardinality of each other. If an LP-type problem does not have this property, it can be made to have it by adding d new dummy elements and by modifying the function f to return the ordered pair of its old value f(A) and of the number min(d,|A|), ordered lexicographically.
Rather than adding elements of S one at a time, or finding samples of the elements, Matoušek, Sharir & Welzl (1996) describe an algorithm that removes elements one at a time. At each step it maintains a basis C that may initially be the set of dummy elements. It may be described with the following pseudocode:

In most of the recursive calls of the algorithm, the violation test succeeds and the if statement is skipped. However, with a small probability the violation test fails and the algorithm makes an additional basis computation and then an additional recursive call. As the authors show, the expected time for the algorithm is linear in n and exponential in the square root of d log n. By combining this method with Clarkson's recursive and iterative procedures, these two forms of time dependence can be separated out from each other, resulting in an algorithm that performs O(dn) violation tests in the outer recursive algorithm and a number that is exponential in the square root of d log d in the lower levels of the algorithm.


== Variations ==


=== Optimization with outliers ===
Matoušek (1995) considers a variation of LP-type optimization problems in which one is given, together with the set S and the objective function f, a number k; the task is to remove k elements from S in order to make the objective function on the remaining set as small as possible. For instance, when applied to the smallest circle problem, this would give the smallest circle that contains all but k of a given set of planar points. He shows that, for all non-degenerate LP-type problems (that is, problems in which all bases have distinct values) this problem may be solved in time O(nkd), by solving a set of O(kd) LP-type problems defined by subsets of S.


=== Implicit problems ===
Some geometric optimization problems may be expressed as LP-type problems in which the number of elements in the LP-type formulation is significantly greater than the number of input data values for the optimization problem. As an example, consider a collection of n points in the plane, each moving with constant velocity. At any point in time, the diameter of this system is the maximum distance between two of its points. The problem of finding a time at which the diameter is minimized can be formulated as minimizing the pointwise maximum of O(n2) quasiconvex functions, one for each pair of points, measuring the Euclidean distance between the pair as a function of time. Thus, it can be solved as an LP-type problem of combinatorial dimension two on a set of O(n2) elements, but this set is significantly larger than the number of input points.
Chan (2004) describes an algorithm for solving implicitly defined LP-type problems such as this one in which each LP-type element is determined by a k-tuple of input values, for some constant k. In order to apply his approach, there must exist a decision algorithm that can determine, for a given LP-type basis B and set S of n input values, whether B is a basis for the LP-type problem determined by S.
Chan's algorithm performs the following steps:
If the number of input values is below some threshold value, find the set of LP-type elements that it determines and solve the resulting explicit LP-type problem.
Otherwise, partition the input values into a suitable number greater than k of equal-sized subsets Si.
If f is the objective function for the implicitly defined LP-type problem to be solved, then define a function g that maps collections of subsets Si to the value of f on the union of the collection. Then the collection of subsets Si and the objective function g itself defines an LP-type problem, of the same dimension as the implicit problem to be solved.
Solve the (explicit) LP-type problem defined by g using Clarkson's algorithm, which performs a linear number of violation tests and a polylogarithmic number of basis evaluations. The basis evaluations for g may be performed by recursive calls to Chan's algorithm, and the violation tests may be performed by calls to the decision algorithm.
With the assumption that the decision algorithm takes an amount of time O(T(n)) that grows at least polynomially as a function of the input size n, Chan shows that the threshold for switching to an explicit LP formulation and the number of subsets in the partition can be chosen in such a way that the implicit LP-type optimization algorithm also runs in time O(T(n)).
For instance, for the minimum diameter of moving points, the decision algorithm needs only to calculate the diameter of a set of points at a fixed time, a problem that can be solved in O(n log n) time using the rotating calipers technique. Therefore, Chan's algorithm for finding the time at which the diameter is minimized also takes time O(n log n). Chan uses this method to find a point of maximal Tukey depth among a given collection of n points in d-dimensional Euclidean space, in time O(nd − 1 + n log n). A similar technique was used by Braß, Heinrich-Litan & Morin (2003) to find a point of maximal Tukey depth for the uniform distribution on a convex polygon.


== History and related problems ==
The discovery of linear time algorithms for linear programming and the observation that the same algorithms could in many cases be used to solve geometric optimization problems that were not linear programs goes back at least to Megiddo (1983, 1984), who gave a linear expected time algorithm for both three-variable linear programs and the smallest circle problem. However, Megiddo formulated the generalization of linear programming geometrically rather than combinatorially, as a convex optimization problem rather than as an abstract problem on systems of sets. Similarly, Dyer (1986) and Clarkson (in the 1988 conference version of Clarkson 1995) observed that their methods could be applied to convex programs as well as linear programs. Dyer (1992) showed that the minimum enclosing ellipsoid problem could also be formulated as a convex optimization problem by adding a small number of non-linear constraints. The use of randomization to improve the time bounds for low dimensional linear programming and related problems was pioneered by Clarkson and by Dyer & Frieze (1989).
The definition of LP-type problems in terms of functions satisfying the axioms of locality and monotonicity is from Sharir & Welzl (1992), but other authors in the same timeframe formulated alternative combinatorial generalizations of linear programs. For instance, in a framework developed by Gärtner (1995), the function f is replaced by a total ordering on the subsets of S. It is possible to break the ties in an LP-type problem to create a total order, but only at the expense of an increase in the combinatorial dimension. Additionally, as in LP-type problems, Gärtner defines certain primitives for performing computations on subsets of elements; however, his formalization does not have an analogue of the combinatorial dimension.
Another abstract generalization of both linear programs and linear complementarity problems, formulated by Stickney & Watson (1978) and later studied by several other authors, concerns orientations of the edges of a hypercube with the property that every face of the hypercube (including the whole hypercube as a face) has a unique sink, a vertex with no outgoing edges. An orientation of this type may be formed from an LP-type problem by corresponding the subsets of S with the vertices of a hypercube in such a way that two subsets differ by a single element if and only if the corresponding vertices are adjacent, and by orienting the edge between neighboring sets A ⊆ B towards B if f(A) ≠ f(B) and towards A otherwise. The resulting orientation has the additional property that it forms a directed acyclic graph, from which it can be shown that a randomized algorithm can find the unique sink of the whole hypercube (the optimal basis of the LP-type problem) in a number of steps exponential in the square root of n.
The more recently developed framework of violator spaces generalizes LP-type problems, in the sense that every LP-type problem can be modeled by a violator space but not necessarily vice versa. Violator spaces are defined similarly to LP-type problems, by a function f that maps sets to objective function values, but the values of f are not ordered. Despite the lack of ordering, every set S has a well-defined set of bases (the minimal sets with the same value as the whole set) that can be found by variations of Clarkson's algorithms for LP-type problems. Indeed, violator spaces have been shown to exactly characterize the systems that can be solved by Clarkson's algorithms.


== Notes ==


== References =="
19,Computational chemistry,6019,36814,"Computational chemistry is a branch of chemistry that uses computer simulation to assist in solving chemical problems. It uses methods of theoretical chemistry, incorporated into efficient computer programs, to calculate the structures and properties of molecules and solids. It is necessary because, apart from relatively recent results concerning the hydrogen molecular ion (dihydrogen cation, see references therein for more details), the quantum many-body problem cannot be solved analytically, much less in closed form. While computational results normally complement the information obtained by chemical experiments, it can in some cases predict hitherto unobserved chemical phenomena. It is widely used in the design of new drugs and materials.
Examples of such properties are structure (i.e., the expected positions of the constituent atoms), absolute and relative (interaction) energies, electronic charge density distributions, dipoles and higher multipole moments, vibrational frequencies, reactivity, or other spectroscopic quantities, and cross sections for collision with other particles.
The methods used cover both static and dynamic situations. In all cases, the computer time and other resources (such as memory and disk space) increase rapidly with the size of the system being studied. That system can be one molecule, a group of molecules, or a solid. Computational chemistry methods range from very approximate to highly accurate; the latter are usually feasible for small systems only. Ab initio methods are based entirely on quantum mechanics and basic physical constants. Other methods are called empirical or semi-empirical because they use additional empirical parameters.
Both ab initio and semi-empirical approaches involve approximations. These range from simplified forms of the first-principles equations that are easier or faster to solve, to approximations limiting the size of the system (for example, periodic boundary conditions), to fundamental approximations to the underlying equations that are required to achieve any solution to them at all. For example, most ab initio calculations make the Born–Oppenheimer approximation, which greatly simplifies the underlying Schrödinger equation by assuming that the nuclei remain in place during the calculation. In principle, ab initio methods eventually converge to the exact solution of the underlying equations as the number of approximations is reduced. In practice, however, it is impossible to eliminate all approximations, and residual error inevitably remains. The goal of computational chemistry is to minimize this residual error while keeping the calculations tractable.
In some cases, the details of electronic structure are less important than the long-time phase space behavior of molecules. This is the case in conformational studies of proteins and protein-ligand binding thermodynamics. Classical approximations to the potential energy surface are used, as they are computationally less intensive than electronic calculations, to enable longer simulations of molecular dynamics. Furthermore, cheminformatics uses even more empirical (and computationally cheaper) methods like machine learning based on physicochemical properties. One typical problem in cheminformatics is to predict the binding affinity of drug molecules to a given target.


== History ==
Building on the founding discoveries and theories in the history of quantum mechanics, the first theoretical calculations in chemistry were those of Walter Heitler and Fritz London in 1927. The books that were influential in the early development of computational quantum chemistry include Linus Pauling and E. Bright Wilson's 1935 Introduction to Quantum Mechanics – with Applications to Chemistry, Eyring, Walter and Kimball's 1944 Quantum Chemistry, Heitler's 1945 Elementary Wave Mechanics – with Applications to Quantum Chemistry, and later Coulson's 1952 textbook Valence, each of which served as primary references for chemists in the decades to follow.
With the development of efficient computer technology in the 1940s, the solutions of elaborate wave equations for complex atomic systems began to be a realizable objective. In the early 1950s, the first semi-empirical atomic orbital calculations were performed. Theoretical chemists became extensive users of the early digital computers. One major advance came with the 1951 paper in Reviews of Modern Physics by Clemens C. J. Roothaan in 1951, largely on the ""LCAO MO"" approach (Linear Combination of Atomic Orbitals Molecular Orbitals), for many years the second-most cited paper in that journal. A very detailed account of such use in the United Kingdom is given by Smith and Sutcliffe. The first ab initio Hartree–Fock method calculations on diatomic molecules were performed in 1956 at MIT, using a basis set of Slater orbitals. For diatomic molecules, a systematic study using a minimum basis set and the first calculation with a larger basis set were published by Ransil and Nesbet respectively in 1960. The first polyatomic calculations using Gaussian orbitals were performed in the late 1950s. The first configuration interaction calculations were performed in Cambridge on the EDSAC computer in the 1950s using Gaussian orbitals by Boys and coworkers. By 1971, when a bibliography of ab initio calculations was published, the largest molecules included were naphthalene and azulene. Abstracts of many earlier developments in ab initio theory have been published by Schaefer.
In 1964, Hückel method calculations (using a simple linear combination of atomic orbitals (LCAO) method to determine electron energies of molecular orbitals of π electrons in conjugated hydrocarbon systems) of molecules, ranging in complexity from butadiene and benzene to ovalene, were generated on computers at Berkeley and Oxford. These empirical methods were replaced in the 1960s by semi-empirical methods such as CNDO.
In the early 1970s, efficient ab initio computer programs such as ATMOL, Gaussian, IBMOL, and POLYAYTOM, began to be used to speed ab initio calculations of molecular orbitals. Of these four programs, only Gaussian, now vastly expanded, is still in use, but many other programs are now in use. At the same time, the methods of molecular mechanics, such as MM2 force field, were developed, primarily by Norman Allinger.
One of the first mentions of the term computational chemistry can be found in the 1970 book Computers and Their Role in the Physical Sciences by Sidney Fernbach and Abraham Haskell Taub, where they state ""It seems, therefore, that 'computational chemistry' can finally be more and more of a reality."" During the 1970s, widely different methods began to be seen as part of a new emerging discipline of computational chemistry. The Journal of Computational Chemistry was first published in 1980.
Computational chemistry has featured in several Nobel Prize awards, most notably in 1998 and 2013. Walter Kohn, ""for his development of the density-functional theory"", and John Pople, ""for his development of computational methods in quantum chemistry"", received the 1998 Nobel Prize in Chemistry. Martin Karplus, Michael Levitt and Arieh Warshel received the 2013 Nobel Prize in Chemistry for ""the development of multiscale models for complex chemical systems"".


== Fields of application ==
The term theoretical chemistry may be defined as a mathematical description of chemistry, whereas computational chemistry is usually used when a mathematical method is sufficiently well developed that it can be automated for implementation on a computer. In theoretical chemistry, chemists, physicists, and mathematicians develop algorithms and computer programs to predict atomic and molecular properties and reaction paths for chemical reactions. Computational chemists, in contrast, may simply apply existing computer programs and methodologies to specific chemical questions.
Computational chemistry has two different aspects:
Computational studies, used to find a starting point for a laboratory synthesis, or to assist in understanding experimental data, such as the position and source of spectroscopic peaks.
Computational studies, used to predict the possibility of so far entirely unknown molecules or to explore reaction mechanisms not readily studied via experiments.
Thus, computational chemistry can assist the experimental chemist or it can challenge the experimental chemist to find entirely new chemical objects.
Several major areas may be distinguished within computational chemistry:
The prediction of the molecular structure of molecules by the use of the simulation of forces, or more accurate quantum chemical methods, to find stationary points on the energy surface as the position of the nuclei is varied.
Storing and searching for data on chemical entities (see chemical databases).
Identifying correlations between chemical structures and properties (see quantitative structure–property relationship (QSPR) and quantitative structure–activity relationship (QSAR)).
Computational approaches to help in the efficient synthesis of compounds.
Computational approaches to design molecules that interact in specific ways with other molecules (e.g. drug design and catalysis).


== Accuracy ==
The words exact and perfect do not apply here, as very few aspects of chemistry can be computed exactly. However, almost every aspect of chemistry can be described in a qualitative or approximate quantitative computational scheme.
Molecules consist of nuclei and electrons, so the methods of quantum mechanics apply. Computational chemists often attempt to solve the non-relativistic Schrödinger equation, with relativistic corrections added, although some progress has been made in solving the fully relativistic Dirac equation. In principle, it is possible to solve the Schrödinger equation in either its time-dependent or time-independent form, as appropriate for the problem in hand; in practice, this is not possible except for very small systems. Therefore, a great number of approximate methods strive to achieve the best trade-off between accuracy and computational cost.
Accuracy can always be improved with greater computational cost. Significant errors can present themselves in ab initio models comprising many electrons, due to the computational cost of full relativistic-inclusive methods. This complicates the study of molecules interacting with high atomic mass unit atoms, such as transitional metals and their catalytic properties. Present algorithms in computational chemistry can routinely calculate the properties of small molecules that contain up to about 40 electrons with errors for energies less than a few kJ/mol. For geometries, bond lengths can be predicted within a few picometres and bond angles within 0.5 degrees. The treatment of larger molecules that contain a few dozen atoms is computationally tractable by more approximate methods such as density functional theory (DFT).
There is some dispute within the field whether or not the latter methods are sufficient to describe complex chemical reactions, such as those in biochemistry. Large molecules can be studied by semi-empirical approximate methods. Even larger molecules are treated by classical mechanics methods that use what are called molecular mechanics (MM). In QM-MM methods, small parts of large complexes are treated quantum mechanically (QM), and the remainder is treated approximately (MM).


== Methods ==
One molecular formula can represent more than one molecular isomer: a set of isomers. Each isomer is a local minimum on the energy surface (called the potential energy surface) created from the total energy (i.e., the electronic energy, plus the repulsion energy between the nuclei) as a function of the coordinates of all the nuclei. A stationary point is a geometry such that the derivative of the energy with respect to all displacements of the nuclei is zero. A local (energy) minimum is a stationary point where all such displacements lead to an increase in energy. The local minimum that is lowest is called the global minimum and corresponds to the most stable isomer. If there is one particular coordinate change that leads to a decrease in the total energy in both directions, the stationary point is a transition structure and the coordinate is the reaction coordinate. This process of determining stationary points is called geometry optimization.
The determination of molecular structure by geometry optimization became routine only after efficient methods for calculating the first derivatives of the energy with respect to all atomic coordinates became available. Evaluation of the related second derivatives allows the prediction of vibrational frequencies if harmonic motion is estimated. More importantly, it allows for the characterization of stationary points. The frequencies are related to the eigenvalues of the Hessian matrix, which contains second derivatives. If the eigenvalues are all positive, then the frequencies are all real and the stationary point is a local minimum. If one eigenvalue is negative (i.e., an imaginary frequency), then the stationary point is a transition structure. If more than one eigenvalue is negative, then the stationary point is a more complex one, and is usually of little interest. When one of these is found, it is necessary to move the search away from it if the experimenter is looking solely for local minima and transition structures.
The total energy is determined by approximate solutions of the time-dependent Schrödinger equation, usually with no relativistic terms included, and by making use of the Born–Oppenheimer approximation, which allows for the separation of electronic and nuclear motions, thereby simplifying the Schrödinger equation. This leads to the evaluation of the total energy as a sum of the electronic energy at fixed nuclei positions and the repulsion energy of the nuclei. A notable exception are certain approaches called direct quantum chemistry, which treat electrons and nuclei on a common footing. Density functional methods and semi-empirical methods are variants on the major theme. For very large systems, the relative total energies can be compared using molecular mechanics. The ways of determining the total energy to predict molecular structures are:


=== Ab initio methods ===

The programs used in computational chemistry are based on many different quantum-chemical methods that solve the molecular Schrödinger equation associated with the molecular Hamiltonian. Methods that do not include any empirical or semi-empirical parameters in their equations – being derived directly from theoretical principles, with no inclusion of experimental data – are called ab initio methods. This does not imply that the solution is an exact one; they are all approximate quantum mechanical calculations. It means that a particular approximation is rigorously defined on first principles (quantum theory) and then solved within an error margin that is qualitatively known beforehand. If numerical iterative methods must be used, the aim is to iterate until full machine accuracy is obtained (the best that is possible with a finite word length on the computer, and within the mathematical and/or physical approximations made).

The simplest type of ab initio electronic structure calculation is the Hartree–Fock method (HF), an extension of molecular orbital theory, in which the correlated electron-electron repulsion is not specifically taken into account; only its average effect is included in the calculation. As the basis set size is increased, the energy and wave function tend towards a limit called the Hartree–Fock limit. Many types of calculations (termed post-Hartree–Fock methods) begin with a Hartree–Fock calculation and subsequently correct for electron-electron repulsion, referred to also as electronic correlation. As these methods are pushed to the limit, they approach the exact solution of the non-relativistic Schrödinger equation. To obtain exact agreement with experiment, it is necessary to include relativistic and spin orbit terms, both of which are far more important for heavy atoms. In all of these approaches, along with choice of method, it is necessary to choose a basis set. This is a set of functions, usually centered on the different atoms in the molecule, which are used to expand the molecular orbitals with the linear combination of atomic orbitals (LCAO) molecular orbital method ansatz. Ab initio methods need to define a level of theory (the method) and a basis set.
The Hartree–Fock wave function is a single configuration or determinant. In some cases, particularly for bond breaking processes, this is inadequate, and several configurations must be used. Here, the coefficients of the configurations, and of the basis functions, are optimized together.
The total molecular energy can be evaluated as a function of the molecular geometry; in other words, the potential energy surface. Such a surface can be used for reaction dynamics. The stationary points of the surface lead to predictions of different isomers and the transition structures for conversion between isomers, but these can be determined without a full knowledge of the complete surface.
A particularly important objective, called computational thermochemistry, is to calculate thermochemical quantities such as the enthalpy of formation to chemical accuracy. Chemical accuracy is the accuracy required to make realistic chemical predictions and is generally considered to be 1 kcal/mol or 4 kJ/mol. To reach that accuracy in an economic way it is necessary to use a series of post-Hartree–Fock methods and combine the results. These methods are called quantum chemistry composite methods.


=== Density functional methods ===

Density functional theory (DFT) methods are often considered to be ab initio methods for determining the molecular electronic structure, even though many of the most common functionals use parameters derived from empirical data, or from more complex calculations. In DFT, the total energy is expressed in terms of the total one-electron density rather than the wave function. In this type of calculation, there is an approximate Hamiltonian and an approximate expression for the total electron density. DFT methods can be very accurate for little computational cost. Some methods combine the density functional exchange functional with the Hartree–Fock exchange term and are termed hybrid functional methods.


=== Semi-empirical methods ===

Semi-empirical quantum chemistry methods are based on the Hartree–Fock method formalism, but make many approximations and obtain some parameters from empirical data. They were very important in computational chemistry from the 60S to the 90s, especially for treating large molecules where the full Hartree–Fock method without the approximations were too costly. The use of empirical parameters appears to allow some inclusion of correlation effects into the methods.
Primitive semi-empirical methods were designed even before, where the two-electron part of the Hamiltonian is not explicitly included. For π-electron systems, this was the Hückel method proposed by Erich Hückel, and for all valence electron systems, the extended Hückel method proposed by Roald Hoffmann. Sometimes, Hückel methods are referred to as ""completely emprirical"" because they do not derive from a Hamiltonian.Yet, the term ""empirical methods"", or ""empirical force fields"" is usually used to describe Molecular Mechanics.


=== Molecular mechanics ===

In many cases, large molecular systems can be modeled successfully while avoiding quantum mechanical calculations entirely. Molecular mechanics simulations, for example, use one classical expression for the energy of a compound, for instance the harmonic oscillator. All constants appearing in the equations must be obtained beforehand from experimental data or ab initio calculations.
The database of compounds used for parameterization, i.e., the resulting set of parameters and functions is called the force field, is crucial to the success of molecular mechanics calculations. A force field parameterized against a specific class of molecules, for instance proteins, would be expected to only have any relevance when describing other molecules of the same class.
These methods can be applied to proteins and other large biological molecules, and allow studies of the approach and interaction (docking) of potential drug molecules.


=== Methods for solids ===

Computational chemical methods can be applied to solid state physics problems. The electronic structure of a crystal is in general described by a band structure, which defines the energies of electron orbitals for each point in the Brillouin zone. Ab initio and semi-empirical calculations yield orbital energies; therefore, they can be applied to band structure calculations. Since it is time-consuming to calculate the energy for a molecule, it is even more time-consuming to calculate them for the entire list of points in the Brillouin zone.


=== Chemical dynamics ===
Once the electronic and nuclear variables are separated (within the Born–Oppenheimer representation), in the time-dependent approach, the wave packet corresponding to the nuclear degrees of freedom is propagated via the time evolution operator (physics) associated to the time-dependent Schrödinger equation (for the full molecular Hamiltonian). In the complementary energy-dependent approach, the time-independent Schrödinger equation is solved using the scattering theory formalism. The potential representing the interatomic interaction is given by the potential energy surfaces. In general, the potential energy surfaces are coupled via the vibronic coupling terms.
The most popular methods for propagating the wave packet associated to the molecular geometry are:
the split operator technique,
the Chebyshev (real) polynomial,
the multi-configuration time-dependent Hartree method (MCTDH),
the semiclassical method.


=== Molecular dynamics ===

Molecular dynamics (MD) use either quantum mechanics, molecular mechanics or a mixture of both to calculate forces which are then used to solve Newton's laws of motion to examine the time-dependent behaviour of systems. The result of a molecular dynamics simulation is a trajectory that describes how the position and velocity of particles varies with time.


=== Quantum mechanics/Molecular mechanics (QM/MM) ===

QM/MM is a hybrid method that attempts to combine the accuracy of quantum mechanics with the speed of molecular mechanics. It is useful for simulating very large molecules such as enzymes.


== Interpreting molecular wave functions ==
The atoms in molecules (QTAIM) model of Richard Bader was developed to effectively link the quantum mechanical model of a molecule, as an electronic wavefunction, to chemically useful concepts such as atoms in molecules, functional groups, bonding, the theory of Lewis pairs, and the valence bond model. Bader has demonstrated that these empirically useful chemistry concepts can be related to the topology of the observable charge density distribution, whether measured or calculated from a quantum mechanical wavefunction. QTAIM analysis of molecular wavefunctions is implemented, for example, in the AIMAll software package.


== Software packages ==
Many self-sufficient computational chemistry software packages exist. Some include many methods covering a wide range, while others concentrate on a very specific range or even on one method. Details of most of them can be found in:
Biomolecular modelling programs: proteins, nucleic acid.
Molecular mechanics programs.
Quantum chemistry and solid state physics software supporting several methods.
Molecular design software
Semi-empirical programs.
Valence bond programs.


== See also ==


== Notes and references ==


== Bibliography ==
C. J. Cramer Essentials of Computational Chemistry, John Wiley & Sons (2002).
T. Clark A Handbook of Computational Chemistry, Wiley, New York (1985).
R. Dronskowski Computational Chemistry of Solid State Materials, Wiley-VCH (2005).
A.K. Hartmann, Practical Guide to Computer Simulations, World Scientific (2009)
F. Jensen Introduction to Computational Chemistry, John Wiley & Sons (1999).
K.I. Ramachandran, G Deepa and Krishnan Namboori. P.K. Computational Chemistry and Molecular Modeling Principles and applications Springer-Verlag GmbH ISBN 978-3-540-77302-3.
D. Rogers Computational Chemistry Using the PC, 3rd Edition, John Wiley & Sons (2003).
P. v. R. Schleyer (Editor-in-Chief). Encyclopedia of Computational Chemistry. Wiley, 1998. ISBN 0-471-96588-X.
D. Sherrill. Notes on Quantum Mechanics and Computational Chemistry.
J. Simons An introduction to Theoretical Chemistry, Cambridge (2003) ISBN 978-0-521-53047-7.
A. Szabo, N.S. Ostlund, Modern Quantum Chemistry, McGraw-Hill (1982).
D. Young Computational Chemistry: A Practical Guide for Applying Techniques to Real World Problems, John Wiley & Sons (2001).
D. Young's Introduction to Computational Chemistry.
Lewars, Errol G. (2011). Computational Chemistry. Heidelberg: Springer. doi:10.1007/978-90-481-3862-3. ISBN 978-90-481-3860-9. 


== Specialized journals on computational chemistry ==
Reviews in Computational Chemistry
Journal of Computational Chemistry
Journal of Chemical Information and Modeling
Journal of Computer-aided Molecular Design
Journal of Chemical Information and Modeling
Journal of Chemical Theory and Computation
Computational and Theoretical Polymer Science
Computational and Theoretical Chemistry
Journal of Theoretical and Computational Chemistry
Journal of Cheminformatics
Journal of Computer Chemistry Japan
Annual Reports in Computational Chemistry
Computers & Chemical Engineering
Journal of Chemical Software
Molecular Informatics
Journal of Computer Aided Chemistry
Theoretical Chemistry Accounts


== External links ==
NIST Computational Chemistry Comparison and Benchmark DataBase – Contains a database of thousands of computational and experimental results for hundreds of systems
American Chemical Society Division of Computers in Chemistry – American Chemical Society Computers in Chemistry Division, resources for grants, awards, contacts and meetings.
CSTB report Mathematical Research in Materials Science: Opportunities and Perspectives – CSTB Report
3.320 Atomistic Computer Modeling of Materials (SMA 5107) Free MIT Course
Chem 4021/8021 Computational Chemistry Free University of Minnesota Course
Technology Roadmap for Computational Chemistry
Applications of molecular and materials modelling.
Impact of Advances in Computing and Communications Technologies on Chemical Science and Technology CSTB Report
MD and Computational Chemistry applications on GPUs"
20,ACM/IEEE Supercomputing Conference,26528420,36587,"SC (formerly Supercomputing), the International Conference for High Performance Computing, Networking, Storage and Analysis, is the name of the annual conference established in 1988 by the Association for Computing Machinery and the IEEE Computer Society. In 2016, about 11,000 people participated overall. The not-for-profit conference is run by a committee of approximately 600 volunteers who spend roughly three years organizing each conference.
Not to be confused with the International Supercomputing Conference.


== Sponsorship and Governance ==
SC is sponsored by the Association for Computing Machinery and the IEEE Computer Society. From its formation through 2011, ACM sponsorship was managed through ACM's Special Interest Group on Computer Architecture (SIGARCH). Sponsors are listed on each proceedings page in the ACM DL; see for example. Beginning in 2012, ACM began the process of transitioning sponsorship from SIGARCH to the recently formed Special Interest Group on High Performance Computing (SIGHPC). This transition was completed after SC15, and for SC16 ACM sponsorship was vested exclusively in SIGHPC (IEEE sponsorship remained unchanged). The conference is non-profit.
The conference is governed by a steering committee that includes representatives of the sponsoring societies, the current conference general chair, the general chairs of the preceding two years, the general chairs of the next two conference years, and a number of elected members. All steering committee members are volunteers, with the exception of the two representatives of the sponsoring societies, who are employees of those societies. The committee selects the conference general chair, approves each year's conference budget, and is responsible for setting policy and strategy for the conference.


== Conference Components ==
Although each conference committee introduces slight variations on the program each year, the core components of the conference remain largely unchanged from year to year.


=== Technical Program ===
The SC Technical Program is competitive with an acceptance rate around 20% for papers (see History). Traditionally, the program includes invited talks, panels, research papers, tutorials, workshops, posters, and Birds of a Feather (BoF) sessions.


=== Awards ===
Each year, SC hosts the following conference and sponsoring society awards:
ACM Gordon Bell Prize
ACM/IEEE-CS George Michael Memorial HPC Fellowship
ACM/IEEE-CS Ken Kennedy Award
ACM SIGHPC Computational & Data Science Fellowships
IEEE-CS Seymour Cray Computer Engineering Award
IEEE-CS Sidney Fernbach Memorial Award
IEEE CS TCHPC Award for Excellence for Early Career Researchers in HPC
Test of Time Award


=== Exhibits ===
In addition to the technical program, SC hosts a research exhibition each year that includes universities, state-sponsored computing research organizations (such as the Federal labs in the US), and vendors of HPC-related hardware and software from many countries around the world. There were 353 exhibitors at SC16 in Salt Lake City, UT.


=== Student Program ===
SC's program for students has gone through a variety of changes and emphases over the years. Beginning with SC15 the program is called ""Students@SC"", and is oriented toward undergraduate and graduate students in computing related fields, and computing-oriented students in science and engineering. The program includes professional development programs, opportunities to learn from mentors, and engagement with SC’s technical sessions.


=== SCinet ===
SCinet is SC’s research network. Started in 1991, SCinet features emerging technologies for very high bandwidth, low latency wide area network communications in addition to operational services necessary to provide conference attendees with connectivity to the commodity Internet and to many national research and engineering networks.


== Name changes ==
Since its establishment in 1988, and until 1995, the full name of the conference was the ""ACM/IEEE Supercomputing Conference"" (sometimes: ""ACM/IEEE Conference on Supercomputing""). The conference's abbreviated (and more commonly used) formal name was ""Supercomputing 'XY"", where XY denotes the last two digits of the year. In 1996, according to the archived front matter of the conference proceedings, the full name was changed to the ACM/IEEE ""International Conference on High Performance Computing and Communications"". The latter document further announced that, as of 1997, the conference will undergo a name change and will be called ""SC97: High Performance Networking and Computing"". The document explained that

1997 [will mark] the first use of ""SC97"" as the name of the annual conference you've known as ""Supercomputing 'XY"". This change reflects our growing attention to networking, distributed computing, data-intensive applications, and other emerging technologies that push the frontiers of communications and computing.

A 1997 HPCwire article discussed at length the reasoning, considerations, and concerns that accompanied the decision to change the name of the conference series from ""Supercomputing 'XY"" to ""SC 'XY"", stating that

It's official: the age of supercomputing has ended. At any rate, the word ""supercomputing"" has been excised from the title of the annual trade shows, sponsored by the IEEE and ACM, that have been known for almost ten years as ""Supercomputing '(final two digits of year)"". The next event, to be held in San Jose next November, has been redesignated ""SC '97."" Like Lewis Carroll's Cheshire Cat, ""supercomputing"" has faded steadily away until only the smile, nose, and whiskers remain. [...] The loss is a real one. An enormous range of ordinary people had some idea, however vague, what ""supercomputing"" meant. No-caf, local alternatives like ""SC"" and ""HPC"" lack this authority. This is not a trivial issue. In these days of rapid change, passing technofancies, and information overload, a rose with the wrong name is just another thorn -- or forgotten immediately. After all, how can businessmen, ordinary consumers, and taxpayers be expected to pay money for something they can't comprehend? More important, will investors and grant-givers hand over money to support further R&D on something whose only identity is an arbitrary clump of capital letters?

Despite these concerns, the abbreviated name of the conference, ""SC"", is still used today, a reminiscent of the abbreviation of the conference's original name—""Supercomputing Conference"".
The full name, in contrast, underwent several changes. Between 1997 and 2003, the name ""High Performance Networking and Computing"" was specified in the front matter of the archived conference proceedings in some years (1997, 1998, 2000, 2002), whereas in other years it was omitted altogether in favor of the abbreviated name (1999, 2001, 2003). In 2004, the stated front matter full name was changed to ""High Performance Computing, Networking and Storage Conference"". In 2005, this name was replaced by the original name of the conference—""supercomputing""— in the front matter. Finally, in 2006, the current full name, as used today, emerged: ""The International Conference for High Performance Computing, Networking, Storage and Analysis"".
Despite all of the name variances in the proceedings through the years, the digital library of ACM, the co-sponsoring society, records the name of the conference as ""The ACM/IEEE Conference on Supercomputing"" from 1998 - 2008, when it changes to """"The International Conference for High Performance Computing, Networking, Storage and Analysis"". It is these two names that are used in the full citations to the conference proceedings provided in this article.


== History ==
The table below provides the location, name of the general chair, and acceptance statistics for each year of SC. Note that references for data in these tables apply to data preceding the reference to the left on the same row; for example, for SC17 the single reference substantiates all the information in that row, but for SC05 the source for the convention center and chair is different than the source for the acceptance statistics.
The following table details the keynote speakers during the history of the conference; as of SC17, 20% of the keynote speakers have been female, with a mix of speakers from corporate, academic, and national government organizations.


== See also ==
Gordon Bell Prize
Sidney Fernbach Award
Seymour Cray Award
Ken Kennedy Award
TOP500
Green500
HPC Challenge Awards
SCinet
Storcloud


== References ==


== External links ==
The SC Conference Website
SC12 - The International Conference for High Performance Computing, Networking, Storage and Analysis"
21,Neuroinformatics,3062721,35909,"Neuroinformatics is a research field concerned with the organization of neuroscience data by the application of computational models and analytical tools. These areas of research are important for the integration and analysis of increasingly large-volume, high-dimensional, and fine-grain experimental data. Neuroinformaticians provide computational tools, mathematical models, and create interoperable databases for clinicians and research scientists. Neuroscience is a heterogeneous field, consisting of many and various sub-disciplines (e.g., cognitive psychology, behavioral neuroscience, and behavioral genetics). In order for our understanding of the brain to continue to deepen, it is necessary that these sub-disciplines are able to share data and findings in a meaningful way; Neuroinformaticians facilitate this.
Neuroinformatics stands at the intersection of neuroscience and information science. Other fields, like genomics, have demonstrated the effectiveness of freely distributed databases and the application of theoretical and computational models for solving complex problems. In Neuroinformatics, such facilities allow researchers to more easily quantitatively confirm their working theories by computational modeling. Additionally, neuroinformatics fosters collaborative research—an important fact that facilitates the field's interest in studying the multi-level complexity of the brain.
There are three main directions where neuroinformatics has to be applied:
the development of tools and databases for management and sharing of neuroscience data at all levels of analysis,
the development of tools for analyzing and modeling neuroscience data,
the development of computational models of the nervous system and neural processes.
In the recent decade, as vast amounts of diverse data about the brain were gathered by many research groups, the problem was raised of how to integrate the data from thousands of publications in order to enable efficient tools for further research. The biological and neuroscience data are highly interconnected and complex, and by itself, integration represents a great challenge for scientists.
Combining informatics research and brain research provides benefits for both fields of science. On one hand, informatics facilitates brain data processing and data handling, by providing new electronic and software technologies for arranging databases, modeling and communication in brain research. On the other hand, enhanced discoveries in the field of neuroscience will invoke the development of new methods in information technologies (IT).


== History ==
Starting in 1989, the United States National Institute of Mental Health (NIMH), the National Institute of Drug Abuse (NIDA) and the National Science Foundation (NSF) provided the National Academy of Sciences Institute of Medicine with funds to undertake a careful analysis and study of the need to create databases, share neuroscientific data and to examine how the field of information technology could create the tools needed for the increasing volume and modalities of neuroscientific data. The positive recommendations were reported in 1991. This positive report enabled NIMH, now directed by Allan Leshner, to create the ""Human Brain Project"" (HBP), with the first grants awarded in 1993. The HBP was led by Koslow along with cooperative efforts of other NIH Institutes, the NSF, the National Aeronautics and Space Administration and the Department of Energy. The HPG and grant-funding initiative in this area slightly preceded the explosive expansion of the World Wide Web. From 1993 through 2004 this program grew to over 100 million dollars in funded grants.
Next, Koslow pursued the globalization of the HPG and neuroinformatics through the European Union and the Office for Economic Co-operation and Development (OECD), Paris, France. Two particular opportunities occurred in 1996.
The first was the existence of the US/European Commission Biotechnology Task force co-chaired by Mary Clutter from NSF. Within the mandate of this committee, of which Koslow was a member the United States European Commission Committee on Neuroinformatics was established and co-chaired by Koslow from the United States. This committee resulted in the European Commission initiating support for neuroinformatics in Framework 5 and it has continued to support activities in neuroinformatics research and training.
A second opportunity for globalization of neuroinformatics occurred when the participating governments of the Mega Science Forum (MSF) of the OECD were asked if they had any new scientific initiatives to bring forward for scientific cooperation around the globe. The White House Office of Science and Technology Policy requested that agencies in the federal government meet at NIH to decide if cooperation were needed that would be of global benefit. The NIH held a series of meetings in which proposals from different agencies were discussed. The proposal recommendation from the U.S. for the MSF was a combination of the NSF and NIH proposals. Jim Edwards of NSF supported databases and data-sharing in the area of biodiversity; Koslow proposed the HPG as a model for sharing neuroscientific data, with the new moniker of neuroinformatics.
The two related initiates were combined to form the United States proposal on ""Biological Informatics"". This initiative was supported by the White House Office of Science and Technology Policy and presented at the OECD MSF by Edwards and Koslow. An MSF committee was established on Biological Informatics with two subcommittees: 1. Biodiversity (Chair, James Edwards, NSF), and 2. Neuroinformatics (Chair, Stephen Koslow, NIH). At the end of two years the Neuroinformatics subcommittee of the Biological Working Group issued a report supporting a global neuroinformatics effort. Koslow, working with the NIH and the White House Office of Science and Technology Policy to establishing a new Neuroinformatics working group to develop specific recommendation to support the more general recommendations of the first report. The Global Science Forum (GSF; renamed from MSF) of the OECD supported this recommendation.


=== The International Neuroinformatics Coordinating Facility ===

This committee presented 3 recommendations to the member governments of GSF. These recommendations were:
National neuroinformatics programs should be continued or initiated in each country should have a national node to both provide research resources nationally and to serve as the contact for national and international coordination.
An International Neuroinformatics Coordinating Facility (INCF) should be established. The INCF will coordinate the implementation of a global neuroinformatics network through integration of national neuroinformatics nodes.
A new international funding scheme should be established. This scheme should eliminate national and disciplinary barriers and provide a most efficient approach to global collaborative research and data sharing. In this new scheme, each country will be expected to fund the participating researchers from their country.
The GSF neuroinformatics committee then developed a business plan for the operation, support and establishment of the INCF which was supported and approved by the GSF Science Ministers at its 2004 meeting. In 2006 the INCF was created and its central office established and set into operation at the Karolinska Institute, Stockholm, Sweden under the leadership of Sten Grillner. Sixteen countries (Australia, Canada, China, the Czech Republic, Denmark, Finland, France, Germany, India, Italy, Japan, the Netherlands, Norway, Sweden, Switzerland, the United Kingdom and the United States), and the EU Commission established the legal basis for the INCF and Programme in International Neuroinformatics (PIN). To date, eighteen countries (Australia, Belgium, Czech Republic, Finland, France, Germany, India, Italy, Japan, Malaysia, Netherlands, Norway, Poland, Republic of Korea, Sweden, Switzerland, the United Kingdom and the United States) are members of the INCF. Membership is pending for several other countries.
The goal of the INCF is to coordinate and promote international activities in neuroinformatics. The INCF contributes to the development and maintenance of database and computational infrastructure and support mechanisms for neuroscience applications. The system is expected to provide access to all freely accessible human brain data and resources to the international research community. The more general task of INCF is to provide conditions for developing convenient and flexible applications for neuroscience laboratories in order to improve our knowledge about the human brain and its disorders.


=== Society for Neuroscience Brain Information Group ===
On the foundation of all of these activities, Huda Akil, the 2003 President of the Society for Neuroscience (SfN) established the Brain Information Group (BIG) to evaluate the importance of neuroinformatics to neuroscience and specifically to the SfN. Following the report from BIG, SfN also established a neuroinformatics committee.
In 2004, SfN announced the Neuroscience Database Gateway (NDG) as a universal resource for neuroscientists through which almost any neuroscience databases and tools may be reached. The NDG was established with funding from NIDA, NINDS and NIMH. The Neuroscience Database Gateway has transitioned to a new enhanced platform, the Neuroscience Information Framework. Funded by the NIH Neuroscience BLueprint, the NIF is a dynamic portal providing access to neuroscience-relevant resources (data, tools, materials) from a single search interface. The NIF builds upon the foundation of the NDG, but provides a unique set of tools tailored especially for neuroscientists: a more expansive catalog, the ability to search multiple databases directly from the NIF home page, a custom web index of neuroscience resources, and a neuroscience-focused literature search function.


== Collaboration with other disciplines ==
Neuroinformatics is formed at the intersections of the following fields:

Biology is concerned with molecular data (from genes to cell specific expression); medicine and anatomy with the structure of synapses and systems level anatomy; engineering – electrophysiology (from single channels to scalp surface EEG), brain imaging; computer science – databases, software tools, mathematical sciences – models, chemistry – neurotransmitters, etc. Neuroscience uses all aforementioned experimental and theoretical studies to learn about the brain through its various levels. Medical and biological specialists help to identify the unique cell types, and their elements and anatomical connections. Functions of complex organic molecules and structures, including a myriad of biochemical, molecular, and genetic mechanisms which regulate and control brain function, are determined by specialists in chemistry and cell biology. Brain imaging determines structural and functional information during mental and behavioral activity. Specialists in biophysics and physiology study physical processes within neural cells neuronal networks. The data from these fields of research is analyzed and arranged in databases and neural models in order to integrate various elements into a sophisticated system; this is the point where neuroinformatics meets other disciplines.
Neuroscience provides the following types of data and information on which neuroinformatics operates:
Molecular and cellular data (ion channel, action potential, genetics, cytology of neurons, protein pathways),
Data from organs and systems (visual cortex, perception, audition, sensory system, pain, taste, motor system, spinal cord),
Cognitive data (language, emotion, motor learning, sexual behavior, decision making, social neuroscience),
Developmental information (neuronal differentiation, cell survival, synaptic formation, motor differentiation, injury and regeneration, axon guidance, growth factors),
Information about diseases and aging (autonomic nervous system, depression, anxiety, Parkinson's disease, addiction, memory loss),
Neural engineering data (brain-computer interface), and
Computational neuroscience data (computational models of various neuronal systems, from membrane currents, proteins to learning and memory).
Neuroinformatics uses databases, the Internet, and visualization in the storage and analysis of the mentioned neuroscience data.


== Research programs and groups ==


=== Australia ===
Neuroimaging & Neuroinformatics, Howard Florey Institute, University of Melbourne
Institute scientists utilize brain imaging techniques, such as magnetic resonance imaging, to reveal the organization of brain networks involved in human thought. Led by Gary Egan.


=== Canada ===
McGill Centre for Integrative Neuroscience (MCIN), Montreal Neurological Institute, McGill University
Led by Alan Evans, MCIN conducts computationally-intensive brain research using innovative mathematical and statistical approaches to integrate clinical, psychological and brain imaging data with genetics. MCIN researchers and staff also develop infrastructure and software tools in the areas of image processing, databasing, and high performance computing. The MCIN community, together with the Ludmer Centre for Neuroinformatics and Mental Health, collaborates with a broad range of researchers and increasingly focuses on open data sharing and open science, including for the Montreal Neurological Institute.


=== Denmark ===
The THOR Center for Neuroinformatics
Established April 1998 at the Department of Mathematical Modelling, Technical University of Denmark. Besides pursuing independent research goals, the THOR Center hosts a number of related projects concerning neural networks, functional neuroimaging, multimedia signal processing, and biomedical signal processing.


=== Germany ===
The Neuroinformatics Portal Pilot
The project is part of a larger effort to enhance the exchange of neuroscience data, data-analysis tools, and modeling software. The portal is supported from many members of the OECD Working Group on Neuroinformatics. The Portal Pilot is promoted by the German Ministry for Science and Education.
Computational Neuroscience, ITB, Humboldt-University Berlin
This group focuses on computational neurobiology, in particular on the dynamics and signal processing capabilities of systems with spiking neurons. Lead by Andreas VM Herz.
The Neuroinformatics Group in Bielefeld
Active in the field of Artificial Neural Networks since 1989. Current research programmes within the group are focused on the improvement of man-machine-interfaces, robot-force-control, eye-tracking experiments, machine vision, virtual reality and distributed systems.


=== Italy ===
Laboratory of Computational Embodied Neuroscience (LOCEN)
This group, part of the Institute of Cognitive Sciences and Technologies, Italian National Research Council (ISTC-CNR) in Rome and founded in 2006 is currently led by Gianluca Baldassarre. It has two objectives: (a) understanding the brain mechanisms underlying learning and expression of sensorimotor behaviour, and related motivations and higher-level cognition grounded on it, on the basis of embodied computational models; (b) transferring the acquired knowledge to building innovative controllers for autonomous humanoid robots capable of learning in an open-ended fashion on the basis of intrinsic and extrinsic motivations.


=== Japan ===
Japan national neuroinformatics resource
The Visiome Platform is the Neuroinformatics Search Service that provides access to mathematical models, experimental data, analysis libraries and related resources. An online portal for neurophysiological data sharing is also available at BrainLiner.jp as part of the MEXT Strategic Research Program for Brain Sciences (SRPBS).
Laboratory for Mathematical Neuroscience, RIKEN Brain Science Institute (Wako, Saitama)
The target of Laboratory for Mathematical Neuroscience is to establish mathematical foundations of brain-style computations toward construction of a new type of information science. Led by Shun-ichi Amari.


=== The Netherlands ===
Netherlands state program in neuroinformatics
Started in the light of the international OECD Global Science Forum which aim is to create a worldwide program in Neuroinformatics.


=== Pakistan ===
NUST-SEECS Neuroinformatics Research Lab
Establishment of the Neuro-Informatics Lab at SEECS-NUST has enabled Pakistani researchers and members of the faculty to actively participate in such efforts, thereby becoming an active part of the above-mentioned experimentation, simulation, and visualization processes. The lab collaborates with the leading international institutions to develop highly skilled human resource in the related field. This lab facilitates neuroscientists and computer scientists in Pakistan to conduct their experiments and analysis on the data collected using state of the art research methodologies without investing in establishing the experimental neuroscience facilities. The key goal of this lab is to provide state of the art experimental and simulation facilities, to all beneficiaries including higher education institutes, medical researchers/practitioners, and technology industry.


=== Switzerland ===
The Blue Brain Project
The Blue Brain Project was founded in May 2005, and uses an 8000 processor Blue Gene/L supercomputer developed by IBM. At the time, this was one of the fastest supercomputers in the world.
The project involves:
Databases: 3D reconstructed model neurons, synapses, synaptic pathways, microcircuit statistics, computer model neurons, virtual neurons.
Visualization: microcircuit builder and simulation results visualizator, 2D, 3D and immersive visualization systems are being developed.
Simulation: a simulation environment for large-scale simulations of morphologically complex neurons on 8000 processors of IBM's Blue Gene supercomputer.
Simulations and experiments: iterations between large-scale simulations of neocortical microcircuits and experiments in order to verify the computational model and explore predictions.

The mission of the Blue Brain Project is to understand mammalian brain function and dysfunction through detailed simulations. The Blue Brain Project will invite researchers to build their own models of different brain regions in different species and at different levels of detail using Blue Brain Software for simulation on Blue Gene. These models will be deposited in an internet database from which Blue Brain software can extract and connect models together to build brain regions and begin the first whole brain simulations.
The Institute of Neuroinformatics (INI)
Established at the University of Zurich at the end of 1995, the mission of the Institute is to discover the key principles by which brains work and to implement these in artificial systems that interact intelligently with the real world.


=== United Kingdom ===
Genes to Cognition Project
A neuroscience research programme that studies genes, the brain and behaviour in an integrated manner. It is engaged in a large-scale investigation of the function of molecules found at the synapse. This is mainly focused on proteins that interact with the NMDA receptor, a receptor for the neurotransmitter, glutamate, which is required for processes of synaptic plasticity such as long-term potentiation (LTP). Many of the techniques used are high-throughout in nature, and integrating the various data sources, along with guiding the experiments has raised numerous informatics questions. The program is primarily run by Professor Seth Grant at the Wellcome Trust Sanger Institute, but there are many other teams of collaborators across the world.
The CARMEN project
The CARMEN project is a multi-site (11 universities in the United Kingdom) research project aimed at using GRID computing to enable experimental neuroscientists to archive their datasets in a structured database, making them widely accessible for further research, and for modellers and algorithm developers to exploit.
EBI Computational Neurobiology, EMBL-EBI (Hinxton)
The main goal of the group is to build realistic models of neuronal function at various levels, from the synapse to the micro-circuit, based on the precise knowledge of molecule functions and interactions (Systems Biology). Led by Nicolas Le Novère.


=== United States ===
Neuroscience Information Framework
The Neuroscience Information Framework (NIF) is an initiative of the NIH Blueprint for Neuroscience Research, which was established in 2004 by the National Institutes of Health. Unlike general search engines, NIF provides deeper access to a more focused set of resources that are relevant to neuroscience, search strategies tailored to neuroscience, and access to content that is traditionally ""hidden"" from web search engines. The NIF is a dynamic inventory of neuroscience databases, annotated and integrated with a unified system of biomedical terminology (i.e. NeuroLex). NIF supports concept-based queries across multiple scales of biological structure and multiple levels of biological function, making it easier to search for and understand the results. NIF will also provide a registry through which resources providers can disclose availability of resources relevant to neuroscience research. NIF is not intended to be a warehouse or repository itself, but a means for disclosing and locating resources elsewhere available via the web.
Neurogenetics GeneNetwork
Genenetwork started as component of the NIH Human Brain Project in 1999 with a focus on the genetic analysis of brain structure and function. This international program consists of tightly integrated genome and phenome data sets for human, mouse, and rat that are designed specifically for large-scale systems and network studies relating gene variants to differences in mRNA and protein expression and to differences in CNS structure and behavior. The great majority of data are open access. GeneNetwork has a companion neuroimaging web site—the Mouse Brain Library—that contains high resolution images for thousands of genetically defined strains of mice.
The Neuronal Time Series Analysis (NTSA)
NTSA Workbench is a set of tools, techniques and standards designed to meet the needs of neuroscientists who work with neuronal time series data. The goal of this project is to develop information system that will make the storage, organization, retrieval, analysis and sharing of experimental and simulated neuronal data easier. The ultimate aim is to develop a set of tools, techniques and standards in order to satisfy the needs of neuroscientists who work with neuronal data.
The Cognitive Atlas
The Cognitive Atlas is a project developing a shared knowledge base in cognitive science and neuroscience. This comprises two basic kinds of knowledge: tasks and concepts, providing definitions and properties thereof, and also relationships between them. An important feature of the site is ability to cite literature for assertions (e.g. ""The Stroop task measures executive control"") and to discuss their validity. It contributes to NeuroLex and the Neuroscience Information Framework, allows programmatic access to the database, and is built around semantic web technologies.
Brain Big Data research group at the Allen Institute for Brain Science (Seattle, WA)
Led by Hanchuan Peng, this group has focused on using large-scale imaging computing and data analysis techniques to reconstruct single neuron models and mapping them in brains of different animals.


== Technologies and developments ==
The main technological tendencies in neuroinformatics are:
Application of computer science for building databases, tools, and networks in neuroscience;
Analysis and modeling of neuronal systems.
In order to organize and operate with neural data scientists need to use the standard terminology and atlases that precisely describe the brain structures and their relationships.
Neuron Tracing and Reconstruction is an essential technique to establish digital models of the morphology of neurons. Such morphology is useful for neuron classification and simulation.
BrainML is a system that provides a standard XML metaformat for exchanging neuroscience data.
The Biomedical Informatics Research Network (BIRN) is an example of a grid system for neuroscience. BIRN is a geographically distributed virtual community of shared resources offering vast scope of services to advance the diagnosis and treatment of disease. BIRN allows combining databases, interfaces and tools into a single environment.
Budapest Reference Connectome is a web-based 3D visualization tool to browse connections in the human brain. Nodes, and connections are calculated from the MRI datasets of the Human Connectome Project.
GeneWays is concerned with cellular morphology and circuits. GeneWays is a system for automatically extracting, analyzing, visualizing and integrating molecular pathway data from the research literature. The system focuses on interactions between molecular substances and actions, providing a graphical view on the collected information and allows researchers to review and correct the integrated information.
Neocortical Microcircuit Database (NMDB). A database of versatile brain's data from cells to complex structures. Researchers are able not only to add data to the database but also to acquire and edit one.
SenseLab. SenseLab is a long-term effort to build integrated, multidisciplinary models of neurons and neural systems. It was founded in 1993 as part of the original Human Brain Project. A collection of multilevel neuronal databases and tools. SenseLab contains six related databases that support experimental and theoretical research on the membrane properties that mediate information processing in nerve cells, using the olfactory pathway as a model system.
BrainMaps.org is an interactive high-resolution digital brain atlas using a high-speed database and virtual microscope that is based on over 12 million megapixels of scanned images of several species, including human.
Another approach in the area of the brain mappings is the probabilistic atlases obtained from the real data from different group of people, formed by specific factors, like age, gender, diseased etc. Provides more flexible tools for brain research and allow obtaining more reliable and precise results, which cannot be achieved with the help of traditional brain atlases.


== See also ==


== Notes and references ==


== Bibliography ==


== Further reading ==


=== Books ===


=== Journals ==="
22,Graph isomorphism problem,1950766,35744,"The graph isomorphism problem is the computational problem of determining whether two finite graphs are isomorphic.
The problem is not known to be solvable in polynomial time nor to be NP-complete, and therefore may be in the computational complexity class NP-intermediate. It is known that the graph isomorphism problem is in the low hierarchy of class NP, which implies that it is not NP-complete unless the polynomial time hierarchy collapses to its second level. At the same time, isomorphism for many special classes of graphs can be solved in polynomial time, and in practice graph isomorphism can often be solved efficiently.
This problem is a special case of the subgraph isomorphism problem, which asks whether a given graph G contains a subgraph that is isomorphic to another given graph H and which is known to be NP-complete. It is also known to be a special case of the non-abelian hidden subgroup problem over the symmetric group.
In the area of image recognition it is known as the exact graph matching.


== State of the art ==
The best currently accepted theoretical algorithm is due to Babai & Luks (1983), and is based on the earlier work by Luks (1982) combined with a subfactorial algorithm of V. N. Zemlyachenko (Zemlyachenko, Korneenko & Tyshkevich 1985). The algorithm has run time 2O(√n log n) for graphs with n vertices and relies on the classification of finite simple groups. Without CFSG, a slightly weaker bound 2O(√n log2 n) was obtained first for strongly regular graphs by László Babai (1980), and then extended to general graphs by Babai & Luks (1983). Improvement of the exponent √n is a major open problem; for strongly regular graphs this was done by Spielman (1996). For hypergraphs of bounded rank, a subexponential upper bound matching the case of graphs was obtained by Babai & Codenotti (2008).
In November 2015, Babai announced a quasipolynomial time algorithm for all graphs, that is, one with running time 
  
    
      
        
          2
          
            O
            (
            (
            log
            ⁡
            n
            
              )
              
                c
              
            
            )
          
        
      
    
    {\displaystyle 2^{O((\log n)^{c})}}
   for some fixed 
  
    
      
        c
        >
        0
      
    
    {\displaystyle c>0}
  . On January 4 2017, Babai retracted the quasi-polynomial claim and stated a sub-exponential time bound instead after Harald Helfgott discovered a flaw in the proof. On January 9 2017, Babai announced a correction (published in full on January 19) and restored the quasi-polynomial claim, with Helfgott confirming the fix. Helfgott further claims that one can take c = 3, so the running time is 2O((log n)3). The new proof has not been fully peer-reviewed yet.
There are several competing practical algorithms for graph isomorphism, such as those due to McKay (1981), Schmidt & Druffel (1976), and Ullman (1976). While they seem to perform well on random graphs, a major drawback of these algorithms is their exponential time performance in the worst case.
The graph isomorphism problem is computationally equivalent to the problem of computing the automorphism group of a graph, and is weaker than the permutation group isomorphism problem and the permutation group intersection problem. For the latter two problems, Babai, Kantor & Luks (1983) obtained complexity bounds similar to that for graph isomorphism.


== Solved special cases ==
A number of important special cases of the graph isomorphism problem have efficient, polynomial-time solutions:
Trees
Planar graphs (In fact, planar graph isomorphism is in log space, a class contained in P)
Interval graphs
Permutation graphs
Circulant graphs
Bounded-parameter graphs
Graphs of bounded treewidth
Graphs of bounded genus (Note: planar graphs are graphs of genus 0)
Graphs of bounded degree
Graphs with bounded eigenvalue multiplicity
k-Contractible graphs (a generalization of bounded degree and bounded genus)
Color-preserving isomorphism of colored graphs with bounded color multiplicity (i.e., at most k vertices have the same color for a fixed k) is in class NC, which is a subclass of P


== Complexity class GI ==
Since the graph isomorphism problem is neither known to be NP-complete nor known to be tractable, researchers have sought to gain insight into the problem by defining a new class GI, the set of problems with a polynomial-time Turing reduction to the graph isomorphism problem. If in fact the graph isomorphism problem is solvable in polynomial time, GI would equal P.
As is common for complexity classes within the polynomial time hierarchy, a problem is called GI-hard if there is a polynomial-time Turing reduction from any problem in GI to that problem, i.e., a polynomial-time solution to a GI-hard problem would yield a polynomial-time solution to the graph isomorphism problem (and so all problems in GI). A problem 
  
    
      
        X
      
    
    {\displaystyle X}
   is called complete for GI, or GI-complete, if it is both GI-hard and a polynomial-time solution to the GI problem would yield a polynomial-time solution to 
  
    
      
        X
      
    
    {\displaystyle X}
  .
The graph isomorphism problem is contained in both NP and co-AM. GI is contained in and low for Parity P, as well as contained in the potentially much smaller class SPP. That it lies in Parity P means that the graph isomorphism problem is no harder than determining whether a polynomial-time nondeterministic Turing machine has an even or odd number of accepting paths. GI is also contained in and low for ZPPNP. This essentially means that an efficient Las Vegas algorithm with access to an NP oracle can solve graph isomorphism so easily that it gains no power from being given the ability to do so in constant time.


=== GI-complete and GI-hard problems ===


==== Isomorphism of other objects ====
There are a number of classes of mathematical objects for which the problem of isomorphism is a GI-complete problem. A number of them are graphs endowed with additional properties or restrictions:
digraphs
labelled graphs, with the proviso that an isomorphism is not required to preserve the labels, but only the equivalence relation consisting of pairs of vertices with the same label
""polarized graphs"" (made of a complete graph Km and an empty graph Kn plus some edges connecting the two; their isomorphism must preserve the partition)
2-colored graphs
explicitly given finite structures
multigraphs
hypergraphs
finite automata
Markov Decision Processes
commutative class 3 nilpotent (i.e., xyz = 0 for every elements x, y, z) semigroups
finite rank associative algebras over a fixed algebraically closed field with zero squared radical and commutative factor over the radical.
context-free grammars
balanced incomplete block designs
Recognizing combinatorial isomorphism of convex polytopes represented by vertex-facet incidences.


==== GI-complete classes of graphs ====
A class of graphs is called GI-complete if recognition of isomorphism for graphs from this subclass is a GI-complete problem. The following classes are GI-complete:
connected graphs
graphs of diameter 2 and radius 1
directed acyclic graphs
regular graphs
bipartite graphs without non-trivial strongly regular subgraphs
bipartite Eulerian graphs
bipartite regular graphs
line graphs
split graphs
chordal graphs
regular self-complementary graphs
polytopal graphs of general, simple, and simplicial convex polytopes in arbitrary dimensions.

Many classes of digraphs are also GI-complete.


==== Other GI-complete problems ====
There are other nontrivial GI-complete problems in addition to isomorphism problems.
The recognition of self-complementarity of a graph or digraph.
A clique problem for a class of so-called M-graphs. It is shown that finding an isomorphism for n-vertex graphs is equivalent to finding an n-clique in an M-graph of size n2. This fact is interesting because the problem of finding an (n − ε)-clique in a M-graph of size n2 is NP-complete for arbitrarily small positive ε.
The problem of homeomorphism of 2-complexes.


==== GI-hard problems ====
The problem of counting the number of isomorphisms between two graphs is polynomial-time equivalent to the problem of telling whether even one exists.
The problem of deciding whether two convex polytopes given by either the V-description or H-description are projectively or affinely isomorphic. The latter means existence of a projective or affine map between the spaces that contain the two polytopes (not necessarily of the same dimension) which induces a bijection between the polytopes.


== Program checking ==
Manuel Blum and Sampath Kannan (1995) have shown a probabilistic checker for programs for graph isomorphism. Suppose P is a claimed polynomial-time procedure that checks if two graphs are isomorphic, but it is not trusted. To check if G and H are isomorphic:
Ask P whether G and H are isomorphic.
If the answer is ""yes':
Attempt to construct an isomorphism using P as subroutine. Mark a vertex u in G and v in H, and modify the graphs to make them distinctive (with a small local change). Ask P if the modified graphs are isomorphic. If no, change v to a different vertex. Continue searching.
Either the isomorphism will be found (and can be verified), or P will contradict itself.

If the answer is ""no"":
Perform the following 100 times. Choose randomly G or H, and randomly permute its vertices. Ask P if the graph is isomorphic to G and H. (As in AM protocol for graph nonisomorphism).
If any of the tests are failed, judge P as invalid program. Otherwise, answer ""no"".

This procedure is polynomial-time and gives the correct answer if P is a correct program for graph isomorphism. If P is not a correct program, but answers correctly on G and H, the checker will either give the correct answer, or detect invalid behaviour of P. If P is not a correct program, and answers incorrectly on G and H, the checker will detect invalid behaviour of P with high probability, or answer wrong with probability 2−100.
Notably, P is used only as a blackbox.


== Applications ==
Graphs are commonly used to encode structural information in many fields, including computer vision and pattern recognition, and graph matching, i.e., identification of similarities between graphs, is an important tools in these areas. In these areas graph isomorphism problem is known as the exact graph matching. 
In cheminformatics and in mathematical chemistry, graph isomorphism testing is used to identify a chemical compound within a chemical database. Also, in organic mathematical chemistry graph isomorphism testing is useful for generation of molecular graphs and for computer synthesis.
Chemical database search is an example of graphical data mining, where the graph canonization approach is often used. In particular, a number of identifiers for chemical substances, such as SMILES and InChI, designed to provide a standard and human-readable way to encode molecular information and to facilitate the search for such information in databases and on the web, use canonization step in their computation, which is essentially the canonization of the graph which represents the molecule.
In electronic design automation graph isomorphism is the basis of the Layout Versus Schematic (LVS) circuit design step, which is a verification whether the electric circuits represented by a circuit schematic and an integrated circuit layout are the same.


== See also ==
Graph automorphism problem
Graph canonization


== Notes ==


== References =="
23,History of software,40601008,34227,"Software can be defined as programmed instructions stored in the memory of stored-program digital computers for execution by the processor. The design for what would have been the first piece of software was written by Ada Lovelace in the 19th century but was never implemented.
Alan Turing is credited with being the first person to come up with a theory for software, which led to the two academic fields of computer science and software engineering. The first generation of software for early stored program digital computers in the late 1940s had its instructions written directly in binary code. Early on, it was very expensive when it was in low quantities, but as it became more popular in the 1980s, prices dropped significantly. It went from being an item that only belonged to the elite to the majority of the population owning one.


== Before stored-program digital computers ==


=== Origins of computer science ===

An outline (algorithm) for what would have been the first piece of software was written by Ada Lovelace in the 19th century, for the planned Analytical Engine. However, neither the Analytical Engine nor any software for it was ever created.
The first theory about software –  prior to the creation of computers as we know them today –  was proposed by Alan Turing in his 1935 essay Computable numbers with an application to the Entscheidungsproblem (decision problem).
This eventually led to the creation of the twin academic fields of computer science and software engineering, which both study software and its creation. Computer science is more theoretical (Turing's essay is an example of computer science), whereas software engineering is focused on more practical concerns.
However, prior to 1946, software as we now understand it –  programs stored in the memory of stored-program digital computers –  did not yet exist. The very first electronic computing devices were instead rewired in order to ""reprogram"" them –  see History of computing hardware.


== Early days of computer software (1948–1979) ==
In his manuscript ""A Mathematical theory of Communication"", Claude Shannon (1916–2001) provided an outline for how binary logic could be implemented to program a computer. Subsequently, the first computer programmers used binary code to instruct computers to perform various tasks. Nevertheless, the process was very arduous. Computer programmers had to enter long strings of binary code to tell the computer what data to store. Computer programmers had to load information onto computers using various tedious mechanisms, including flicking switches or punching holes at predefined positions in cards and loading these punched cards into a computer. With such methods, if a mistake was made, the whole program might have to be loaded again from the beginning.
The very first time a stored-program computer held a piece of software in an electronic memory, and executed it successfully, was 11am, 21 June 1948, at the University of Manchester, on the Small Scale Experimental Machine, also known as the ""Baby"" computer. It was written by Tom Kilburn, and calculated the highest factor of the integer 2^18 = 262,144. Starting with a large trial divisor, it performed division of 262,144 by repeated subtraction then checked if the remainder was zero. If not, it decremented the trial divisor by one and repeated the process. Google released a tribute to the Manchester Baby, celebrating it as the ""birth of software"".


=== Bundling of software with hardware and its legal issues ===
Later, software was sold to multiple customers by being bundled with the hardware by original equipment manufacturers (OEMs) such as Data General, Digital Equipment and IBM. When a customer bought a minicomputer, at that time the smallest computer on the market, the computer did not come with Pre-installed software, but needed to be installed by engineers employed by the OEM.
This bundling attracted the attention of US antitrust regulators, who sued IBM for improper ""tying"" in 1969, alleging that it was an antitrust violation that customers who wanted to obtain its software had to also buy or lease its hardware in order to do so. Although the case was dropped by the US Justice Department after many years of attrition as ""without merit"".
Very quickly, commercial software started to be pirated, and commercial software producers were very unhappy at this. Bill Gates, cofounder of Microsoft, was an early moraliser against software piracy with his famous Open Letter to Hobbyists in 1976.
Data General also encountered legal problems related to bundling –  although in this case, it was due to a civil suit from a would-be competitor. When Data General introduced the Data General Nova, a company called Digidyne wanted to use its RDOS operating system on its own hardware clone. Data General refused to license their software and claimed their ""bundling rights"". The US Supreme Court set a precedent called Digidyne v. Data General in 1985 by letting a 9th circuit appeal court decision on the case stand, and Data General was eventually forced into licensing the operating system because it was ruled that restricting the license to only DG hardware was an illegal tying arrangement. Even though the District Court noted that ""no reasonable juror could find that within this large and dynamic market with much larger competitors"", Data General ""had the market power to restrain trade through an illegal tie-in arrangement"", the tying of the operating system to the hardware was ruled as per se illegal on appeal.
In 2008, Psystar Corporation was sued by Apple Inc. for distributing unauthorized Macintosh clones with OS X preinstalled, and countersued. One of the arguments in the countersuit - citing the Data General case - was that Apple dominates the market for OS X compatible computers by illegally tying the operating system to Apple computers. District Court Judge William Alsup rejected this argument, saying, as the District Court had ruled in the Data General case over 20 years prior, that the relevant market was not simply one operating system (Mac OS) but all PC operating systems, including Mac OS, and noting that Mac OS did not enjoy a dominant position in that broader market. Alsup's judgement also noted that the surprising Data General precedent that tying of copyrighted products was always illegal had since been ""implicitly overruled"" by the verdict in the Illinois Tool Works Inc. v. Independent Ink, Inc. case.


=== Unix (1970s–present) ===

Unix was an early operating system which became popular and very influential, and still exists today. The most popular variant of Unix today is macOS (previously OS X and Mac OS X), while Linux is closely related to Unix.


=== Pre-Internet source code sharing ===
Before the Internet –  and indeed in the period after the internet was created, but before it came into widespread use by the public –  computer programming enthusiasts had to find other ways to share their efforts with each other, and also with potentially-interested computer users who were not themselves programmers. Such sharing techniques included distribution of tapes, such as the DECUS tapes, and later, electronic bulletin board systems. However, a particularly popular and mainstream early technique involved computer magazines.


==== Source code listings in computer magazines ====

Tiny BASIC was published as a type-in program in Dr. Dobb's Journal in 1975, and developed collaboratively (in effect, an early example of open source software, although that particular term was not to be coined until two decades later).
It was an inconvenient and slow process to type in source code from a computer magazine, and a single mistyped –  or worse, misprinted –  character could render the program inoperable, yet people still did so. (Optical character recognition technology to scan in the listings rather than transcribe them by hand was not yet available).
However, even with the widespread use of cartridges and cassette tapes in the 1980s for distribution of commercial software, free programs (such as simple educational programs for the purpose of teaching programming techniques) were still often printed, because it was cheaper than manufacturing and attaching cassette tapes to each copy of a magazine. Many of today's IT professionals who were children at the time had a lifelong interest in computing in general or programming in particular sparked by such first encounters with source code.
However, eventually a combination of four factors brought this practice of printing complete source code listings of entire programs in computer magazines to an end:
programs started to become very large
floppy discs started to be used for distributing software, and then came down in price
more and more people started to use computers –  computing became a mass market phenomenon, and most ordinary people were far less likely to want to spend hours typing in listings than the earlier enthusiasts
partly as a consequence of all of the above factors, computer magazines started to attach free cassette tapes, and free floppy discs, with free or trial versions of software on them, to their covers


== 1980s–present ==
Before the microcomputer, a successful software program typically sold up to 1,000 units at $50,000–60,000 each. By the mid-1980s, personal computer software sold thousands of copies for $50–700 each. Companies like Microsoft, MicroPro, and Lotus Development had tens of millions of dollars in annual sales. Just like the auto industry, the software industry has grown from a few visionaries operating (figuratively or literally) out of their garage with prototypes. Steve Jobs and Bill Gates were the Henry Ford and Louis Chevrolet of their times, who capitalized on ideas already commonly known before they started in the business. A pivotal moment in computing history was the publication in the 1980s of the specifications for the IBM Personal Computer published by IBM employee Philip Don Estridge, which quickly led to the dominance of the PC in the worldwide desktop and later laptop markets –  a dominance which continues to this day.


=== Free and open source software ===


=== Recent developments ===


==== App stores ====

Applications for mobile devices (cellphones and tablets) have been termed ""apps"" in recent years. Apple chose to funnel iPhone and iPad app sales through their App Store, and thus both vet apps, and get a cut of every paid app sold. Apple does not allow apps which could be used to circumvent their app store (e.g. virtual machines such as the Java or Flash virtual machines).
The Android platform, by contrast, has multiple app stores available for it, and users can generally select which to use (although Google Play requires a compatible or rooted device).
This move was replicated for desktop operating systems with GNOME Software (for Linux), the Mac App Store (for macOS), and the Windows Store (for Windows). All of these platforms remain, as they have always been, non-exclusive: they allow applications to be installed from outside the app store, and indeed from other app stores.
The explosive rise in popularity of apps, for the iPhone in particular but also for Android, led to a kind of ""gold rush"", with some hopeful programmers dedicating a significant amount of time to creating apps in the hope of striking it rich. As in real gold rushes, not all of these hopeful entrepreneurs were successful.


== Formalization of software development ==
The development of curricula in computer science has resulted in improvements in software development. Components of these curricula include:
Structured and Object Oriented programming
Data structures
Analysis of Algorithms
Formal languages and compiler construction
Computer Graphics Algorithms
Sorting and Searching
Numerical Methods, Optimization and Statistics
Artificial Intelligence and Machine Learning


== How software has affected hardware ==
As more and more programs enter the realm of firmware, and the hardware itself becomes smaller, cheaper and faster as predicted by Moore's law, an increasing number of types of functionality of computing first carried out by software, have joined the ranks of hardware, as for example with graphics processing units. (However, the change has sometimes gone the other way for cost or other reasons, as for example with softmodems and microcode.)
Most hardware companies today have more software programmers on the payroll than hardware designers, since software tools have automated many tasks of printed circuit board (PCB) engineers.


== Computer software and programming language timeline ==
The following tables include year by year development of many different aspects of computer software including:
High level languages

Operating systems

Networking software and applications

Computer graphics hardware, algorithms and applications

Spreadsheets

Word processing

Computer aided design


=== 1971–1974 ===


=== 1975–1978 ===


=== 1979–1982 ===


=== 1983–1986 ===


=== 1987–1990 ===


=== 1991–1994 ===


=== 1995–1998 ===


=== 1999–2002 ===


=== 2003–2006 ===


=== 2007–2010 ===


=== 2011–2014 ===


== See also ==

Forensic software engineering
History of computing hardware
History of operating systems
History of software engineering
List of failed and overbudget custom software projects


== References =="
24,Computational neuroscience,271430,31195,"Computational neuroscience (also known as theoretical neuroscience or mathematical neuroscience) is a branch of neuroscience which employs mathematical models, theoretical analysis and abstractions of the brain to understand the principles that govern the development, structure, information-processing, physiology and cognitive abilities of the nervous system.
Computational neuroscience focuses on the description of functional and biologically realistic neurons (and neural systems) and their physiology and dynamics, distinguishing it from psychological connectionism and disciplines such as machine learning, neural networks, and computational learning theory.
These models are useful since they capture the essential features of the biological system at multiple spatial-temporal scales, from membrane currents, proteins, and chemical coupling to network oscillations, columnar and topographic architecture, and learning and memory. Furthermore, these computational models frame hypotheses that can be directly tested by biological or psychological experiments.


== History ==
The term ""computational neuroscience"" was introduced by Eric L. Schwartz, who organized a conference, held in 1985 in Carmel, California, at the request of the Systems Development Foundation to provide a summary of the current status of a field which until that point was referred to by a variety of names, such as neural modeling, brain theory and neural networks. The proceedings of this definitional meeting were published in 1990 as the book Computational Neuroscience. The first open international meeting focused on Computational Neuroscience was organized by James M. Bower and John Miller in San Francisco, California in 1989 and has continued each year since as the annual CNS meeting. The first graduate educational program in computational neuroscience was organized as the Computational and Neural Systems Ph.D. program at the California Institute of Technology in 1985.
The early historical roots of the field can be traced to the work of people such as Louis Lapicque, Hodgkin & Huxley, Hubel & Wiesel, and David Marr, to name a few. Lapicque introduced the integrate and fire model of the neuron in a seminal article published in 1907. This model is still popular today for mathematical, biological, and artificial neural networks studies because of its simplicity (see a recent review) and, in fact, has seen recent experimental and biophysical support.
About 40 years later, Hodgkin & Huxley developed the voltage clamp and created the first biophysical model of the action potential. Hubel & Wiesel discovered that neurons in the primary visual cortex, the first cortical area to process information coming from the retina, have oriented receptive fields and are organized in columns. David Marr's work focused on the interactions between neurons, suggesting computational approaches to the study of how functional groups of neurons within the hippocampus and neocortex interact, store, process, and transmit information. Computational modeling of biophysically realistic neurons and dendrites began with the work of Wilfrid Rall, with the first multicompartmental model using cable theory.


== Major topics ==
Research in computational neuroscience can be roughly categorized into several lines of inquiry. Most computational neuroscientists collaborate closely with experimentalists in analyzing novel data and synthesizing new models of biological phenomena.


=== Single-neuron modeling ===

Even single neurons have complex biophysical characteristics and can perform computations (e.g.). Hodgkin and Huxley's original model only employed two voltage-sensitive currents (Voltage sensitive ion channels are glycoprotein molecules which extend through the lipid bilayer, allowing ions to traverse under certain conditions through the axolemma), the fast-acting sodium and the inward-rectifying potassium. Though successful in predicting the timing and qualitative features of the action potential, it nevertheless failed to predict a number of important features such as adaptation and shunting. Scientists now believe that there are a wide variety of voltage-sensitive currents, and the implications of the differing dynamics, modulations, and sensitivity of these currents is an important topic of computational neuroscience.
The computational functions of complex dendrites are also under intense investigation. There is a large body of literature regarding how different currents interact with geometric properties of neurons.
Some models are also tracking biochemical pathways at very small scales such as spines or synaptic clefts.
There are many software packages, such as GENESIS and NEURON, that allow rapid and systematic in silico modeling of realistic neurons. Blue Brain, a project founded by Henry Markram from the École Polytechnique Fédérale de Lausanne, aims to construct a biophysically detailed simulation of a cortical column on the Blue Gene supercomputer.
Modeling the richness of biophysical properties on the single-neuron scale can supply mechanisms that serve as the building blocks for network dynamics. However, detailed neuron descriptions are computationally expensive and this can handicap the pursuit of realistic network investigations, where many neurons need to be simulated. As a result, researchers that study large neural circuits typically represent each neuron and synapse with an artificially simple model, ignoring much of the biological detail. Hence there is a drive to produce simplified neuron models that can retain significant biological fidelity at a low computational overhead. Algorithms have been developed to produce faithful, faster running, simplified surrogate neuron models from computationally expensive, detailed neuron models.


=== Development, axonal patterning, and guidance ===
Computational neuroscience aims to address a wide array of questions. How do axons and dendrites form during development? How do axons know where to target and how to reach these targets? How do neurons migrate to the proper position in the central and peripheral systems? How do synapses form? We know from molecular biology that distinct parts of the nervous system release distinct chemical cues, from growth factors to hormones that modulate and influence the growth and development of functional connections between neurons.
Theoretical investigations into the formation and patterning of synaptic connection and morphology are still nascent. One hypothesis that has recently garnered some attention is the minimal wiring hypothesis, which postulates that the formation of axons and dendrites effectively minimizes resource allocation while maintaining maximal information storage.


=== Sensory processing ===
Early models of sensory processing understood within a theoretical framework are credited to Horace Barlow. Somewhat similar to the minimal wiring hypothesis described in the preceding section, Barlow understood the processing of the early sensory systems to be a form of efficient coding, where the neurons encoded information which minimized the number of spikes. Experimental and computational work have since supported this hypothesis in one form or another.
Current research in sensory processing is divided among a biophysical modelling of different subsystems and a more theoretical modelling of perception. Current models of perception have suggested that the brain performs some form of Bayesian inference and integration of different sensory information in generating our perception of the physical world.


=== Memory and synaptic plasticity ===

Earlier models of memory are primarily based on the postulates of Hebbian learning. Biologically relevant models such as Hopfield net have been developed to address the properties of associative, rather than content-addressable, style of memory that occur in biological systems. These attempts are primarily focusing on the formation of medium- and long-term memory, localizing in the hippocampus. Models of working memory, relying on theories of network oscillations and persistent activity, have been built to capture some features of the prefrontal cortex in context-related memory.
One of the major problems in neurophysiological memory is how it is maintained and changed through multiple time scales. Unstable synapses are easy to train but also prone to stochastic disruption. Stable synapses forget less easily, but they are also harder to consolidate. One recent computational hypothesis involves cascades of plasticity that allow synapses to function at multiple time scales. Stereochemically detailed models of the acetylcholine receptor-based synapse with the Monte Carlo method, working at the time scale of microseconds, have been built. It is likely that computational tools will contribute greatly to our understanding of how synapses function and change in relation to external stimulus in the coming decades.


=== Behaviors of networks ===
Biological neurons are connected to each other in a complex, recurrent fashion. These connections are, unlike most artificial neural networks, sparse and usually specific. It is not known how information is transmitted through such sparsely connected networks, although specific areas of the brain, such as the Visual cortex, are understood in some detail. It is also unknown what the computational functions of these specific connectivity patterns are, if any.
The interactions of neurons in a small network can be often reduced to simple models such as the Ising model. The statistical mechanics of such simple systems are well-characterized theoretically. There has been some recent evidence that suggests that dynamics of arbitrary neuronal networks can be reduced to pairwise interactions. It is not known, however, whether such descriptive dynamics impart any important computational function. With the emergence of two-photon microscopy and calcium imaging, we now have powerful experimental methods with which to test the new theories regarding neuronal networks.
In some cases the complex interactions between inhibitory and excitatory neurons can be simplified using mean field theory, which gives rise to the population model of neural networks. While many neurotheorists prefer such models with reduced complexity, others argue that uncovering structural functional relations depends on including as much neuronal and network structure as possible. Models of this type are typically built in large simulation platforms like GENESIS or NEURON. There have been some attempts to provide unified methods that bridge and integrate these levels of complexity.


=== Cognition, discrimination, and learning ===
Computational modeling of higher cognitive functions has only recently begun. Experimental data comes primarily from single-unit recording in primates. The frontal lobe and parietal lobe function as integrators of information from multiple sensory modalities. There are some tentative ideas regarding how simple mutually inhibitory functional circuits in these areas may carry out biologically relevant computation.
The brain seems to be able to discriminate and adapt particularly well in certain contexts. For instance, human beings seem to have an enormous capacity for memorizing and recognizing faces. One of the key goals of computational neuroscience is to dissect how biological systems carry out these complex computations efficiently and potentially replicate these processes in building intelligent machines.
The brain's large-scale organizational principles are illuminated by many fields, including biology, psychology, and clinical practice. Integrative neuroscience attempts to consolidate these observations through unified descriptive models and databases of behavioral measures and recordings. These are the bases for some quantitative modeling of large-scale brain activity.
The Computational Representational Understanding of Mind (CRUM) is another attempt at modeling human cognition through simulated processes like acquired rule-based systems in decision making and the manipulation of visual representations in decision making.


=== Consciousness ===
One of the ultimate goals of psychology/neuroscience is to be able to explain the everyday experience of conscious life. Francis Crick and Christof Koch made some attempts to formulate a consistent framework for future work in neural correlates of consciousness (NCC), though much of the work in this field remains speculative.


=== Computational clinical neuroscience ===
It is a field that brings together experts in neuroscience, neurology, psychiatry, decision sciences and computational modeling to quantitatively define and investigate problems in neurological and psychiatric diseases, and to train scientists and clinicians that wish to apply these models to diagnosis and treatment.


== Notable persons ==
David Marr, neuroscientist and professor of psychology at MIT, noted for his theories of perception
Phil Husbands, professor of computer science and artificial intelligence at the English University of Sussex
Read Montague, American neuroscientist and popular science author
Tomaso Poggio, Eugene McDermott professor in the Department of Brain and Cognitive Sciences, investigator at the McGovern Institute for Brain Research, a member of the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) and director of both the Center for Biological and Computational Learning at MIT and the Center for Brains, Minds, and Machines.
Terry Sejnowski, investigator at the Howard Hughes Medical Institute and the Francis Crick Professor at The Salk Institute for Biological Studies where he directs the Computational Neurobiology Laboratory
Haim Sompolinsky, William N. Skirball Professor of Neuroscience at the Edmond and Lily Safra Center for Brain Sciences (formerly the Interdisciplinary Center for Neural Computation), and a Professor of Physics at the Racah Institute of Physics at The Hebrew University of Jerusalem, Israel.
Peter Dayan, Professor and Director of the Gatsby Computational Neuroscience Unit at University College London.


== See also ==
Biological neuron models
Bayesian Brain
Brain-computer interface
Brain simulation
Computational anatomy
Connectionism
Medical image computing
Mind uploading
Neural coding
Neural engineering
Neural network
Neurocomputational speech processing
Neuroinformatics
Simulated reality
Artificial consciousness
Cognitive architecture
Technological singularity, a hypothetical artificial intelligence that would exceed the capabilities of the human brain


== Notes and references ==


== Bibliography ==
Chklovskii DB (2004). ""Synaptic connectivity and neuronal morphology: two sides of the same coin"". Neuron. 43 (5): 609–17. doi:10.1016/j.neuron.2004.08.012. PMID 15339643. 
Sejnowski, Terrence J.; Churchland, Patricia Smith (1992). The computational brain. Cambridge, Mass: MIT Press. ISBN 0-262-03188-4. 
Gerstner, W.; Kistler, W.; Naud, R.; Paninski, L. (2014). Neuronal Dynamics. Cambridge, UK: Cambridge University Press. ISBN 9781107447615. 
Abbott, L. F.; Dayan, Peter (2001). Theoretical neuroscience: computational and mathematical modeling of neural systems. Cambridge, Mass: MIT Press. ISBN 0-262-04199-5. 
Eliasmith, Chris; Anderson, Charles H. (2003). Neural engineering: Representation, computation, and dynamics in neurobiological systems. Cambridge, Mass: MIT Press. ISBN 0-262-05071-4. 
Hodgkin AL, Huxley AF (28 August 1952). ""A quantitative description of membrane current and its application to conduction and excitation in nerve"". J. Physiol. 117 (4): 500–44. doi:10.1113/jphysiol.1952.sp004764. PMC 1392413 . PMID 12991237. 
William Bialek; Rieke, Fred; David Warland; Rob de Ruyter van Steveninck (1999). Spikes: exploring the neural code. Cambridge, Mass: MIT. ISBN 0-262-68108-0. CS1 maint: Multiple names: authors list (link)
Schutter, Erik de (2001). Computational neuroscience: realistic modeling for experimentalists. Boca Raton: CRC. ISBN 0-8493-2068-2. 
Sejnowski, Terrence J.; Hemmen, J. L. van (2006). 23 problems in systems neuroscience. Oxford [Oxfordshire]: Oxford University Press. ISBN 0-19-514822-3. 
Michael A. Arbib; Shun-ichi Amari; Prudence H. Arbib (2002). The Handbook of Brain Theory and Neural Networks. Cambridge, Massachusetts: The MIT Press. ISBN 0-262-01197-2. 


== External links ==


=== Journals ===
Network: Computation in Neural Systems
Biological Cybernetics
Journal of Computational Neuroscience
Neural Computation
Neural Networks
Neurocomputing
Cognitive Neurodynamics
Frontiers in Computational Neuroscience
PLoS Computational Biology
Frontiers in Neuroinformatics
Journal of Mathematical Neuroscience


=== Software ===
BRIAN, a Python based simulator
Budapest Reference Connectome, web based 3D visualization tool to browse connections in the human brain
Emergent, neural simulation software.
GENESIS, a general neural simulation system.


=== Conferences ===
Computational and Systems Neuroscience (COSYNE) – a computational neuroscience meeting with a systems neuroscience focus.
Annual Computational Neuroscience Meeting (CNS)– a yearly computational neuroscience meeting.
Neural Information Processing Systems (NIPS)– a leading annual conference covering other machine learning topics as well.
International Conference on Cognitive Neurodynamics (ICCN)– a yearly conference.
UK Mathematical Neurosciences Meeting– a new yearly conference, focused on mathematical aspects.
The NeuroComp Conference– a yearly computational neuroscience conference (France).
Bernstein Conference on Computational Neuroscience (BCCN)– a yearly conference in Germany, organized by the Bernstein Network for Computational Neuroscience.
AREADNE Conferences– a biennial meeting that includes theoretical and experimental results, held in even years in Santorini, Greece.


=== Websites ===
Perlewitz's computational neuroscience on the web
Encyclopedia of Computational Neuroscience, part of Scholarpedia, an online expert curated encyclopedia on computational neuroscience, dynamical systems and machine intelligence
Ph.D studies in Computational Neuroscience in Jerusalem


=== Research Groups ===
Laboratory of Computational Embodied Neuroscience (LOCEN), Institute of Cognitive Sciences and Technologies, Italian National Research Council (ISTC-CNR), Rome, Italy. This group, founded in 2006 and currently led by Gianluca Baldassarre, has two objectives: (a) understanding the brain mechanisms underlying learning and the expression of sensorimotor behaviour, the motivations driving it, and higher cognition grounded on it, on the basis of embodied computational models; (b) in synergies with these studies, building innovative controllers for autonomous humanoid robots able to learn in an open-ended fashion driven by intrinsic and extrinsic motivations."
25,History of programming languages,896120,31047,"The first electronic computers had limited speed and memory capacity, forcing programmers to write programs in assembly language, i.e., the native language of the hardware. Once computer capacity increased it became practical to implement higher level languages.
The first high-level programming language was Plankalkül, created by Konrad Zuse between 1942 and 1945. The first high-level language to have an associated compiler, was created by Corrado Böhm in 1951, for his PhD thesis. The first commercially available language was FORTRAN (FORmula TRANslation); developed in 1956 (first manual appeared in 1956, but first developed in 1954) by John Backus, a worker at IBM.
When FORTRAN was first introduced it was treated with suspicion because of the belief that programs compiled from high-level language would be less efficient than those written directly in machine code. FORTRAN became popular because it provided a means of porting existing code to new computers, in a hardware market that was rapidly evolving. FORTRAN eventually became known for its efficiency. Over the years, FORTRAN had been updated, with standards released for FORTRAN-66, FORTRAN-77 and FORTRAN-92.


== Early history ==
During a nine-month period in 1842–1843, Ada Lovelace translated the memoir of Italian mathematician Luigi Menabrea about Charles Babbage's newest proposed machine, the analytical engine. With the article she appended a set of notes which specified in complete detail a method for calculating Bernoulli numbers with the engine, recognized by some historians as the world's first computer program.
The first computer codes were specialized for their applications. In the first decades of the 20th century, numerical calculations were based on decimal numbers. Eventually it was realized that logic could be represented with numbers, not only with words. For example, Alonzo Church was able to express the lambda calculus in a formulatic way. The Turing machine was an abstraction of the operation of a tape-marking machine, for example, in use at the telephone companies. Turing machines set the basis for storage of programs as data in the von Neumann architecture of computers by representing a machine through a finite number. However, unlike the lambda calculus, Turing's code does not serve well as a basis for higher-level languages—its principal use is in rigorous analyses of algorithmic complexity.
To some people, what was the first modern programming language depends on how much power and human-readability is required before the status of ""programming language"" is granted. Jacquard Looms and Charles Babbage's Difference Engine both had simple, extremely limited languages for describing the actions that these machines should perform.


== First programming languages ==
In the 1940s, the first recognizably modern electrically powered computers were created. The limited speed and memory capacity forced programmers to write hand tuned assembly language programs. It was eventually realized that programming in assembly language required a great deal of intellectual effort.
The first programming languages designed to communicate instructions to a computer were written in the 1950s. An early high-level programming language to be designed for a computer was Plankalkül, developed by the Germans for Z1 by Konrad Zuse between 1943 and 1945. However, it was not implemented until 1998 and 2000.
John Mauchly's Short Code, proposed in 1949, was one of the first high-level languages ever developed for an electronic computer. Unlike machine code, Short Code statements represented mathematical expressions in understandable form. However, the program had to be translated into machine code every time it ran, making the process much slower than running the equivalent machine code.
At the University of Manchester, Alick Glennie developed Autocode in the early 1950s, with the second iteration developed for the Mark 1 by R. A. Brooker in 1954, known as the ""Mark 1 Autocode"". Brooker also developed an autocode for the Ferranti Mercury in the 1950s in conjunction with the University of Manchester. The version for the EDSAC 2 was devised by D. F. Hartley of University of Cambridge Mathematical Laboratory in 1961. Known as EDSAC 2 Autocode, it was a straight development from Mercury Autocode adapted for local circumstances, and was noted for its object code optimisation and source-language diagnostics which were advanced for the time. A contemporary but separate thread of development, Atlas Autocode was developed for the University of Manchester Atlas 1 machine.
In 1954, language FORTRAN was invented at IBM by a team led by John Backus; it was the first widely used high level general purpose programming language to have a functional implementation, as opposed to just a design on paper. It is still a popular language for high-performance computing and is used for programs that benchmark and rank the world's fastest supercomputers.
Another early programming language was devised by Grace Hopper in the US, called FLOW-MATIC. It was developed for the UNIVAC I at Remington Rand during the period from 1955 until 1959. Hopper found that business data processing customers were uncomfortable with mathematical notation, and in early 1955, she and her team wrote a specification for an English programming language and implemented a prototype. The FLOW-MATIC compiler became publicly available in early 1958 and was substantially complete in 1959. Flow-Matic was a major influence in the design of COBOL, since only it and its direct descendent AIMACO were in actual use at the time.
Other languages still in use today include LISP (1958), invented by John McCarthy and COBOL (1959), created by the Short Range Committee. Another milestone in the late 1950s was the publication, by a committee of American and European computer scientists, of ""a new language for algorithms""; the ALGOL 60 Report (the ""ALGOrithmic Language""). This report consolidated many ideas circulating at the time and featured three key language innovations:
nested block structure: code sequences and associated declarations could be grouped into blocks without having to be turned into separate, explicitly named procedures;
lexical scoping: a block could have its own private variables, procedures and functions, invisible to code outside that block, that is, information hiding.
Another innovation, related to this, was in how the language was described:
a mathematically exact notation, Backus–Naur form (BNF), was used to describe the language's syntax. Nearly all subsequent programming languages have used a variant of BNF to describe the context-free portion of their syntax.
Algol 60 was particularly influential in the design of later languages, some of which soon became more popular. The Burroughs large systems were designed to be programmed in an extended subset of Algol.
Algol's key ideas were continued, producing ALGOL 68:
syntax and semantics became even more orthogonal, with anonymous routines, a recursive typing system with higher-order functions, etc.;
not only the context-free part, but the full language syntax and semantics were defined formally, in terms of Van Wijngaarden grammar, a formalism designed specifically for this purpose.
Algol 68's many little-used language features (for example, concurrent and parallel blocks) and its complex system of syntactic shortcuts and automatic type coercions made it unpopular with implementers and gained it a reputation of being difficult. Niklaus Wirth actually walked out of the design committee to create the simpler Pascal language.

Some notable languages that were developed in this period include:


== Establishing fundamental paradigms ==

The period from the late 1960s to the late 1970s brought a major flowering of programming languages. Most of the major language paradigms now in use were invented in this period:
Speakeasy (computational environment), developed in 1964 at Argonne National Laboratory (ANL) by Stanley Cohen, is an OOPS (object-oriented programming, much like the later MATLAB, IDL (programming language) and Mathematica) numerical package. Speakeasy has a clear Fortran foundation syntax. It first addressed efficient physics computation internally at ANL, was modified for research use (as ""Modeleasy"") for the Federal Reserve Board in the early 1970s and then was made available commercially; Speakeasy and Modeleasy are still in use currently.
Simula, invented in the late 1960s by Nygaard and Dahl as a superset of Algol 60, was the first language designed to support object-oriented programming.
C, an early systems programming language, was developed by Dennis Ritchie and Ken Thompson at Bell Labs between 1969 and 1973.
Smalltalk (mid-1970s) provided a complete ground-up design of an object-oriented language.
Prolog, designed in 1972 by Colmerauer, Roussel, and Kowalski, was the first logic programming language.
ML built a polymorphic type system (invented by Robin Milner in 1973) on top of Lisp, pioneering statically typed functional programming languages.
Each of these languages spawned an entire family of descendants, and most modern languages count at least one of them in their ancestry.
The 1960s and 1970s also saw considerable debate over the merits of ""structured programming"", which essentially meant programming without the use of ""goto"". A significant fraction of programmers believed that, even in languages that provide ""goto"", it is bad programming style to use it except in rare circumstances. This debate was closely related to language design: some languages did not include a ""goto"" at all, which forced structured programming on the programmer.
To provide even faster compile times, some languages were structured for ""one-pass compilers"" which expect subordinate routines to be defined first, as with Pascal, where the main routine, or driver function, is the final section of the program listing.
Some notable languages that were developed in this period include:


== 1980s: consolidation, modules, performance ==

The 1980s were years of relative consolidation in imperative languages. Rather than inventing new paradigms, all of these movements elaborated upon the ideas invented in the previous decade. C++ combined object-oriented and systems programming. The United States government standardized Ada, a systems programming language intended for use by defense contractors. In Japan and elsewhere, vast sums were spent investigating so-called fifth-generation programming languages that incorporated logic programming constructs. The functional languages community moved to standardize ML and Lisp. Research in Miranda, a functional language with lazy evaluation, began to take hold in this decade.
One important new trend in language design was an increased focus on programming for large-scale systems through the use of modules, or large-scale organizational units of code. Modula, Ada, and ML all developed notable module systems in the 1980s. Module systems were often wedded to generic programming constructs---generics being, in essence, parametrized modules (see also polymorphism in object-oriented programming).
Although major new paradigms for imperative programming languages did not appear, many researchers expanded on the ideas of prior languages and adapted them to new contexts. For example, the languages of the Argus and Emerald systems adapted object-oriented programming to distributed systems.
The 1980s also brought advances in programming language implementation. The RISC movement in computer architecture postulated that hardware should be designed for compilers rather than for human assembly programmers. Aided by processor speed improvements that enabled increasingly aggressive compilation techniques, the RISC movement sparked greater interest in compilation technology for high-level languages.
Language technology continued along these lines well into the 1990s.
Some notable languages that were developed in this period include:


== 1990s: the Internet age ==

The rapid growth of the Internet in the mid-1990s was the next major historic event in programming languages. By opening up a radically new platform for computer systems, the Internet created an opportunity for new languages to be adopted. In particular, the JavaScript programming language rose to popularity because of its early integration with the Netscape Navigator web browser. Various other scripting languages achieved widespread use in developing customized applications for web servers such as PHP. The 1990s saw no fundamental novelty in imperative languages, but much recombination and maturation of old ideas. This era began the spread of functional languages. A big driving philosophy was programmer productivity. Many ""rapid application development"" (RAD) languages emerged, which usually came with an IDE, garbage collection, and were descendants of older languages. All such languages were object-oriented. These included Object Pascal, Visual Basic, and Java. Java in particular received much attention.
More radical and innovative than the RAD languages were the new scripting languages. These did not directly descend from other languages and featured new syntaxes and more liberal incorporation of features. Many consider these scripting languages to be more productive than even the RAD languages, but often because of choices that make small programs simpler but large programs more difficult to write and maintain. Nevertheless, scripting languages came to be the most prominent ones used in connection with the Web.
Some notable languages that were developed in this period include:


== Current trends ==
Programming language evolution continues, in both industry and research. Some of the recent trends have included:

Increasing support for functional programming in mainstream languages used commercially, including pure functional programming for making code easier to reason about and easier to parallelise (at both micro- and macro- levels)
Constructs to support concurrent and distributed programming.
Mechanisms for adding security and reliability verification to the language: extended static checking, dependent typing, information flow control, static thread safety.
Alternative mechanisms for composability and modularity: mixins, traits, delegates, aspects.
Component-oriented software development.
Metaprogramming, reflection or access to the abstract syntax tree
AOP or Aspect Oriented Programming allowing developers to insert code in another module or class at ""join points""
Domain specific languages and code generation
XML for graphical interface (XUL, XAML)

Increased interest in distribution and mobility.
Integration with databases, including XML and relational databases.
Open source as a developmental philosophy for languages, including the GNU Compiler Collection and languages such as Python, Ruby, and Scala.
Massively parallel languages for coding 2000 processor GPU graphics processing units and supercomputer arrays including OpenCL
Early research into (as-yet-unimplementable) quantum computing programming languages
More interest in visual programming languages like Scratch
Some notable languages developed during this period include:


== Prominent people ==

Some key people who helped develop programming languages:
Alan Cooper, developer of Visual Basic.
Alan Kay, pioneering work on object-oriented programming, and originator of Smalltalk.
Anders Hejlsberg, developer of Turbo Pascal, Delphi, C#, and TypeScript.
Bertrand Meyer, inventor of Eiffel.
Bjarne Stroustrup, developer of C++.
Brian Kernighan, co-author of the first book on the C programming language with Dennis Ritchie, coauthor of the AWK and AMPL programming languages.
Chris Lattner, creator of Swift and LLVM.
Dennis Ritchie, inventor of C. Unix Operating System, Plan 9 Operating System.
Grace Hopper, first to use the term compiler and developer of Flow-Matic, influenced development of COBOL. Popularized machine-independent programming languages and the term ""debugging"".
Guido van Rossum, creator of Python.
James Gosling, lead developer of Java and its precursor, Oak.
Jean Ichbiah, chief designer of Ada, Ada 83.
Jean-Yves Girard, co-inventor of the polymorphic lambda calculus (System F).
Jeff Bezanson, main designer, and one of the core developers of Julia.
Joe Armstrong, creator of Erlang.
John Backus, inventor of Fortran and cooperated in the design of ALGOL 58 and ALGOL 60.
John C. Reynolds, co-inventor of the polymorphic lambda calculus (System F).
John McCarthy, inventor of LISP.
John von Neumann, originator of the operating system concept.
Graydon Hoare, inventor of Rust.
Ken Thompson, inventor of B, Go Programming Language, Inferno Programming Language, and Unix Operating System co-author.
Kenneth E. Iverson, developer of APL, and co-developer of J along with Roger Hui.
Konrad Zuse, designed the first high-level programming language, Plankalkül (which influenced ALGOL 58).
Kristen Nygaard, pioneered object-oriented programming, co-invented Simula.
Larry Wall, creator of the Perl programming language (see Perl and Perl 6).
Martin Odersky, creator of Scala, and previously a contributor to the design of Java.
Nathaniel Rochester, inventor of first assembler (IBM 701).
Niklaus Wirth, inventor of Pascal, Modula and Oberon.
Ole-Johan Dahl, pioneered object-oriented programming, co-invented Simula.
Rasmus Lerdorf, creator of PHP
Rich Hickey, creator of Clojure.
Robin Milner, inventor of ML, and sharing credit for Hindley–Milner polymorphic type inference.
Stephen Wolfram, creator of Mathematica.
Tom Love and Brad Cox, creator of Objective-C.
Walter Bright, creator of D.
Yukihiro Matsumoto, creator of Ruby.


== See also ==


== References ==


== Further reading ==
Rosen, Saul, (editor), Programming Systems and Languages, McGraw-Hill, 1967.
Sammet, Jean E., Programming Languages: History and Fundamentals, Prentice-Hall, 1969.
Sammet, Jean E. (July 1972). ""Programming Languages: History and Future"". Communications of the ACM. 15 (7): 601–610. doi:10.1145/361454.361485. 
Richard L. Wexelblat (ed.): History of Programming Languages, Academic Press 1981.
Thomas J. Bergin and Richard G. Gibson (eds.): History of Programming Languages, Addison Wesley, 1996.


== External links ==
History and evolution of programming languages
Graph of programming language history"
26,Here document,1426425,30591,"In computing, a here document (here-document, here-text, heredoc, hereis, here-string or here-script) is a file literal or input stream literal: it is a section of a source code file that is treated as if it were a separate file. The term is also used for a form of multiline string literals that use similar syntax, preserving line breaks and other whitespace (including indentation) in the text.
Here documents originate in the Unix shell, and are found in sh, csh, ksh, bash and zsh, among others. Here document-style string literals are found in various high-level languages, notably the Perl programming language (syntax inspired by Unix shell) and languages influenced by Perl, such as PHP and Ruby. Other high-level languages such as Python and Tcl have other facilities for multiline strings.
Here documents can be treated either as files or strings. Some shells treat them as a format string literal, allowing variable substitution and command substitution inside the literal.
The most common syntax for here documents, originating in Unix shells, is << followed by a delimiting identifier (often EOF or END), followed, starting on the next line, by the text to be quoted, and then closed by the same delimiting identifier on its own line. This syntax is because here documents are formally stream literals, and the content of the document is redirected to stdin (standard input) of the preceding command; the here document syntax is by analogy with the syntax for input redirection, which is < ""take input from the following file"".
Other languages often use substantially similar syntax, but details of syntax and actual functionality can vary significantly. When used simply for string literals, the << does not indicate indirection, but is simply a starting delimiter convention. In some languages, such as Ruby, << is also used for input redirection, thus resulting in << being used twice if one wishes to redirect from a here document string literal.


== File literals ==
Narrowly speaking, here documents are file literals or stream literals. These originate in the Unix shell, though similar facilities are available in some other languages.


=== Unix shells ===
Here documents are available in many Unix shells.
In the following example, text is passed to the tr command (transliterating lower to upper-case) using a here document. This could be in a shell file, or entered interactively at a prompt.

END_TEXT was used as the delimiting identifier. It specified the start and end of the here document. The redirect and the delimiting identifier do not need to be separated by a space: <<END_TEXT or << END_TEXT both work equally well.
Appending a minus sign to the << has the effect that leading tabs are ignored. This allows indenting here documents in shell scripts (primarily for alignment with existing indentation) without changing their value:

This yields the same output, notably not indented.
By default, behavior is largely identical to the contents of double quotes: variables are interpolated, commands in backticks are evaluated, etc.

This can be disabled by quoting any part of the label, which is then ended by the unquoted value; the behavior is essentially identical to that if the contents were enclosed in single quotes. Thus for example by setting it in single quotes:

Double quotes may also be used, but this is subject to confusion, because expansion does occur in a double-quoted string, but does not occur in a here document with double-quoted delimiter. Single- and double-quoted delimiters are distinguished in some other languages, notably Perl (see below), where behavior parallels the corresponding string quoting.


==== Here strings ====
A here string (available in Bash, ksh, or zsh) is syntactically similar, consisting of <<<, and effects input redirection from a word (a sequence treated as a unit by the shell, in this context generally a string literal). In this case the usual shell syntax is used for the word (""here string syntax""), with the only syntax being the redirection: a here string is an ordinary string used for input redirection, not a special kind of string.
A single word need not be quoted:

In case of a string with spaces, it must be quoted:

This could also be written as:

Multiline strings are acceptable, yielding:

Note that leading and trailing newlines, if present, are included:

The key difference from here documents is that, in here documents, the delimiters are on separate lines; the leading and trailing newlines are stripped. Here, the terminating delimiter can be specified.
Here strings are particularly useful for commands that often take short input, such as the calculator bc:

Note that here string behavior can also be accomplished (reversing the order) via piping and the echo command, as in:

however here strings are particularly useful when the last command needs to run in the current process, as is the case with the read builtin:

yields nothing, while

This happens because in the previous example piping causes read to run in a subprocess, and as such can not affect the environment of the parent process.


=== Microsoft NMAKE ===
In Microsoft NMAKE, here documents are referred to as inline files. Inline files are referenced as << or <<pathname: the first notation creates a temporary file, the second notation creates (or overwrites) the file with the specified pathname. An inline file is terminated with << on a line by itself, optionally followed by the (case-insensitive) keyword KEEP or NOKEEP to indicate whether the created file should be kept.

target0: dependent0
    command0 <<
temporary inline file
...
<<

target1: dependent1
    command1 <<
temporary, but preserved inline file
...
<<KEEP

target2: dependent2
    command2 <<filename2
named, but discarded inline file
...
<<NOKEEP

target3: dependent3
    command3 <<filename3
named inline file
...
<<KEEP


=== R ===
R does not have file literals, but provides equivalent functionality by combining string literals with a string-to-file function. R allows arbitrary whitespace, including newlines, in strings. A string then can be turned into a file descriptor using the textConnection() function. For example, the following turns a data table embedded in the source code into a data-frame variable:


=== Data segment ===
Perl and Ruby have a form of file literal, which can be considered a form of data segment. In these languages, including the line __DATA__ (Perl) or __END__ (Ruby, old Perl) marks the end of the code segment and the start of the data segment. Only the contents prior to this line are executed, and the contents of the source file after this line are available as a file object: PACKAGE::DATA in Perl (e.g., main::DATA) and DATA in Ruby. As an inline file, these are semantically similar to here documents, though there can be only one per script. However, in these languages the term ""here document"" instead refers to multiline string literals, as discussed below.


=== Data URI Scheme ===
As further explained in Data URI scheme, all major web browsers understand URIs that start with data: as here document.


== Multiline string literals ==
The term ""here document"" or ""here string"" is also used for multiline string literals in various programming languages, notably Perl (syntax influenced by Unix shell), and languages influenced by Perl, notably PHP and Ruby. The shell-style << syntax is often retained, despite not being used for input redirection.


=== Perl-influenced ===


==== Perl ====
In Perl there are several different ways to invoke here docs. The delimiters around the tag have the same effect within the here doc as they would in a regular string literal: For example, using double quotes around the tag allows variables to be interpolated, but using single quotes doesn't, and using the tag without either behaves like double quotes. Using backticks as the delimiters around the tag runs the contents of the heredoc as a shell script. It is necessary to make sure that the end tag is at the beginning of the line or the tag will not be recognized by the interpreter.
Note that the here doc does not start at the tag—but rather starts on the next line. So the statement containing the tag continues on after the tag.
Here is an example with double quotes:

Output:

Dear Spike,

I wish you to leave Sunnydale and never return.

Not Quite Love,
Buffy the Vampire Slayer

Here is an example with single quotes:

Output:

Dear $recipient,

I wish you to leave Sunnydale and never return.

Not Quite Love,
$sender

And an example with backticks (may not be portable):

It is possible to start multiple heredocs on the same line:

The tag itself may contain whitespace, which may allow heredocs to be used without breaking indentation.

In addition to these strings, Perl also features file literals, namely the contents of the file following __DATA__ (formerly __END__) on a line by itself. This is accessible as the file object PACKAGE::DATA such as main::DATA, and can be viewed as a form of data segment.


==== PHP ====
In PHP, here documents are referred to as heredocs.

Outputs

This is a heredoc section.
For more information talk to Joe Smith, your local Programmer.

Thanks!

Hey Joe Smith! You can actually assign the heredoc section to a variable!

The line containing the closing identifier must not contain any other characters, except an optional ending semicolon. Otherwise, it will not be considered to be a closing identifier, and PHP will continue looking for one. If a proper closing identifier is not found, a parse error will result at the last line of the script.
In PHP 5.3 and later, like Perl, it is possible to not interpolate variables by surrounding the tag with single quotes; this is called a nowdoc:

In PHP 5.3+ it is also possible to surround the tag with double quotes, which like Perl has the same effect as not surrounding the tag with anything at all.


==== Ruby ====
The following Ruby code displays a grocery list by using a here document.

The result:

The << in a here document does not indicate input redirection, but Ruby also uses << for input redirection, so redirecting to a file from a here document involves using << twice, in different senses:

As with Unix shells, Ruby also allows for the delimiting identifier not to start on the first column of a line, if the start of the here document is marked with the slightly different starter ""<<-"". Besides, Ruby treats here documents as a double-quoted string, and as such, it is possible to use the #{} construct to interpolate code. The following example illustrates both of these features:

Ruby expands on this by providing the ""<<~"" syntax for omitting indentation on the here document:

The common indentation of two spaces is omitted from all lines:

Like Perl, Ruby allows for starting multiple here documents in one line:

As with Perl, Ruby features file literals, namely the contents of the file following __END__ on a line by itself. This is accessible as the file object DATA and can be viewed as a form of data segment.


=== Others ===


==== Python ====
Python supports multi-line strings as a ""verbatim"" string.

From 3.6, verbatim f-strings support variable and expression interpolation.


==== D ====
Since version 2.0, D has support for here document-style strings using the 'q' prefix character. These strings begin with q""IDENT followed immediately by a newline (for an arbitrary identifier IDENT), and end with IDENT"" at the start of a line.

D also supports a few quoting delimiters, with similar syntax, with such strings starting with q""[ and ending with ]"" or similarly for other delimiter character (any of () <> {} or []).


==== OS/JCL ====
On IBM's Job Control Language (JCL) used on its earlier MVS and current z/OS operating systems, data which is inline to a job stream can be identified by an * on a DD statement, such as //SYSIN DD * or //SYSIN DD *,DLM=text In the first case, the lines of text follow and are combined into a pseudo file with the DD name SYSIN. All records following the command are combined until either another OS/JCL command occurs (any line beginning with //), the default EOF sequence (/*) is found, or the physical end of data occurs. In the second case, the conditions are the same, except the DLM= operand is used to specify the text string signalling end of data, which can be used if a data stream contains JCL (again, any line beginning with //), or the /* sequence (such as comments in C or C++ source code). The following compiles and executes an assembly language program, supplied as in-line data to the assembler.

The //SYSIN DD * statement is the functional equivalent of <</* Indicating s stream of data follows, terminated by /*.


==== Racket ====
Racket's here strings start with #<< followed by characters that define a terminator for the string. The content of the string includes all characters between the #<< line and a line whose only content is the specified terminator. More precisely, the content of the string starts after a newline following #<<, and it ends before a newline that is followed by the terminator.

Outputs:

This is a simple here string in Racket.
  * One
  * Two
  * Three

No escape sequences are recognized between the starting and terminating lines; all characters are included in the string (and terminator) literally.

Outputs:

This string spans for multiple lines
and can contain any Unicode symbol.
So things like λ, ☠, α, β, are all fine.

In the next line comes the terminator. It can contain any Unicode symbol as well, even spaces and smileys!

Here strings can be used normally in contexts where normal strings would:

Outputs:

Dear Isaac,

Thanks for the insightful conversation yesterday.

                Carl

An interesting alternative is to use the language extension at-exp to write @-expressions. They look like this:

#lang at-exp racket

(displayln @string-append{
This is a long string,
very convenient when a
long chunk of text is
needed.

No worries about escaping
""quotes"" or \escapes. It's
also okay to have λ, γ, θ, ...

Embed code: @(number->string (+ 3 4))
})

Outputs:

This is a long string,
very convenient when a
long chunk of text is
needed.

No worries about escaping
""quotes"" or \escapes. It's
also okay to have λ, γ, θ, ...

Embed code: 7

An @-expression is not specific nor restricted to strings, it is a syntax form that can be composed with the rest of the language.


==== Windows PowerShell ====
In Windows PowerShell, here documents are referred to as here-strings. A here-string is a string which starts with an open delimiter (@"" or @') and ends with a close delimiter (""@ or '@) on a line by itself, which terminates the string. All characters between the open and close delimiter are considered the string literal. Using a here-string with double quotes allows variables to be interpreted, using single quotes doesn't. Variable interpolation occurs with simple variables (e.g. $x but NOT $x.y or $x[0]). You can execute a set of statements by putting them in $() (e.g. $($x.y) or $(Get-Process | Out-String)).
In the following PowerShell code, text is passed to a function using a here-string. The function ConvertTo-UpperCase is defined as follows:

Here is an example that demonstrates variable interpolation and statement execution using a here-string with double quotes:

Using a here-string with single quotes instead, the output would look like this: Output:


==== DIGITAL Command Language (DCL) ====
In DCL scripts, any input line which does not begin with a $ symbol is implicitly treated as input to the preceding command - all lines which do not begin with $ are here-documents. The input is either passed to the program, or can be explicitly referenced by the logical name SYS$INPUT (analogous to the Unix concept of stdin).
For instance, explicitly referencing the input as SYS$INPUT:

produces:

Additionally, the DECK command, initially intended for punched card support (hence its name: it signified the beginning of a data deck) can be used to supply input to the preceding command. The input deck is ended either by the command $ EOD, or the character pattern specified by the /DOLLARS parameter to DECK.
Example of a program totalling up monetary values:

Would produce the following output (presuming ADD_SUMS was written to read the values and add them):

Example of using DECK /DOLLARS to create one command file from another:


== See also ==
Pipeline (Unix) for information about pipes
String literal


== References ==


=== General ===


== External links ==
Here document. Link to Rosetta Code task with examples of here documents in over 15 languages."
27,History of computer science,3271413,29971,"The history of computer science began long before our modern discipline of computer science. Developments in previous centuries alluded to the disipline that we now know as computer science. This progression, from mechanical inventions and mathematical theories towards modern computer concepts and machines, led to the development of a major academic field and the basis of a massive worldwide industry.


== Prehistory ==
The earliest known tool for use in computation was the abacus, developed in the period between 2700–2300 BCE in Sumer. The Sumerians' abacus consisted of a table of successive columns which delimited the successive orders of magnitude of their sexagesimal number system. Its original style of usage was by lines drawn in sand with pebbles . Abaci of a more modern design are still used as calculation tools today, such as the Chinese abacus.
In the 5th century BC in ancient India, the grammarian Pāṇini formulated the grammar of Sanskrit in 3959 rules known as the Ashtadhyayi which was highly systematized and technical. Panini used metarules, transformations and recursions.
The Antikythera mechanism is believed to be an early mechanical analog computer. It was designed to calculate astronomical positions. It was discovered in 1901 in the Antikythera wreck off the Greek island of Antikythera, between Kythera and Crete, and has been dated to circa 100 BC.
Mechanical analog computer devices appeared again a thousand years later in the medieval Islamic world and were developed by Muslim astronomers, such as the mechanical geared astrolabe by Abū Rayhān al-Bīrūnī, and the torquetum by Jabir ibn Aflah. According to Simon Singh, Muslim mathematicians also made important advances in cryptography, such as the development of cryptanalysis and frequency analysis by Alkindus. Programmable machines were also invented by Muslim engineers, such as the automatic flute player by the Banū Mūsā brothers, and Al-Jazari's programmable humanoid automata and castle clock, which is considered to be the first programmable analog computer. Technological artifacts of similar complexity appeared in 14th century Europe, with mechanical astronomical clocks.
When John Napier discovered logarithms for computational purposes in the early 17th century, there followed a period of considerable progress by inventors and scientists in making calculating tools. In 1623 Wilhelm Schickard designed a calculating machine, but abandoned the project, when the prototype he had started building was destroyed by a fire in 1624 . Around 1640, Blaise Pascal, a leading French mathematician, constructed a mechanical adding device based on a design described by Greek mathematician Hero of Alexandria. Then in 1672 Gottfried Wilhelm Leibniz invented the Stepped Reckoner which he completed in 1694.
In 1837 Charles Babbage first described his Analytical Engine which is accepted as the first design for a modern computer. The analytical engine had expandable memory, an arithmetic unit, and logic processing capabilities able to interpret a programming language with loops and conditional branching. Although never built, the design has been studied extensively and is understood to be Turing equivalent. The analytical engine would have had a memory capacity of less than 1 kilobyte of memory and a clock speed of less than 10 Hertz .
Considerable advancement in mathematics and electronics theory was required before the first modern computers could be designed.


== Binary logic ==
In 1702, Gottfried Wilhelm Leibniz developed logic in a formal, mathematical sense with his writings on the binary numeral system. In his system, the ones and zeros also represent true and false values or on and off states. But it took more than a century before George Boole published his Boolean algebra in 1854 with a complete system that allowed computational processes to be mathematically modeled .
By this time, the first mechanical devices driven by a binary pattern had been invented. The industrial revolution had driven forward the mechanization of many tasks, and this included weaving. Punched cards controlled Joseph Marie Jacquard's loom in 1801, where a hole punched in the card indicated a binary one and an unpunched spot indicated a binary zero. Jacquard's loom was far from being a computer, but it did illustrate that machines could be driven by binary systems .


== Creation of the computer ==
Before the 1920s, computers (sometimes computors) were human clerks that performed computations. They were usually under the lead of a physicist. Many thousands of computers were employed in commerce, government, and research establishments. Most of these computers were women. Some performed astronomical calculations for calendars, others ballistic tables for the military.
After the 1920s, the expression computing machine referred to any machine that performed the work of a human computer, especially those in accordance with effective methods of the Church-Turing thesis. The thesis states that a mathematical method is effective if it could be set out as a list of instructions able to be followed by a human clerk with paper and pencil, for as long as necessary, and without ingenuity or insight.
Machines that computed with continuous values became known as the analog kind. They used machinery that represented continuous numeric quantities, like the angle of a shaft rotation or difference in electrical potential.
Digital machinery, in contrast to analog, were able to render a state of a numeric value and store each individual digit. Digital machinery used difference engines or relays before the invention of faster memory devices.
The phrase computing machine gradually gave way, after the late 1940s, to just computer as the onset of electronic digital machinery became common. These computers were able to perform the calculations that were performed by the previous human clerks.
Since the values stored by digital machines were not bound to physical properties like analog devices, a logical computer, based on digital equipment, was able to do anything that could be described ""purely mechanical."" The theoretical Turing Machine, created by Alan Turing, is a hypothetical device theorized in order to study the properties of such hardware.


== Emergence of a discipline ==


=== Charles Babbage and Ada Lovelace ===

Charles Babbage is often regarded as one of the first pioneers of computing. Beginning in the 1810s, Babbage had a vision of mechanically computing numbers and tables. Putting this into reality, Babbage designed a calculator to compute numbers up to 8 decimal points long. Continuing with the success of this idea, Babbage worked to develop a machine that could compute numbers with up to 20 decimal places. By the 1830s, Babbage had devised a plan to develop a machine that could use punched cards to perform arithmetical operations. The machine would store numbers in memory units, and there would be a form of sequential control. This means that one operation would be carried out before another in such a way that the machine would produce an answer and not fail. This machine was to be known as the “Analytical Engine”, which was the first true representation of what is the modern computer.
Ada Lovelace (Augusta Ada Byron) is credited as the pioneer of computer programming and is regarded as a mathematical genius, a result of the mathematically heavy tutoring regimen her mother assigned to her as a young girl. Lovelace began working with Charles Babbage as an assistant while Babbage was working on his “Analytical Engine”, the first mechanical computer. During her work with Babbage, Ada Lovelace became the designer of the first computer algorithm, which had the ability to compute Bernoulli numbers. Moreover, Lovelace’s work with Babbage resulted in her prediction of future computers to not only perform mathematical calculations, but also manipulate symbols, mathematical or not. While she was never able to see the results of her work, as the “Analytical Engine” was not created in her lifetime, her efforts in later years, beginning in the 1840s, did not go unnoticed.


=== Alan Turing and the Turing machine ===

The mathematical foundations of modern computer science began to be laid by Kurt Gödel with his incompleteness theorem (1931). In this theorem, he showed that there were limits to what could be proved and disproved within a formal system. This led to work by Gödel and others to define and describe these formal systems, including concepts such as mu-recursive functions and lambda-definable functions.
In 1936 Alan Turing and Alonzo Church independently, and also together, introduced the formalization of an algorithm, with limits on what can be computed, and a ""purely mechanical"" model for computing. This became the Church–Turing thesis, a hypothesis about the nature of mechanical calculation devices, such as electronic computers. The thesis claims that any calculation that is possible can be performed by an algorithm running on a computer, provided that sufficient time and storage space are available.
In 1936, Alan Turing also published his seminal work on the Turing machines, an abstract digital computing machine which is now simply referred to as the Universal Turing machine. This machine invented the principle of the modern computer and was the birthplace of the stored program concept that almost all modern day computers use. These hypothetical machines were designed to formally determine, mathematically, what can be computed, taking into account limitations on computing ability. If a Turing machine can complete the task, it is considered Turing computable or more commonly, Turing complete.
The Los Alamos physicist Stanley Frankel, has described John von Neumann's view of the fundamental importance of Turing's 1936 paper, in a letter:

I know that in or about 1943 or ‘44 von Neumann was well aware of the fundamental importance of Turing's paper of 1936… Von Neumann introduced me to that paper and at his urging I studied it with care. Many people have acclaimed von Neumann as the ""father of the computer"" (in a modern sense of the term) but I am sure that he would never have made that mistake himself. He might well be called the midwife, perhaps, but he firmly emphasized to me, and to others I am sure, that the fundamental conception is owing to Turing...


=== Akira Nakashima and switching circuit theory ===
Up to and during the 1930s, electrical engineers were able to build electronic circuits to solve mathematical and logic problems, but most did so in an ad hoc manner, lacking any theoretical rigor. This changed with NEC engineer Akira Nakashima's switching circuit theory in the 1930s. From 1934 to 1936, Nakashima published a series of papers showing that the two-valued Boolean algebra, which he discovered independently (he was unaware of George Boole's work until 1938), can describe the operation of switching circuits. This concept, of utilizing the properties of electrical switches to do logic, is the basic concept that underlies all electronic digital computers. Switching circuit theory provided the mathematical foundations and tools for digital system design in almost all areas of modern technology.
Nakashima's work was later cited and elaborated on in Claude Elwood Shannon's seminal 1937 master's thesis ""A Symbolic Analysis of Relay and Switching Circuits"". While taking an undergraduate philosophy class, Shannon had been exposed to Boole's work, and recognized that it could be used to arrange electromechanical relays (then used in telephone routing switches) to solve logic problems. His thesis became the foundation of practical digital circuit design when it became widely known among the electrical engineering community during and after World War II.


=== Early computer hardware ===
In 1941, Konrad Zuse developed the world's first functional program-controlled computer, the Z3. In 1998, it was shown to be Turing-complete in principle. Zuse also developed the S2 computing machine, considered the first process control computer. He founded one of the earliest computer businesses in 1941, producing the Z4, which became the world's first commercial computer. In 1946, he designed the first high-level programming language, Plankalkül.
In 1948, the Manchester Baby was completed, it was the world's first general purpose electronic digital computer that also ran stored programs like almost all modern computers. The influence on Max Newman of Turing's seminal 1936 paper on the Turing Machines and of his logico-mathematical contributions to the project, were both crucial to the successful development of the Manchester SSEM.
In 1950, Britain's National Physical Laboratory completed Pilot ACE, a small scale programmable computer, based on Turing's philosophy. With an operating speed of 1 MHz, the Pilot Model ACE was for some time the fastest computer in the world. Turing's design for ACE had much in common with today's RISC architectures and it called for a high-speed memory of roughly the same capacity as an early Macintosh computer, which was enormous by the standards of his day. Had Turing's ACE been built as planned and in full, it would have been in a different league from the other early computers.


=== Shannon and information theory ===
Claude Shannon went on to found the field of information theory with his 1948 paper titled A Mathematical Theory of Communication, which applied probability theory to the problem of how to best encode the information a sender wants to transmit. This work is one of the theoretical foundations for many areas of study, including data compression and cryptography .


=== Wiener and cybernetics ===
From experiments with anti-aircraft systems that interpreted radar images to detect enemy planes, Norbert Wiener coined the term cybernetics from the Greek word for ""steersman."" He published ""Cybernetics"" in 1948, which influenced artificial intelligence. Wiener also compared computation, computing machinery, memory devices, and other cognitive similarities with his analysis of brain waves.
The first actual computer bug was a moth. It was stuck in between the relays on the Harvard Mark II. While the invention of the term 'bug' is often but erroneously attributed to Grace Hopper, a future rear admiral in the U.S. Navy, who supposedly logged the ""bug"" on September 9, 1945, most other accounts conflict at least with these details. According to these accounts, the actual date was September 9, 1947 when operators filed this 'incident' — along with the insect and the notation ""First actual case of bug being found"" (see software bug for details).


=== John von Neumann and the von Neumann architecture ===

In 1946, a model for computer architecture was introduced and became known as Von Neumann architecture. Since 1950, the von Neumann model provided uniformity in subsequent computer designs. The von Neumann architecture was considered innovative as it introduced an idea of allowing machine instructions and data to share memory space. The von Neumann model is composed of three major parts, the arithmetic logic unit (ALU), the memory, and the instruction processing unit (IPU). In von Neumann machine design, the IPU passes addresses to memory, and memory, in turn, is routed either back to the IPU if an instruction is being fetched or to the ALU if data is being fetched.
Von Neumann’s machine design uses a RISC (Reduced instruction set computing) architecture, which means the instruction set uses a total of 21 instructions to perform all tasks. (This is in contrast to CISC, complex instruction set computing, instruction sets which have more instructions from which to choose.) With von Neumann architecture, main memory along with the accumulator (the register that holds the result of logical operations) are the two memories that are addressed. Operations can be carried out as simple arithmetic (these are performed by the ALU and include addition, subtraction, multiplication and division), conditional branches (these are more commonly seen now as if statements or while loops. The branches serve as go to statements), and logical moves between the different components of the machine, i.e., a move from the accumulator to memory or vice versa. Von Neumann architecture accepts fractions and instructions as data types. Finally, as the von Neumann architecture is a simple one, its register management is also simple. The architecture uses a set of seven registers to manipulate and interpret fetched data and instructions. These registers include the ""IR"" (instruction register), ""IBR"" (instruction buffer register), ""MQ"" (multiplier quotient register), ""MAR"" (memory address register), and ""MDR"" (memory data register)."" The architecture also uses a program counter (""PC"") to keep track of where in the program the machine is.


== See also ==
Computer Museum
History of computing
History of computing hardware
History of software
List of computer term etymologies, the origins of computer science words
List of prominent pioneers in computer science
Timeline of algorithms
History of personal computers


== References ==


== Further reading ==
Tedre, Matti (2014). The Science of Computing: Shaping a Discipline. Taylor and Francis / CRC Press. ISBN 978-1-4822-1769-8. 
Kak, Subhash : Computing Science in Ancient India; Munshiram Manoharlal Publishers Pvt. Ltd (2001)
The Development of Computer Science: A Sociocultural Perspective Matti Tedre's Ph.D. Thesis, University of Joensuu (2006)
Ceruzzi, Paul E. (1998). A History of a Modern Computing. The MIT Press. ISBN 978-0-262-03255-1. 
Copeland, B. Jack. ""The Modern History of Computing"". In Zalta, Edward N. Stanford Encyclopedia of Philosophy. 


== External links ==
Computer History Museum
Computers: From the Past to the Present
The First ""Computer Bug"" at the Naval History and Heritage Command Photo Archives.
Bitsavers, an effort to capture, salvage, and archive historical computer software and manuals from minicomputers and mainframes of the 1950s, 1960s, 1970s, and 1980s
Oral history interviews"
28,Association for Computing Machinery,2928,28997,"The Association for Computing Machinery (ACM) is an international learned society for computing. It was founded in 1947, and is the world's largest scientific and educational computing society. It is a not-for-profit professional membership group. Its membership is more than 100,000 as of 2011. Its headquarters are in New York City.
The ACM is an umbrella organization for academic and scholarly interests in computer science. Its motto is ""Advancing Computing as a Science & Profession"".


== History ==
The ACM was founded in 1947 under the name Eastern Association for Computing Machinery, which was changed the following year to the Association of Computing Machinery.


== Activities ==

ACM is organized into over 171 local chapters and 37 Special Interest Groups (SIGs), through which it conducts most of its activities. Additionally, there are over 500 college and university chapters. The first student chapter was founded in 1961 at the University of Louisiana at Lafayette.
Many of the SIGs, such as SIGGRAPH, SIGPLAN, SIGCSE and SIGCOMM, sponsor regular conferences, which have become famous as the dominant venue for presenting innovations in certain fields. The groups also publish a large number of specialized journals, magazines, and newsletters.
ACM also sponsors other computer science related events such as the worldwide ACM International Collegiate Programming Contest (ICPC), and has sponsored some other events such as the chess match between Garry Kasparov and the IBM Deep Blue computer.


== Services ==


=== Publications ===

ACM publishes over 50 journals including the prestigious Journal of the ACM, and two general magazines for computer professionals, Communications of the ACM (also known as Communications or CACM) and Queue. Other publications of the ACM include:
ACM XRDS, formerly ""Crossroads"", was redesigned in 2010 and is the most popular student computing magazine in the US.
ACM Interactions, an interdisciplinary HCI publication focused on the connections between experiences, people and technology, and the third largest ACM publication.
ACM Computing Surveys (CSUR)
ACM Computers in Entertainment (CIE)
ACM Special Interest Group: Computers and Society (SIGCAS) 
A number of journals, specific to subfields of computer science, titled ACM Transactions. Some of the more notable transactions include:
ACM Transactions on Computer Systems (TOCS)
IEEE/ACM Transactions on Computational Biology and Bioinformatics (TCBB)
ACM Transactions on Computational Logic (TOCL)
ACM Transactions on Computer-Human Interaction (TOCHI)
ACM Transactions on Database Systems (TODS)
ACM Transactions on Graphics (TOG)
ACM Transactions on Mathematical Software (TOMS)
ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)
IEEE/ACM Transactions on Networking (TON)
ACM Transactions on Programming Languages and Systems (TOPLAS)

Although Communications no longer publishes primary research, and is not considered a prestigious venue, many of the great debates and results in computing history have been published in its pages.
ACM has made almost all of its publications available to paid subscribers online at its Digital Library and also has a Guide to Computing Literature. Individual members additionally have access to Safari Books Online and Books24x7. ACM also offers insurance, online courses, and other services to its members.
In 1997, ACM Press published Wizards and Their Wonders: Portraits in Computing (ISBN 0897919602), written by Christopher Morgan, with new photographs by Louis Fabian Bachrach. The book is a collection of historic and current portrait photographs of figures from the computer industry.


== Portal and Digital Library ==
The ACM Portal is an online service of the ACM. Its core are two main sections: ACM Digital Library and the ACM Guide to Computing Literature.
The ACM Digital Library is the full-text collection of all articles published by the ACM in its articles, magazines and conference proceedings. The Guide is a bibliography in computing with over one million entries. The ACM Digital Library contains a comprehensive archive starting in the 1950s of the organization's journals, magazines, newsletters and conference proceedings. Online services include a forum called Ubiquity and Tech News digest. There is an extensive underlying bibliographic database containing key works of all genres from all major publishers of computing literature. This secondary database is a rich discovery service known as The ACM Guide to Computing Literature.
ACM adopted a hybrid Open Access (OA) publishing model in 2013. Authors who do not choose to pay the OA fee must grant ACM publishing rights by either a copyright transfer agreement or a publishing license agreement.
ACM was a ""green"" publisher before the term was invented. Authors may post documents on their own websites and in their institutional repositories with a link back to the ACM Digital Library's permanently maintained Version of Record.
All metadata in the Digital Library is open to the world, including abstracts, linked references and citing works, citation and usage statistics, as well as all functionality and services. Other than the free articles, the full-texts are accessed by subscription.
There is also a mounting challenge to the ACM's publication practices coming from the open access movement. Some authors see a centralized peer–review process as less relevant and publish on their home pages or on unreviewed sites like arXiv. Other organizations have sprung up which do their peer review entirely free and online, such as Journal of Artificial Intelligence Research (JAIR), Journal of Machine Learning Research (JMLR) and the Journal of Research and Practice in Information Technology.


== Membership grades ==
In addition to student and regular members, ACM has several advanced membership grades to recognize those with multiple years of membership and ""demonstrated performance that sets them apart from their peers"".


=== Fellows ===

The ACM Fellows Program was established by Council of the Association for Computing Machinery in 1993 ""to recognize and honor outstanding ACM members for their achievements in computer science and information technology and for their significant contributions to the mission of the ACM."" There are presently about 958 Fellows out of about 75,000 professional members.


=== Distinguished Members ===
In 2006 ACM began recognizing two additional membership grades, one which was called Distinguished Members. Distinguished Members (Distinguished Engineers, Distinguished Scientists, and Distinguished Educators) have at least 15 years of professional experience and 5 years of continuous ACM membership and ""have made a significant impact on the computing field"". Note that in 2006 when the Distinguished Members first came out, one of the three levels was called ""Distinguished Member"" and was changed about two years later to ""Distinguished Educator"". Those who already had the Distinguished Member title had their titles changed to one of the other three titles.


=== Senior Members ===
Also in 2006, ACM began recognizing Senior Members. Senior Members have ten or more years of professional experience and 5 years of continuous ACM membership.


=== Distinguished Speakers ===
While not technically a membership grade, the ACM recognizes distinguished speakers on topics in computer science. A distinguished speaker is appointed for a three-year period. There are usually about 125 current distinguished speakers. The ACM website describes these people as 'Renowned International Thought Leaders'. The distinguished speaker program is overseen by a committee 
Norman E. Gibbs served as the president of the ACM.


== Chapters ==
ACM has three kinds of chapters: Special Interest Groups, Professional Chapters, and Student Chapters.
As of 2011, ACM has professional & SIG Chapters in 56 countries.
As of 2014, there exist ACM student chapters in 41 different countries.


=== Special Interest Groups ===


== Conferences ==

ACM and its Special Interest Groups (SIGs) sponsors numerous conferences with 170 hosted worldwide in 2017. ACM Conferences page has an up-to-date complete list while a partial list is shown below. Most of the SIGs also have an annual conference. ACM conferences are often very popular publishing venues and are therefore very competitive. For example, the 2007 SIGGRAPH conference attracted about 30000 visitors, and CIKM only accepted 15% of the long papers that were submitted in 2005.

MobiHoc: International Symposium on Mobile Ad Hoc Networking and Computing
The ACM is a co–presenter and founding partner of the Grace Hopper Celebration of Women in Computing (GHC) with the Anita Borg Institute for Women and Technology.
There are some conferences hosted by ACM student branches; this includes Reflections Projections, which is hosted by UIUC ACM. . In addition, ACM sponsors regional conferences. Regional conferences facilitate increased opportunities for collaboration between nearby institutions and they are well attended.
For additional non-ACM conferences, see this list of computer science conferences.


== Awards ==
The ACM presents or co–presents a number of awards for outstanding technical and professional achievements and contributions in computer science and information technology.

Over 30 of ACM's Special Interest Groups also award individuals for their contributions with a few listed below.


== Leadership ==

The President of ACM for 2016–2018 is Vicki L. Hanson, Distinguished Professor in the Department of Information Sciences and Technologies at the Rochester Institute of Technology and Professor and Chair of Inclusive Technologies at the University of Dundee, UK. She is successor of Alexander L. Wolf (2014–2016), Dean of the Jack Baskin School of Engineering at the University of California, Santa Cruz; Vint Cerf (2012–2014), an American computer scientist who is recognized as one of ""the fathers of the Internet""; Alain Chesnais (2010–2012), a French citizen living in Toronto, Ontario, Canada, where he runs his company named Visual Transitions; and Dame Wendy Hall of the University of Southampton, UK (2008–2010).
ACM is led by a Council consisting of the President, Vice-President, Treasurer, Past President, SIG Governing Board Chair, Publications Board Chair, three representatives of the SIG Governing Board, and seven Members–At–Large. This institution is often referred to simply as ""Council"" in Communications of the ACM.


== Infrastructure ==
ACM has five ""Boards"" that make up various committees and subgroups, to help Headquarters staff maintain quality services and products. These boards are as follows:
Publications Board
SIG Governing Board
Education Board
Membership Services Board
Practitioners Board


== ACM Council on Women in Computing ==
ACM-W, the ACM council on women in computing, supports, celebrates, and advocates internationally for the full engagement of women in computing. ACM–W's main programs are regional celebrations of women in computing, ACM-W chapters, and scholarships for women CS students to attend research conferences. In India and Europe these activities are overseen by ACM-W India and ACM-W Europe respectively. ACM-W collaborates with organizations such as the Anita Borg Institute, the National Center for Women & Information Technology (NCWIT), and Committee on the Status of Women in Computing Research (CRA-W).


=== Athena Lectures ===
The ACM-W gives an annual Athena Lecturer Award to honor outstanding women researchers who have made fundamental contributions to computer science. This program began in 2006. Speakers are nominated by SIG officers.
2006–2007: Deborah Estrin of UCLA
2007–2008: Karen Spärck Jones of Cambridge University
2008–2009: Shafi Goldwasser of MIT and the Weitzmann Institute of Science
2009–2010: Susan J. Eggers of the University of Washington
2010–2011: Mary Jane Irwin of the Pennsylvania State University
2011–2012: Judith S. Olson of the University of California, Irvine
2012–2013: Nancy Lynch of MIT
2013–2014: Katherine Yelick of LBNL
2014–2015: Susan Dumais of Microsoft Research
2015–2016: Jennifer Widom of Stanford University
2016–2017: Jennifer Rexford of Princeton University


== Cooperation ==
ACM's primary partner has been the IEEE Computer Society (IEEE-CS), which is the largest subgroup of the Institute of Electrical and Electronics Engineers (IEEE). The IEEE focuses more on hardware and standardization issues than theoretical computer science, but there is considerable overlap with ACM's agenda. They have many joint activities including conferences, publications and awards. ACM and its SIGs co-sponsor about 20 conferences each year with IEEE-CS and other parts of IEEE. Eckert-Mauchly Award and Ken Kennedy Award, both major awards in computer science, are given jointly by ACM and the IEEE-CS. They occasionally cooperate on projects like developing computing curricula.
ACM has also jointly sponsored on events with other professional organizations like the Society for Industrial and Applied Mathematics (SIAM).


== See also ==


== References ==


== External links ==
Official website
ACM portal for publications
ACM Digital Library
Association for Computing Machinery Records, 1947-2009, Charles Babbage Institute, University of Minnesota."
29,Computer program,5783,28866,"A computer program is a structured collection of instruction sequences that perform a specific task when executed by a computer. A computer requires programs to function.
A computer program is usually written by a computer programmer in a programming language. From the program in its human-readable form of source code, a compiler can derive machine code—a form consisting of instructions that the computer can directly execute. Alternatively, a computer program may be executed with the aid of an interpreter.
The evolution of a process is directed by a pattern of rules called a program. People create programs to direct processes.
A formal model of some part of a computer program that performs a general and well-defined task is called an algorithm. A collection of computer programs, libraries, and related data are referred to as software. Computer programs may be categorized along functional lines, such as application software and system software.


== HistoryEdit ==


=== Early programmable machinesEdit ===
The earliest programmable machines preceded the invention of the digital computer. In 1801, Joseph-Marie Jacquard devised a loom that would weave a pattern by following a series of perforated cards. Patterns could be woven and repeated by arranging the cards.


=== Analytical EngineEdit ===

In 1837, Charles Babbage was inspired by Jacquard's loom to attempt to build the Analytical Engine. The names of the components of the calculating device were borrowed from the textile industry. In the textile industry, yarn was brought from the store to be milled. The device would have had a ""store""—memory to hold 1,000 numbers of 40 decimal digits each. Numbers from the ""store"" would then have then been transferred to the ""mill"" (analogous to the CPU of a modern machine), for processing. It was programmed using two sets of perforated cards—one to direct the operation and the other for the input variables.  However, after more than 17,000 pounds of the British government's money, the thousands of cogged wheels and gears never fully worked together.
During a nine-month period in 1842–43, Ada Lovelace translated the memoir of Italian mathematician Luigi Menabrea. The memoir covered the Analytical Engine. The translation contained Note G which completely detailed a method for calculating Bernoulli numbers using the Analytical Engine. This note is recognized by some historians as the world's first written computer program.


=== Universal Turing machineEdit ===
In 1936, Alan Turing introduced the Universal Turing machine—a theoretical device that can model every computation that can be performed on a Turing complete computing machine. It is a finite-state machine that has an infinitely long read/write tape. The machine can move the tape back and forth, changing its contents as it performs an algorithm. The machine starts in the initial state, goes through a sequence of steps, and halts when it encounters the halt state. This machine is considered by some to be the origin of the stored-program computer—used by John von Neumann (1946) for the ""Electronic Computing Instrument"" that now bears the von Neumann architecture name.


=== Early programmable computersEdit ===
The Z3 computer, invented by Konrad Zuse (1941) in Germany, was a digital and programmable computer. A digital computer uses electricity as the calculating component. The Z3 contained 2,400 relays to create the circuits. The circuits provided a binary, floating-point, nine-instruction computer. Programming the Z3 was through a specially designed keyboard and punched tape.
The Electronic Numerical Integrator And Computer (Fall 1945) was a Turing complete, general-purpose computer that used 17,468 vacuum tubes to create the circuits. At its core, it was a series of Pascalines wired together. Its 40 units weighed 30 tons, occupied 1,800 square feet (167 m2), and consumed $650 per hour (in 1940s currency) in electricity when idle. It had 20 base-10 accumulators. Programming the ENIAC took up to two months. Three function tables were on wheels and needed to be rolled to fixed function panels. Function tables were connected to function panels using heavy black cables. Each function table had 728 rotating knobs. Programming the ENIAC also involved setting some of the 3,000 switches. Debugging a program took a week. The ENIAC featured parallel operations. Different sets of accumulators could simultaneously work on different algorithms. It used punched card machines for input and output, and it was controlled with a clock signal. It ran for eight years, calculating hydrogen bomb parameters, predicting weather patterns, and producing firing tables to aim artillery guns.
The Manchester Small-Scale Experimental Machine (June 1948) was a stored-program computer. Programming transitioned away from moving cables and setting dials; instead, a computer program was stored in memory as numbers. Only three bits of memory were available to store each instruction, so it was limited to eight instructions. 32 switches were available for programming.


=== Later computersEdit ===

Computers manufactured until the 1970s had front-panel switches for programming. The computer program was written on paper for reference. An instruction was represented by a configuration of on/off settings. After setting the configuration, an execute button was pressed. This process was then repeated. Computer programs also were manually input via paper tape or punched cards. After the medium was loaded, the starting address was set via switches and the execute button pressed.
In 1961, the Burroughs B5000 was built specifically to be programmed in the ALGOL 60 language. The hardware featured circuits to ease the compile phase.
In 1964, the IBM System/360 was a line of six computers each having the same instruction set architecture. The Model 30 was the smallest and least expensive. Customers could upgrade and retain the same application software. Each System/360 model featured multiprogramming. With operating system support, multiple programs could be in memory at once. When one was waiting for input/output, another could compute. Each model also could emulate other computers. Customers could upgrade to the System/360 and retain their IBM 7094 or IBM 1401 application software.


== Computer programmingEdit ==

Computer programming is the process of writing or editing source code. Editing source code involves testing, analyzing, refining, and sometimes coordinating with other programmers on a jointly developed program. A person who practices this skill is referred to as a computer programmer, software developer, and sometimes coder.
The sometimes lengthy process of computer programming is usually referred to as software development. The term software engineering is becoming popular as the process is seen as an engineering discipline.


=== Programming languagesEdit ===

Computer programs can be categorized by the programming language paradigm used to produce them. Two of the main paradigms are imperative and declarative.


==== Imperative languagesEdit ====
Imperative programming languages specify a sequential algorithm using declarations, expressions, and statements:
A declaration couples a variable name to a datatype – for example: var x: integer;
An expression yields a value – for example: 2 + 2 yields 4
A statement might assign an expression to a variable or use the value of a variable to alter the program's control flow – for example: x := 2 + 2; if x = 4 then do_something();
One criticism of imperative languages is the side effect of an assignment statement on a class of variables called non-local variables.


==== Declarative languagesEdit ====
Declarative programming languages describe what computation should be performed and not how to compute it. Declarative programs omit the control flow and are considered sets of instructions. Two broad categories of declarative languages are functional languages and logical languages. The principle behind functional languages (like Haskell) is to not allow side effects, which makes it easier to reason about programs like mathematical functions. The principle behind logical languages (like Prolog) is to define the problem to be solved – the goal – and leave the detailed solution to the Prolog system itself. The goal is defined by providing a list of subgoals. Then each subgoal is defined by further providing a list of its subgoals, etc. If a path of subgoals fails to find a solution, then that subgoal is backtracked and another path is systematically attempted.


=== Compilation and interpretationEdit ===
A computer program in the form of a human-readable, computer programming language is called source code. Source code may be converted into an executable image by a compiler or executed immediately with the aid of an interpreter.
Compilers are used to translate source code from a programming language into either object code or machine code. Object code needs further processing to become machine code, and machine code consists of the central processing unit's native instructions, ready for execution. Compiled computer programs are commonly referred to as executables, binary images, or simply as binaries – a reference to the binary file format used to store the executable code.
Interpreters are used to execute source code from a programming language line-by-line. The interpreter decodes each statement and performs its behavior. One advantage of interpreters is that they can easily be extended to an interactive session. The programmer is presented with a prompt, and individual lines of code are typed in and performed immediately.
The main disadvantage of interpreters is computer programs run slower than when compiled. Interpreting code is slower because the interpreter must decode each statement and then perform it. However, software development may be faster using an interpreter because testing is immediate when the compiling step is omitted. Another disadvantage of interpreters is an interpreter must be present on the executing computer. By contrast, compiled computer programs need no compiler present during execution.
Just in time compilers pre-compile computer programs just before execution. For example, the Java virtual machine Hotspot contains a Just In Time Compiler which selectively compiles Java bytecode into machine code - but only code which Hotspot predicts is likely to be used many times.
Either compiled or interpreted programs might be executed in a batch process without human interaction.
Scripting languages are often used to create batch processes. One common scripting language is Unix shell, and its executing environment is called the command-line interface.
No properties of a programming language require it to be exclusively compiled or exclusively interpreted. The categorization usually reflects the most popular method of language execution. For example, Java is thought of as an interpreted language and C a compiled language, despite the existence of Java compilers and C interpreters.


== Storage and executionEdit ==

Typically, computer programs are stored in non-volatile memory until requested either directly or indirectly to be executed by the computer user. Upon such a request, the program is loaded into random-access memory, by a computer program called an operating system, where it can be accessed directly by the central processor. The central processor then executes (""runs"") the program, instruction by instruction, until termination. A program in execution is called a process. Termination is either by normal self-termination or by error – software or hardware error.


=== Simultaneous executionEdit ===

Many operating systems support multitasking which enables many computer programs to appear to run simultaneously on one computer. Operating systems may run multiple programs through process scheduling – a software mechanism to switch the CPU among processes often so users can interact with each program while it runs. Within hardware, modern day multiprocessor computers or computers with multicore processors may run multiple programs.
Multiple lines of the same computer program may be simultaneously executed using threads. Multithreading processors are optimized to execute multiple threads efficiently.


=== Self-modifying programsEdit ===

A computer program in execution is normally treated as being different from the data the program operates on. However, in some cases, this distinction is blurred when a computer program modifies itself. The modified computer program is subsequently executed as part of the same program. Self-modifying code is possible for programs written in machine code, assembly language, Lisp, C, COBOL, PL/1, and Prolog.


== Functional categoriesEdit ==
Computer programs may be categorized along functional lines. The main functional categories are application software and system software. System software includes the operating system which couples computer hardware with application software. The purpose of the operating system is to provide an environment in which application software executes in a convenient and efficient manner. In addition to the operating system, system software includes embedded programs, boot programs, and micro programs. Application software designed for end users have a user interface. Application software not designed for the end user includes middleware, which couples one application with another. Application software also includes utility programs. The distinction between system software and application software is under debate.


=== Application softwareEdit ===

There are many types of application software:
The word app came to being in 21st century. It is a clipping of the word ""application"". They have been designed for many platforms, but the word was first used for smaller mobile apps. Desktop apps are traditional computer programs that run on desktop computers. Mobile apps run on mobile devices. Web apps run inside a web browser. Both mobile and desktop apps may be downloaded from the developers' website or purchased from app stores such as Microsoft Store, Apple App Store, Mac App Store, Google Play or Intel AppUp.
An application suite consists of multiple applications bundled together. Examples include Microsoft Office, LibreOffice, and iWork. They bundle a word processor, spreadsheet, and other applications.
Enterprise applications bundle accounting, personnel, customer, and vendor applications. Examples include enterprise resource planning, customer relationship management, and supply chain management software.
Enterprise infrastructure software supports the enterprise's software systems. Examples include databases, email servers, and network servers.
Information worker software are designed for workers at the departmental level. Examples include time management, resource management, analytical, collaborative and documentation tools. Word processors, spreadsheets, email and blog clients, personal information system, and individual media editors may aid in multiple information worker tasks.
Media development software generates print and electronic media for others to consume, most often in a commercial or educational setting. These produce graphics, publications, animations, and videos.
Product engineering software is used to help develop large machines and other application software. Examples includes computer-aided design (CAD), computer-aided engineering (CAE), and integrated development environments.
Entertainment Software can refer to video games, movie recorders and players, and music recorders and players.


=== Utility programsEdit ===
Utility programs are application programs designed to aid system administrators and computer programmers.


=== Operating systemEdit ===

An operating system is a computer program that acts as an intermediary between a user of a computer and the computer hardware. 
In the 1950s, the programmer, who was also the operator, would write a program and run it. After the program finished executing, the output may have been printed, or it may have been punched onto paper tape or cards for later processing. More often than not the program did not work.  The programmer then looked at the console lights and fiddled with the console switches. If less fortunate, a memory printout was made for further study. In the 1960s, programmers reduced the amount of wasted time by automating the operator's job. A program called an operating system was kept in the computer at all times.
Originally, operating systems were programmed in assembly; however, modern operating systems are typically written in C.


=== Boot programEdit ===
A stored-program computer requires an initial computer program stored in its read-only memory to boot. The boot process is to identify and initialize all aspects of the system, from processor registers to device controllers to memory contents. Following the initialization process, this initial computer program loads the operating system and sets the program counter to begin normal operations.


=== Embedded programsEdit ===

Independent of the host computer, a hardware device might have embedded firmware to control its operation. Firmware is used when the computer program is rarely or never expected to change, or when the program must not be lost when the power is off.


=== Microcode programsEdit ===

Microcode programs control some central processing units and some other hardware. This code moves data between the registers, buses, arithmetic logic units, and other functional units in the CPU. Unlike conventional programs, microcode is not usually written by, or even visible to, the end users of systems, and is usually provided by the manufacturer, and is considered internal to the device.


== See alsoEdit ==
Automatic programming
Firmware
Killer application
Software
Software bug


== ReferencesEdit ==


== Further readingEdit ==
Knuth, Donald E. (1997). The Art of Computer Programming, Volume 1, 3rd Edition. Boston: Addison-Wesley. ISBN 0-201-89683-4. 
Knuth, Donald E. (1997). The Art of Computer Programming, Volume 2, 3rd Edition. Boston: Addison-Wesley. ISBN 0-201-89684-2. 
Knuth, Donald E. (1997). The Art of Computer Programming, Volume 3, 3rd Edition. Boston: Addison-Wesley. ISBN 0-201-89685-0."
30,Computational electromagnetics,3849994,28632,"Computational electromagnetics, computational electrodynamics or electromagnetic modeling is the process of modeling the interaction of electromagnetic fields with physical objects and the environment.
It typically involves using computationally efficient approximations to Maxwell's equations and is used to calculate antenna performance, electromagnetic compatibility, radar cross section and electromagnetic wave propagation when not in free space.
A specific part of computational electromagnetics deals with electromagnetic radiation scattered and absorbed by small particles.


== Background ==
Several real-world electromagnetic problems like electromagnetic scattering, electromagnetic radiation, modeling of waveguides etc., are not analytically calculable, for the multitude of irregular geometries found in actual devices. Computational numerical techniques can overcome the inability to derive closed form solutions of Maxwell's equations under various constitutive relations of media, and boundary conditions. This makes computational electromagnetics (CEM) important to the design, and modeling of antenna, radar, satellite and other communication systems, nanophotonic devices and high speed silicon electronics, medical imaging, cell-phone antenna design, among other applications.
CEM typically solves the problem of computing the E (electric) and H (magnetic) fields across the problem domain (e.g., to calculate antenna radiation pattern for an arbitrarily shaped antenna structure). Also calculating power flow direction (Poynting vector), a waveguide's normal modes, media-generated wave dispersion, and scattering can be computed from the E and H fields. CEM models may or may not assume symmetry, simplifying real world structures to idealized cylinders, spheres, and other regular geometrical objects. CEM models extensively make use of symmetry, and solve for reduced dimensionality from 3 spatial dimensions to 2D and even 1D.
An eigenvalue problem formulation of CEM allows us to calculate steady state normal modes in a structure. Transient response and impulse field effects are more accurately modeled by CEM in time domain, by FDTD. Curved geometrical objects are treated more accurately as finite elements FEM, or non-orthogonal grids. Beam propagation method (BPM) can solve for the power flow in waveguides. CEM is application specific, even if different techniques converge to the same field and power distributions in the modeled domain.


== Overview of methods ==
One approach is to discretize the space in terms of grids (both orthogonal, and non-orthogonal) and solving Maxwell's equations at each point in the grid. Discretization consumes computer memory, and solving the equations takes significant time. Large-scale CEM problems face memory and CPU limitations. As of 2007, CEM problems require supercomputers, high performance clusters, vector processors and/or parallelism. Typical formulations involve either time-stepping through the equations over the whole domain for each time instant; or through banded matrix inversion to calculate the weights of basis functions, when modeled by finite element methods; or matrix products when using transfer matrix methods; or calculating integrals when using method of moments (MoM); or using fast fourier transforms, and time iterations when calculating by the split-step method or by BPM.


== Choice of methods ==
Choosing the right technique for solving a problem is important, as choosing the wrong one can either result in incorrect results, or results which take excessively long to compute. However, the name of a technique does not always tell one how it is implemented, especially for commercial tools, which will often have more than one solver.
Davidson gives two tables comparing the FEM, MoM and FDTD techniques in the way they are normally implemented. One table is for both open region (radiation and scattering problems) and another table is for guided wave problems.


== Maxwell's equations in hyperbolic PDE form ==
Maxwell's equations can be formulated as a hyperbolic system of partial differential equations. This gives access to powerful techniques for numerical solutions.
It is assumed that the waves propagate in the (x,y)-plane and restrict the direction of the magnetic field to be parallel to the z-axis and thus the electric field to be parallel to the (x,y) plane. The wave is called a transverse magnetic (TM) wave. In 2D and no polarization terms present, Maxwell's equations can then be formulated as:

  
    
      
        
          
            ∂
            
              ∂
              t
            
          
        
        
          
            
              u
              ¯
            
          
        
        +
        A
        
          
            ∂
            
              ∂
              x
            
          
        
        
          
            
              u
              ¯
            
          
        
        +
        B
        
          
            ∂
            
              ∂
              y
            
          
        
        
          
            
              u
              ¯
            
          
        
        +
        C
        
          
            
              u
              ¯
            
          
        
        =
        
          
            
              g
              ¯
            
          
        
      
    
    {\displaystyle {\frac {\partial }{\partial t}}{\bar {u}}+A{\frac {\partial }{\partial x}}{\bar {u}}+B{\frac {\partial }{\partial y}}{\bar {u}}+C{\bar {u}}={\bar {g}}}
  
where u, A, B, and C are defined as

  
    
      
        
          
            
              u
              ¯
            
          
        
        =
        
          (
          
            
              
                
                  
                    E
                    
                      x
                    
                  
                
              
              
                
                  
                    E
                    
                      y
                    
                  
                
              
              
                
                  
                    H
                    
                      z
                    
                  
                
              
            
          
          )
        
        ,
      
    
    {\displaystyle {\bar {u}}=\left({\begin{matrix}E_{x}\\E_{y}\\H_{z}\end{matrix}}\right),}
  

  
    
      
        A
        =
        
          (
          
            
              
                
                  0
                
                
                  0
                
                
                  0
                
              
              
                
                  0
                
                
                  0
                
                
                  
                    
                      1
                      ϵ
                    
                  
                
              
              
                
                  0
                
                
                  
                    
                      1
                      μ
                    
                  
                
                
                  0
                
              
            
          
          )
        
        ,
      
    
    {\displaystyle A=\left({\begin{matrix}0&0&0\\0&0&{\frac {1}{\epsilon }}\\0&{\frac {1}{\mu }}&0\end{matrix}}\right),}
  

  
    
      
        B
        =
        
          (
          
            
              
                
                  0
                
                
                  0
                
                
                  
                    
                      
                        −
                        1
                      
                      ϵ
                    
                  
                
              
              
                
                  0
                
                
                  0
                
                
                  0
                
              
              
                
                  
                    
                      
                        −
                        1
                      
                      μ
                    
                  
                
                
                  0
                
                
                  0
                
              
            
          
          )
        
        ,
      
    
    {\displaystyle B=\left({\begin{matrix}0&0&{\frac {-1}{\epsilon }}\\0&0&0\\{\frac {-1}{\mu }}&0&0\end{matrix}}\right),}
  

  
    
      
        C
        =
        
          (
          
            
              
                
                  
                    
                      σ
                      ϵ
                    
                  
                
                
                  0
                
                
                  0
                
              
              
                
                  0
                
                
                  
                    
                      σ
                      ϵ
                    
                  
                
                
                  0
                
              
              
                
                  0
                
                
                  0
                
                
                  0
                
              
            
          
          )
        
        .
      
    
    {\displaystyle C=\left({\begin{matrix}{\frac {\sigma }{\epsilon }}&0&0\\0&{\frac {\sigma }{\epsilon }}&0\\0&0&0\end{matrix}}\right).}
  
In this representation, 
  
    
      
        
          
            
              g
              ¯
            
          
        
      
    
    {\displaystyle {\bar {g}}}
   is the forcing function, and is in the same space as 
  
    
      
        
          
            
              u
              ¯
            
          
        
      
    
    {\displaystyle {\bar {u}}}
  . It can be used to express an externally applied field or to describe an optimization constraint. As formulated above:

  
    
      
        
          
            
              g
              ¯
            
          
        
        =
        
          (
          
            
              
                
                  
                    E
                    
                      x
                      ,
                      c
                      o
                      n
                      s
                      t
                      r
                      a
                      i
                      n
                      t
                    
                  
                
              
              
                
                  
                    E
                    
                      y
                      ,
                      c
                      o
                      n
                      s
                      t
                      r
                      a
                      i
                      n
                      t
                    
                  
                
              
              
                
                  
                    H
                    
                      z
                      ,
                      c
                      o
                      n
                      s
                      t
                      r
                      a
                      i
                      n
                      t
                    
                  
                
              
            
          
          )
        
        .
      
    
    {\displaystyle {\bar {g}}=\left({\begin{matrix}E_{x,constraint}\\E_{y,constraint}\\H_{z,constraint}\end{matrix}}\right).}
  

  
    
      
        
          
            
              g
              ¯
            
          
        
      
    
    {\displaystyle {\bar {g}}}
   may also be explicitly defined equal to zero to simplify certain problems, or to find a characteristic solution, which is often the first step in a method to find the particular inhomogeneous solution.


== Integral equation solvers ==


=== The discrete dipole approximation ===
The discrete dipole approximation is a flexible technique for computing scattering and absorption by targets of arbitrary geometry. The formulation is based on integral form of Maxwell equations. The DDA is an approximation of the continuum target by a finite array of polarizable points. The points acquire dipole moments in response to the local electric field. The dipoles of course interact with one another via their electric fields, so the DDA is also sometimes referred to as the coupled dipole approximation. The resulting linear system of equations is commonly solved using conjugate gradient iterations. The discretization matrix has symmetries (the integral form of Maxwell equations has form of convolution) enabling Fast Fourier Transform to multiply matrix times vector during conjugate gradient iterations.


=== Method of moments element method ===
The method of moments (MoM) or boundary element method (BEM) is a numerical computational method of solving linear partial differential equations which have been formulated as integral equations (i.e. in boundary integral form). It can be applied in many areas of engineering and science including fluid mechanics, acoustics, electromagnetics, fracture mechanics, and plasticity.
MoM has become more popular since the 1980s. Because it requires calculating only boundary values, rather than values throughout the space, it is significantly more efficient in terms of computational resources for problems with a small surface/volume ratio. Conceptually, it works by constructing a ""mesh"" over the modeled surface. However, for many problems, BEM are significantly computationally less efficient than volume-discretization methods (finite element method, finite difference method, finite volume method). Boundary element formulations typically give rise to fully populated matrices. This means that the storage requirements and computational time will tend to grow according to the square of the problem size. By contrast, finite element matrices are typically banded (elements are only locally connected) and the storage requirements for the system matrices typically grow linearly with the problem size. Compression techniques (e.g. multipole expansions or adaptive cross approximation/hierarchical matrices) can be used to ameliorate these problems, though at the cost of added complexity and with a success-rate that depends heavily on the nature and geometry of the problem.
BEM is applicable to problems for which Green's functions can be calculated. These usually involve fields in linear homogeneous media. This places considerable restrictions on the range and generality of problems suitable for boundary elements. Nonlinearities can be included in the formulation, although they generally introduce volume integrals which require the volume to be discretized before solution, removing an oft-cited advantage of BEM.


=== Fast multipole method ===
The fast multipole method (FMM) is an alternative to MoM or Ewald summation. It is an accurate simulation technique and requires less memory and processor power than MoM. The FMM was first introduced by Greengard and Rokhlin and is based on the multipole expansion technique. The first application of the FMM in computational electromagnetics was by Engheta et al.(1992). FMM can also be used to accelerate MoM.


=== Plane wave time-domain ===
While the fast multipole method is useful for accelerating MoM solutions of integral equations with static or frequency-domain oscillatory kernels, the plane wave time-domain (PWTD) algorithm employs similar ideas to accelerate the MoM solution of time-domain integral equations involving the retarded potential. The PWTD algorithm was introduced in 1998 by Ergin, Shanker, and Michielssen.


=== Partial element equivalent circuit method ===
The partial element equivalent circuit (PEEC) is a 3D full-wave modeling method suitable for combined electromagnetic and circuit analysis. Unlike MoM, PEEC is a full spectrum method valid from dc to the maximum frequency determined by the meshing. In the PEEC method, the integral equation is interpreted as Kirchhoff's voltage law applied to a basic PEEC cell which results in a complete circuit solution for 3D geometries. The equivalent circuit formulation allows for additional SPICE type circuit elements to be easily included. Further, the models and the analysis apply to both the time and the frequency domains. The circuit equations resulting from the PEEC model are easily constructed using a modified loop analysis (MLA) or modified nodal analysis (MNA) formulation. Besides providing a direct current solution, it has several other advantages over a MoM analysis for this class of problems since any type of circuit element can be included in a straightforward way with appropriate matrix stamps. The PEEC method has recently been extended to include nonorthogonal geometries. This model extension, which is consistent with the classical orthogonal formulation, includes the Manhattan representation of the geometries in addition to the more general quadrilateral and hexahedral elements. This helps in keeping the number of unknowns at a minimum and thus reduces computational time for nonorthogonal geometries.


== Differential equation solvers ==


=== Finite-difference time-domain ===
Finite-difference time-domain (FDTD) is a popular CEM technique. It is easy to understand. It has an exceptionally simple implementation for a full wave solver. It is at least an order of magnitude less work to implement a basic FDTD solver than either an FEM or MoM solver. FDTD is the only technique where one person can realistically implement oneself in a reasonable time frame, but even then, this will be for a quite specific problem. Since it is a time-domain method, solutions can cover a wide frequency range with a single simulation run, provided the time step is small enough to satisfy the Nyquist–Shannon sampling theorem for the desired highest frequency.
FDTD belongs in the general class of grid-based differential time-domain numerical modeling methods. Maxwell's equations (in partial differential form) are modified to central-difference equations, discretized, and implemented in software. The equations are solved in a cyclic manner: the electric field is solved at a given instant in time, then the magnetic field is solved at the next instant in time, and the process is repeated over and over again.
The basic FDTD algorithm traces back to a seminal 1966 paper by Kane Yee in IEEE Transactions on Antennas and Propagation. Allen Taflove originated the descriptor ""Finite-difference time-domain"" and its corresponding ""FDTD"" acronym in a 1980 paper in IEEE Transactions on Electromagnetic Compatibility. Since about 1990, FDTD techniques have emerged as the primary means to model many scientific and engineering problems addressing electromagnetic wave interactions with material structures. An effective technique based on a time-domain finite-volume discretization procedure was introduced by Mohammadian et al. in 1991. Current FDTD modeling applications range from near-DC (ultralow-frequency geophysics involving the entire Earth-ionosphere waveguide) through microwaves (radar signature technology, antennas, wireless communications devices, digital interconnects, biomedical imaging/treatment) to visible light (photonic crystals, nanoplasmonics, solitons, and biophotonics). Approximately 30 commercial and university-developed software suites are available.


=== Multiresolution time-domain ===
MRTD is an adaptive alternative to the finite difference time domain method (FDTD) based on wavelet analysis.


=== Finite element method ===
The finite element method (FEM) is used to find approximate solution of partial differential equations (PDE) and integral equations. The solution approach is based either on eliminating the time derivatives completely (steady state problems), or rendering the PDE into an equivalent ordinary differential equation, which is then solved using standard techniques such as finite differences, etc.
In solving partial differential equations, the primary challenge is to create an equation which approximates the equation to be studied, but which is numerically stable, meaning that errors in the input data and intermediate calculations do not accumulate and destroy the meaning of the resulting output. There are many ways of doing this, with various advantages and disadvantages. The finite element method is a good choice for solving partial differential equations over complex domains or when the desired precision varies over the entire domain.


=== Finite integration technique ===
The finite integration technique (FIT) is a spatial discretization scheme to numerically solve electromagnetic field problems in time and frequency domain. It preserves basic topological properties of the continuous equations such as conservation of charge and energy. FIT was proposed in 1977 by Thomas Weiland and has been enhanced continually over the years. This method covers the full range of electromagnetics (from static up to high frequency) and optic applications and is the basis for commercial simulation tools.
The basic idea of this approach is to apply the Maxwell equations in integral form to a set of staggered grids. This method stands out due to high flexibility in geometric modeling and boundary handling as well as incorporation of arbitrary material distributions and material properties such as anisotropy, non-linearity and dispersion. Furthermore, the use of a consistent dual orthogonal grid (e.g. Cartesian grid) in conjunction with an explicit time integration scheme (e.g. leap-frog-scheme) leads to compute and memory-efficient algorithms, which are especially adapted for transient field analysis in radio frequency (RF) applications.


=== Pseudo-spectral time domain ===
This class of marching-in-time computational techniques for Maxwell's equations uses either discrete Fourier or discrete Chebyshev transforms to calculate the spatial derivatives of the electric and magnetic field vector components that are arranged in either a 2-D grid or 3-D lattice of unit cells. PSTD causes negligible numerical phase velocity anisotropy errors relative to FDTD, and therefore allows problems of much greater electrical size to be modeled.


=== Pseudo-spectral spatial domain ===
PSSD solves Maxwell's equations by propagating them forward in a chosen spatial direction. The fields are therefore held as a function of time, and (possibly) any transverse spatial dimensions. The method is pseudo-spectral because temporal derivatives are calculated in the frequency domain with the aid of FFTs. Because the fields are held as functions of time, this enables arbitrary dispersion in the propagation medium to be rapidly and accurately modelled with minimal effort. However, the choice to propagate forward in space (rather than in time) brings with it some subtleties, particularly if reflections are important.


=== Transmission line matrix ===
Transmission line matrix (TLM) can be formulated in several means as a direct set of lumped elements solvable directly by a circuit solver (ala SPICE, HSPICE, et al.), as a custom network of elements or via a scattering matrix approach. TLM is a very flexible analysis strategy akin to FDTD in capabilities, though more codes tend to be available with FDTD engines.


=== Locally one-dimensional ===
This is an implicit method. In this method, in two-dimensional case, Maxwell equations are computed in two steps, whereas in three-dimensional case Maxwell equations are divided into three spatial coordinate directions. Stability and dispersion analysis of the three-dimensional LOD-FDTD method have been discussed in detail.


== Other methods ==


=== EigenMode expansion ===
Eigenmode expansion (EME) is a rigorous bi-directional technique to simulate electromagnetic propagation which relies on the decomposition of the electromagnetic fields into a basis set of local eigenmodes. The eigenmodes are found by solving Maxwell's equations in each local cross-section. Eigenmode expansion can solve Maxwell's equations in 2D and 3D and can provide a fully vectorial solution provided that the mode solvers are vectorial. It offers very strong benefits compared with the FDTD method for the modelling of optical waveguides, and it is a popular tool for the modelling of fiber optics and silicon photonics devices.


=== Physical optics ===
Physical optics (PO) is the name of a high frequency approximation (short-wavelength approximation) commonly used in optics, electrical engineering and applied physics. It is an intermediate method between geometric optics, which ignores wave effects, and full wave electromagnetism, which is a precise theory. The word ""physical"" means that it is more physical than geometrical optics and not that it is an exact physical theory.
The approximation consists of using ray optics to estimate the field on a surface and then integrating that field over the surface to calculate the transmitted or scattered field. This resembles the Born approximation, in that the details of the problem are treated as a perturbation.


=== Uniform theory of diffraction ===
The uniform theory of diffraction (UTD) is a high frequency method for solving electromagnetic scattering problems from electrically small discontinuities or discontinuities in more than one dimension at the same point.
The uniform theory of diffraction approximates near field electromagnetic fields as quasi optical and uses ray diffraction to determine diffraction coefficients for each diffracting object-source combination. These coefficients are then used to calculate the field strength and phase for each direction away from the diffracting point. These fields are then added to the incident fields and reflected fields to obtain a total solution.


== Validation ==
Validation is one of the key issues facing electromagnetic simulation users. The user must understand and master the validity domain of its simulation. The measure is, ""how far from the reality are the results?""
Answering this question involves three steps: comparison between simulation results and analytical formulation, cross-comparison between codes, and comparison of simulation results with measurement.


=== Comparison between simulation results and analytical formulation ===
For example, assessing the value of the radar cross section of a plate with the analytical formula:

  
    
      
        
          
            RCS
          
          
            Plate
          
        
        =
        
          
            
              4
              π
              
                A
                
                  2
                
              
            
            
              λ
              
                2
              
            
          
        
        ,
      
    
    {\displaystyle {\text{RCS}}_{\text{Plate}}={\frac {4\pi A^{2}}{\lambda ^{2}}},}
  

where A is the surface of the plate and 
  
    
      
        λ
      
    
    {\displaystyle \lambda }
   is the wavelength. The next curve presenting the RCS of a plate computed at 35 GHz can be used as reference example.


=== Cross-comparison between codes ===
One example is the cross comparison of results from method of moments and asymptotic methods in their validity domains.


=== Comparison of simulation results with measurement ===
The final validation step is made by comparison between measurements and simulation. For example, the RCS calculation and the measurement of a complex metallic object at 35 GHz. The computation implements GO, PO and PTD for the edges.
Validation processes can clearly reveal that some differences can be explained by the differences between the experimental setup and its reproduction in the simulation environment.


== Light scattering codes ==
There are now many efficient codes for solving electromagnetic scattering problems. They are listed as discrete dipole approximation codes, codes for electromagnetic scattering by cylinders, codes for electromagnetic scattering by spheres. Solutions which are analytical, such as Mie solution for scattering by spheres or cylinders, can be used to validate more involved techniques.


== See also ==
EM simulation software
Analytical regularization
Electromagnetic field solver
Electromagnetic wave equation
Finite-difference time-domain method
Finite-difference frequency-domain
Mie theory
Physical optics
Rigorous coupled-wave analysis
Space mapping
Uniform theory of diffraction
https://en.wikipedia.org/wiki/HOBBIES_(electromagnetic_solver)


== References ==


== Further reading ==
Detailed and highly visual lecture notes and videos on Computational Electromagnetics
R. F. Harrington (1993). Field Computation by Moment Methods. Wiley-IEEE Press. ISBN 0-7803-1014-4. 
W. C. Chew; J.-M. Jin; E. Michielssen; J. Song (2001). Fast and Efficient Algorithms in Computational Electromagnetics. Artech House Publishers. ISBN 1-58053-152-0. 
J. Jin (2002). The Finite Element Method in Electromagnetics, 2nd. ed. Wiley-IEEE Press. ISBN 0-471-43818-9. 
Allen Taflove and Susan C. Hagness (2005). Computational Electrodynamics: The Finite-Difference Time-Domain Method, 3rd ed. Artech House Publishers. ISBN 1-58053-832-0. 


== External links ==
Computational electromagnetics at the Open Directory Project
Computational electromagnetics: a review"
31,Stylometry,2097760,28620,"Stylometry is the application of the study of linguistic style, usually to written language, but it has successfully been applied to music and to fine-art paintings as well.
Stylometry is often used to attribute authorship to anonymous or disputed documents. It has legal as well as academic and literary applications, ranging from the question of the authorship of Shakespeare's works to forensic linguistics.


== History ==
Stylometry grew out of earlier techniques of analyzing texts for evidence of authenticity, author identity, and other questions.
The modern practice of the discipline received major impetus from the study of authorship problems in English Renaissance drama. Researchers and readers observed that some playwrights of the era had distinctive patterns of language preferences, and attempted to use those patterns to identify authors in uncertain or collaborative works. Early efforts were not always successful: in 1901, one researcher attempted to use John Fletcher's preference for ""'em"", the contractional form of ""them"", as a marker to distinguish between Fletcher and Philip Massinger in their collaborations—but he mistakenly employed an edition of Massinger's works in which the editor had expanded all instances of ""'em"" to ""them"".
The basics of stylometry were set out by Polish philosopher Wincenty Lutosławski in Principes de stylométrie (1890). Lutosławski used this method to build a chronology of Plato's Dialogues.
The development of computers and their capacities for analyzing large quantities of data enhanced this type of effort by orders of magnitude. The great capacity of computers for data analysis, however, did not guarantee quality output. In the early 1960s, Rev. A. Q. Morton produced a computer analysis of the fourteen Epistles of the New Testament attributed to St. Paul, which showed that six different authors had written that body of work. A check of his method, applied to the works of James Joyce, gave the result that Ulysses, Joyce's multi-perspective, multi-style masterpiece, was written by five separate individuals; none of whom had any part in the crafting of Joyce's first novel, A Portrait of the Artist as a Young Man.
In time, however, and with practice, researchers and scholars have refined their approaches and methods, to yield better results. One notable early success was the resolution of disputed authorship in twelve of The Federalist Papers by Frederick Mosteller and David Wallace. While questions of initial assumptions and methodology still arise (and, perhaps, always will), few now dispute the basic premise that linguistic analysis of written texts can produce valuable information and insight. (Indeed, this was apparent even before the advent of computers: the successful application of a textual/linguistic approach to the Fletcher canon by Cyrus Hoy and others yielded clear results in the late 1950s and early '60s.)


== Applications ==
Applications of stylometry include literary studies, historical studies, social studies, gender studies, and many forensic cases and studies.


== Current research ==
Modern stylometry draws heavily on the aid of computers for statistical analysis, artificial intelligence and access to the growing corpus of texts available via the Internet. Software systems such as Signature (freeware produced by Dr Peter Millican of Oxford University), JGAAP (the Java Graphical Authorship Attribution Program—freeware produced by Dr Patrick Juola of Duquesne University), stylo (an open-source R package for a variety of stylometric analyses, including authorship attribution) and Stylene for Dutch (online freeware by Prof Walter Daelemans of University of Antwerp and Dr Véronique Hoste of University of Ghent) make


== Academic venues and events ==
Stylometric methods are discussed in several academic fields, mostly as a tangential field of application for e.g. machine learning, natural language processing, or lexicography.


=== Forensic linguistics ===
The International Association of Forensic Linguists (IAFL) organises the Biennial Conference of the International Association of Forensic Linguists (13th edition in 2016 in Porto) and publishes The International Journal of Speech, Language and the Law with forensic stylistics as one of its central topics.


=== AAAI ===
The Association for the Advancement of Artificial Intelligence (AAAI) has hosted several events on subjective and stylistic analysis of text.


=== PAN ===
PAN workshops (originally, plagiarism analysis, authorship identification, and near-duplicate detection, later more generally workshop on uncovering plagiarism, authorship, and social software misuse) organised since 2007 mainly in conjunction with information access conferences such as ACM SIGIR, FIRE, and CLEF. PAN formulates shared challenge tasks for plagiarism detection, authorship identification, author gender identification, author profiling, vandalism detection, and other related text analysis tasks, many of which hinge on stylometry.


== Case studies of interest ==
Around 1370–1070 BC, as recorded in the Book of Judges, one tribe identified members of another tribe in order to kill them by asking them to say the word Shibboleth which in the dialect of the intended victims sounded like ""sibboleth"".
In 1439, Lorenzo Valla showed that the Donation of Constantine was a forgery, an argument based partly on a comparison of the Latin with that used in authentic 4th-century documents.
In 1952, the Swedish bishop Dick Helander was elected bishop of Strängnäs. The campaign was competitive and Helander was accused of writing a series of a hundred-some anonymous libelous letters about other candidates to the electorate of the bishopric of Strängnäs. Helander was first convicted of writing the letters and lost his position as bishop but later partially exonerated. The letters were studied using a number of stylometric measures (and also typewriter characteristics) and the various court cases and further examinations, many contracted by Helander himself during the years up to his death in 1978 discussed stylometric methodology and its value as evidence in some detail.
In 1975, after Ronald Reagan had served as governor of California, he began giving weekly radio commentaries syndicated to hundreds of stations. After his personal notes were made public on his 90th birthday in 2001, a study to determine which of those talks were written by him and which were written by various aides used stylostatistical methods.
In 1996, the stylometric analysis of the controversial, pseudonymously authored book Primary Colors, performed by Vassar professor Donald Foster brought the field to the attention of a wider audience after correctly identifying the author as Joe Klein. (This case was only resolved after a handwriting analysis confirmed the authorship).
In 1996, stylometric methods were used to compare the Unabomber manifesto with letters written by one of the suspects, Theodor Kaczynski to his brother, which led to his apprehension and later conviction.
In April 2015, researchers using stylometry techniques identified a play, Double Falsehood, as being the work of William Shakespeare. Researchers analyzed 54 plays by Shakespeare and John Fletcher and compared average sentence length, studied the use of unusual words and quantified the complexity and psychological valence of its language.
In 2017, a group of linguists, computer scientists, and scholars analysed the authoship of Elena Ferrante. Based on a corpus created at University of Padua containing 150 novels written by 40 authors, they analyzed Ferrante's style based on seven of her novels. They were able to compare her writing style with 39 other novelists using, for example, stylo. The conclusion was the same for all of them: Domenico Starnone is the secret hand behind Elena Ferrante


== Data and methods ==
Since stylometry has both descriptive use cases, used to characterise the content of a collection, and identificatory use cases, e.g. identifying authors or categories of texts, the methods used to analyse the data and features above range from those built to classify items into sets or to distribute items in a space of feature variation. Most methods are statistical in nature, such as cluster analysis and discriminant analysis, are typically based on philological data and features, and are fruitful application domains for modern machine learning approaches.
Whereas in the past, stylometry emphasized the rarest or most striking elements of a text, contemporary techniques can isolate identifying patterns even in common parts of speech. Most systems are based on lexical statistics, i.e. using the frequencies of words and terms in the text to characterise the text (or its author). In this context, unlike in information retrieval, the observed occurrence patterns of the most common words are more interesting than the topical terms which are less frequent.
The primary stylometric method is the writer invariant: a property held in common by all texts, or at least all texts long enough to admit of analysis yielding statistically significant results, written by a given author. An example of a writer invariant is frequency of function words used by the writer.
In one such method, the text is analyzed to find the 50 most common words. The text is then broken into 5,000 word chunks and each of the chunks is analyzed to find the frequency of those 50 words in that chunk. This generates a unique 50-number identifier for each chunk. These numbers place each chunk of text into a point in a 50-dimensional space. This 50-dimensional space is flattened into a plane using principal components analysis (PCA). This results in a display of points that correspond to an author's style. If two literary works are placed on the same plane, the resulting pattern may show if both works were by the same author or different authors.


=== Neural networks ===
Neural networks, a special case of statistical machine learning methods, have been used to analyze authorship of texts. Text of undisputed authorship are used to train the neural network through processes such as backpropagation, where training error is calculated and used to update the process to increase accuracy. Through a process akin to non-linear regression, the network gains the ability to generalize its recognition ability to new texts to which it has not yet been exposed, classifying them to a stated degree of confidence. Such techniques were applied to the long-standing claims of collaboration of Shakespeare with his contemporaries Fletcher and Christopher Marlowe, and confirmed the view, based on more conventional scholarship, that such collaboration had indeed taken place.
A 1999 study showed that a neural network program reached 70% accuracy in determining authorship of poems it had not yet analyzed. This study from Vrije Universiteit examined identification of poems by three Dutch authors using only letter sequences such as ""den"".
A study used deep belief networks (DBN) for authorship verification model applicable for continuous authentication (CA).
One problem with this method of analysis is that the network can become biased based on its training set, possibly selecting authors the network has more often analyzed.


=== Genetic algorithms ===
The genetic algorithm is another machine learning technique used in stylometry. This involves a method that starts out with a set of rules. An example rule might be, ""If but appears more than 1.7 times in every thousand words, then the text is author X"". The program is presented with text and uses the rules to determine authorship. The rules are tested against a set of known texts and each rule is given a fitness score. The 50 rules with the lowest scores are thrown out. The remaining 50 rules are given small changes and 50 new rules are introduced. This is repeated until the evolved rules correctly attribute the texts.


=== Rare pairs ===
One method for identifying style is called ""rare pairs"", and relies upon individual habits of collocation. The use of certain words may, for a particular author, idiosyncratically entail the use of other, predictable words.


== Authorship attribution in instant messaging ==
The diffusion of Internet has shifted the authorship attribution attention towards online texts (web pages, blogs, etc.) electronic messages (e-mails, tweets, posts, etc.), and other types of written information that are far shorter than an average book, much less formal and more diverse in terms of expressive elements such as colors, layout, fonts, graphics, emoticons, etc. Efforts to take into account such aspects at the level of both structure and syntax were reported in. In addition, content-specific and idiosyncratic cues (e.g., topic models and grammar checking tools) were introduced to unveil deliberate stylistic choices.
Standard stylometric features have been employed to categorize the content of a chat over instant messaging, or the behavior of the participants, but attempts of identifying chat participants are still few and early. Furthermore, the similarity between spoken conversations and chat interactions has been neglected while being a key difference between chat data and any other type of written information.


== See also ==
Linguistics and the Book of Mormon, Stylometry (Wordprint Studies)
Moshe Koppel
Writeprint


== Notes ==


== References ==
Brocardo, Marcelo Luiz; Issa Traore; Sherif Saad; Isaac Woungang (2013). Authorship Verification for Short Messages Using Stylometry. IEEE Intl. Conference on Computer, Information and Telecommunication Systems (CITS). 
Can F, Patton JM (2004). ""Change of writing style with time"". Computers and the Humanities. 38 (1): 61–82. doi:10.1023/b:chum.0000009225.28847.77. 
Brennan, Michael Robert; Greenstadt, Rachel. ""Practical Attacks Against Authorship Recognition Techniques"". Innovative Applications of Artificial Intelligence. 
Hope, Jonathan (1994). The Authorship of Shakespeare's Plays. Cambridge: Cambridge University Press. 
Hoy C (1956–62). ""The Shares of Fletcher and His Collaborators in the Beaumont and Fletcher Canon"". Studies in Bibliography. 7–15. 
Juola, Patrick (2006). ""Authorship Attribution"" (PDF). Foundations and Trends in Information Retrieval. 1: 3. doi:10.1561/1500000005. 
Kenny, Anthony (1982). The Computation of Style: An Introduction to Statistics for Students of Literature and Humanities. Oxford: Pergamon Press. 
Romaine, Suzanne (1982). Socio-Historical Linguistics. Cambridge: Cambridge University Press. 
Samuels, M. L. (1972). Linguistic Evolution: With Special Reference to English. Cambridge: Cambridge University Press. 
Schoenbaum, Samuel (1966). Internal Evidence and Elizabethan Dramatic Authorship: An Essay in Literary History and Method. Evanston, IL, USA: Northwestern University Press. 
Van Droogenbroeck, Frans J. (2016) ""Handling the Zipf distribution in computerized authorship attribution""
Zenkov A.V. A Method of Text Attribution Based on the Statistics of Numerals // Journal of Quantitative Linguistics, 2017, https://dx.doi.org/10.1080/09296174.2017.1371915


=== Further reading ===
See also the academic journal Literary and Linguistic Computing (published by the University of Oxford) and the Language Resources and Evaluation journal.


== External links ==
Association for Computers and the Humanities
Literary and Linguistic Computing
Computational Stylistics Group
Signature Stylometric System
JGAAP Authorship Attribution Program
Uncovering the Mystery of J.K. Rowling's Latest Novel"
32,Financial modeling,2844974,28549,"Financial modeling is the task of building an abstract representation (a model) of a real world financial situation. This is a mathematical model designed to represent (a simplified version of) the performance of a financial asset or portfolio of a business, project, or any other investment. Financial modeling is a general term that means different things to different users; the reference usually relates either to accounting and corporate finance applications, or to quantitative finance applications. While there has been some debate in the industry as to the nature of financial modeling—whether it is a tradecraft, such as welding, or a science—the task of financial modeling has been gaining acceptance and rigor over the years. Typically, financial modeling is understood to mean an exercise in either asset pricing or corporate finance, of a quantitative nature. In other words, financial modelling is about translating a set of hypotheses about the behavior of markets or agents into numerical predictions; for example, a firm's decisions about investments (the firm will invest 20% of assets), or investment returns (returns on ""stock A"" will, on average, be 10% higher than the market's returns).


== Accounting ==
In corporate finance and the accounting profession, financial modeling typically entails financial statement forecasting; usually the preparation of detailed company-specific models used for decision making purposes and financial analysis.
Applications include:
Business valuation, especially discounted cash flow, but including other valuation problems
Scenario planning and management decision making (""what is""; ""what if""; ""what has to be done"")
Capital budgeting
Cost of capital (i.e. WACC) calculations
Financial statement analysis (including of operating- and finance leases, and R&D)
Project finance
To generalize as to the nature of these models: firstly, as they are built around financial statements, calculations and outputs are monthly, quarterly or annual; secondly, the inputs take the form of “assumptions”, where the analyst specifies the values that will apply in each period for external / global variables (exchange rates, tax percentage, etc.…; may be thought of as the model parameters), and for internal / company specific variables (wages, unit costs, etc.…). Correspondingly, both characteristics are reflected (at least implicitly) in the mathematical form of these models: firstly, the models are in discrete time; secondly, they are deterministic. For discussion of the issues that may arise, see below; for discussion as to more sophisticated approaches sometimes employed, see Corporate finance# Quantifying uncertainty, and Financial economics #Corporate finance theory.
Modelers are are often designated ""financial analyst"" (and are sometimes referred to (tongue in cheek) as ""number crunchers"") . Typically, the modeler will have completed an MBA or MSF with (optional) coursework in ""financial modeling"". Accounting qualifications and finance certifications such as the CIIA and CFA generally do not provide direct or explicit training in modeling. At the same time, numerous commercial training courses are offered, both through universities and privately.
Although purpose built software does exist, the vast proportion of the market is spreadsheet-based; this is largely since the models are almost always company specific. Also, analysts will each have their own criteria and methods for financial modeling. Microsoft Excel now has by far the dominant position, having overtaken Lotus 1-2-3 in the 1990s. Spreadsheet-based modelling can have its own problems, and several standardizations and ""best practices"" have been proposed. ""Spreadsheet risk"" is increasingly studied and managed.
One critique here, is that model outputs, i.e. line items, often incorporate “unrealistic implicit assumptions” and “internal inconsistencies”. (For example, a forecast for growth in revenue but without corresponding increases in working capital, fixed assets and the associated financing, may imbed unrealistic assumptions about asset turnover, leverage and / or equity financing.) What is required, but often lacking, is that all key elements are explicitly and consistently forecasted. Related to this, is that modellers often additionally ""fail to identify crucial assumptions"" relating to inputs, ""and to explore what can go wrong"". Here, in general, modellers ""use point values and simple arithmetic instead of probability distributions and statistical measures"" — i.e., as mentioned, the problems are treated as deterministic in nature — and thus calculate a single value for the asset or project, but without providing information on the range, variance and sensitivity of outcomes. Other critiques discuss the lack of basic computer programming concepts. More serious criticism, in fact, relates to the nature of budgeting itself, and its impact on the organization.
The Financial Modeling World Championships, known as ModelOff, have been held since 2012. ModelOff is a global online financial modeling competition which culminates in a Live Finals Event for top competitors. From 2012-2014 the Live Finals were held in New York City and in 2015, in London.


== Quantitative finance ==
In quantitative finance, financial modeling entails the development of a sophisticated mathematical model. Models here deal with asset prices, market movements, portfolio returns and the like. A general distinction is between: ""quantitative financial management"", models of the financial situation of a large, complex firm; ""quantitative asset pricing"", models of the returns of different stocks; ""financial engineering"", models of the price or returns of derivative securities; ""quantitative corporate finance"", models of the firm's financial decisions.
Relatedly, applications include:
Option pricing and calculation of their ""Greeks""
Other derivatives, especially interest rate derivatives, credit derivatives and exotic derivatives
Modeling the term structure of interest rates (Bootstrapping, short rate modelling, building ""curve sets"") and credit spreads
Credit scoring and provisioning
Corporate financing activity prediction problems
Portfolio optimization.
Real options
Risk modeling (Financial risk modeling) and value at risk
Dynamic financial analysis (DFA)
Credit valuation adjustment, CVA, as well as the various XVA
These problems are generally stochastic and continuous in nature, and models here thus require complex algorithms, entailing computer simulation, advanced numerical methods (such as numerical differential equations, numerical linear algebra, dynamic programming) and/or the development of optimization models. The general nature of these problems is discussed under Mathematical finance, while specific techniques are listed under Outline of finance# Mathematical tools. For further discussion here see also: Financial models with long-tailed distributions and volatility clustering; Brownian model of financial markets; Martingale pricing; Extreme value theory; Historical simulation (finance).
Modellers are generally referred to as ""quants"" (quantitative analysts), and typically have advanced (Ph.D. level) backgrounds in quantitative disciplines such as physics, engineering, computer science, mathematics or operations research. Alternatively, or in addition to their quantitative background, they complete a finance masters with a quantitative orientation, such as the Master of Quantitative Finance, or the more specialized Master of Computational Finance or Master of Financial Engineering; the CQF is increasingly common.
Although spreadsheets are widely used here also (almost always requiring extensive VBA), custom C++, Fortran or Python, or numerical analysis software such as MATLAB, are often preferred, particularly where stability or speed is a concern. MATLAB is often used at the research or prototyping stage because of its intuitive programming, graphical and debugging tools, but C++/Fortran are preferred for conceptually simple but high computational-cost applications where MATLAB is too slow; Python is increasingly used due to its simplicity and large standard library. Additionally, for many (of the standard) derivative and portfolio applications, commercial software is available, and the choice as to whether the model is to be developed in-house, or whether existing products are to be deployed, will depend on the problem in question.
The complexity of these models may result in incorrect pricing or hedging or both. This Model risk is the subject of ongoing research by finance academics, and is a topic of great, and growing, interest in the risk management arena.
Criticism of the discipline (often preceding the financial crisis of 2007–08 by several years) emphasizes the differences between the mathematical and physical sciences, and finance, and the resultant caution to be applied by modelers, and by traders and risk managers using their models. Notable here are Emanuel Derman and Paul Wilmott, authors of the Financial Modelers' Manifesto. Some go further and question whether mathematical- and statistical modeling may be applied to finance at all, at least with the assumptions usually made (for options; for portfolios). In fact, these may go so far as to question the ""empirical and scientific validity... of modern financial theory"". Notable here are Nassim Taleb and Benoit Mandelbrot. See also Mathematical finance #Criticism and Financial economics #Challenges and criticism.


== See also ==


== References ==


== Bibliography =="
33,Fat object,42247504,28269,"In geometry, a fat object is an object in two or more dimensions, whose lengths in the different dimensions are similar. For example, a square is fat because its length and width are identical. A 2-by-1 rectangle is thinner than a square, but it is fat relative to a 10-by-1 rectangle. Similarly, a circle is fatter than a 1-by-10 ellipse and an equilateral triangle is fatter than a very obtuse triangle.
Fat objects are especially important in computational geometry. Many algorithms in computational geometry can perform much better if their input consists of only fat objects; see the applications section below.


== Global fatness ==

Given a constant R≥1, an object o is called R-fat if its ""slimness factor"" is at most R. The ""slimness factor"" has different definitions in different papers. A common definition is:

  
    
      
        
          
            
              
                side of smallest cube enclosing
              
               
              o
            
            
              
                side of largest cube enclosed in
              
               
              o
            
          
        
      
    
    {\displaystyle {\frac {{\text{side of smallest cube enclosing}}\ o}{{\text{side of largest cube enclosed in}}\ o}}}
  
where o and the cubes are d-dimensional. A 2-dimensional cube is a square, so the slimness factor of a square is 1 (since its smallest enclosing square is the same as its largest enclosed disk). The slimness factor of a 10-by-1 rectangle is 10. The slimness factor of a circle is √2. Hence, by this definition, a square is 1-fat but a disk and a 10×1 rectangle are not 1-fat. A square is also 2-fat (since its slimness factor is less than 2), 3-fat, etc. A disk is also 2-fat (and also 3-fat etc.), but a 10×1 rectangle is not 2-fat. Every shape is ∞-fat, since by definition the slimness factor is always at most ∞.
The above definition can be termed two-cubes fatness since it is based on the ratio between the side-lengths of two cubes. Similarly, it is possible to define two-balls fatness, in which a d-dimensional ball is used instead. A 2-dimensional ball is a disk. According to this alternative definition, a disk is 1-fat but a square is not 1-fat, since its two-balls-slimness is √2.
An alternative definition, that can be termed enclosing-ball fatness (also called ""thickness"") is based on the following slimness factor:

  
    
      
        
          
            (
            
              
                
                  
                    volume of smallest ball enclosing
                  
                   
                  o
                
                
                  
                    volume of
                  
                   
                  o
                
              
            
            )
          
          
            1
            
              /
            
            d
          
        
      
    
    {\displaystyle \left({\frac {{\text{volume of smallest ball enclosing}}\ o}{{\text{volume of}}\ o}}\right)^{1/d}}
  
The exponent 1/d makes this definition a ratio of two lengths, so that it is comparable to the two-balls-fatness.
Here, too, a cube can be used instead of a ball.
Similarly it is possible to define the enclosed-ball fatness based on the following slimness factor:

  
    
      
        
          
            (
            
              
                
                  
                    volume of
                  
                   
                  o
                
                
                  
                    volume of largest  ball enclosed in
                  
                   
                  o
                
              
            
            )
          
          
            1
            
              /
            
            d
          
        
      
    
    {\displaystyle \left({\frac {{\text{volume of}}\ o}{{\text{volume of largest  ball enclosed in}}\ o}}\right)^{1/d}}
  


=== Enclosing-fatness vs. enclosed-fatness ===
The enclosing-ball/cube-slimness might be very different from the enclosed-ball/cube-slimness.
For example, consider a lollipop with a candy in the shape of a 1×1 square and a stick in the shape of a b×(1/b) rectangle (with b>1>(1/b)). As b increases, the area of the enclosing cube (≈b2) increases, but the area of the enclosed cube remains constant (=1) and the total area of the shape also remains constant (=2). Thus the enclosing-cube-slimness can grow arbitrarily while the enclosed-cube-slimness remains constant (=√2). See this GeoGebra page for a demonstration.
On the other hand, consider a rectilinear 'snake' with width 1/b and length b, that is entirely folded within a square of side length 1. As b increases, the area of the enclosed cube(≈1/b2) decreases, but the total areas of the snake and of the enclosing cube remain constant (=1). Thus the enclosed-cube-slimness can grow arbitrarily while the enclosing-cube-slimness remains constant (=1).
With both the lollipop and the snake, the two-cubes-slimness grows arbitrarily, since in general:

enclosing-ball-slimness ⋅ enclosed-ball-slimness = two-balls-slimness
enclosing-cube-slimness ⋅ enclosed-cube-slimness = two-cubes-slimness

Since all slimness factor are at least 1, it follows that if an object o is R-fat according to the two-balls/cubes definition, it is also R-fat according to the enclosing-ball/cube and enclosed-ball/cube definitions (but the opposite is not true, as exemplified above).


=== Balls vs. cubes ===
The volume of a d-dimensional ball of radius r is: 
  
    
      
        
          V
          
            d
          
        
        ⋅
        
          r
          
            d
          
        
      
    
    {\displaystyle V_{d}\cdot r^{d}}
  , where Vd is a dimension-dependent constant:

  
    
      
        
          V
          
            d
          
        
        =
        
          
            
              π
              
                d
                
                  /
                
                2
              
            
            
              Γ
              (
              
                
                  d
                  2
                
              
              +
              1
              )
            
          
        
      
    
    {\displaystyle V_{d}={\frac {\pi ^{d/2}}{\Gamma ({\frac {d}{2}}+1)}}}
  
A d-dimensional cube with side-length 2a has volume (2a)d. It is enclosed in a d-dimensional ball with radius a√d whose volume is Vd(a√d)d. Hence for every d-dimensional object:

enclosing-ball-slimness ≤ enclosing-cube-slimness ⋅ 
  
    
      
        
          
            
              V
              
                d
              
            
          
          
            1
            
              /
            
            d
          
        
        ⋅
        
          
            d
          
        
        
          /
        
        2
      
    
    {\displaystyle {V_{d}}^{1/d}\cdot {\sqrt {d}}/2}
  .

For even dimensions (d=2k), the factor simplifies to: 
  
    
      
        
          
            0.5
            π
            k
          
        
        
          /
        
        
          
            
              (
              k
              !
              )
            
            
              1
              
                /
              
              2
              k
            
          
        
      
    
    {\displaystyle {\sqrt {0.5\pi k}}/{{(k!)}^{1/2k}}}
  . In particular, for two-dimensional shapes V2=π and the factor is: √(0.5 π)≈1.25, so:

enclosing-disk-slimness ≤ enclosing-square-slimness ⋅ 1.25

From similar considerations:

enclosed-cube-slimness ≤ enclosed-ball-slimness ⋅ 
  
    
      
        
          
            
              V
              
                d
              
            
          
          
            1
            
              /
            
            d
          
        
        ⋅
        
          
            d
          
        
        
          /
        
        2
      
    
    {\displaystyle {V_{d}}^{1/d}\cdot {\sqrt {d}}/2}
  
enclosed-square-slimness ≤ enclosed-disk-slimness ⋅ 1.25

A d-dimensional ball with radius a is enclosed in a d-dimensional cube with side-length 2a. Hence for every d-dimensional object:

enclosing-cube-slimness ≤ enclosing-ball-slimness ⋅ 
  
    
      
        2
        
          /
        
        
          
            
              V
              
                d
              
            
          
          
            1
            
              /
            
            d
          
        
      
    
    {\displaystyle 2/{V_{d}}^{1/d}}
  

For even dimensions (d=2k), the factor simplifies to: 
  
    
      
        2
        
          /
        
        
          
            (
            k
            !
            )
          
          
            1
            
              /
            
            2
            k
          
        
        
          /
        
        
          
            π
          
        
      
    
    {\displaystyle 2/{(k!)}^{1/2k}/{\sqrt {\pi }}}
  . In particular, for two-dimensional shapes the factor is: 2/√π≈1.13, so:

enclosing-square-slimness ≤ enclosing-disk-slimness ⋅ 1.13

From similar considerations:

enclosed-ball-slimness ≤ enclosed-cube-slimness ⋅ 
  
    
      
        2
        
          /
        
        
          
            
              V
              
                d
              
            
          
          
            1
            
              /
            
            d
          
        
      
    
    {\displaystyle 2/{V_{d}}^{1/d}}
  
enclosed-disk-slimness ≤ enclosed-square-slimness ⋅ 1.13

Multiplying the above relations gives the following simple relations:

two-balls-slimness ≤ two-cubes-slimness ⋅ √d
two-cubes-slimness ≤ two-balls-slimness ⋅ √d

Thus, an R-fat object according to the either the two-balls or the two-cubes definition is at most R√d-fat according to the alternative definition.


== Local fatness ==
The above definitions are all global in the sense that they don't care about small thin areas that are part of a large fat object.
For example, consider a lollipop with a candy in the shape of a 1×1 square and a stick in the shape of a 1×(1/b) rectangle (with b>1>(1/b)). As b increases, the area of the enclosing cube (=4) and the area of the enclosed cube (=1) remain constant, while the total area of the shape changes only slightly (=1+1/b). Thus all three slimness factors are bounded: enclosing-cube-slimness≤2, enclosed-cube-slimness≤2, two-cube-slimness=2. Thus by all definitions the lollipop is 2-fat. However, the stick-part of the lollipop obviously becomes thinner and thinner.
In some applications, such thin parts are unacceptable, so local fatness, based on a local slimness factor, may be more appropriate. For every global slimness factor, it is possible to define a local version. For example, for the enclosing-ball-slimness, it is possible to define the local-enclosing-ball slimness factor of an object o by considering the set B of all balls whose center is inside o and whose boundary intersects the boundary of o (i.e. not entirely containing o). The local-enclosing-ball-slimness factor is defined as:

  
    
      
        
          
            1
            2
          
        
        ⋅
        
          sup
          
            b
            ∈
            B
          
        
        
          
            (
            
              
                
                  
                    volume of
                  
                   
                  B
                
                
                  
                    volume of
                  
                   
                  B
                  ∩
                  o
                
              
            
            )
          
          
            1
            
              /
            
            d
          
        
      
    
    {\displaystyle {\frac {1}{2}}\cdot \sup _{b\in B}\left({\frac {{\text{volume of}}\ B}{{\text{volume of}}\ B\cap o}}\right)^{1/d}}
  
The 1/2 is a normalization factor that makes the local-enclosing-ball-slimness of a ball equal to 1. The local-enclosing-ball-slimness of the lollipop-shape described above is dominated by the 1×(1/b) stick, and it goes to ∞ as b grows. Thus by the local definition the above lollipop is not 2-fat.


=== Global vs. local definitions ===
Local-fatness implies global-fatness. Here is a proof sketch for fatness based on enclosing balls. By definition, the volume of the smallest enclosing ball is ≤ the volume of any other enclosing ball. In particular, it is ≤ the volume of any enclosing ball whose center is inside o and whose boundary touches the boundary of o. But every such enclosing ball is in the set B considered by the definition of local-enclosing-ball slimness. Hence:

enclosing-ball-slimnessd =
= volume(smallest-enclosing-ball)/volume(o)
≤ volume(enclosing-ball-b-in-B)/volume(o)
= volume(enclosing-ball-b-in-B)/volume(b ∩ o)
≤ (2 local-enclosing-ball-slimness)d

Hence:

enclosing-ball-slimness ≤ 2⋅local-enclosing-ball-slimness

For a convex body, the opposite is also true: local-fatness implies global-fatness. The proof is based on the following lemma. Let o be a convex object. Let P be a point in o. Let b and B be two balls centered at P such that b is smaller than B. Then o intersects a larger portion of b than of B, i.e.:

  
    
      
        
          
            
              
                volume
              
               
              (
              b
              ∩
              o
              )
            
            
              
                volume
              
               
              (
              b
              )
            
          
        
        ≥
        
          
            
              
                volume
              
               
              (
              B
              ∩
              o
              )
            
            
              
                volume
              
               
              (
              B
              )
            
          
        
      
    
    {\displaystyle {\frac {{\text{volume}}\ (b\cap o)}{{\text{volume}}\ (b)}}\geq {\frac {{\text{volume}}\ (B\cap o)}{{\text{volume}}\ (B)}}}
  

Proof sketch: standing at the point P, we can look at different angles θ and measure the distance to the boundary of o. Because o is convex, this distance is a function, say r(θ). We can calculate the left-hand side of the inequality by integrating the following function (multiplied by some determinant function) over all angles:

  
    
      
        f
        (
        θ
        )
        =
        min
        
          (
          
            
              
                r
                (
                θ
                )
              
              
                
                  radius
                
                 
                (
                b
                )
              
            
          
          ,
          1
          )
        
      
    
    {\displaystyle f(\theta )=\min {({\frac {r(\theta )}{{\text{radius}}\ (b)}},1)}}
  

Similarly we can calculate the right-hand side of the inequality by integrating the following function:

  
    
      
        F
        (
        θ
        )
        =
        min
        
          (
          
            
              
                r
                (
                θ
                )
              
              
                
                  radius
                
                 
                (
                B
                )
              
            
          
          ,
          1
          )
        
      
    
    {\displaystyle F(\theta )=\min {({\frac {r(\theta )}{{\text{radius}}\ (B)}},1)}}
  

By checking all 3 possible cases, it is possible to show that always 
  
    
      
        f
        (
        θ
        )
        ≥
        F
        (
        θ
        )
      
    
    {\displaystyle f(\theta )\geq F(\theta )}
  . Thus the integral of f is at least the integral of F, and the lemma follows.
The definition of local-enclosing-ball slimness considers all balls that are centered in a point in o and intersect the boundary of o. However, when o is convex, the above lemma allows us to consider, for each point in o, only balls that are maximal in size, i.e., only balls that entirely contain o (and whose boundary intersects the boundary of o). For every such ball b:

  
    
      
        
          volume
        
         
        (
        b
        )
        ≤
        
          C
          
            d
          
        
        ⋅
        
          diameter
        
         
        (
        o
        
          )
          
            d
          
        
      
    
    {\displaystyle {\text{volume}}\ (b)\leq C_{d}\cdot {\text{diameter}}\ (o)^{d}}
  

where 
  
    
      
        
          C
          
            d
          
        
      
    
    {\displaystyle C_{d}}
   is some dimension-dependent constant.
The diameter of o is at most the diameter of the smallest ball enclosing o, and the volume of that ball is: 
  
    
      
        
          C
          
            d
          
        
        ⋅
        (
        
          diameter(smallest ball enclosing
        
         
        o
        )
        
          /
        
        2
        
          )
          
            d
          
        
      
    
    {\displaystyle C_{d}\cdot ({\text{diameter(smallest ball enclosing}}\ o)/2)^{d}}
  . Combining all inequalities gives that for every convex object:

local-enclosing-ball-slimness ≤ enclosing-ball-slimness

For non-convex objects, this inequality of course doesn't hold, as exemplified by the lollipop above.


== Examples ==
The following table shows the slimness factor of various shapes based on the different definitions. The two columns of the local definitions are filled with ""*"" when the shape is convex (in this case, the value of the local slimness equals the value of the corresponding global slimness):


== Fatness of a triangle ==
Slimness is invariant to scale, so the slimness factor of a triangle (as of any other polygon) can be presented as a function of its angles only. The three ball-based slimness factors can be calculated using well-known trigonometric identities.


=== Enclosed-ball slimness ===
The largest circle contained in a triangle is called its incircle. It is known that:

  
    
      
        Δ
        =
        
          r
          
            2
          
        
        ⋅
        (
        cot
        ⁡
        
          
            
              ∠
              A
            
            2
          
        
        +
        cot
        ⁡
        
          
            
              ∠
              B
            
            2
          
        
        +
        cot
        ⁡
        
          
            
              ∠
              C
            
            2
          
        
        )
      
    
    {\displaystyle \Delta =r^{2}\cdot (\cot {\frac {\angle A}{2}}+\cot {\frac {\angle B}{2}}+\cot {\frac {\angle C}{2}})}
  
where Δ is the area of a triangle and r is the radius of the incircle. Hence, the enclosed-ball slimness of a triangle is:

  
    
      
        
          
            
              
                cot
                ⁡
                
                  
                    
                      ∠
                      A
                    
                    2
                  
                
                +
                cot
                ⁡
                
                  
                    
                      ∠
                      B
                    
                    2
                  
                
                +
                cot
                ⁡
                
                  
                    
                      ∠
                      C
                    
                    2
                  
                
              
              π
            
          
        
      
    
    {\displaystyle {\sqrt {\frac {\cot {\frac {\angle A}{2}}+\cot {\frac {\angle B}{2}}+\cot {\frac {\angle C}{2}}}{\pi }}}}
  


=== Enclosing-ball slimness ===
The smallest containing circle for an acute triangle is its circumcircle, while for an obtuse triangle it is the circle having the triangle's longest side as a diameter.
It is known that:

  
    
      
        Δ
        =
        
          R
          
            2
          
        
        ⋅
        2
        sin
        ⁡
        A
        sin
        ⁡
        B
        sin
        ⁡
        C
      
    
    {\displaystyle \Delta =R^{2}\cdot 2\sin A\sin B\sin C}
  
where again Δ is the area of a triangle and R is the radius of the circumcircle. Hence, for an acute triangle, the enclosing-ball slimness factor is:

  
    
      
        
          
            
              π
              
                2
                sin
                ⁡
                A
                sin
                ⁡
                B
                sin
                ⁡
                C
              
            
          
        
      
    
    {\displaystyle {\sqrt {\frac {\pi }{2\sin A\sin B\sin C}}}}
  
It is also known that:

  
    
      
        Δ
        =
        
          
            
              c
              
                2
              
            
            
              2
              (
              cot
              ⁡
              ∠
              
                A
              
              +
              cot
              ⁡
              ∠
              
                B
              
              )
            
          
        
        =
        
          
            
              
                c
                
                  2
                
              
              (
              sin
              ⁡
              ∠
              
                A
              
              )
              (
              sin
              ⁡
              ∠
              
                B
              
              )
            
            
              2
              sin
              ⁡
              (
              ∠
              
                A
              
              +
              ∠
              
                B
              
              )
            
          
        
      
    
    {\displaystyle \Delta ={\frac {c^{2}}{2(\cot \angle {A}+\cot \angle {B})}}={\frac {c^{2}(\sin \angle {A})(\sin \angle {B})}{2\sin(\angle {A}+\angle {B})}}}
  
where c is any side of the triangle and A,B are the adjacent angles. Hence, for an obtuse triangle with acute angles A and B (and longest side c), the enclosing-ball slimness factor is:

  
    
      
        
          
            
              
                π
                ⋅
                (
                cot
                ⁡
                ∠
                
                  A
                
                +
                cot
                ⁡
                ∠
                
                  B
                
                )
              
              2
            
          
        
        =
        
          
            
              
                π
                ⋅
                sin
                ⁡
                (
                ∠
                
                  A
                
                +
                ∠
                
                  B
                
                )
              
              
                2
                (
                sin
                ⁡
                ∠
                
                  A
                
                )
                (
                sin
                ⁡
                ∠
                
                  B
                
                )
              
            
          
        
      
    
    {\displaystyle {\sqrt {\frac {\pi \cdot (\cot \angle {A}+\cot \angle {B})}{2}}}={\sqrt {\frac {\pi \cdot \sin(\angle {A}+\angle {B})}{2(\sin \angle {A})(\sin \angle {B})}}}}
  
Note that in a right triangle, 
  
    
      
        sin
        ⁡
        
          ∠
          
            C
          
        
        =
        sin
        ⁡
        
          ∠
          
            A
          
          +
          ∠
          
            B
          
        
        =
        1
      
    
    {\displaystyle \sin {\angle {C}}=\sin {\angle {A}+\angle {B}}=1}
  , so the two expressions coincide.


=== Two-balls slimness ===
The inradius r and the circumradius R are connected via a couple of formulae which provide two alternative expressions for the two-balls slimness of an acute triangle:

  
    
      
        
          
            R
            r
          
        
        =
        
          
            1
            
              4
              sin
              ⁡
              (
              
                
                  
                    ∠
                    
                      A
                    
                  
                  2
                
              
              )
              sin
              ⁡
              (
              
                
                  
                    ∠
                    
                      B
                    
                  
                  2
                
              
              )
              sin
              ⁡
              (
              
                
                  
                    ∠
                    
                      C
                    
                  
                  2
                
              
              )
            
          
        
        =
        
          
            1
            
              cos
              ⁡
              ∠
              
                A
              
              +
              cos
              ⁡
              ∠
              
                B
              
              +
              cos
              ⁡
              ∠
              
                C
              
              −
              1
            
          
        
      
    
    {\displaystyle {\frac {R}{r}}={\frac {1}{4\sin({\frac {\angle {A}}{2}})\sin({\frac {\angle {B}}{2}})\sin({\frac {\angle {C}}{2}})}}={\frac {1}{\cos \angle {A}+\cos \angle {B}+\cos \angle {C}-1}}}
  
For an obtuse triangle, c/2 should be used instead of R. By the Law of sines:

  
    
      
        
          
            c
            2
          
        
        =
        R
        sin
        ⁡
        
          ∠
          
            C
          
        
      
    
    {\displaystyle {\frac {c}{2}}=R\sin {\angle {C}}}
  
Hence the slimness factor of an obtuse triangle with obtuse angle C is:

  
    
      
        
          
            
              c
              
                /
              
              2
            
            r
          
        
        =
        
          
            
              sin
              ⁡
              
                ∠
                
                  C
                
              
            
            
              4
              sin
              ⁡
              (
              
                
                  
                    ∠
                    
                      A
                    
                  
                  2
                
              
              )
              sin
              ⁡
              (
              
                
                  
                    ∠
                    
                      B
                    
                  
                  2
                
              
              )
              sin
              ⁡
              (
              
                
                  
                    ∠
                    
                      C
                    
                  
                  2
                
              
              )
            
          
        
        =
        
          
            
              sin
              ⁡
              
                ∠
                
                  C
                
              
            
            
              cos
              ⁡
              ∠
              
                A
              
              +
              cos
              ⁡
              ∠
              
                B
              
              +
              cos
              ⁡
              ∠
              
                C
              
              −
              1
            
          
        
      
    
    {\displaystyle {\frac {c/2}{r}}={\frac {\sin {\angle {C}}}{4\sin({\frac {\angle {A}}{2}})\sin({\frac {\angle {B}}{2}})\sin({\frac {\angle {C}}{2}})}}={\frac {\sin {\angle {C}}}{\cos \angle {A}+\cos \angle {B}+\cos \angle {C}-1}}}
  
Note that in a right triangle, 
  
    
      
        sin
        ⁡
        
          ∠
          
            C
          
        
        =
        1
      
    
    {\displaystyle \sin {\angle {C}}=1}
  , so the two expressions coincide.
The two expressions can be combined in the following way to get a single expression for the two-balls slimness of any triangle with smaller angles A and B:

  
    
      
        
          
            
              sin
              ⁡
              
                max
                (
                ∠
                
                  A
                
                ,
                ∠
                
                  B
                
                ,
                ∠
                
                  C
                
                ,
                π
                
                  /
                
                2
                )
              
            
            
              4
              sin
              ⁡
              (
              
                
                  
                    ∠
                    
                      A
                    
                  
                  2
                
              
              )
              sin
              ⁡
              (
              
                
                  
                    ∠
                    
                      B
                    
                  
                  2
                
              
              )
              sin
              ⁡
              (
              
                
                  
                    π
                    −
                    ∠
                    
                      A
                    
                    −
                    ∠
                    
                      B
                    
                  
                  2
                
              
              )
            
          
        
        =
        
          
            
              sin
              ⁡
              
                max
                (
                ∠
                
                  A
                
                ,
                ∠
                
                  B
                
                ,
                ∠
                
                  C
                
                ,
                π
                
                  /
                
                2
                )
              
            
            
              cos
              ⁡
              ∠
              
                A
              
              +
              cos
              ⁡
              ∠
              
                B
              
              −
              cos
              ⁡
              (
              ∠
              
                A
              
              +
              ∠
              
                B
              
              )
              −
              1
            
          
        
      
    
    {\displaystyle {\frac {\sin {\max(\angle {A},\angle {B},\angle {C},\pi /2)}}{4\sin({\frac {\angle {A}}{2}})\sin({\frac {\angle {B}}{2}})\sin({\frac {\pi -\angle {A}-\angle {B}}{2}})}}={\frac {\sin {\max(\angle {A},\angle {B},\angle {C},\pi /2)}}{\cos \angle {A}+\cos \angle {B}-\cos(\angle {A}+\angle {B})-1}}}
  
To get a feeling of the rate of change in fatness, consider what this formula gives for an isosceles triangle with head angle θ when θ is small:

  
    
      
        
          
            
              sin
              ⁡
              
                max
                (
                θ
                ,
                π
                
                  /
                
                2
                )
              
            
            
              4
              
                sin
                
                  2
                
              
              ⁡
              (
              
                
                  
                    π
                    −
                    θ
                  
                  4
                
              
              )
              sin
              ⁡
              (
              
                
                  θ
                  2
                
              
              )
            
          
        
        ≈
        
          
            1
            
              4
              
                
                  
                    1
                    
                      /
                    
                    2
                  
                
                
                  2
                
              
              θ
              
                /
              
              2
            
          
        
        =
        
          
            1
            θ
          
        
      
    
    {\displaystyle {\frac {\sin {\max(\theta ,\pi /2)}}{4\sin ^{2}({\frac {\pi -\theta }{4}})\sin({\frac {\theta }{2}})}}\approx {\frac {1}{4{\sqrt {1/2}}^{2}\theta /2}}={\frac {1}{\theta }}}
  

The following graphs show the 2-balls slimness factor of a triangle:
Slimness of a general triangle when one angle (a) is a constant parameter while the other angle (x) changes.
Slimness of an isosceles triangle as a function of its head angle (x).


== Fatness of circles, ellipses and their parts ==
The ball-based slimness of a circle is of course 1 - the smallest possible value.

For a circular segment with central angle θ, the circumcircle diameter is the length of the chord and the incircle diameter is the height of the segment, so the two-balls slimness (and its approximation when θ is small) is:

  
    
      
        
          
            length of chord
            height of segment
          
        
        =
        
          
            
              2
              R
              sin
              ⁡
              
                
                  θ
                  2
                
              
            
            
              R
              
                (
                
                  1
                  −
                  cos
                  ⁡
                  
                    
                      θ
                      2
                    
                  
                
                )
              
            
          
        
        =
        
          
            
              2
              sin
              ⁡
              
                
                  θ
                  2
                
              
            
            
              (
              
                1
                −
                cos
                ⁡
                
                  
                    θ
                    2
                  
                
              
              )
            
          
        
        ≈
        
          
            θ
            
              
                θ
                
                  2
                
              
              
                /
              
              8
            
          
        
        =
        
          
            8
            θ
          
        
      
    
    {\displaystyle {\frac {\text{length of chord}}{\text{height of segment}}}={\frac {2R\sin {\frac {\theta }{2}}}{R\left(1-\cos {\frac {\theta }{2}}\right)}}={\frac {2\sin {\frac {\theta }{2}}}{\left(1-\cos {\frac {\theta }{2}}\right)}}\approx {\frac {\theta }{\theta ^{2}/8}}={\frac {8}{\theta }}}
  

For a circular sector with central angle θ (when θ is small), the circumcircle diameter is the radius of the circle and the incircle diameter is the chord length, so the two-balls slimness is:

  
    
      
        
          
            radius of circle
            length of chord
          
        
        =
        
          
            R
            
              2
              R
              sin
              ⁡
              
                
                  θ
                  2
                
              
            
          
        
        =
        
          
            1
            
              2
              sin
              ⁡
              
                
                  θ
                  2
                
              
            
          
        
        ≈
        
          
            1
            
              2
              θ
              
                /
              
              2
            
          
        
        =
        
          
            1
            θ
          
        
      
    
    {\displaystyle {\frac {\text{radius of circle}}{\text{length of chord}}}={\frac {R}{2R\sin {\frac {\theta }{2}}}}={\frac {1}{2\sin {\frac {\theta }{2}}}}\approx {\frac {1}{2\theta /2}}={\frac {1}{\theta }}}
  
For an ellipse, the slimness factors are different in different locations. For example, consider an ellipse with short axis a and long axis b. the length of a chord ranges between 
  
    
      
        2
        a
        sin
        ⁡
        
          
            θ
            2
          
        
      
    
    {\displaystyle 2a\sin {\frac {\theta }{2}}}
   at the narrow side of the ellipse and 
  
    
      
        2
        b
        sin
        ⁡
        
          
            θ
            2
          
        
      
    
    {\displaystyle 2b\sin {\frac {\theta }{2}}}
   at its wide side; similarly, the height of the segment ranges between 
  
    
      
        b
        
          (
          
            1
            −
            cos
            ⁡
            
              
                θ
                2
              
            
          
          )
        
      
    
    {\displaystyle b\left(1-\cos {\frac {\theta }{2}}\right)}
   at the narrow side and 
  
    
      
        a
        
          (
          
            1
            −
            cos
            ⁡
            
              
                θ
                2
              
            
          
          )
        
      
    
    {\displaystyle a\left(1-\cos {\frac {\theta }{2}}\right)}
   at its wide side. So the two-balls slimness ranges between:

  
    
      
        
          
            
              2
              a
              sin
              ⁡
              
                
                  θ
                  2
                
              
            
            
              b
              
                (
                
                  1
                  −
                  cos
                  ⁡
                  
                    
                      θ
                      2
                    
                  
                
                )
              
            
          
        
        ≈
        
          
            
              8
              a
            
            
              b
              θ
            
          
        
      
    
    {\displaystyle {\frac {2a\sin {\frac {\theta }{2}}}{b\left(1-\cos {\frac {\theta }{2}}\right)}}\approx {\frac {8a}{b\theta }}}
  
and:

  
    
      
        
          
            
              2
              b
              sin
              ⁡
              
                
                  θ
                  2
                
              
            
            
              a
              
                (
                
                  1
                  −
                  cos
                  ⁡
                  
                    
                      θ
                      2
                    
                  
                
                )
              
            
          
        
        ≈
        
          
            
              8
              b
            
            
              a
              θ
            
          
        
      
    
    {\displaystyle {\frac {2b\sin {\frac {\theta }{2}}}{a\left(1-\cos {\frac {\theta }{2}}\right)}}\approx {\frac {8b}{a\theta }}}
  
In general, when the secant starts at angle Θ the slimness factor can be approximated by:

  
    
      
        
          
            
              2
              sin
              ⁡
              
                
                  θ
                  2
                
              
            
            
              (
              
                1
                −
                cos
                ⁡
                
                  
                    θ
                    2
                  
                
              
              )
            
          
        
      
    
    {\displaystyle {\frac {2\sin {\frac {\theta }{2}}}{\left(1-\cos {\frac {\theta }{2}}\right)}}}
   
  
    
      
        
          (
          
            
              
                b
                a
              
            
            
              cos
              
                2
              
            
            ⁡
            (
            Θ
            +
            
              
                θ
                2
              
            
            )
            +
            
              
                a
                b
              
            
            
              sin
              
                2
              
            
            ⁡
            (
            Θ
            +
            
              
                θ
                2
              
            
            )
          
          )
        
      
    
    {\displaystyle \left({\frac {b}{a}}\cos ^{2}(\Theta +{\frac {\theta }{2}})+{\frac {a}{b}}\sin ^{2}(\Theta +{\frac {\theta }{2}})\right)}
  


== Fatness of a convex polygon ==
A convex polygon is called r-separated if the angle between each pair of edges (not necessarily adjacent) is at least r.
Lemma: The enclosing-ball-slimness of an r-separated convex polygon is at most 
  
    
      
        O
        (
        1
        
          /
        
        r
        )
      
    
    {\displaystyle O(1/r)}
  .
A convex polygon is called k,r-separated if:
It does not have parallel edges, except maybe two horizontal and two vertical.
Each non-axis-parallel edge makes an angle of at least r with any other edge, and with the x and y axes.
If there are two horizontal edges, then diameter/height is at most k.
If there are two vertical edges, then diameter/width is at most k.
Lemma: The enclosing-ball-slimness of a k,r-separated convex polygon is at most 
  
    
      
        O
        (
        max
        (
        k
        ,
        1
        
          /
        
        r
        )
        )
      
    
    {\displaystyle O(\max(k,1/r))}
  . improve the upper bound to 
  
    
      
        O
        (
        d
        )
      
    
    {\displaystyle O(d)}
  .


== Counting fat objects ==
If an object o has diameter 2a, then every ball enclosing o must have radius at least a and volume at least Vdad. Hence, by definition of enclosing-ball-fatness, the volume of an R-fat object with diameter 2a must be at least: Vdad/Rd. Hence:
Lemma 1: Let R≥1 and C≥0 be two constants. Consider a collection of non-overlapping d-dimensional objects that are all globally R-fat (i.e. with enclosing-ball-slimness ≤ R). The number of such objects of diameter at least 2a, contained in a ball of radius C⋅a, is at most:

  
    
      
        
          V
          
            d
          
        
        ⋅
        (
        C
        a
        
          )
          
            d
          
        
        
          /
        
        (
        
          V
          
            d
          
        
        ⋅
        
          a
          
            d
          
        
        
          /
        
        
          R
          
            d
          
        
        )
        =
        (
        R
        C
        
          )
          
            d
          
        
      
    
    {\displaystyle V_{d}\cdot (Ca)^{d}/(V_{d}\cdot a^{d}/R^{d})=(RC)^{d}}
  

For example (taking d=2, R=1 and C=3): The number of non-overlapping disks with radius at least 1 contained in a circle of radius 3 is at most 32=9. (Actually, it is at most 7).
If we consider local-fatness instead of global-fatness, we can get a stronger lemma:
Lemma 2: Let R≥1 and C≥0 be two constants. Consider a collection of non-overlapping d-dimensional objects that are all locally R-fat (i.e. with local-enclosing-ball-slimness ≤ R). Let o be a single object in that collection with diameter 2a. Then the number of objects in the collection with diameter larger than 2a that lie within distance 2C⋅a from object o is at most:

  
    
      
        (
        4
        R
        ⋅
        (
        C
        +
        1
        )
        
          )
          
            d
          
        
      
    
    {\displaystyle (4R\cdot (C+1))^{d}}
  

For example (taking d=2, R=1 and C=0): the number of non-overlapping disks with radius larger than 1 that touch a given unit disk is at most 42=16 (this is not a tight bound since in this case it is easy to prove an upper bound of 5).


== Generalizations ==
The following generalization of fatness were studied by  for 2-dimensional objects.
A triangle ∆ is a (β, δ)-triangle of a planar object o (0<β≤π/3, 0<δ< 1), if ∆ ⊆ o, each of the angles of ∆ is at least β, and the length of each of its edges is at least δ·diameter(o). An object o in the plane is (β,δ)-covered if for each point P ∈ o there exists a (β, δ)-triangle ∆ of o that contains P.
For convex objects, the two definitions are equivalent, in the sense that if o is α-fat, for some constant α, then it is also (β,δ)-covered, for appropriate constants β and δ, and vice versa. However, for non-convex objects the definition of being fat is more general than the definition of being (β, δ)-covered.


== Applications ==
Fat objects are used in various problems, for example:
Motion planning - planning a path for a robot moving amidst obstacles becomes easier when the obstacles are fat objects.
Fair cake-cutting - dividing a cake becomes more difficult when the pieces have to be fat objects. This requirement is common, for example, when the ""cake"" to be divided is a land-estate.
More applications can be found in the references below.


== References =="
34,Computational law,38358886,28154,"Computational law is a branch of legal informatics concerned with the mechanization of legal reasoning (whether done by humans or by computers). It emphasizes explicit behavioral constraints and eschews implicit rules of conduct. Importantly, there is a commitment to a level of rigor in specifying laws that is sufficient to support entirely mechanical processing.
Philosophically, computational law sits within the Legal Formalist school of jurisprudence. Given its emphasis on rigorously specified laws, computational law is most applicable in civil law settings, where laws are taken more or less literally. It is less applicable to legal systems based on common law, which provides more scope for unspecified normative considerations. However, even in common law systems, computational law still has relevance in the case of categorical statutes and in settings where the handling of cases has resulted in de facto rules.
From a pragmatic perspective, computational law is important as the basis for computer systems capable of doing useful legal calculations, such as compliance checking, legal planning, regulatory analysis, and so forth. Some systems of this sort already exist. TurboTax is a good example. And the potential is particularly significant now due to recent technological advances – including the prevalence of the Internet in human interaction and the proliferation of embedded computer systems (such as smart phones, self-driving cars, and robots).


== History ==
Speculation about potential benefits to legal practice through applying methods from computational science and AI research to automate parts of the law date back at least to the middle 1940s. Further, AI and law and computational law do not seem easily separable, as perhaps most of AI research focusing on the law and its automation appears to utilize computational methods. The forms that speculation took are multiple and not all related in ways to readily show closeness to one another. This history will sketch them as they were, attempting to show relationships where they can be found to have existed.
By 1949, a minor academic field aiming to incorporate electronic and computational methods to legal problems had been founded by American legal scholars, called jurimetrics. Though broadly said to be concerned with the application of the ""methods of science"" to the law, these methods were actually of a quite specifically defined scope. Jurimetrics was to be ""concerned with such matters as the quantitative analysis of judicial behavior, the application of communication and information theory to legal expression, the use of mathematical logic in law, the retrieval of legal data by electronic and mechanical means, and the formulation of a calculus of legal predictability"". These interests led in 1959 to the founding a journal, Modern Uses of Logic in Law, as a forum wherein articles would be published about the applications of techniques such as mathematical logic, engineering, statistics, etc. to the legal study and development. In 1966, this Journal was renamed as Jurimetrics. Today, however, the journal and meaning of jurimetrics seems to have broadened far beyond what would fit under the areas of applications of computers and computational methods to law. Today the journal not only publishes articles on such practices as found in computational law, but has broadened jurimetrical concerns to mean also things like the use of social science in law or the ""policy implications [of] and legislative and administrative control of science"".
Independently in 1958, at the Conference for the Mechanization of Thought held at the National Physical Laboratory in Teddington, Middlesex, UK, the French jurist Lucien Mehl presented a paper both on the benefits of using computational methods for law and on the potential means to use such methods to automate law for a discussion that included AI luminaries like Marvin Minsky. Mehl believed that the law could by automated by two basic distinct, though not wholly separable, types of machine. These were the ""documentary or information machine"", which would provide the legal researcher quick access to relevant case precedents and legal scholarship, and the ""consultation machine"", which would be ""capable of answering any question put to it over a vast field of law"". The latter type of machine would be able to basically do much of a lawyer's job by simply giving the ""exact answer to a [legal] problem put to it"".
By 1970, Mehl's first type of machine, one that would be able to retrieve information, had been accomplished but there seems to have been little consideration of further fruitful intersections between AI and legal research. There were, however, still hopes that computers could model the lawyer's thought processes through computational methods and then apply that capacity to solve legal problems, thus automating and improving legal services via increased efficiency as well as shedding light on the nature of legal reasoning. By the late 1970s, computer science and the affordability of computer technology had progressed enough that the retrieval of ""legal data by electronic and mechanical means"" had been achieved by machines fitting Mehl's first type and were in common use in American law firms. During this time, research focused on improving the goals of the early 1970s occurred, with programs like Taxman being worked on in order to both bring useful computer technology into the law as practical aids and to help specify the exact nature of legal concepts.
Nonetheless, progress on the second type of machine, one that would more fully automate the law, remained relatively inert. Research into machines that could answer questions in the way that Mehl's consultation machine would picked up somewhat in the late 1970s and 1980s. A 1979 convention in Swansea, Wales marked the first international effort solely to focus upon applying artificial intelligence research to legal problems in order to ""consider how computers can be used to discover and apply the legal norms embedded within the written sources of the law"". That said, little substantial progress seems to have been made in the following decade of the 1980s. In a 1988 review of Anne Gardner's book An Artificial Intelligence Approach to Legal Reasoning (1987), the Harvard academic legal scholar and computer scientist Edwina Rissland wrote that ""She plays, in part, the role of pioneer; artificial intelligence (""AI"") techniques have not yet been widely applied to perform legal tasks. Therefore, Gardner, and this review, first describe and define the field, then demonstrate a working model in the domain of contract offer and acceptance."" Eight years after the Swansea conference had passed, and still AI and law researchers merely trying to delineate the field could be described by their own kind as ""pioneer[s]"".
In the 1990s and early 2000s more progress occurred. Computational research generated insights for law. The First International Conference on AI and the Law occurred it 1987, but it is in the 1990s and 2000s that the biannual conference began to build up steam and to delve more deeply into the issues involved with work intersecting computational methods, AI, and law. Classes began to be taught to undergraduates on the uses of computational methods to automating, understanding, and obeying the law. Further, by 2005, a team largely composed of Stanford computer scientists from the Stanford Logic group had devoted themselves to studying the uses of computational techniques to the law. Computational methods in fact advanced enough that members of the legal profession began in the 2000s to both analyze, predict and worry about the potential future of computational law and a new academic field of computational legal studies seems to be now well established. As insight into what such scholars see in the law's future due in part to computational law, here is quote from a recent conference about the ""New Normal"" for the legal profession:
""Over the last 5 years, in the fallout of the Great Recession, the legal profession has entered the era of the New Normal. Notably, a series of forces related to technological change, globalization, and the pressure to do more with less (in both corporate America and law firms) has changed permanently the legal services industry. As one article put it, firms are cutting back on hiring ""in order to increase efficiency, improve profit margins, and reduce client costs."" Indeed, in its recently noted cutbacks, Weil Gotshal's leaders remarked that it had initially expected old work to return, but came ""around to the view that this is the ‘new normal.’""The New Normal provides lawyers with an opportunity to rethink—and reimagine—the role of lawyers in our economy and society. To the extent that law firms enjoyed, or still enjoy, the ability to bundle work together, that era is coming to an end, as clients unbundle legal services and tasks. Moreover, in other cases, automation and technology can change the roles of lawyers, both requiring them to oversee processes and use technology more aggressively as well as doing less of the work that is increasingly managed by computers (think: electronic discovery). The upside is not only greater efficiencies for society, but new possibilities for legal craftsmanship. The emerging craft of lawyering in the New Normal is likely to require lawyers to be both entrepreneurial and fluent with a range of competencies that will enable them to add value for clients. Apropos of the trends noted above, there are emerging opportunities for ""legal entrepreneurs"" in a range of roles from legal process management to developing technologies to manage legal operations (such as overseeing automated processes) to supporting online dispute resolution processes. In other cases, effective legal training as well as domain specific knowledge (finance, sales, IT, entrepreneurship, human resources, etc.) can form a powerful combination that prepares law school grads for a range of opportunities (business development roles, financial operations roles, HR roles, etc.). In both cases, traditional legal skills alone will not be enough to prepare law students for these roles. But the proper training, which builds on the traditional law school curriculum and goes well beyond it including practical skills, relevant domain knowledge (e.g., accounting), and professional skills (e.g., working in teams), will provide law school students a huge advantage over those with a one-dimensional skill set.""
Many see perks to oncoming changes brought about by the computational automation of law. For one thing, legal experts have predicted that it will aid legal self-help, especially in the areas of contract formation, enterprise planning, and the prediction of rule changes. For another thing, those with knowledge about computers see the potential for computational law to really fully bloom as eminent. In this vein, it seems that machines like Mehl's second type may come into existence. Stephen Wolfram has said that:
""So we're slowly moving toward people being educated in the kind of computational paradigm. Which is good, because the way I see it, computation is going to become central to almost every field. Let's talk about two examples—classic professions: law and medicine. It's funny, when Leibniz was first thinking about computation at the end of the 1600s, the thing he wanted to do was to build a machine that would effectively answer legal questions. It was too early then. But now we’re almost ready, I think, for computational law. Where for example contracts become computational. They explicitly become algorithms that decide what's possible and what's not.You know, some pieces of this have already happened. Like with financial derivatives, like options and futures. In the past these used to just be natural language contracts. But then they got codified and parametrized. So they’re really just algorithms, which of course one can do meta-computations on, which is what has launched a thousand hedge funds, and so on. Well, eventually one's going to be able to make computational all sorts of legal things, from mortgages to tax codes to perhaps even patents. Now to actually achieve that, one has to have ways to represent many aspects of the real world, in all its messiness. Which is what the whole knowledge-based computing of Wolfram|Alpha is about.""


== Approaches ==


=== Algorithmic law ===
There have also been many attempts to create a machine readable or machine executable legal code. A machine readable code would simplify the analysis of legal code, allowing the rapid construction and analysis of databases, without the need for advanced text processing techniques. A machine executable format would allow the specifics of a case to be input, and would return the decision based on the case.
Machine readable legal code is already quite common. METAlex, an XML-based standard proposed and developed by the Leibniz Center for Law of the University of Amsterdam, is used by the governments of both the United Kingdom and the Netherlands to encode their laws. In the United States, an executive order issued by President Barack Obama in the May 2013 mandated that all public government documentation be released in a machine readable format by default, although no specific format was mentioned.
Machine executable legal code is much less common. Notable among current efforts is the Hammurabi Project, an attempt to rewrite parts of the United States legal code in such a way that a law can take facts as input and return decisions. The Hammurabi Project currently focuses on the aspects of law that lend themselves to this type of specification, such as tax or immigration laws, although in the long-term the developers of the Hammurabi Project plan to include as many laws as possible.


=== Empirical analysis ===
Many current efforts in computational law are focused on the empirical analysis of legal decisions, and their relation to legislation. These efforts usually make use of citation analysis, which examines patterns in citations between works. Due to the widespread practice of legal citation, it is possible to construct citation indices and large graphs of legal precedent, called citation networks. Citation networks allow the use of graph traversal algorithms in order to relate cases to one another, as well as the use of various distance metrics to find mathematical relationships between them. These analyses can reveal important overarching patterns and trends in judicial proceedings and the way law is used.
There have been several breakthroughs in the analysis of judicial rulings in recent research on legal citation networks. These analyses have made use of citations in Supreme Court majority opinions to build citation networks, and analyzed the patterns in these networks to identify meta-information about individual decisions, such as the importance of the decision, as well as general trends in judicial proceedings, such as the role of precedent over time. These analyses have been used to predict which cases the Supreme Court will choose to consider.
Another effort has examined United States Tax Court decisions, compiling a publicly available database of Tax Court decisions, opinions, and citations between the years of 1990 and 2008, and constructing a citation network from this database. Analysis of this network revealed that large sections of the tax code were rarely, if ever, cited, and that other sections of code, such as those that dealt with ""divorce, dependents, nonprofits, hobby and business expenses and losses, and general definition of income,"" were involved the vast majority of disputes.
Some research has also been focused on hierarchical networks, in combination with citation networks, and the analysis of United States Code. This research has been used to analyze various aspects of the Code, including its size, the density of citations within and between sections of the Code, the type of language used in the Code, and how these features vary over time. This research has been used to provide commentary on the nature of the Code's change over time, which is characterized by an increase in size and in interdependence between sections.


=== Visualization ===
Visualization of legal code, and of the relationships between various laws and decisions, is also a hot topic in computational law. Visualizations allow both professionals and laypeople to see large-scale relationships and patterns, which may be difficult to see using standard legal analysis or empirical analysis.
Legal citation networks lend themselves to visualization, and many citation networks which are analyzed empirically also have sub-sections of the network that are represented visually as a result. However, there are still many technical problems in network visualization. The density of connections between nodes, and the sheer number of nodes in some cases, can make the visualization incomprehensible to humans. There are a variety of methods that can be used to reduce the complexity of the displayed information, for example by defining semantic sub-groups within the network, and then representing relationships between these semantic groups, rather than between every node. This allows the visualization to be human readable, but the reduction in complexity can obscure relationships. Despite this limitation, visualization of legal citation networks remains a popular field and practice.


== Examples of tools ==
OASIS Legal XML, UNDESA Akoma Ntoso, and CEN Metalex, which are standardizations created by legal and technical experts for the electronic exchange of legal data.
Creative Commons, which correspond to custom-generated copyright licenses for internet content.
Legal Analytics, which combines big data, critical expertise, and intuitive tools to deliver business intelligence and benchmarking solutions.
Legal visualizations. Examples include Katz's map of supreme court decisions and Starger's Opinion Lines for the commerce clause and stare decisis.


== Online legal resources and databases ==
PACER is an online repository of judicial rulings, maintained by the Federal Judiciary.
The Law Library of Congress maintains a comprehensive online repository of legal information, including legislation at the international, national, and state levels.
The Supreme Court Database is a comprehensive database containing detailed information about decisions made by the Supreme Court from 1946 to the present.
The United States Reports contained detailed information about every Supreme Court decision from 1791 to the near-present.


== See also ==
Artificial intelligence and law
Legal informatics
Legal expert systems
Robot lawyer


== References ==


== External links ==
CodeX Techindex, Stanford Law School Legal Tech List
LawBots.info Annual Lawbot Awards
LawSites List of Legal Tech Startups"
35,Maximum disjoint set,41701177,28122,"In computational geometry, a maximum disjoint set (MDS) is a largest set of non-overlapping geometric shapes selected from a given set of candidate shapes.
Finding an MDS is important in applications such as automatic label placement, VLSI circuit design, and cellular frequency division multiplexing.
Every set of non-overlapping shapes is an independent set in the intersection graph of the shapes. Therefore, the MDS problem is a special case of the maximum independent set (MIS) problem. Both problems are NP complete, but finding a MDS may be easier than finding a MIS in two respects:
For the general MIS problem, the best known exact algorithms are exponential. In some geometric intersection graphs, there are sub-exponential algorithms for finding a MDS.
The general MIS problem is hard to approximate and doesn't even have a constant-factor approximation. In some geometric intersection graphs, there are polynomial-time approximation schemes (PTAS) for finding a MDS.
The MDS problem can be generalized by assigning a different weight to each shape and searching for a disjoint set with a maximum total weight.
In the following text, MDS(C) denotes the maximum disjoint set in a set C.


== Greedy algorithms ==
Given a set C of shapes, an approximation to MDS(C) can be found by the following greedy algorithm:
INITIALIZATION: Initialize an empty set, S.
SEARCH: For every shape x in C:
Calculate N(x) - the subset of all shapes in C that intersect x (including x itself).
Calculate the largest independent set in this subset: MDS(N(x)).
Select an x such that |MDS(N(x))| is minimized.

Add x to S.
Remove x and N(x) from C.
If there are shapes in C, go back to Search.
END: return the set S.
For every shape x that we add to S, we lose the shapes in N(x), because they are intersected by x and thus cannot be added to S later on. However, some of these shapes themselves intersect each other, and thus in any case it is not possible that they all be in the optimal solution MDS(S). The largest subset of shapes that can all be in the optimal solution is MDS(N(x)). Therefore, selecting an x that minimizes |MDS(N(x))| minimizes the loss from adding x to S.
In particular, if we can guarantee that there is an x for which |MDS(N(x))| is bounded by a constant (say, M), then this greedy algorithm yields a constant M-factor approximation, as we can guarantee that:

  
    
      
        
          |
        
        S
        
          |
        
        ≥
        
          
            
              
                |
              
              M
              D
              S
              (
              C
              )
              
                |
              
            
            M
          
        
      
    
    {\displaystyle |S|\geq {\frac {|MDS(C)|}{M}}}
  
Such an upper bound M exists for several interesting cases:


=== 1-dimensional intervals: exact polynomial algorithm ===

When C is a set of intervals on a line, M=1, and thus the greedy algorithm finds the exact MDS. To see this, assume w.l.o.g. that the intervals are vertical, and let x be the interval with the highest bottom endpoint. All other intervals intersected by x must cross its bottom endpoint. Therefore, all intervals in N(x) intersect each other, and MDS(N(x)) has a size of at most 1 (see figure).
Therefore, in the 1-dimensional case, the MDS can be found exactly in time O(n log n):
Sort the intervals in ascending order of their bottom endpoints (this takes time O(n log n)).
Add an interval with the highest bottom endpoint, and delete all intervals intersecting it.
Continue until no intervals remain.
This algorithm is analogous to the earliest deadline first scheduling solution to the interval scheduling problem.
In contrast to the 1-dimensional case, in 2 or more dimensions the MDS problem becomes NP-complete, and thus has either exact super-polynomial algorithms or approximate polynomial algorithms.


=== Fat shapes: constant-factor approximations ===

When C is a set of unit disks, M=3, because the leftmost disk (the disk whose center has the smallest x coordinate) intersects at most 3 other disjoint disks (see figure). Therefore the greedy algorithm yields a 3-approximation, i.e., it finds a disjoint set with a size of at least MDS(C)/3.
Similarly, when C is a set of axis-parallel unit squares, M=2.

When C is a set of arbitrary-size disks, M=5, because the disk with the smallest radius intersects at most 5 other disjoint disks (see figure).
Similarly, when C is a set of arbitrary-size axis-parallel squares, M=4.
Other constants can be calculated for other regular polygons.


== Divide-and-conquer algorithms ==
The most common approach to finding a MDS is divide-and-conquer. A typical algorithm in this approach looks like the following:
Divide the given set of shapes into two or more subsets, such that the shapes in each subset cannot overlap the shapes in other subsets because of geometric considerations.
Recursively find the MDS in each subset separately.
Return the union of the MDSs from all subsets.
The main challenge with this approach is to find a geometric way to divide the set into subsets. This may require to discard a small number of shapes that do not fit into any one of the subsets, as explained in the following subsections.


=== Axis-parallel rectangles: Logarithmic-factor approximation ===
Let C be a set of n axis-parallel rectangles in the plane. The following algorithm finds a disjoint set with a size of at least 
  
    
      
        
          
            
              
                |
              
              M
              D
              S
              (
              C
              )
              
                |
              
            
            
              log
              ⁡
              
                n
              
            
          
        
      
    
    {\displaystyle {\frac {|MDS(C)|}{\log {n}}}}
   in time 
  
    
      
        O
        (
        n
        log
        ⁡
        
          n
        
        )
      
    
    {\displaystyle O(n\log {n})}
  :
INITIALIZATION: sort the horizontal edges of the given rectangles by their y-coordinate, and the vertical edges by their x-coordinate (this step takes time O(n log n)).
STOP CONDITION: If there are at most n ≤ 2 shapes, compute the MDS directly and return.
RECURSIVE PART:
Let 
  
    
      
        
          x
          
            
              m
              e
              d
            
          
        
      
    
    {\displaystyle x_{\mathrm {med} }}
   be the median x-coordinate.
Partition the input rectangles into three groups according to their relation to the line 
  
    
      
        x
        =
        
          x
          
            
              m
              e
              d
            
          
        
      
    
    {\displaystyle x=x_{\mathrm {med} }}
  : those entirely to its left (
  
    
      
        
          R
          
            
              l
              e
              f
              t
            
          
        
      
    
    {\displaystyle R_{\mathrm {left} }}
  ), those entirely to its right (
  
    
      
        
          R
          
            
              r
              i
              g
              h
              t
            
          
        
      
    
    {\displaystyle R_{\mathrm {right} }}
  ), and those intersected by it (
  
    
      
        
          R
          
            
              i
              n
              t
            
          
        
      
    
    {\displaystyle R_{\mathrm {int} }}
  ). By construction, the cardinalities of 
  
    
      
        
          R
          
            
              l
              e
              f
              t
            
          
        
      
    
    {\displaystyle R_{\mathrm {left} }}
   and 
  
    
      
        
          R
          
            
              r
              i
              g
              h
              t
            
          
        
      
    
    {\displaystyle R_{\mathrm {right} }}
   are at most n/2.
Recursively compute an approximate MDS in 
  
    
      
        
          R
          
            
              l
              e
              f
              t
            
          
        
      
    
    {\displaystyle R_{\mathrm {left} }}
   (
  
    
      
        
          M
          
            
              l
              e
              f
              t
            
          
        
      
    
    {\displaystyle M_{\mathrm {left} }}
  ) and in 
  
    
      
        
          R
          
            
              r
              i
              g
              h
              t
            
          
        
      
    
    {\displaystyle R_{\mathrm {right} }}
   (
  
    
      
        
          M
          
            
              r
              i
              g
              h
              t
            
          
        
      
    
    {\displaystyle M_{\mathrm {right} }}
  ), and calculate their union. By construction, the rectangles in 
  
    
      
        
          M
          
            
              l
              e
              f
              t
            
          
        
      
    
    {\displaystyle M_{\mathrm {left} }}
   and 
  
    
      
        
          M
          
            
              r
              i
              g
              h
              t
            
          
        
      
    
    {\displaystyle M_{\mathrm {right} }}
   are all disjoint, so 
  
    
      
        
          M
          
            
              l
              e
              f
              t
            
          
        
        ∪
        
          M
          
            
              r
              i
              g
              h
              t
            
          
        
      
    
    {\displaystyle M_{\mathrm {left} }\cup M_{\mathrm {right} }}
   is a disjoint set.
Compute an exact MDS in 
  
    
      
        
          R
          
            
              i
              n
              t
            
          
        
      
    
    {\displaystyle R_{\mathrm {int} }}
   (
  
    
      
        
          M
          
            
              i
              n
              t
            
          
        
      
    
    {\displaystyle M_{\mathrm {int} }}
  ). Since all rectangles in 
  
    
      
        
          R
          
            
              i
              n
              t
            
          
        
      
    
    {\displaystyle R_{\mathrm {int} }}
   intersect a single vertical line 
  
    
      
        x
        =
        
          x
          
            
              m
              e
              d
            
          
        
      
    
    {\displaystyle x=x_{\mathrm {med} }}
  , this computation is equivalent to finding an MDS from a set of intervals, and can be solved exactly in time O(n log n) (see above).

Return either 
  
    
      
        
          M
          
            
              l
              e
              f
              t
            
          
        
        ∪
        
          M
          
            
              r
              i
              g
              h
              t
            
          
        
      
    
    {\displaystyle M_{\mathrm {left} }\cup M_{\mathrm {right} }}
   or 
  
    
      
        
          M
          
            
              i
              n
              t
            
          
        
      
    
    {\displaystyle M_{\mathrm {int} }}
   – whichever of them is larger.
It is provable by induction that, at the last step, either 
  
    
      
        
          M
          
            
              l
              e
              f
              t
            
          
        
        ∪
        
          M
          
            
              r
              i
              g
              h
              t
            
          
        
      
    
    {\displaystyle M_{\mathrm {left} }\cup M_{\mathrm {right} }}
   or 
  
    
      
        
          M
          
            
              i
              n
              t
            
          
        
      
    
    {\displaystyle M_{\mathrm {int} }}
   have a cardinality of at least 
  
    
      
        
          
            
              
                |
              
              M
              D
              S
              (
              C
              )
              
                |
              
            
            
              log
              ⁡
              
                n
              
            
          
        
      
    
    {\displaystyle {\frac {|MDS(C)|}{\log {n}}}}
  .
The approximation factor has been reduced to 
  
    
      
        O
        (
        log
        ⁡
        
          log
          ⁡
          
            n
          
        
        )
      
    
    {\displaystyle O(\log {\log {n}})}
   and generalized to the case in which rectangles have different weights.


=== Axis-parallel rectangles with the same height: 2-approximation ===
Let C be a set of n axis-parallel rectangles in the plane, all with the same height H but with varying lengths. The following algorithm finds a disjoint set with a size of at least |MDS(C)|/2 in time O(n log n):
Draw m horizontal lines, such that:
The separation between two lines is strictly more than H.
Each line intersects at least one rectangle (hence m ≤ n).
Each rectangle is intersected by exactly one line.

Since the height of all rectangles is H, it is not possible that a rectangle is intersected by more than one line. Therefore the lines partition the set of rectangles into m subsets (
  
    
      
        
          R
          
            i
          
        
        ,
        …
        ,
        
          R
          
            m
          
        
      
    
    {\displaystyle R_{i},\ldots ,R_{m}}
  ) – each subset includes the rectangles intersected by a single line.
For each subset 
  
    
      
        
          R
          
            i
          
        
      
    
    {\displaystyle R_{i}}
  , compute an exact MDS 
  
    
      
        
          M
          
            i
          
        
      
    
    {\displaystyle M_{i}}
   using the one-dimensional greedy algorithm (see above).
By construction, the rectangles in (
  
    
      
        
          R
          
            i
          
        
      
    
    {\displaystyle R_{i}}
  ) can intersect only rectangles in 
  
    
      
        
          R
          
            i
            +
            1
          
        
      
    
    {\displaystyle R_{i+1}}
   or in 
  
    
      
        
          R
          
            i
            −
            1
          
        
      
    
    {\displaystyle R_{i-1}}
  . Therefore, each of the following two unions is a disjoint sets:
Union of odd MDSs: 
  
    
      
        
          M
          
            1
          
        
        ∪
        
          M
          
            3
          
        
        ∪
        ⋯
      
    
    {\displaystyle M_{1}\cup M_{3}\cup \cdots }
  
Union of even MDSs: 
  
    
      
        
          M
          
            2
          
        
        ∪
        
          M
          
            4
          
        
        ∪
        ⋯
      
    
    {\displaystyle M_{2}\cup M_{4}\cup \cdots }
  

Return the largest of these two unions. Its size must be at least |MDS|/2.


=== Axis-parallel rectangles with the same height: PTAS ===
Let C be a set of n axis-parallel rectangles in the plane, all with the same height but with varying lengths. There is an algorithm that finds a disjoint set with a size of at least |MDS(C)|/(1 + 1/k) in time O(n2k−1), for every constant k > 1.
The algorithm is an improvement of the above-mentioned 2-approximation, by combining dynamic programming with the shifting technique of.
This algorithm can be generalized to d dimensions. If the labels have the same size in all dimensions except one, it is possible to find a similar approximation by applying dynamic programming along one of the dimensions. This also reduces the time to n^O(1/e).


=== Fat objects with identical sizes: PTAS ===
Let C be a set of n squares or circles of identical size. There is a polynomial-time approximation scheme for finding an MDS using a simple shifted-grid strategy. It finds a solution within (1 − e) of the maximum in time nO(1/e2) time and linear space. The strategy generalizes to any collection of fat objects of roughly the same size (i.e., when the maximum-to-minimum size ratio is bounded by a constant).


=== Fat objects with arbitrary sizes: PTAS ===
Let C be a set of n fat objects (e.g. squares or circles) of arbitrary sizes. There is a PTAS for finding an MDS based on multi-level grid alignment. It has been discovered by two groups in approximately the same time, and described in two different ways.
Version 1 finds a disjoint set with a size of at least (1 − 1/k)2 · |MDS(C)| in time nO(k2), for every constant k > 1:
Scale the disks so that the smallest disk has diameter 1. Partition the disks to levels, based on the logarithm of their size. I.e., the j-th level contains all disks with diameter between (k + 1)j and (k + 1)j+1, for j ≤ 0 (the smallest disk is in level 0).
For each level j, impose a grid on the plane that consists of lines that are (k + 1)j+1 apart from each other. By construction, every disk can intersect at most one horizontal line and one vertical line from its level.
For every r, s between 0 and k, define D(r,s) as the subset of disks that are not intersected by any horizontal line whose index modulo k is r, nor by any vertical line whose index modulu k is s. By the pigeonhole principle, there is at least one pair (r,s) such that 
  
    
      
        
          |
        
        
          M
          D
          S
        
        (
        D
        (
        r
        ,
        s
        )
        )
        
          |
        
        ≥
        (
        1
        −
        
          
            1
            k
          
        
        
          )
          
            2
          
        
        ⋅
        
          |
        
        
          M
          D
          S
        
        
          |
        
      
    
    {\displaystyle |\mathrm {MDS} (D(r,s))|\geq (1-{\frac {1}{k}})^{2}\cdot |\mathrm {MDS} |}
  , i.e., we can find the MDS only in D(r,s) and miss only a small fraction of the disks in the optimal solution:
For all k2 possible values of r,s (0 ≤ r,s < k), calculate D(r,s) using dynamic programming.
Return the largest of these k2 sets.

Version 2 finds a disjoint set with a size of at least (1 − 2/k)·|MDS(C)| in time nO(k), for every constant k > 1.
The algorithm uses shifted quadtrees. The key concept of the algorithm is alignment to the quadtree grid. An object of size r is called k-aligned (where k ≥ 1 is a constant) if it is inside a quadtree cell of size at most kr (R ≤ kr).
By definition, a k-aligned object that intersects the boundary of a quatree cell of size R must have a size of at least R/k (r > R/k). The boundary of a cell of size R can be covered by 4k squares of size R/k; hence the number of disjoint fat objects intersecting the boundary of that cell is at most 4kc, where c is a constant measuring the fatness of the objects.
Therefore, if all objects are fat and k-aligned, it is possible to find the exact maximum disjoint set in time nO(kc) using a divide-and-conquer algorithm. Start with a quadtree cell that contains all objects. Then recursively divide it to smaller quadtree cells, find the maximum in each smaller cell, and combine the results to get the maximum in the larger cell. Since the number of disjoint fat objects intersecting the boundary of every quadtree cell is bounded by 4kc, we can simply ""guess"" which objects intersect the boundary in the optimal solution, and then apply divide-and-conquer to the objects inside.
If almost all objects are k-aligned, we can just discard the objects that are not k-aligned, and find a maximum disjoint set of the remaining objects in time nO(k). This results in a (1 − e) approximation, where e is the fraction of objects that are not k-aligned.
If most objects are not k-aligned, we can try to make them k-aligned by shifting the grid in multiples of (1/k,1/k). First, scale the objects such that they are all contained in the unit square. Then, consider k shifts of the grid: (0,0), (1/k,1/k), (2/k,2/k), ..., ((k − 1)/k,(k − 1)/k). I.e., for each j in {0,...,k − 1}, consider a shift of the grid in (j/k,j/k). It is possible to prove that every label will be 2k-aligned for at least k − 2 values of j. Now, for every j, discard the objects that are not k-aligned in the (j/k,j/k) shift, and find a maximum disjoint set of the remaining objects. Call that set A(j). Call the real maximum disjoint set is A*. Then:

  
    
      
        
          ∑
          
            j
            =
            0
            ,
            …
            ,
            k
            −
            1
          
        
        
          
            |
          
          A
          (
          j
          )
          
            |
          
        
        ≥
        (
        k
        −
        2
        )
        
          |
        
        A
        ∗
        
          |
        
      
    
    {\displaystyle \sum _{j=0,\ldots ,k-1}{|A(j)|}\geq (k-2)|A*|}
  
Therefore, the largest A(j) has a size of at least: (1 − 2/k)|A*|. The return value of the algorithm is the largest A(j); the approximation factor is (1 − 2/k), and the run time is nO(k). We can make the approximation factor as small as we want, so this is a PTAS.
Both versions can be generalized to d dimensions (with different approximation ratios) and to the weighted case.


== Geometric separator algorithms ==
Several divide-and-conquer algorithms are based on a certain geometric separator theorem. A geometric separator is a line or shape that separates a given set of shapes to two smaller subsets, such that the number of shapes lost during the division is relatively small. This allows both PTASs and sub-exponential exact algorithms, as explained below.


=== Fat objects with arbitrary sizes: PTAS using geometric separators ===
Let C be a set of n fat objects of arbitrary sizes. The following algorithm finds a disjoint set with a size of at least (1 − O(√b))·|MDS(C)| in time nO(b), for every constant b > 1.
The algorithm is based on the following geometric separator theorem, which can be proved similarly to the proof of the existence of geometric separator for disjoint squares:

For every set C of fat objects, there is a rectangle that partitions C into three subsets of objects – Cinside, Coutside and Cboundary, such that:
|MDS(Cinside)| ≤ a|MDS(C)|
|MDS(Coutside)| ≤ a|MDS(C)|
|MDS(Cboundary)| c√|MDS(C)|

where a and c are constants. If we could calculate MDS(C) exactly, we could make the constant a as low as 2/3 by a proper selection of the separator rectangle. But since we can only approximate MDS(C) by a constant factor, the constant a must be larger. Fortunately, a remains a constant independent of |C|.
This separator theorem allows to build the following PTAS:
Select a constant b. Check all possible combinations of up to b + 1 labels.
If |MDS(C)| has a size of at most b (i.e. all sets of b + 1 labels are not disjoint) then just return that MDS and exit. This step takes nO(b) time.
Otherwise, use a geometric separator to separate C to two subsets. Find the approximate MDS in Cinside and Coutside separately, and return their combination as the approximate MDS in C.
Let E(m) be the error of the above algorithm when the optimal MDS size is MDS(C) = m. When m ≤ b, the error is 0 because the maximum disjoint set is calculated exactly; when m > b, the error increases by at most c√m the number of labels intersected by the separator. The worst case for the algorithm is when the split in each step is in the maximum possible ratio which is a:(1 − a). Therefore the error function satisfies the following recurrence relation:

  
    
      
        E
        (
        m
        )
        =
        0
         
         
         
         
        
           if 
        
        m
        ≤
        b
      
    
    {\displaystyle E(m)=0\ \ \ \ {\text{ if }}m\leq b}
  

  
    
      
        E
        (
        m
        )
        =
        E
        (
        a
        ⋅
        m
        )
        +
        E
        (
        (
        1
        −
        a
        )
        ⋅
        m
        )
        +
        c
        ⋅
        
          
            m
          
        
        
           if 
        
        m
        >
        b
      
    
    {\displaystyle E(m)=E(a\cdot m)+E((1-a)\cdot m)+c\cdot {\sqrt {m}}{\text{ if }}m>b}
  
The solution to this recurrence is:

  
    
      
        E
        (
        m
        )
        =
        (
        
          
            0
            b
          
        
        +
        
          
            c
            
              
                
                  b
                
              
              (
              
                
                  a
                
              
              +
              
                
                  1
                  −
                  a
                
              
              −
              1
              )
            
          
        
        )
        ⋅
        m
        −
        
          
            c
            
              
                
                  a
                
              
              +
              
                
                  1
                  −
                  a
                
              
              −
              1
            
          
        
        ⋅
        
          
            m
          
        
        .
      
    
    {\displaystyle E(m)=({\frac {0}{b}}+{\frac {c}{{\sqrt {b}}({\sqrt {a}}+{\sqrt {1-a}}-1)}})\cdot m-{\frac {c}{{\sqrt {a}}+{\sqrt {1-a}}-1}}\cdot {\sqrt {m}}.}
  
i.e., 
  
    
      
        E
        (
        m
        )
        =
        O
        (
        m
        
          /
        
        
          
            b
          
        
        )
      
    
    {\displaystyle E(m)=O(m/{\sqrt {b}})}
  . We can make the approximation factor as small as we want by a proper selection of b.
This PTAS is more space-efficient than the PTAS based on quadtrees, and can handle a generalization where the objects may slide, but it cannot handle the weighted case.


=== Disks with a bounded size-ratio: exact sub-exponential algorithm ===
Let C be a set of n disks, such that the ratio between the largest radius and the smallest radius is at most r. The following algorithm finds MDS(C) exactly in time 
  
    
      
        
          2
          
            O
            (
            r
            ⋅
            
              
                n
              
            
            )
          
        
      
    
    {\displaystyle 2^{O(r\cdot {\sqrt {n}})}}
  .
The algorithm is based on a width-bounded geometric separator on the set Q of the centers of all disks in C. This separator theorem allows to build the following exact algorithm:
Find a separator line such that at most 2n/3 centers are to its right (Cright), at most 2n/3 centers are to its left (Cleft), and at most O(√n) centers are at a distance of less than r/2 from the line (Cint).
Consider all possible non-overlapping subsets of Cint. There are at most 
  
    
      
        
          2
          
            O
            (
            r
            ⋅
            
              
                n
              
            
            )
          
        
      
    
    {\displaystyle 2^{O(r\cdot {\sqrt {n}})}}
   such subsets. For each such subset, recursively compute the MDS of Cleft and the MDS of Cright, and return the largest combined set.
The run time of this algorithm satisfies the following recurrence relation:

  
    
      
        T
        (
        1
        )
        =
        1
      
    
    {\displaystyle T(1)=1}
  

  
    
      
        T
        (
        n
        )
        =
        
          2
          
            O
            (
            r
            ⋅
            
              
                n
              
            
            )
          
        
        T
        
          (
          
            
              
                2
                n
              
              3
            
          
          )
        
        
           if 
        
        n
        >
        1
      
    
    {\displaystyle T(n)=2^{O(r\cdot {\sqrt {n}})}T\left({\frac {2n}{3}}\right){\text{ if }}n>1}
  
The solution to this recurrence is:

  
    
      
        T
        (
        n
        )
        =
        
          2
          
            O
            (
            r
            ⋅
            
              
                n
              
            
            )
          
        
      
    
    {\displaystyle T(n)=2^{O(r\cdot {\sqrt {n}})}}
  


== Local search algorithms ==


=== Pseudo-disks: a PTAS ===
A pseudo-disks-set is a set of objects in which the boundaries of every pair of objects intersect at most twice. (Note that this definition relates to a whole collection, and does not say anything about the shapes of the specific objects in the collection). A pseudo-disks-set has a bounded union complexity, i.e., the number of intersection points on the boundary of the union of all objects is linear in the number of objects.
Let C be a pseudo-disks-set with n objects. The following local search algorithm finds a disjoint set of size at least 
  
    
      
        (
        1
        −
        O
        (
        
          
            1
            
              b
            
          
        
        )
        )
        ⋅
        
          |
        
        M
        D
        S
        (
        C
        )
        
          |
        
      
    
    {\displaystyle (1-O({\frac {1}{\sqrt {b}}}))\cdot |MDS(C)|}
   in time 
  
    
      
        O
        (
        
          n
          
            b
            +
            3
          
        
        )
      
    
    {\displaystyle O(n^{b+3})}
  , for every integer constant 
  
    
      
        b
        ≥
        0
      
    
    {\displaystyle b\geq 0}
  :
INITIALIZATION: Initialize an empty set, 
  
    
      
        S
      
    
    {\displaystyle S}
  .
SEARCH: Loop over all the subsets of 
  
    
      
        C
        −
        S
      
    
    {\displaystyle C-S}
   whose size is between 1 and 
  
    
      
        b
        +
        1
      
    
    {\displaystyle b+1}
  . For each such subset X:
Verify that X itself is independent (otherwise go to the next subset);
Calculate the set Y of objects in S that intersect X.
If 
  
    
      
        
          |
        
        Y
        
          |
        
        <
        
          |
        
        X
        
          |
        
      
    
    {\displaystyle |Y|<|X|}
  , then remove Y from S and insert X: 
  
    
      
        S
        :=
        S
        −
        Y
        +
        X
      
    
    {\displaystyle S:=S-Y+X}
  .

END: return the set S.
Every exchange in the search step increases the size of S by at least 1, and thus can happen at most n times.
The algorithm is very simple; the difficult part is to prove the approximation ratio.
See also.


== Linear programming relaxation algorithms ==


=== Pseudo-disks: a PTAS ===
Let C be a pseudo-disks-set with n objects and union complexity u. Using linear programming relaxation, it is possible to find a disjoint set of size at least 
  
    
      
        
          
            n
            u
          
        
        ⋅
        
          |
        
        M
        D
        S
        (
        C
        )
        
          |
        
      
    
    {\displaystyle {\frac {n}{u}}\cdot |MDS(C)|}
  . This is possible either with a randomized algorithm that has a high probability of success and run time 
  
    
      
        O
        (
        
          n
          
            3
          
        
        )
      
    
    {\displaystyle O(n^{3})}
  , or a deterministic algorithm with a slower run time (but still polynomial). This algorithm can be generalized to the weighted case.


== Other classes of shapes for which approximations are known ==
Line segments in the two-dimensional plane.
Arbitrary two-dimensional convex objects.
Curves with a bounded number of intersection points.


== External links ==
Approximation Algorithms for Maximum Independent Set of Pseudo-Disks - presentation by Sariel Har-Peled.
Javascript code for calculating exact maximum disjoint set of rectangles.


== Notes =="
36,Secondary School Mathematics Curriculum Improvement Study,40556829,27001,"The Secondary School Mathematics Curriculum Improvement Study (SSMCIS) was the name of an American mathematics education program that stood for both the name of a curriculum and the name of the project that was responsible for developing curriculum materials. It is considered part of the second round of initiatives in the ""New Math"" movement of the 1960s. The program was led by Howard F. Fehr, a professor at Columbia University Teachers College.
The program's signature goal was to create a unified treatment of mathematics and eliminate the traditional separate per-year studies of algebra, geometry, trigonometry, and so forth, that was typical of American secondary schools. Instead, the treatment unified those branches by studying fundamental concepts such as sets, relations, operations, and mappings, and fundamental structures such as groups, rings, fields, and vector spaces. The SSMCIS program produced six courses' worth of class material, intended for grades 7 through 12, in textbooks called Unified Modern Mathematics. Some 25,000 students took SSMCIS courses nationwide during the late 1960s and early 1970s.


== Background ==
The program was led by Howard F. Fehr, a professor at Columbia University Teachers College who was internationally known and had published numerous mathematics textbooks and hundreds of articles about mathematics teaching. In 1961 he had been the principal author of the 246-page report ""New Thinking in School Mathematics"", which held that traditional teaching of mathematics approaches did not meet the needs of the new technical society being entered into or of the current language of mathematicians and scientists. Fehr considered the separation of mathematical study into separate years of distinct subjects to be an American failing that followed an educational model two hundred years old.
The new curriculum was inspired by the seminar reports from the Organisation for Economic Co-operation and Development in the early 1960s and by the Cambridge Conference on School Mathematics (1963), which also inspired the Comprehensive School Mathematics Program. There were some interactions among these initiatives in the early stages, and the development of SSMCIS was part of a general wave of cooperation in the mathematics education reform movement between Europe and the U.S.


== Curriculum ==

Work on the SSMCIS program began in 1965 and took place mainly at Teachers College. Fehr was the director of the project from 1965 to 1973. The principal consultants in the initial stages and subsequent yearly planning sessions were Marshall H. Stone of the University of Chicago, Albert W. Tucker of Princeton University, Edgar Lorch of Columbia University, and Meyer Jordan of Brooklyn College. The program was targeted at the junior high and high school level and the 15–20 percent best students in a grade.
Funding for the initiative began with the U.S. Office of Education and covered the development first three courses produced; the last three courses produced, as well as teacher training, were funded by the National Science Foundation and by Teachers College itself. The scope and sequence of the curriculum was developed by eighteen mathematicians from the U.S. and Europe in 1966 and subsequently refined in experimental course material by mathematical educators with high school level teaching experience. By 1971, some thirty-eight contributors to course materials were identified, eight from Teachers College, four from Europe, one from Canada, and the rest from various other universities (and a couple of high schools) in the United States. Fehr did not do much curriculum development himself, but rather recruited and led the others and organized the whole process. Graduate students from the Department of Mathematical Education at Teachers College also served each year in various capacities on the SSMCIS program.
The central idea of the program was to organize mathematics not by algebra, geometry, etc., but rather to unify those branches by studying the fundamental concepts of sets, relations, operations, and mappings, and fundamental structures such as groups, rings, fields, and vector spaces. Other terms used for this approach included ""global"" or ""integrated""; Fehr himself spoke of a ""spiral"" or ""helical"" development, and wrote of ""the spirit of global organization that is at the heart of the SSMCIS curriculum – important mathematical systems unified by a core of fundamental concepts and structures common to all.
For example, as the courses progressed, the concept of mappings were used to describe, and visually illustrate, the traditionally disparate topics of translation, line reflection, probability of an event, trigonometric functions, isomorphism and complex numbers, and analysis and linear mappings. Traditional subjects were broken up, such that the course material for each year included some material related to algebra, some to geometry, and so forth.
Even when abstract concepts were being introduced, they were introduced in concrete, intuitive forms, especially at the younger levels. Logical proofs were introduced early on and built in importance as the years developed. At least one year of university-level mathematics education was incorporated into the later courses. Solving traditional applications problems was de-emphasized, especially in the earlier courses, but the intent of the project was to make up for that with its focus on real numbers in measurements, computer programming, and probability and statistics. In particular, the last of these was a pronounced element of SSMCIS, with substantial material on it present in all six courses, from measures of statistical dispersion to combinatorics to Bayes' theorem and more.
The curriculum that SSMCIS devised had influences from earlier reform work in Europe, going back to the Bourbaki group's work in France in the 1930s and the Synopses for Modern Secondary School Mathematics published in Paris in 1961. Indeed, most European secondary schools were teaching a more integrated approach. Also, this was one of several American efforts to address a particularly controversial issue, the teaching of a full year of Euclidean geometry in secondary school. Like many of the others, it did this by teaching geometric transformations as a unifying approach between algebra and geometry.
Regardless of all these influences and other projects, the SSMCIS study group considered its work unique in scope and breadth, and Fehr wrote that ""nowhere [else] had a total 7–12 unified mathematics program been designed, produced, and tested."" It was thus considered one of the more radical of the reform efforts lumped under the ""New Math"" label. Moreover, Fehr believed that the SSMCIS could not just improve students' thinking in mathematics, but in all subjects, by ""develop[ing] the capacity of the human mind for the observation, selection, generalization, abstraction, and construction of models for use in the other disciplines.""


== Materials ==

The course books put out by SSMCIS were titled Unified Modern Mathematics, and labeled as Course I through Course VI, with the two volumes in each year labeled as Part I and Part II. Materials for the next year's course were prepared each year, thus keeping up with the early adoption programs underway. Using largely formative evaluation methods for gaining teacher feedback, revised versions were put out after the first year's teaching experience. By 1973, the revised version of all six courses had been completed. The first three volumes were put into the public domain for any organization to use.
The pages of the books were formatted by typewriter, augmented by some mathematical symbols and inserted graphs, bound in paper, and published by Teachers College itself. A more polished hardcover version of Courses I through IV was put out in subsequent years by Addison-Wesley; these were adaptations made by Fehr and others and targeted to students with a broader range of mathematical ability.
Computer programming on time-shared computer systems was included in the curriculum both for its own importance and for understanding numerical methods. The first course introduced flow charts and the notion of algorithms. The beginning portion of the fourth-year course was devoted to introducing the BASIC programming language, with an emphasis on fundamental control flow statements, continued use of flow charts for design, and numerical programming applications. Interactive teletype interfaces on slow and erratic dial-up connections, with troublesome paper tape for offline storage, was the typical physical environment.


== Adoption ==
Starting in 1966, teachers from nine junior high and high schools, mostly in the New York metropolitan area, began getting training in the study program at Teachers College. Such training was crucial since few junior high or high school teachers knew all the material that would be introduced. They then returned to their schools and began teaching the experimental courses, two teachers per grade. For instance, Leonia High School, which incorporated grades 8–12 (since there was no middle school then), called the program ""Math X"" for experimental, with individual courses called Math 8X, Math 9X, etc. Hunter College High School used it as the basis for its Extended Honors Program; the school's description stated that the program ""includes many advanced topics and requires extensive preparation and a considerable commitment of time to the study of mathematics."" Students were periodically given standardized tests to make sure there was no decline in performance due to the unusual organization of material. Some 400 students were involved in this initial phase.
Because the program was so different from standard U.S. mathematics curricula, it was quite difficult to students to enter after the first year; students did, however, sometimes drop out of it and return to standard courses. As teaching the program was a specialized activity, teachers tended to move along from each grade to the next with their students, and so it was typical for students to have one of the same two teachers, or even the same teacher, for five or six years in a row.
More teachers were added in 1968 and 1969 and the University of Maryland and University of Arizona were added as teaching sites. Eighteen schools in Los Angeles adopted SSMCIS in what was called the Accelerated Mathematics Instruction program; some 2,500 gifted students took part. By 1971, teacher education programs were being conducted in places like Austin Peay State University in Tennessee, which was attended by junior high school teachers from seventeen states and one foreign country. By 1974, Fehr stated that 25,000 students were taking SSMCIS courses across the U.S.


== Results ==

The Secondary School Mathematics Curriculum Improvement Study program did show some success in its educational purpose. A study of the Los Angeles program found that SSMCIS-taught students had a better attitude toward their program than did students using School Mathematics Study Group courses (another ""New Math"" initiative) or traditional courses. In New York State schools, special examinations were given to tenth and eleventh grade SSMCIS students in lieu of the standard Regents Examinations due to a mismatch in curriculum. However, SSMCIS was one of the direct inspirations for the New York State Education Department, in the late 1970s and 1980s, adopting an integrated, three-year mathematics curriculum for all its students, combining algebra, geometry, and trigonometry with an increased emphasis in probability and statistics.
Given the differences in subject matter and approach, how SSMCIS-taught students would perform on College Entrance Examination Board tests became a major concern of parents and students and teachers. A 1973 report compared the test performance of such students with those from traditional mathematics curricula. It found that the SSMCIS students did better on the mathematics portion of the Preliminary Scholastic Aptitude Test (PSAT), even when matched for background and performance on the verbal portion. It also found that SSMCIS students did just as well on the Mathematics Level II Achievement Test as traditional students taking college preparatory courses or, indeed, as college freshmen taking introductory calculus courses. Another study found SSMCIS students well prepared for the mathematics portion of the regular Scholastic Aptitude Test.
However, SSMCIS developed slowly. Funding became an issue, and indeed it was never funded as well as some other mathematics curriculum efforts had been. Despite the federal funding source, there was no centralized, national focal point in the U.S. for curriculum changes – such as some European countries had – and that made adoption of SSMCIS innovations a harder task. By the mid-1970s there was a growing backlash against the ""New Math"" movement, spurred in part by a perceived decline in standardized test scores and by Morris Kline's critical book Why Johnny Can't Add: The Failure of the New Math. Many reform efforts had underestimated the difficulty of getting the public and the mathematics educational community to believe that major changes were really necessary, especially for secondary school programs where college entrance performance was always the key concern of administrators. Federal funding for curriculum development also came under attack from American conservatives in the U.S. Congress. As one of the participants in creating SSMCIS, James T. Fey of Teachers College, later wrote, ""Schools and societal expectations of schools appear to change very slowly."" In the end, SSMCIS never became widely adopted.


== Legacy ==
One SSMCIS student, Toomas Hendrik Ilves of Leonia High School, decades later became Foreign Minister and then President of Estonia. He credited the SSMCIS course, the early exposure it gave him to computer programming, and the teacher of the course, Christine Cummings, with his subsequent interest in computer infrastructure, which in part resulted in the country leaping over its Soviet-era technological backwardness; computer-accessible education became pervasive in Estonian schools, and the Internet in Estonia has one of the highest penetration rates in the world. As his tenure as president came to a close in 2016, Ilves visited his old school building with Cummings and said, ""I owe everything to her. Because of what she taught us, my country now uses it."" Cummings said that SSMCIS not only introduced beginning computer programming but also taught students ""how to think"".
SSMCIS did represent a productive exercise in thinking about mathematics curriculum, and the mathematics education literature would cite it in subsequent years, including references to it as a distinct, and the most radical, approach to teaching geometry; as using functions as a unifying element of teaching mathematics; and as its course materials having value when used as the vehicle for further research in mathematics education.


== References =="
37,Conformal geometric algebra,34629138,25636,"Conformal geometric algebra (CGA) is the geometric algebra constructed over the resultant space of a projective map from an n-dimensional base space ℝp,q into ℝp+1,q+1. This allows operations on the base space, including reflections, rotations and translations to be represented using versors of the geometric algebra; and it is found that points, lines, planes, circles and spheres gain particularly natural and computationally amenable representations.
The effect of the mapping is that generalized (i.e. including zero curvature) k-spheres in the base space map onto (k + 2)-blades, and so that the effect of a translation (or any conformal mapping) of the base space corresponds to a rotation in the higher-dimensional space. In the algebra of this space, based on the geometric product of vectors, such transformations correspond to the algebra's characteristic sandwich operations, similar to the use of quaternions for spatial rotation in 3D, which combine very efficiently. A consequence of rotors representing transformations is that the representations of spheres, planes, circles and other geometrical objects, and equations connecting them, all transform covariantly. A geometric object (a k-sphere) can be synthesized as the wedge product of k + 2 linearly independent vectors representing points on the object; conversely, the object can be decomposed as the repeated wedge product of vectors representing k + 2 distinct points in its surface. Some intersection operations also acquire a tidy algebraic form: for example, for the Euclidean base space ℝ3, applying the wedge product to the dual of the tetravectors representing two spheres produces the dual of the trivector representation of their circle of intersection.
As this algebraic structure lends itself directly to effective computation, it facilitates exploration of the classical methods of projective geometry and inversive geometry in a concrete, easy-to-manipulate setting. It has also been used as an efficient structure to represent and facilitate calculations in screw theory. CGA has particularly been applied in connection with the projective mapping of the everyday Euclidean space ℝ3 into a five-dimensional vector space ℝ4,1, which has been investigated for applications in robotics and computer vision. It can be applied generally to any pseudo-Euclidean space, and the mapping of Minkowski space ℝ3,1 to the space ℝ4,2 is being investigated for applications to relativistic physics.


== Construction of CGA ==


=== Notation and terminology ===
In this article, the focus is on the algebra 
  
    
      
        
          
            G
          
        
        (
        4
        ,
        1
        )
      
    
    {\displaystyle {\mathcal {G}}(4,1)}
   as it is this particular algebra that has been the subject of most attention over time; other cases are briefly covered in a separate section. The space containing the objects being modelled is referred to here as the base space, and the algebraic space used to model these objects as the representation or conformal space. A homogeneous subspace refers to a linear subspace of the algebraic space.
The terms for objects: point, line, circle, sphere, quasi-sphere etc. are used to mean either the geometric object in the base space, or the homogeneous subspace of the representation space that represents that object, with the latter generally being intended unless indicated otherwise. Algebraically, any nonzero null element of the homogeneous subspace will be used, with one element being referred to as normalized by some criterion.
Boldface lowercase Latin letters are used to represent position vectors from the origin to a point in the base space. Italic symbols are used for other elements of the representation space.


=== Base and representation spaces ===
The base space ℝ3 is represented by extending a basis for the displacements from a chosen origin and adding two basis vectors e− and e+ orthogonal to the base space and to each other, with e−2 = −1 and e+2 = +1, creating the representation space 
  
    
      
        
          
            G
          
        
        (
        4
        ,
        1
        )
      
    
    {\displaystyle {\mathcal {G}}(4,1)}
  .
It is convenient to use two null vectors no and n∞ as basis vectors in place of e+ and e−, where no = (e− − e+)/2, and n∞ = e− + e+. It can be verified, where x is in the base space, that:

  
    
      
        
          
            
              
                
                  
                    
                      n
                      
                        o
                      
                    
                  
                  
                    2
                  
                
              
              
                =
                0
                
                
                  n
                  
                    o
                  
                
                ⋅
                
                  n
                  
                    ∞
                  
                
              
              
                =
                −
                1
                
              
              
                
                  n
                  
                    o
                  
                
                ⋅
                
                  x
                
              
              
                =
                0
              
            
            
              
                
                  
                    
                      n
                      
                        ∞
                      
                    
                  
                  
                    2
                  
                
              
              
                =
                0
                
                
                  n
                  
                    o
                  
                
                ∧
                
                  n
                  
                    ∞
                  
                
              
              
                =
                
                  e
                  
                    −
                  
                
                
                  e
                  
                    +
                  
                
                
              
              
                
                  n
                  
                    ∞
                  
                
                ⋅
                
                  x
                
              
              
                =
                0
              
            
          
        
      
    
    {\displaystyle {\begin{array}{lllll}{n_{\text{o}}}^{2}&=0\qquad n_{\text{o}}\cdot n_{\infty }&=-1\qquad &n_{\text{o}}\cdot \mathbf {x} &=0\\{n_{\infty }}^{2}&=0\qquad n_{\text{o}}\wedge n_{\infty }&=e_{-}e_{+}\qquad &n_{\infty }\cdot \mathbf {x} &=0\end{array}}}
  
These properties lead to the following formulas for the basis vector coefficients of a general vector r in the representation space for a basis with elements ei orthogonal to every other basis element:
The coefficient of no for r is −n∞ ⋅ r
The coefficient of n∞ for r is −no ⋅ r
The coefficient of ei for r is ei−1 ⋅ r.


=== Mapping between the base space and the representation space ===
The mapping from a vector in the base space (being from the origin to a point in the affine space represented) is given by the formula:

  
    
      
        F
        :
        
          x
        
        ↦
        
          n
          
            o
          
        
        +
        
          x
        
        +
        
          
            
              1
              2
            
          
        
        
          
            x
          
          
            2
          
        
        
          n
          
            ∞
          
        
      
    
    {\displaystyle F:\mathbf {x} \mapsto n_{\text{o}}+\mathbf {x} +{\tfrac {1}{2}}\mathbf {x} ^{2}n_{\infty }}
  
Points and other objects that differ only by a nonzero scalar factor all map to the same object in the base space. When normalisation is desired, as for generating a simple reverse map of a point from the representation space to the base space or determining distances, the condition F(x) ⋅ n∞ = −1 may be used.

The forward mapping is equivalent to:
first conformally projecting x from e123 onto a unit 3-sphere in the space e+ ∧ e123 (in 5-D this is in the subspace r ⋅ (−no − 1/2n∞) = 0);
then lift this into a projective space, by adjoining e– = 1, and identifying all points on the same ray from the origin (in 5-D this is in the subspace r ⋅ (−no − 1/2n∞) = 1);
then change the normalisation, so the plane for the homogeneous projection is given by the no co-ordinate having a value 1, i.e. r ⋅ n∞ = −1.


=== Inverse mapping ===
An inverse mapping for X on the null cone is given (Perwass eqn 4.37) by

  
    
      
        X
        ↦
        
          
            
              P
            
          
          
            
              n
              
                ∞
              
            
            ∧
            
              n
              
                o
              
            
          
          
            ⊥
          
        
        
          (
          
            
              X
              
                −
                X
                ⋅
                
                  n
                  
                    ∞
                  
                
              
            
          
          )
        
      
    
    {\displaystyle X\mapsto {\mathcal {P}}_{n_{\infty }\wedge n_{\text{o}}}^{\perp }\left({\frac {X}{-X\cdot n_{\infty }}}\right)}
  
This first gives a stereographic projection from the light-cone onto the plane r ⋅ n∞ = −1, and then throws away the no and n∞ parts, so that the overall result is to map all of the equivalent points αX = α(no + x + 1/2x2n∞) to x.


=== Origin and point at infinity ===
The point x = 0 in ℝp,q maps to no in ℝp+1,q+1, so no is identified as the (representation) vector of the point at the origin.
A vector in ℝp+1,q+1 with a nonzero n∞ coefficient, but a zero no coefficient, must (considering the inverse map) be the image of an infinite vector in ℝp,q. The direction n∞ therefore represents the (conformal) point at infinity. This motivates the subscripts o and ∞ for identifying the null basis vectors.
The choice of the origin is arbitrary: any other point may be chosen, as the representation is of an affine space. The origin merely represents a reference point, and is algebraically equivalent to any other point. As with any translation, changing the origin corresponds to a rotation in the representation space.


== Geometrical objects ==


=== Basis ===
Together with 
  
    
      
        
          I
          
            5
          
        
        =
        
          e
          
            123
          
        
        E
      
    
    {\displaystyle I_{5}=e_{123}E}
   and 
  
    
      
        1
      
    
    {\displaystyle 1}
  , these are the 32 basis blades of the algebra. The Flat Point Origin is written as an outer product because the geometric product is of mixed grade.(
  
    
      
        E
        =
        
          e
          
            +
          
        
        
          e
          
            −
          
        
      
    
    {\displaystyle E=e_{+}e_{-}}
  ).


=== As the solution of a pair of equations ===
Given any nonzero blade A of the representing space, the set of vectors that are solutions to a pair of homogeneous equations of the form

  
    
      
        
          X
          
            2
          
        
        =
        0
      
    
    {\displaystyle X^{2}=0}
  

  
    
      
        X
        ∧
        A
        =
        0
      
    
    {\displaystyle X\wedge A=0}
  
is the union of homogeneous 1-d subspaces of null vectors, and is thus a representation of a set of points in the base space. This leads to the choice of a blade A as being a useful way to represent a particular class of geometric object. Specific cases for the blade A (independent of the number of dimensions of the space) when the base space is Euclidean space are:
a scalar: the empty set
a vector: a single point
a bivector: a pair of points
a trivector: a generalized circle
a 4-vector: a generalized sphere
etc.
These each may split into three cases according to whether A2 is positive, zero or negative, corresponding (in reversed order in some cases) to the object as listed, a degenerate case of a single point, or no points (where the nonzero solutions of X ∧ A exclude null vectors).
The listed geometric objects (generalized n-spheres) become quasi-spheres in the more general case of the base space being pseudo-Euclidean.
Flat objects may be identified by the point at infinity being included in the solutions. Thus, if n∞ ∧ A = 0, the object will be a line, plane, etc., for the blade A respectively being of grade 3, 4, etc.


=== As derived from points of the object ===
A blade A representing of one of this class of object may be found as the outer product of linearly independent vectors representing points on the object. In the base space, this linear independence manifests as each point lying outside the object defined by the other points. So, for example, a fourth point lying on the generalized circle defined by three distinct points cannot be used as a fourth point to define a sphere.


=== odds ===
Points in e123 map onto the null cone—the null parabola if we set r . n∞ = -1.
We can consider the locus of points in e123 s.t. in conformal space g(x) . A = 0, for various types of geometrical object A.
We start by observing that 
  
    
      
        g
        (
        
          a
        
        )
        .
        g
        (
        
          b
        
        )
        =
        −
        
          
            1
            2
          
        
        ‖
        
          a
        
        −
        
          b
        
        
          ‖
          
            2
          
        
      
    
    {\displaystyle g(\mathbf {a} ).g(\mathbf {b} )=-{\frac {1}{2}}\|\mathbf {a} -\mathbf {b} \|^{2}}
  
compare:
x. a = 0 => x perp a; x.(a∧b) = 0 => x perp a and x perp b
x∧a = 0 => x parallel to a; x∧(a∧b) = 0 => x parallel to a or to b (or to some linear combination)
the inner product and outer product representations are related by dualisation
x∧A = 0 <=> x . A* = 0 (check—works if x is 1-dim, A is n-1 dim)


==== g(x) . A = 0 ====

A point: the locus of x in R3 is a point if A in R4,1 is a vector on the null cone.

(N.B. that because it's a homogeneous projective space, vectors of any length on a ray through the origin are equivalent, so g(x).A =0 is equivalent to g(x).g(a) = 0).
*** warning: apparently wrong codimension—go to the sphere as the general case, then restrict to a sphere of size zero. Is the dual of the equation affected by being on the null cone?

A sphere: the locus of x is a sphere if A = S, a vector off the null cone.

If

  
    
      
        
          S
        
        =
        g
        (
        
          a
        
        )
        −
        
          
            1
            2
          
        
        
          ρ
          
            2
          
        
        
          
            e
          
          
            ∞
          
        
      
    
    {\displaystyle \mathbf {S} =g(\mathbf {a} )-{\frac {1}{2}}\rho ^{2}\mathbf {e} _{\infty }}
  

then S.X = 0 => 
  
    
      
        −
        
          
            1
            2
          
        
        (
        
          a
        
        −
        
          x
        
        
          )
          
            2
          
        
        +
        
          
            1
            2
          
        
        
          ρ
          
            2
          
        
        =
        0
      
    
    {\displaystyle -{\frac {1}{2}}(\mathbf {a} -\mathbf {x} )^{2}+{\frac {1}{2}}\rho ^{2}=0}
  
these are the points corresponding to a sphere

make pic to show hyperbolic orthogonality --> for a vector S off the null-cone, which directions are hyperbolically orthogonal? (cf Lorentz transformation pix)
in 2+1 D, if S is (1,a,b), (using co-ords e-, {e+, ei}), the points hyperbolically orthogonal to S are those euclideanly orthogonal to (-1,a,b)—i.e., a plane; or in n dimensions, a hyperplane through the origin. This would cut another plane not through the origin in a line (a hypersurface in an n-2 surface), and then the cone in two points (resp. some sort of n-3 conic surface). So it's going to probably look like some kind of conic. This is the surface that is the image of a sphere under g.

A plane: the locus of x is a plane if A = P, a vector with a zero no component. In a homogeneous projective space such a vector P represents a vector on the plane no=1 that would be infinitely far from the origin (ie infinitely far outside the null cone) , so g(x).P =0 corresponds to x on a sphere of infinite radius, a plane.
In particular:

  
    
      
        
          P
        
        =
        
          
            
              
                a
              
              ^
            
          
        
        +
        α
        
          
            e
          
          
            ∞
          
        
      
    
    {\displaystyle \mathbf {P} ={\hat {\mathbf {a} }}+\alpha \mathbf {e} _{\infty }}
   corresponds to x on a plane with normal 
  
    
      
        
          
            
              
                a
              
              ^
            
          
        
      
    
    {\displaystyle {\hat {\mathbf {a} }}}
   an orthogonal distance α from the origin.

  
    
      
        
          P
        
        =
        g
        (
        
          a
        
        )
        −
        g
        (
        
          b
        
        )
      
    
    {\displaystyle \mathbf {P} =g(\mathbf {a} )-g(\mathbf {b} )}
   corresponds to a plane half way between a and b, with normal a - b

circles
tangent planes
lines
lines at infinity
point pairs


== Transformations ==

reflections
It can be verified that forming P g(x) P gives a new direction on the null-cone, g(x' ), where x' corresponds to a reflection in the plane of points p in R3 that satisfy g(p) . P = 0.
g(x) . A = 0 => P g(x) . A P = 0 => P g(x) P . P A P (and similarly for the wedge product), so the effect of applying P sandwich-fashion to any the quantities A in the section above is similarly to reflect the corresponding locus of points x, so the corresponding circles, spheres, lines and planes corresponding to particular types of A are reflected in exactly the same way that applying P to g(x) reflects a point x.

This reflection operation can be used to build up general translations and rotations:

translations
Reflection in two parallel planes gives a translation,

  
    
      
        g
        (
        
          
            x
          
          
            ′
          
        
        )
        =
        
          
            P
          
          
            β
          
        
        
          
            P
          
          
            α
          
        
        
        g
        (
        
          x
        
        )
        
        
          
            P
          
          
            α
          
        
        
          
            P
          
          
            β
          
        
      
    
    {\displaystyle g(\mathbf {x} ^{\prime })=\mathbf {P} _{\beta }\mathbf {P} _{\alpha }\;g(\mathbf {x} )\;\mathbf {P} _{\alpha }\mathbf {P} _{\beta }}
  
If 
  
    
      
        
          
            P
          
          
            α
          
        
        =
        
          
            
              
                a
              
              ^
            
          
        
        +
        α
        
          
            e
          
          
            ∞
          
        
      
    
    {\displaystyle \mathbf {P} _{\alpha }={\hat {\mathbf {a} }}+\alpha \mathbf {e} _{\infty }}
   and 
  
    
      
        
          
            P
          
          
            β
          
        
        =
        
          
            
              
                a
              
              ^
            
          
        
        +
        β
        
          
            e
          
          
            ∞
          
        
      
    
    {\displaystyle \mathbf {P} _{\beta }={\hat {\mathbf {a} }}+\beta \mathbf {e} _{\infty }}
   then 
  
    
      
        
          
            x
          
          
            ′
          
        
        =
        
          x
        
        +
        2
        (
        β
        −
        α
        )
        
          
            
              
                a
              
              ^
            
          
        
      
    
    {\displaystyle \mathbf {x} ^{\prime }=\mathbf {x} +2(\beta -\alpha ){\hat {\mathbf {a} }}}
  

rotations

  
    
      
        g
        (
        
          
            x
          
          
            ′
          
        
        )
        =
        
          
            
              
                b
              
              ^
            
          
        
        
          
            
              
                a
              
              ^
            
          
        
        
        g
        (
        
          x
        
        )
        
        
          
            
              
                a
              
              ^
            
          
        
        
          
            
              
                b
              
              ^
            
          
        
      
    
    {\displaystyle g(\mathbf {x} ^{\prime })={\hat {\mathbf {b} }}{\hat {\mathbf {a} }}\;g(\mathbf {x} )\;{\hat {\mathbf {a} }}{\hat {\mathbf {b} }}}
   corresponds to an x' that is rotated about the origin by an angle 2 θ where θ is the angle between a and b -- the same effect that this rotor would have if applied directly to x.

general rotations
rotations about a general point can be achieved by first translating the point to the origin, then rotating around the origin, then translating the point back to its original position, i.e. a sandwiching by the operator 
  
    
      
        
          T
          R
          
            
              
                T
                ~
              
            
          
        
      
    
    {\displaystyle \mathbf {TR{\tilde {T}}} }
   so

  
    
      
        g
        (
        
          
            G
          
        
        x
        )
        =
        
          T
          R
          
            
              
                T
                ~
              
            
          
        
        
        g
        (
        
          x
        
        )
        
        
          T
          
            
              
                R
                ~
              
            
          
          
            
              
                T
                ~
              
            
          
        
      
    
    {\displaystyle g({\mathcal {G}}x)=\mathbf {TR{\tilde {T}}} \;g(\mathbf {x} )\;\mathbf {T{\tilde {R}}{\tilde {T}}} }
  

screws
the effect a screw, or motor, (a rotation about a general point, followed by a translation parallel to the axis of rotation) can be achieved by sandwiching g(x) by the operator 
  
    
      
        
          M
        
        =
        
          
            T
            
              2
            
          
          
            T
            
              1
            
          
          R
          
            
              
                
                  T
                  
                    1
                  
                
                ~
              
            
          
        
      
    
    {\displaystyle \mathbf {M} =\mathbf {T_{2}T_{1}R{\tilde {T_{1}}}} }
  .
M can also be parametrised 
  
    
      
        
          M
        
        =
        
          
            T
            
              ′
            
          
          
            R
            
              ′
            
          
        
      
    
    {\displaystyle \mathbf {M} =\mathbf {T^{\prime }R^{\prime }} }
   (Chasles' theorem)

inversions
an inversion is a reflection in a sphere – various operations that can be achieved using such inversions are discussed at inversive geometry. In particular, the combination of inversion together with the Euclidean transformations translation and rotation is sufficient to express any conformal mapping – i.e. any mapping that universally preserves angles. (Liouville's theorem).

dilations
two inversions with the same centre produce a dilation.


== Generalizations ==


== History ==


== Notes ==


== References ==


== Bibliography =="
38,Computational biology,149353,25169,"Computational biology involves the development and application of data-analytical and theoretical methods, mathematical modeling and computational simulation techniques to the study of biological, behavioral, and social systems. The field is broadly defined and includes foundations in biology, applied mathematics, statistics, biochemistry, chemistry, biophysics, molecular biology, genetics, genomics, computer science and evolution.
Computational biology is different from biological computation, which is a subfield of computer science and computer engineering using bioengineering and biology to build computers, but is similar to bioinformatics, which is an interdisciplinary science using computers to store and process biological data.


== Introduction ==
Computational Biology, which includes many aspects of bioinformatics, is the science of using biological data to develop algorithms or models to understand among various biological systems and relationships. Until recently, biologists did not have access to very large amounts of data which have become commonplace, particularly in molecular biology and genomics. Researchers were able to develop analytical methods for interpreting biological information, but were unable to share them quickly among colleagues.
Bioinformatics began to develop in the early 1970s. It was considered the science of analyzing informatics processes of various biological systems. At this time, research in artificial intelligence was using network models of the human brain in order to generate new algorithms. This use of biological data to develop other fields pushed biological researchers to revisit the idea of using computers to evaluate and compare large data sets. By 1982, information was being shared amongst researchers through the use of punch cards. The amount of data being shared began to grow exponentially by the end of the 1980s. This required the development of new computational methods in order to quickly analyze and interpret relevant information.
Since the late 1990s, computational biology has become an important part of developing emerging technologies for the field of biology. The terms computational biology and evolutionary computation have a similar name, but are not to be confused. Unlike computational biology, evolutionary computation is not concerned with modeling and analyzing biological data. It instead creates algorithms based on the ideas of evolution across species. Sometimes referred to as genetic algorithms, the research of this field can be applied to computational biology. While evolutionary computation is not inherently a part of computational biology, Computational evolutionary biology is a subfield of it.
Computational biology has been used to help sequence the human genome, create accurate models of the human brain, and assist in modeling biological systems.


== Subfields ==


=== Computational anatomy ===

Computational anatomy is a discipline focusing on the study of anatomical shape and form at the visible or gross anatomical 
  
    
      
        50
        −
        100
        μ
      
    
    {\displaystyle 50-100\mu }
   scale of morphology. It involves the development and application of computational, mathematical and data-analytical methods for modeling and simulation of biological structures. The field is broadly defined and includes foundations in anatomy, applied mathematics and pure mathematics, machine learning, computational mechanics, computational science, medical imaging, neuroscience, physics, probability, and statistics; it also has strong connections with fluid mechanics and geometric mechanics. It focuses on the anatomical structures being imaged, rather than the medical imaging devices. It is similar in spirit to the history of Computational linguistics, a discipline that focuses on the linguistic structures rather than the sensor acting as the transmission and communication medium(s).Due to the availability of dense 3D measurements via technologies such as magnetic resonance imaging (MRI), Computational anatomy has emerged as a subfield of medical imaging and bioengineering for extracting anatomical coordinate systems at the morphome scale in 3D.
In computational anatomy, the diffeomorphism group is used to study different coordinate systems via coordinate transformations as generated via the Lagrangian and Eulerian velocities of flow from one anatomical configuration in 
  
    
      
        
          
            
              R
            
          
          
            3
          
        
      
    
    {\displaystyle {\mathbb {R} }^{3}}
   to another. Computational anatomy intersects the study of Riemannian manifolds where groups of diffeomorphisms are the central focus, intersecting with emerging high-dimensional theories of shape emerging from the field of shape statistics. The metric structures in Computational anatomy are related in spirit to morphometrics, with the distinction that Computational anatomy focuses on an infinite-dimensional space of coordinate systems transformed by a diffeomorphism, hence the central use of the terminology diffeomorphometry, the metric space study of coordinate systems via diffeomorphisms. At Computational anatomy's heart is the comparison of shape by recognizing in one shape the other. This connects it to D'Arcy Wentworth Thompson's developments On Growth and Form which has led to scientific explanations of morphogenesis, the process by which patterns are formed in Biology. The original formulation of Computational anatomy is as a generative model of shape and form from exemplars acted upon via transformations.
The spirit of this discipline shares strong overlap with areas such as computer vision and kinematics of rigid bodies, where objects are studied by analysing the groups responsible for the movement in question. It is a branch of the image analysis and pattern theory school at Brown University pioneered by Ulf Grenander. Making spaces of anatomical patterns in Pattern Theory, into a metric space is one of the fundamental operations since being able to cluster and recognize anatomical configurations often requires a metric of close and far between shapes. The diffeomorphometry metric of Computational anatomy measures how far two diffeomorphic changes of coordinates are from each other, which in turn induces a metric on the shapes and images indexed to them. The models of metric pattern theory, in particular group action on the orbit of shapes and forms is a central tool to the formal definitions in Computational anatomy.


=== Computational biomodeling ===

Computational biomodeling is a field concerned with building computer models of biological systems. Computational biomodeling aims to develop and use visual simulations in order to assess the complexity of biological systems. This is accomplished through the use of specialized algorithms, and visualization software. These models allow for prediction of how systems will react under different environments. This is useful for determining if a system is robust. A robust biological system is one that “maintain their state and functions against external and internal perturbations”, which is essential for a biological system to survive. Computational biomodeling generates a large archive of such data, allowing for analysis from multiple users. While current techniques focus on small biological systems, researchers are working on approaches that will allow for larger networks to be analyzed and modeled. A majority of researchers believe that this will be essential in developing modern medical approaches to creating new drugs and gene therapy. A useful modelling approach is to use Petri nets via tools such as esyN 


=== Computational genomics (Computational genetics) ===

Computational genomics is a field within genomics which studies the genomes of cells and organisms. It is sometimes referred to as Computational and Statistical Genetics and encompasses much of Bioinformatics. The Human Genome Project is one example of computational genomics. This project looks to sequence the entire human genome into a set of data. Once fully implemented, this could allow for doctors to analyze the genome of an individual patient. This opens the possibility of personalized medicine, prescribing treatments based on an individual’s pre-existing genetic patterns. This project has created many similar programs. Researchers are looking to sequence the genomes of animals, plants, bacteria, and all other types of life.
One of the main ways that genomes are compared is by homology. Homology is the study of biological structures and nucleotide sequences in different organisms that come from a common ancestor. Research suggests that between 80 and 90% of genes in newly sequenced prokaryotic genomes can be identified this way.
This field is still in development. An untouched project in the development of computational genomics is the analysis of intergenic regions. Studies show that roughly 97% of the human genome consists of these regions. Researchers in computational genomics are working on understanding the functions of non-coding regions of the human genome through the development of computational and statistical methods and via large consortia projects such as ENCODE (The Encyclopedia of DNA Elements) and the Roadmap Epigenomics Project.


=== Computational neuroscience ===

Computational neuroscience is the study of brain function in terms of the information processing properties of the structures that make up the nervous system. It is a subset of the field of neuroscience, and looks to analyze brain data to create practical applications. It looks to model the brain in order to examine specific types aspects of the neurological system. Various types of models of the brain include:
Realistic Brain Models: These models look to represent every aspect of the brain, including as much detail at the cellular level as possible. Realistic models provide the most information about the brain, but also have the largest margin for error. More variables in a brain model create the possibility for more error to occur. These models do not account for parts of the cellular structure that scientists do not know about. Realistic brain models are the most computationally heavy and the most expensive to implement.
Simplifying Brain Models: These models look to limit the scope of a model in order to assess a specific physical property of the neurological system. This allows for the intensive computational problems to be solved, and reduces the amount of potential error from a realistic brain model.
It is the work of computational neuroscientists to improve the algorithms and data structures currently used to increase the speed of such calculations.


=== Computational pharmacology ===
Computational pharmacology (from a computational biology perspective) is “the study of the effects of genomic data to find links between specific genotypes and diseases and then screening drug data”. The pharmaceutical industry requires a shift in methods to analyze drug data. Pharmacologists were able to use Microsoft Excel to compare chemical and genomic data related to the effectiveness of drugs. However, the industry has reached what is referred to as the Excel barricade. This arises from the limited number of cells accessible on a spreadsheet. This development led to the need for computational pharmacology. Scientists and researchers develop computational methods to analyze these massive data sets. This allows for an efficient comparison between the notable data points and allows for more accurate drugs to be developed.
Analysts project that if major medications fail due to patents, that computational biology will be necessary to replace current drugs on the market. Doctoral students in computational biology are being encouraged to pursue careers in industry rather than take Post-Doctoral positions. This is a direct result of major pharmaceutical companies needing more qualified analysts of the large data sets required for producing new drugs.


=== Computational evolutionary biology ===
Computational biology has assisted the field of evolutionary biology in many capacities. This includes:
Using DNA data to reconstruct the tree of life with computational phylogenetics
Fitting population genetics models (either forward time or backward time) to DNA data to make inferences about demographic or selective history
Building population genetics models of evolutionary systems from first principles in order to predict what is likely to evolve.


=== Cancer computational biology ===
Cancer computational biology is a field that aims to determine the future mutations in cancer through an algorithmic approach to analyzing data. Research in this field has led to the use of high-throughput measurement. High throughput measurement allows for the gathering of millions of data points using robotics and other sensing devices. This data is collected from DNA, RNA, and other biological structures. Areas of focus include determining the characteristics of tumors, analyzing molecules that are deterministic in causing cancer, and understanding how the human genome relates to the causation of tumors and cancer.


=== Computational neuropsychiatry ===
Computational neuropsychiatry is the emerging field that uses mathematical and computer-assisted modeling of brain mechanisms involved in mental disorders. It was already demonstrated by several initiatives that computational modeling is an important contribution to understand neuronal circuits that could generate mental functions and dysfunctions.


== Software and tools ==
Computational Biologists use a wide range of software. These range from command line programs to graphical and web-based programs.


=== Open source software ===
Open source software provides a platform to develop computational biological methods. Specifically, open source means that every person and/or entity can access and benefit from software developed in research. PLOS cites four main reasons for the use of open source software including:
Reproducibility: This allows for researchers to use the exact methods used to calculate the relations between biological data.
Faster Development: developers and researchers do not have to reinvent existing code for minor tasks. Instead they can use pre-existing programs to save time on the development and implementation of larger projects.
Increased quality: Having input from multiple researchers studying the same topic provides a layer of assurance that errors will not be in the code.
Long-term availability: Open source programs are not tied to any businesses or patents. This allows for them to be posted to multiple web pages and ensure that they are available in the future.


== Conferences ==
There are several large conferences that are concerned with computational biology. Some notable examples are Intelligent Systems for Molecular Biology (ISMB), European Conference on Computational Biology (ECCB) and Research in Computational Molecular Biology (RECOMB).


== Journals ==
There are numerous journals dedicated to computational biology. Some notable examples include Journal of Computational Biology and PLOS Computational Biology. The PLOS computational biology journal is a peer-reviewed journal that has many notable research projects in the field of computational biology. They provide reviews on software, tutorials for open source software, and display information on upcoming computational biology conferences. PLOS Computational Biology is an open access journal. The publication may be openly used provided the author is cited. Recently a new open access journal Computational Molecular Biology was launched.


== Related fields ==
Computational biology, bioinformatics and mathematical biology are all interdisciplinary approaches to the life sciences that draw from quantitative disciplines such as mathematics and information science. The NIH describes computational/mathematical biology as the use of computational/mathematical approaches to address theoretical and experimental questions in biology and, by contrast, bioinformatics as the application of information science to understand complex life-sciences data.
Specifically, the NIH defines

Computational biology: The development and application of data-analytical and theoretical methods, mathematical modeling and computational simulation techniques to the study of biological, behavioral, and social systems.

Bioinformatics: Research, development, or application of computational tools and approaches for expanding the use of biological, medical, behavioral or health data, including those to acquire, store, organize, archive, analyze, or visualize such data.

While each field is distinct, there may be significant overlap at their interface.


== See also ==


== References ==


== External links ==
bioinformatics.org"
39,Aanderaa–Karp–Rosenberg conjecture,21681084,25145,"In theoretical computer science, the Aanderaa–Karp–Rosenberg conjecture (also known as the Aanderaa–Rosenberg conjecture or the evasiveness conjecture) is a group of related conjectures about the number of questions of the form ""Is there an edge between vertex u and vertex v?"" that have to be answered to determine whether or not an undirected graph has a particular property such as planarity or bipartiteness. They are named after Stål Aanderaa, Richard M. Karp, and Arnold L. Rosenberg. According to the conjecture, for a wide class of properties, no algorithm can guarantee that it will be able to skip any questions: any algorithm for determining whether the graph has the property, no matter how clever, might need to examine every pair of vertices before it can give its answer. A property satisfying this conjecture is called evasive.
More precisely, the Aanderaa–Rosenberg conjecture states that any deterministic algorithm must test at least a constant fraction of all possible pairs of vertices, in the worst case, to determine any non-trivial monotone graph property; in this context, a property is monotone if it remains true when edges are added (so planarity is not monotone, but non-planarity is monotone). A stronger version of this conjecture, called the evasiveness conjecture or the Aanderaa–Karp–Rosenberg conjecture, states that exactly n(n − 1)/2 tests are needed. Versions of the problem for randomized algorithms and quantum algorithms have also been formulated and studied.
The deterministic Aanderaa–Rosenberg conjecture was proven by Rivest & Vuillemin (1975), but the stronger Aanderaa–Karp–Rosenberg conjecture remains unproven. Additionally, there is a large gap between the conjectured lower bound and the best proven lower bound for randomized and quantum query complexity.


== Example ==
The property of being non-empty (that is, having at least one edge) is monotone, because adding another edge to a non-empty graph produces another non-empty graph. There is a simple algorithm for testing whether a graph is non-empty: loop through all of the pairs of vertices, testing whether each pair is connected by an edge. If an edge is ever found in this way, break out of the loop, and report that the graph is non-empty, and if the loop completes without finding any edges, then report that the graph is empty. On some graphs (for instance the complete graphs) this algorithm will terminate quickly, without testing every pair of vertices, but on the empty graph it tests all possible pairs before terminating. Therefore, the query complexity of this algorithm is n(n − 1)/2: in the worst case, the algorithm performs n(n − 1)/2 tests.
The algorithm described above is not the only possible method of testing for non-emptiness, but the Aanderaa–Karp–Rosenberg conjecture implies that every deterministic algorithm for testing non-emptiness has the same query complexity, n(n − 1)/2. That is, the property of being non-empty is evasive. For this property, the result is easy to prove directly: if an algorithm does not perform n(n − 1)/2 tests, it cannot distinguish the empty graph from a graph that has one edge connecting one of the untested pairs of vertices, and must give an incorrect answer on one of these two graphs.


== Definitions ==
In the context of this article, all graphs will be simple and undirected, unless stated otherwise. This means that the edges of the graph form a set (and not a multiset) and each edge is a pair of distinct vertices. Graphs are assumed to have an implicit representation in which each vertex has a unique identifier or label and in which it is possible to test the adjacency of any two vertices, but for which adjacency testing is the only allowed primitive operation.
Informally, a graph property is a property of a graph that is independent of labeling. More formally, a graph property is a mapping from the set of all graphs to {0,1} such that isomorphic graphs are mapped to the same value. For example, the property of containing at least 1 vertex of degree 2 is a graph property, but the property that the first vertex has degree 2 is not, because it depends on the labeling of the graph (in particular, it depends on which vertex is the ""first"" vertex). A graph property is called non-trivial if it doesn't assign the same value to all graphs. For instance, the property of being a graph is a trivial property, since all graphs possess this property. On the other hand, the property of being empty is non-trivial, because the empty graph possesses this property, but non-empty graphs do not. A graph property is said to be monotone if the addition of edges does not destroy the property. Alternately, if a graph possesses a monotone property, then every supergraph of this graph on the same vertex set also possesses it. For instance, the property of being nonplanar is monotone: a supergraph of a nonplanar graph is itself nonplanar. However, the property of being regular is not monotone.
The big O notation is often used for query complexity. In short, f(n) is O(g(n)) if for large enough n, f(n) ≤ c g(n) for some positive constant c. Similarly, f(n) is Ω(g(n)) if for large enough n, f(n) ≥ c g(n) for some positive constant c. Finally, f(n) is Θ(g(n)) if it is both O(g(n)) and Ω(g(n)).


== Query complexity ==
The deterministic query complexity of evaluating a function on n bits (x1, x2, ..., xn) is the number of bits xi that have to be read in the worst case by a deterministic algorithm to determine the value of the function. For instance, if the function takes value 0 when all bits are 0 and takes value 1 otherwise (this is the OR function), then the deterministic query complexity is exactly n. In the worst case, the first n − 1 bits read could all be 0, and the value of the function now depends on the last bit. If an algorithm doesn't read this bit, it might output an incorrect answer. (Such arguments are known as adversary arguments.) The number of bits read are also called the number of queries made to the input. One can imagine that the algorithm asks (or queries) the input for a particular bit and the input responds to this query.
The randomized query complexity of evaluating a function is defined similarly, except the algorithm is allowed to be randomized, i.e., it can flip coins and use the outcome of these coin flips to decide which bits to query. However, the randomized algorithm must still output the correct answer for all inputs: it is not allowed to make errors. Such algorithms are called Las Vegas algorithms, which distinguishes them from Monte Carlo algorithms which are allowed to make some error. Randomized query complexity can also be defined in the Monte Carlo sense, but the Aanderaa–Karp–Rosenberg conjecture is about the Las Vegas query complexity of graph properties.
Quantum query complexity is the natural generalization of randomized query complexity, of course allowing quantum queries and responses. Quantum query complexity can also be defined with respect to Monte Carlo algorithms or Las Vegas algorithms, but it is usually taken to mean Monte Carlo quantum algorithms.
In the context of this conjecture, the function to be evaluated is the graph property, and the input is a string of size n(n − 1)/2, which gives the locations of the edges on an n vertex graph, since a graph can have at most n(n − 1)/2 possible edges. The query complexity of any function is upper bounded by n(n − 1)/2, since the whole input is read after making n(n − 1)/2 queries, thus determining the input graph completely.


== Deterministic query complexity ==
For deterministic algorithms, Rosenberg (1973) originally conjectured that for all nontrivial graph properties on n vertices, deciding whether a graph possesses this property requires Ω(n2) queries. The non-triviality condition is clearly required because there are trivial properties like ""is this a graph?"" which can be answered with no queries at all.

The conjecture was disproved by Aanderaa, who exhibited a directed graph property (the property of containing a ""sink"") which required only O(n) queries to test. A sink, in a directed graph, is a vertex of indegree n-1 and outdegree 0. This property can be tested with less than 3n queries (Best, van Emde Boas & Lenstra 1974). An undirected graph property which can also be tested with O(n) queries is the property of being a scorpion graph, first described in Best, van Emde Boas & Lenstra (1974). A scorpion graph is a graph containing a three-vertex path, such that one endpoint of the path is connected to all remaining vertices, while the other two path vertices have no incident edges other than the ones in the path.
Then Aanderaa and Rosenberg formulated a new conjecture (the Aanderaa–Rosenberg conjecture) which says that deciding whether a graph possesses a non-trivial monotone graph property requires Ω(n2) queries. This conjecture was resolved by Rivest & Vuillemin (1975) by showing that at least n2/16 queries are needed to test for any nontrivial monotone graph property. The bound was further improved to n2/9 by Kleitman & Kwiatkowski (1980), then to n2/4 - o(n2) by Kahn, Saks & Sturtevant (1983), then to (8/25)n2 - o(n2) by Korneffel & Triesch (2010), and then to n2/3 - o(n2) by Scheidweiler & Triesch (2013).
Richard Karp conjectured the stronger statement (which is now called the evasiveness conjecture or the Aanderaa–Karp–Rosenberg conjecture) that ""every nontrivial monotone graph property for graphs on n vertices is evasive."" A property is called evasive if determining whether a given graph has this property sometimes requires all n(n − 1)/2 queries. This conjecture says that the best algorithm for testing any nontrivial monotone property must (in the worst case) query all possible edges. This conjecture is still open, although several special graph properties have shown to be evasive for all n. The conjecture has been resolved for the case where n is a prime power by Kahn, Saks & Sturtevant (1983) using a topological approach. The conjecture has also been resolved for all non-trivial monotone properties on bipartite graphs by Yao (1988). Minor-closed properties have also been shown to be evasive for large n (Chakrabarti, Khot & Shi 2001).


== Randomized query complexity ==
Richard Karp also conjectured that Ω(n2) queries are required for testing nontrivial monotone properties even if randomized algorithms are permitted. No nontrivial monotone property is known which requires less than n2/4 queries to test. A linear lower bound (i.e., Ω(n)) on all monotone properties follows from a very general relationship between randomized and deterministic query complexities. The first superlinear lower bound for all monotone properties was given by Yao (1991) who showed that Ω(n log1/12 n) queries are required. This was further improved by King (1988) to Ω(n5/4), and then by Hajnal (1991) to Ω(n4/3). This was subsequently improved to the current best known lower bound (among bounds that hold for all monotone properties) of Ω(n4/3 log1/3 n) by Chakrabarti & Khot (2001).
Some recent results give lower bounds which are determined by the critical probability p of the monotone graph property under consideration. The critical probability p is defined as the unique p such that a random graph G(n, p) possesses this property with probability equal to 1/2. A random graph G(n, p) is a graph on n vertices where each edge is chosen to be present with probability p independent of all the other edges. Friedgut, Kahn & Wigderson (2002) showed that any monotone property with critical probability p requires 
  
    
      
        Ω
        
          (
          
            min
            
              {
              
                
                  
                    n
                    
                      min
                      (
                      p
                      ,
                      1
                      −
                      p
                      )
                    
                  
                
                ,
                
                  
                    
                      n
                      
                        2
                      
                    
                    
                      log
                      ⁡
                      n
                    
                  
                
              
              }
            
          
          )
        
      
    
    {\displaystyle \Omega \left(\min \left\{{\frac {n}{\min(p,1-p)}},{\frac {n^{2}}{\log n}}\right\}\right)}
   queries. For the same problem, O'Donnell et al. (2005) showed a lower bound of Ω(n4/3/p1/3).
As in the deterministic case, there are many special properties for which an Ω(n2) lower bound is known. Moreover, better lower bounds are known for several classes of graph properties. For instance, for testing whether the graph has a subgraph isomorphic to any given graph (the so-called subgraph isomorphism problem), the best known lower bound is Ω(n3/2) due to Gröger (1992).


== Quantum query complexity ==
For bounded-error quantum query complexity, the best known lower bound is Ω(n2/3 log1/6 n) as observed by Andrew Yao. It is obtained by combining the randomized lower bound with the quantum adversary method. The best possible lower bound one could hope to achieve is Ω(n), unlike the classical case, due to Grover's algorithm which gives an O(n) query algorithm for testing the monotone property of non-emptiness. Similar to the deterministic and randomized case, there are some properties which are known to have an Ω(n) lower bound, for example non-emptiness (which follows from the optimality of Grover's algorithm) and the property of containing a triangle. More interestingly, there are some graph properties which are known to have an Ω(n3/2) lower bound, and even some properties with an Ω(n2) lower bound. For example, the monotone property of nonplanarity requires Θ(n3/2) queries (Ambainis et al. 2008) and the monotone property of containing more than half the possible number of edges (also called the majority function) requires Θ(n2) queries (Beals et al. 2001).


== Notes ==


== References ==


== Further reading ==
Bollobás, Béla (2004), ""Chapter VIII. Complexity and packing"", Extremal Graph Theory, New York: Dover Publications, pp. 401–437, ISBN 978-0-486-43596-1 .
László Lovász; Young, Neal E. (2002). ""Lecture Notes on Evasiveness of Graph Properties"". arXiv:cs/0205031v1  [cs.CC]. 
Chronaki, Catherine E (1990), A survey of Evasiveness: Lower Bounds on the Decision-Tree Complexity of Boolean Functions, CiteSeerX 10.1.1.37.1041 . 
Michael Saks. ""Decision Trees: Problems and Results, Old and New"" (PDF)."
40,Informatics,23997153,25130,"Informatics is a branch of information engineering. It involves the practice of information processing and the engineering of information systems, and as an academic field it is an applied form of information science. The field considers the interaction between humans and information alongside the construction of interfaces, organisations, technologies and systems. As such, the field of informatics has great breadth and encompasses many subspecialties, including disciplines of computer science, information systems, information technology and statistics. Since the advent of computers, individuals and organizations increasingly process information digitally. This has led to the study of informatics with computational, mathematical, biological, cognitive and social aspects, including study of the social impact of information technologies.


== Etymology ==

In 1956 the German computer scientist Karl Steinbuch coined the word Informatik by publishing a paper called Informatik: Automatische Informationsverarbeitung (""Informatics: Automatic Information Processing""). The English term Informatics is sometimes understood as meaning the same as computer science. The German word Informatik is usually translated to English as computer science.
The French term informatique was coined in 1962 by Philippe Dreyfus together with various translations—informatics (English), also proposed independently and simultaneously by Walter F. Bauer and associates who co-founded Informatics Inc., and informatica (Italian, Spanish, Romanian, Portuguese, Dutch), referring to the application of computers to store and process information.
The term was coined as a combination of ""information"" and ""automatic"" to describe the science of automating information interactions. The morphology—informat-ion + -ics—uses ""the accepted form for names of sciences, as conics, linguistics, optics, or matters of practice, as economics, politics, tactics"", and so, linguistically, the meaning extends easily to encompass both the science of information and the practice of information processing.


== History ==

The culture of library science promotes policies and procedures for managing information that fosters the relationship between library science and the development of information science to provide benefits for health informatics development; which is traced to the 1950s with the beginning of computer uses in healthcare (Nelson & Staggers p.4). Early practitioners interested in the field soon learned that there were no formal education programs set up to educate them on the informatics science until the late 1960s and early 1970s. Professional development began to emerge, playing a significant role in the development of health informatics (Nelson &Staggers p.7) According to Imhoff et al., 2001, healthcare informatics is not only the application of computer technology to problems in healthcare but covers all aspects of generation, handling, communication, storage, retrieval, management, analysis, discovery, and synthesis of data information and knowledge in the entire scope of healthcare. Furthermore, they stated that the primary goal of health informatics can be distinguished as follows: To provide solutions for problems related to data, information, and knowledge processing. To study general principles of processing data information and knowledge in medicine and healthcare.
Reference Imhoff, M., Webb. A,.&Goldschmidt, A., (2001). Health Informatics. Intensive Care Med, 27: 179-186. doi:10.1007//s001340000747.
Nelson, R. & Staggers, N. Health Informatics: An Interprofessional Approach. St. Louis: Mosby, 2013. Print. (p.4,7)

This new term was adopted across Western Europe, and, except in English, developed a meaning roughly translated by the English ‘computer science’, or ‘computing science’. Mikhailov advocated the Russian term informatika (1966), and the English informatics (1967), as names for the theory of scientific information, and argued for a broader meaning, including study of the use of information technology in various communities (for example, scientific) and of the interaction of technology and human organizational structures.
Informatics is the discipline of science which investigates the structure and properties (not specific content) of scientific information, as well as the regularities of scientific information activity, its theory, history, methodology and organization.
Usage has since modified this definition in three ways. First, the restriction to scientific information is removed, as in business informatics or legal informatics. Second, since most information is now digitally stored, computation is now central to informatics. Third, the representation, processing and communication of information are added as objects of investigation, since they have been recognized as fundamental to any scientific account of information. Taking information as the central focus of study distinguishes informatics from computer science. Informatics includes the study of biological and social mechanisms of information processing whereas computer science focuses on the digital computation. Similarly, in the study of representation and communication, informatics is indifferent to the substrate that carries information. For example, it encompasses the study of communication using gesture, speech and language, as well as digital communications and networking.
In the English-speaking world the term informatics was first widely used in the compound medical informatics, taken to include ""the cognitive, information processing, and communication tasks of medical practice, education, and research, including information science and the technology to support these tasks"". Many such compounds are now in use; they can be viewed as different areas of ""applied informatics"". Indeed, ""In the U.S., however, informatics is linked with applied computing, or computing in the context of another domain.""
Informatics encompasses the study of systems that represent, process, and communicate information. However, the theory of computation in the specific discipline of theoretical computer science, which evolved from Alan Turing, studies the notion of a complex system regardless of whether or not information actually exists. Since both fields process information, there is some disagreement among scientists as to field hierarchy; for example Arizona State University attempted to adopt a broader definition of informatics to even encompass cognitive science at the launch of its School of Computing and Informatics in September 2006.
A broad interpretation of informatics, as ""the study of the structure, algorithms, behaviour, and interactions of natural and artificial computational systems,"" was introduced by the University of Edinburgh in 1994 when it formed the grouping that is now its School of Informatics. This meaning is now (2006) increasingly used in the United Kingdom.
The 2008 Research Assessment Exercise, of the UK Funding Councils, includes a new, Computer Science and Informatics, unit of assessment (UoA), whose scope is described as follows:
The UoA includes the study of methods for acquiring, storing, processing, communicating and reasoning about information, and the role of interactivity in natural and artificial systems, through the implementation, organisation and use of computer hardware, software and other resources. The subjects are characterised by the rigorous application of analysis, experimentation and design.


== Academic schools and departments ==
Academic research in the informatics area can be found in a number of disciplines such as computer science, information technology, Information and Computer Science, information system, business information management and health informatics.
In France, the first degree level qualifications in Informatics (computer science) appeared in the mid-1960s.
In English-speaking countries, the first example of a degree level qualification in Informatics occurred in 1982 when Plymouth Polytechnic (now the University of Plymouth) offered a four-year BSc(Honours) degree in Computing and Informatics – with an initial intake of only 35 students. The course still runs today  making it the longest available qualification in the subject.
At the Indiana University School of Informatics (Bloomington, Indianapolis and Southeast), informatics is defined as ""the art, science and human dimensions of information technology"" and ""the study, application, and social consequences of technology."" It is also defined in Informatics 101, Introduction to Informatics as ""the application of information technology to the arts, sciences, and professions."" These definitions are widely accepted in the United States, and differ from British usage in omitting the study of natural computation.
Texas Woman's University places its informatics degrees in its department of Mathematics and Computer Science within the College of Arts & Sciences, though it offers interdisciplinary Health Informatics degrees. Informatics is presented in a generalist framework, as evidenced by their definition of informatics (""Using technology and data analytics to derive meaningful information from data for data and decision driven practice in user centered systems""), though TWU is also known for its nursing and health informatics programs.
At the University of California, Irvine Department of Informatics, informatics is defined as ""the interdisciplinary study of the design, application, use and impact of information technology. The discipline of informatics is based on the recognition that the design of this technology is not solely a technical matter, but must focus on the relationship between the technology and its use in real-world settings. That is, informatics designs solutions in context, and takes into account the social, cultural and organizational settings in which computing and information technology will be used.""
At the University of Michigan, Ann Arbor Informatics interdisciplinary major, informatics is defined as ""the study of information and the ways information is used by and affects human beings and social systems. The major involves coursework from the College of Literature, Science and the Arts, where the Informatics major is housed, as well as the School of Information and the College of Engineering. Key to this growing field is that it applies both technological and social perspectives to the study of information. Michigan's interdisciplinary approach to teaching Informatics gives a solid grounding in contemporary computer programming, mathematics, and statistics, combined with study of the ethical and social science aspects of complex information systems. Experts in the field help design new information technology tools for specific scientific, business, and cultural needs."" Michigan offers four curricular tracks within the informatics degree to provide students with increased expertise. These four track topics include:
Internet Informatics: An applied track in which students experiment with technologies behind Internet-based information systems and acquire skills to map problems to deployable Internet-based solutions. This track will replace Computational Informatics in Fall 2013.
Data Mining & Information Analysis: Integrates the collection, analysis, and visualization of complex data and its critical role in research, business, and government to provide students with practical skills and a theoretical basis for approaching challenging data analysis problems.
Life Science Informatics: Examines artificial information systems, which has helped scientists make great progress in identifying core components of organisms and ecosystems.
Social Computing: Advances in computing have created opportunities for studying patterns of social interaction and developing systems that act as introducers, recommenders, coordinators, and record-keepers. Students, in this track, craft, evaluate, and refine social software computer applications for engaging technology in unique social contexts. This track will be phased out in Fall 2013 in favor of the new bachelor of science in information. This will be the first undergraduate degree offered by the School of Information since its founding in 1996. The School of Information already contains a Master's program, Doctorate program, and a professional master's program in conjunction with the School of Public Health. The BS in Information at the University of Michigan will be the first curriculum program of its kind in the United States, with the first graduating class to emerge in 2015. Students will be able to apply for this unique degree in 2013 for the 2014 Fall semester; the new degree will be a stem off of the most popular Social Computing track in the current Informatics interdisciplinary major in LSA. Applications will be open to upper-classmen, juniors and seniors, along with a variety of information classes available for first and second year students to gauge interest and value in the specific sector of study. The degree was approved by the University on June 11, 2012. Along with a new degree in the School of Information, there has also been the first and only chapter of an Informatics Professional Fraternity, Kappa Theta Pi, chartered in Fall 2012.
At the University of Washington, Seattle Informatics Undergraduate Program, Informatics is an undergraduate program offered by the Information School. Bachelor of Science in Informatics is described as ""[a] program that focuses on computer systems from a user-centered perspective and studies the structure, behavior and interactions of natural and artificial systems that store, process and communicate information. Includes instruction in information sciences, human computer interaction, information system analysis and design, telecommunications structure and information architecture and management."" Washington offers three degree options as well as a custom track.
Data Science Option: Data Science is an emerging interdisciplinary field that works to extract knowledge or insight from data. It combines fields such as information science, computer science, statistics, design, and social science.
Human-Computer Interaction: The iSchool’s work in human-computer interaction (HCI) strives to make information and computing useful, usable, and accessible to all. The Informatics HCI option allows one to blend your technical skills and expertise with a broader perspective on how design and development work impacts users. Courses explore the design, construction, and evaluation of interactive technologies for use by individuals, groups, and organizations, and the social implications of these systems. This work encompasses user interfaces, accessibility concerns, new design techniques and methods for interactive systems and collaboration. Coursework also examines the values implicit in the design and development of technology.
Information Architecture: Information architecture (IA) is a crucial component in the development of successful Web sites, software, intranets, and online communities. Architects structure the underlying information and its presentation in a logical and intuitive way so that people can put information to use. As an Informatics major with an IA option, one will master the skills needed to organize and label information for improved navigation and search. One will build frameworks to effectively collect, store and deliver information. One will also learn to design the databases and XML storehouses that drive complex and interactive websites, including the navigation, content layout, personalization, and transactional features of the site.
Information Assurance and Cybersecurity: Information Assurance and Cybersecurity (IAC) is the practice of creating and managing safe and secure systems. It is crucial for organizations public and private, large and small. In the IAC option, one will be equipped with the knowledge to create, deploy, use, and manage systems that preserve individual and organizational privacy and security. This tri-campus concentration leverages the strengths of the Information School, the Computing and Software Systems program at UW Bothell, and the Institute of Technology at UW Tacoma. After a course in the technical, policy, and management foundations of IAC, one may take electives at any campus to learn such specialties as information assurance policy, secure coding, or networking and systems administration.
Custom (Student-Designed Concentration): Students may choose to develop their own concentration, with approval from the academic adviser. Student-designed concentrations are created out of a list of approved courses and also result in the Bachelor of Science degree.


== Applied disciplines ==


=== Organizational informatics ===

One of the most significant areas of application of informatics is that of organizational informatics. Organizational informatics is fundamentally interested in the application of information, information systems and ICT within organisations of various forms including private sector, public sector and voluntary sector organisations. As such, organisational informatics can be seen to be a sub-category of social informatics and a super-category of business informatics. Organizational informatics are also present in the computer science and information technology industry. 


== See also ==
Artificial intelligence
Behavior informatics
Biomimetics
Cognitive science
Computer science
Communication studies
Information science
Information systems
Information theory
Information technology
Knowledge Management
Robotics
Urban informatics


== Notes ==


== External links ==
informatics: entry from International Encyclopedia of Information and Library Science
Informatics Studies: Journal of Centre for Informatics Research and Development
Software History Center: First usage of informatics in the US
What is Informatics? : Indiana University
Q&A about informatics
Prior Art Database: Informatics: An Early Software Company
Informatics Europe
The Council of European Professional Informatics Societies (CEPIS)
Informatics Department, College of Computing and Information, University at Albany - State University of New York
Department of Informatics, King's College London
An Informatics Education: What and who is it for?, from Northern Kentucky University
Texas Woman's University's Informatics on Facebook
Institution of Mechanical Engineers - Mechatronics, Informatics and Control Group (MICG)
https://www.coursehero.com/file/23728173/Academic-schools-and-departments-on-literature-infomatics/
Informatics: 10 Years Back, 10 years Ahead"
41,Computational science,1181008,24110,"Computational science (also scientific computing or scientific computation (SC)) is a rapidly growing multidisciplinary field that uses advanced computing capabilities to understand and solve complex problems. It is an area of science which spans many disciplines, but at its core it involves the development of models and simulations to understand natural systems.
Algorithms (numerical and non-numerical), mathematical and computational modeling and simulation developed to solve science (e.g., biological, physical, and social), engineering, and humanities problems
Computer and information science that develops and optimizes the advanced system hardware, software, networking, and data management components needed to solve computationally demanding problems
The computing infrastructure that supports both the science and engineering problem solving and the developmental computer and information science
In practical use, it is typically the application of computer simulation and other forms of computation from numerical analysis and theoretical computer science to solve problems in various scientific disciplines. The field is different from theory and laboratory experiment which are the traditional forms of science and engineering. The scientific computing approach is to gain understanding, mainly through the analysis of mathematical models implemented on computers. Scientists and engineers develop computer programs, application software, that model systems being studied and run these programs with various sets of input parameters. The essence of computational science is the application of numerical algorithms and/or computational mathematics. In some cases, these models require massive amounts of calculations (usually floating-point) and are often executed on supercomputers or distributed computing platforms.


== The computational scientist ==

The term computational scientist is used to describe someone skilled in scientific computing. This person is usually a scientist, an engineer or an applied mathematician who applies high-performance computing in different ways to advance the state-of-the-art in their respective applied disciplines in physics, chemistry or engineering.
Computational science is now commonly considered a third mode of science, complementing and adding to experimentation/observation and theory (see image on the right). Here, we define a system as a potential source of data, a experiment as a process of extracting data from a system by exerting it through its inputs and a model (M) for a system (S) and an experiment (E) as anything to which E can be applied in order to answer questions about S. A computational scientist should be capable of:
recognizing complex problems
adequately conceptualise the system containing these problems
design a framework of algorithms suitable for studying this system: the simulation
choose a suitable computing infrastructure (parallel computing/grid computing/supercomputers)
hereby, maximising the computational power of the simulation
assessing to what level the output of the simulation resembles the systems: the model is validated
adjust the conceptualisation of the system accordingly
repeat cycle until a suitable level of validation is obtained: the computational scientists trusts that the simulation generates adequately realistic results for the system, under the studied conditions
In fact, substantial effort in computational sciences has been devoted to the development of algorithms, the efficient implementation in programming languages, and validation of computational results. A collection of problems and solutions in computational science can be found in Steeb, Hardy, Hardy and Stoop (2004).
Philosophers of science addressed the question to what degree computational science qualifies as science, among them Humphreys and Gelfert They address the general question of epistemology: how do we gain insight from such computational science approaches. Tolk uses these insights to show the epistemological constraints of computer-based simulation research. As computational science uses mathematical models representing the underlying theory in executable form, in essence they apply modeling (theory building) and simulation (implementation and execution). While simulation and computational science are our most sophisticated way to express our knowledge and understanding, they also come with all constraints and limits already known for computational solutions.


== Applications of computational science ==
Problem domains for computational science/scientific computing include:


=== Urban complex systems ===
Now in 2015 over half the worlds population live in cities. By the middle of the 21st century, it is estimated that 75% of the world’s population will be urban. This urban growth is focused in the urban populations of developing counties where cities dwellers will more than double, increasing from 2.5 billion in 2009 to almost 5.2 billion in 2050. Cities are massive complex systems created by humans, made up of humans and governed by humans. Trying to predict, understand and somehow shape the development of cities in the future requires complexity thinking, and requires computational models and simulations to help mitigate challenges and possible disasters. The focus of research in urban complex systems is, through modelling and simulation, build greater understanding of city dynamics and help prepare for the coming urbanisation.


=== Computational finance ===

In today’s financial markets huge volumes of interdependent assets are traded by a large number of interacting market participants in different locations and time zones. Their behavior is of unprecedented complexity and the characterization and measurement of the risk inherent to these highly diverse set of instruments is typically based on complicated mathematical and computational models. Solving these models exactly in closed form, even at a single instrument level, is typically not possible, and therefore we have to look for efficient numerical algorithms. This has become even more urgent and complex recently, as the credit crisis has clearly demonstrated the role of cascading effects going from single instruments through portfolios of single institutions to even the interconnected trading network. Understanding this requires a multi-scale and holistic approach where interdependent risk factors such as market, credit and liquidity risk are modelled simultaneously and at different interconnected scales.


=== Computational biology ===

Exciting new developments in biotechnology are now revolutionizing biology and biomedical research. Examples of these techniques are high-throughput sequencing, high-throughput quantitative PCR, intra-cellular imaging, in-situ hybridization of gene expression, three-dimensional imaging techniques like Light Sheet Fluorescence Microscopy and Optical Projection, (micro)-Computer Tomography. Given the massive amounts of complicated data that is generated by these techniques, their meaningful interpretation, and even their storage, form major challenges calling for new approaches. Going beyond current bioinformatics approaches, computational biology needs to develop new methods to discover meaningful patterns in these large data sets. Model-based reconstruction of gene networks can be used to organize the gene expression data in systematic way and to guide future data collection. A major challenge here is to understand how gene regulation is controlling fundamental biological processes like biomineralisation and embryogenesis. The sub-processes like gene regulation, organic molecules interacting with the mineral deposition process, cellular processes, physiology and other processes at the tissue and environmental levels are linked. Rather than being directed by a central control mechanism, biomineralisation and embryogenesis can be viewed as an emergent behavior resulting from a complex system in which several sub-processes on very different temporal and spatial scales (ranging from nanometer and nanoseconds to meters and years) are connected into a multi-scale system. One of the few available options to understand such systems is by developing a multi-scale model of the system.


=== Complex systems theory ===

Using information theory, non-equilibrium dynamics and explicit simulations computational systems theory tries to uncover the true nature of complex adaptive systems.


=== Computational science in engineering ===

Computational science and engineering (CSE) is a relatively new discipline that deals with the development and application of computational models and simulations, often coupled with high-performance computing, to solve complex physical problems arising in engineering analysis and design (computational engineering) as well as natural phenomena (computational science). CSE has been described as the ""third mode of discovery"" (next to theory and experimentation). In many fields, computer simulation is integral and therefore essential to business and research. Computer simulation provides the capability to enter fields that are either inaccessible to traditional experimentation or where carrying out traditional empirical inquiries is prohibitively expensive. CSE should neither be confused with pure computer science, nor with computer engineering, although a wide domain in the former is used in CSE (e.g., certain algorithms, data structures, parallel programming, high performance computing) and some problems in the latter can be modeled and solved with CSE methods (as an application area).


== Methods and algorithms ==
Algorithms and mathematical methods used in computational science are varied. Commonly applied methods include:

Both historically and today, Fortran remains popular for most applications of scientific computing. Other programming languages and computer algebra systems commonly used for the more mathematical aspects of scientific computing applications include GNU Octave, Haskell, Julia, Maple, Mathematica, MATLAB, Python (with third-party SciPy library), Perl (with third-party PDL library), R, SciLab, and TK Solver. The more computationally intensive aspects of scientific computing will often use some variation of C or Fortran and optimized algebra libraries such as BLAS or LAPACK.
Computational science application programs often model real-world changing conditions, such as weather, air flow around a plane, automobile body distortions in a crash, the motion of stars in a galaxy, an explosive device, etc. Such programs might create a 'logical mesh' in computer memory where each item corresponds to an area in space and contains information about that space relevant to the model. For example, in weather models, each item might be a square kilometer; with land elevation, current wind direction, humidity, temperature, pressure, etc. The program would calculate the likely next state based on the current state, in simulated time steps, solving equations that describe how the system operates; and then repeat the process to calculate the next state.


== Conferences and journals ==
In the year 2001, the International Conference on Computational Science (ICCS) was first organised. Since then it has been organised yearly. ICCS is an A-rank conference in CORE classification.
The international Journal of Computational Science published its first issue in May 2010. A new initiative was launched in 2012, the Journal of Open Research Software. In 2015, ReSciencededicated to the replication of computational results has been started on GitHub.


== Education ==
At some institutions a specialization in scientific computation can be earned as a ""minor"" within another program (which may be at varying levels). However, there are increasingly many bachelor's, master's and doctoral programs in computational science. The joint degree programme master program computational science at the University of Amsterdam and the Vrije Universiteit was the first full academic degree offered in computational science, and started in 2004. In this programme, students:
learn to build computational models from real-life observations;
develop skills in turning these models into computational structures and in performing large-scale simulations;
learn theory that will give a firm basis for the analysis of complex systems;
learn to analyse the results of simulations in a virtual laboratory using advanced numerical algorithms.


== Related fields ==


== See also ==

Computer simulations in science
Computational science and engineering
Comparison of computer algebra systems
List of molecular modeling software
List of numerical analysis software
List of statistical packages
Timeline of scientific computing
Simulated reality
Extensions for Scientific Computation (XSC)


== References ==


== Additional sources ==
E. Gallopoulos and A. Sameh, ""CSE: Content and Product"". IEEE Computational Science and Engineering Magazine, 4(2):39–43 (1997)
G. Hager and G. Wellein, Introduction to High Performance Computing for Scientists and Engineers, Chapman and Hall (2010)
A.K. Hartmann, Practical Guide to Computer Simulations, World Scientific (2009)
Journal Computational Methods in Science and Technology (open access), Polish Academy of Sciences
Journal Computational Science and Discovery, Institute of Physics
R.H. Landau, C.C. Bordeianu, and M. Jose Paez, A Survey of Computational Physics: Introductory Computational Science, Princeton University Press (2008)


== External links ==
John von Neumann-Institut for Computing (NIC) at Juelich (Germany)
The National Center for Computational Science at Oak Ridge National Laboratory
Educational Materials for Undergraduate Computational Studies
Computational Science at the National Laboratories
Bachelor in Computational Science, University of Medellin, Colombia, South America
Simulation Optimization Systems (SOS) Research Laboratory, McMaster University, Hamilton, ON"
42,Gödel Prize,643342,23764,"The Gödel Prize is an annual prize for outstanding papers in the area of theoretical computer science, given jointly by European Association for Theoretical Computer Science (EATCS) and the Association for Computing Machinery Special Interest Group on Algorithms and Computational Theory (ACM SIGACT). The award is named in honor of Kurt Gödel. Gödel's connection to theoretical computer science is that he was the first to mention the ""P versus NP"" question, in a 1956 letter to John von Neumann in which Gödel asked whether a certain NP-complete problem could be solved in quadratic or linear time.
The Gödel Prize has been awarded since 1993. The prize is awarded either at STOC (ACM Symposium on Theory of Computing, one of the main North American conferences in theoretical computer science) or ICALP (International Colloquium on Automata, Languages and Programming, one of the main European conferences in the field). To be eligible for the prize, a paper must be published in a refereed journal within the last 14 (formerly 7) years. The prize includes a reward of US$5000.
The winner of the Prize is selected by a committee of six members. The EATCS President and the SIGACT Chair each appoint three members to the committee, to serve staggered three-year terms. The committee is chaired alternately by representatives of EATCS and SIGACT.


== Recipients ==


== Winning papers ==


== References ==
Prize website with list of winners"
43,Mexican International Conference on Artificial Intelligence,36494971,23605,"The Mexican International Conference on Artificial Intelligence (MICAI) is the name of an annual conference covering all areas of Artificial Intelligence (AI), held in Mexico. The first MICAI conference was held in 2000. The conference is attended every year by about two hundred of AI researchers and PhD students and 500−1000 local graduate students.


== Overview ==
MICAI is a high-level peer-reviewed international conference covering all areas of Artificial Intelligence. All editions of MICAI have been published in Springer Springer LNAI (N 1793, 2313, 2972, 3789, 4293, 4827, 5317, 5845, 6437–6438). Recent MICAI events (2006, 2007, 2008, 2009, and 2010) received over 300 submissions from over 40 countries each. The conference's scientific program includes keynote lectures, paper presentations, tutorials, panels, posters, and workshops. MICAI is organized by the Mexican Society for Artificial Intelligence (SMIA) in cooperation with various national institutions.
Their topics of interest include, but are not limited to: Applications of artificial intelligence, Automated theorem proving, Belief revision, Bioinformatics and Medical applications of artificial intelligence, Case-based reasoning, Common-sense reasoning, Computer vision and image processing, Constraint programming, Data mining, Expert systems and knowledge-based systems, Fuzzy logic, Genetic algorithms, Hybrid intelligent systems, Intelligent interfaces: multimedia, virtual reality, Intelligent organizations, Intelligent tutoring systems, Knowledge acquisition, Knowledge representation and knowledge management, Logic programming, Machine learning, Model-based reasoning, Multiagent systems and distributed artificial intelligence, Natural Language Processing, Neural Networks, Non-monotonic Reasoning, Ontologies, Pattern Recognition, Philosophical and methodological issues of artificial intelligence, Planning and scheduling, Qualitative reasoning, Robotics, Spatial and remporal reasoning, Uncertainty reasoning and probabilistic reasoning.


== Specific MICAI conferences ==
In the table below, the figures for the number of accepted papers and acceptance rate refer to the main proceedings volume and do not include supplemental proceedings volumes. The number of countries corresponds to submissions, not to accepted papers.


== Keynote speakers and program chairs ==
The following persons were honored by being selected by the organizers as keynote speakers or program chairs:


== Awards ==
The authors of the following papers received the Best Paper Award:


== See also ==
The list of computer science conferences contains other academic conferences in computer science.


== References ==


== External links ==
MICAI series website
Mexican Society for Artificial Intelligence (SMIA)"
44,Bentley–Ottmann algorithm,16329810,23334,"In computational geometry, the Bentley–Ottmann algorithm is a sweep line algorithm for listing all crossings in a set of line segments, i.e. it finds the intersection points (or, simply, intersections) of line segments. It extends the Shamos–Hoey algorithm, a similar previous algorithm for testing whether or not a set of line segments has any crossings. For an input consisting of 
  
    
      
        n
      
    
    {\displaystyle n}
   line segments with 
  
    
      
        k
      
    
    {\displaystyle k}
   crossings (or intersections), the Bentley–Ottmann algorithm takes time 
  
    
      
        
          
            O
          
        
        (
        (
        n
        +
        k
        )
        log
        ⁡
        n
        )
      
    
    {\displaystyle {\mathcal {O}}((n+k)\log n)}
  . In cases where 
  
    
      
        k
        =
        
          
            o
          
        
        
          (
          
            
              
                n
                
                  2
                
              
              
                log
                ⁡
                n
              
            
          
          )
        
      
    
    {\displaystyle k={\mathcal {o}}\left({\frac {n^{2}}{\log n}}\right)}
  , this is an improvement on a naïve algorithm that tests every pair of segments, which takes 
  
    
      
        Θ
        (
        
          n
          
            2
          
        
        )
      
    
    {\displaystyle \Theta (n^{2})}
  .
The algorithm was initially developed by Jon Bentley and Thomas Ottmann (1979); it is described in more detail in the textbooks Preparata & Shamos (1985), O'Rourke (1998), and de Berg et al. (2000). Although asymptotically faster algorithms are now known, the Bentley–Ottmann algorithm remains a practical choice due to its simplicity and low memory requirements.


== Overall strategy ==
The main idea of the Bentley–Ottmann algorithm is to use a sweep line approach, in which a vertical line L moves from left to right (or, e.g., from top to bottom) across the plane, intersecting the input line segments in sequence as it moves. The algorithm is described most easily in its general position, meaning:
No two line segment endpoints or crossings have the same x-coordinate
No line segment endpoint lies upon another line segment
No three line segments intersect at a single point.
In such a case, L will always intersect the input line segments in a set of points whose vertical ordering changes only at a finite set of discrete events. Thus, the continuous motion of L can be broken down into a finite sequence of steps, and simulated by an algorithm that runs in a finite amount of time.
There are two types of events that may happen during the course of this simulation. When L sweeps across an endpoint of a line segment s, the intersection of L with s is added to or removed from the vertically ordered set of intersection points. These events are easy to predict, as the endpoints are known already from the input to the algorithm. The remaining events occur when L sweeps across a crossing between (or intersection of) two line segments s and t. These events may also be predicted from the fact that, just prior to the event, the points of intersection of L with s and t are adjacent in the vertical ordering of the intersection points.
The Bentley–Ottmann algorithm itself maintains data structures representing the current vertical ordering of the intersection points of the sweep line with the input line segments, and a collection of potential future events formed by adjacent pairs of intersection points. It processes each event in turn, updating its data structures to represent the new set of intersection points.


== Data structures ==
In order to efficiently maintain the intersection points of the sweep line L with the input line segments and the sequence of future events, the Bentley–Ottmann algorithm maintains two data structures:
A binary search tree (the ""sweep line status tree""), containing the set of input line segments that cross L, ordered by the y-coordinates of the points where these segments cross L. The crossing points themselves are not represented explicitly in the binary search tree. The Bentley–Ottmann algorithm will insert a new segment s into this data structure when the sweep line L crosses the left endpoint p of this segment (i.e. the endpoint of the segment with the smallest x-coordinate, provided the sweep line L starts from the left, as explained above in this article). The correct position of segment s in the binary search tree may be determined by a binary search, each step of which tests whether p is above or below some other segment that is crossed by L. Thus, an insertion may be performed in logarithmic time. The Bentley–Ottmann algorithm will also delete segments from the binary search tree, and use the binary search tree to determine the segments that are immediately above or below other segments; these operations may be performed using only the tree structure itself without reference to the underlying geometry of the segments.
A priority queue (the ""event queue""), used to maintain a sequence of potential future events in the Bentley–Ottmann algorithm. Each event is associated with a point p in the plane, either a segment endpoint or a crossing point, and the event happens when line L sweeps over p. Thus, the events may be prioritized by the x-coordinates of the points associated with each event. In the Bentley–Ottmann algorithm, the potential future events consist of line segment endpoints that have not yet been swept over, and the points of intersection of pairs of lines containing pairs of segments that are immediately above or below each other.
The algorithm does not need to maintain explicitly a representation of the sweep line L or its position in the plane. Rather, the position of L is represented indirectly: it is the vertical line through the point associated with the most recently processed event.
The binary search tree may be any balanced binary search tree data structure, such as a red-black tree; all that is required is that insertions, deletions, and searches take logarithmic time. Similarly, the priority queue may be a binary heap or any other logarithmic-time priority queue; more sophisticated priority queues such as a Fibonacci heap are not necessary. Note that the space complexity of the priority queue depends on the data structure used to implement it.


== Detailed algorithm ==
The Bentley–Ottmann algorithm performs the following steps.
Initialize a priority queue Q of potential future events, each associated with a point in the plane and prioritized by the x-coordinate of the point. So, initially, Q contains an event for each of the endpoints of the input segments.
Initialize a self-balancing binary search tree T of the line segments that cross the sweep line L, ordered by the y-coordinates of the crossing points. Initially, T is empty. (Even though the line sweep L is not explicitly represented, it may be helpful to imagine it as a vertical line which, initially, is at the left of all input segments.)
While Q is nonempty, find and remove the event from Q associated with a point p with minimum x-coordinate. Determine what type of event this is and process it according to the following case analysis:
If p is the left endpoint of a line segment s, insert s into T. Find the segments r and t that are respectively immediately below and above s in T (if they exist); if their crossing forms a potential future event in the event queue, remove it. If s crosses r or t, add those crossing points as potential future events in the event queue.
If p is the right endpoint of a line segment s, remove s from T. Find the segments r and t that (prior to the removal of s) were respectively immediately above and below it in T (if they exist). If r and t cross, add that crossing point as a potential future event in the event queue.
If p is the crossing point of two segments s and t (with s below t to the left of the crossing), swap the positions of s and t in T. Find the segments r and u (if they exist) that are immediately below and above t and s respectively (after the swap). Remove any crossing points rs and tu from the event queue, and, if r and t cross or s and u cross, add those crossing points to the event queue.


== Analysis ==
The algorithm processes one event per segment endpoint or crossing point, in the sorted order of the 
  
    
      
        x
      
    
    {\displaystyle x}
  -coordinates of these points, as may be proven by induction. This follows because, once the 
  
    
      
        i
      
    
    {\displaystyle i}
  th event has been processed, the next event (if it is a crossing point) must be a crossing of two segments that are adjacent in the ordering of the segments represented by 
  
    
      
        T
      
    
    {\displaystyle T}
  , and because the algorithm maintains all crossings between adjacent segments as potential future events in the event queue; therefore, the correct next event will always be present in the event queue. As a consequence, it correctly finds all crossings of input line segments, the problem it was designed to solve.
The Bentley–Ottmann algorithm processes a sequence of 
  
    
      
        2
        n
        +
        k
      
    
    {\displaystyle 2n+k}
   events, where 
  
    
      
        n
      
    
    {\displaystyle n}
   denotes the number of input line segments and 
  
    
      
        k
      
    
    {\displaystyle k}
   denotes the number of crossings. Each event is processed by a constant number of operations in the binary search tree and the event queue, and (because it contains only segment endpoints and crossings between adjacent segments) the event queue never contains more than 
  
    
      
        3
        n
      
    
    {\displaystyle 3n}
   events. All operations take time 
  
    
      
        
          
            O
          
        
        (
        log
        ⁡
        n
        )
      
    
    {\displaystyle {\mathcal {O}}(\log n)}
  . Hence the total time for the algorithm is 
  
    
      
        
          
            O
          
        
        (
        (
        n
        +
        k
        )
        log
        ⁡
        n
        )
      
    
    {\displaystyle {\mathcal {O}}((n+k)\log n)}
  .
If the crossings found by the algorithm do not need to be stored once they have been found, the space used by the algorithm at any point in time is 
  
    
      
        
          
            O
          
        
        (
        n
        )
      
    
    {\displaystyle {\mathcal {O}}(n)}
  : each of the 
  
    
      
        n
      
    
    {\displaystyle n}
   input line segments corresponds to at most one node of the binary search tree T, and as stated above the event queue contains at most 
  
    
      
        3
        n
      
    
    {\displaystyle 3n}
   elements. This space bound is due to Brown (1981); the original version of the algorithm was slightly different (it did not remove crossing events from 
  
    
      
        Q
      
    
    {\displaystyle Q}
   when some other event causes the two crossing segments to become non-adjacent) causing it to use more space.
Chen & Chan (2003) described a highly space-efficient version of the Bentley–Ottmann algorithm that encodes most of its information in the ordering of the segments in an array representing the input, requiring only 
  
    
      
        
          
            O
          
        
        (
        
          log
          
            2
          
        
        ⁡
        n
        )
      
    
    {\displaystyle {\mathcal {O}}(\log ^{2}n)}
   additional memory cells. However, in order to access the encoded information, the algorithm is slowed by a logarithmic factor.


== Special position ==
The algorithm description above assumes that line segments are not vertical, that line segment endpoints do not lie on other line segments, that crossings are formed by only two line segments, and that no two event points have the same x-coordinate. In other words, it doesn't take into account corner cases, i.e. it assumes general position of the endpoints of the input segments. However, these general position assumptions are not reasonable for most applications of line segment intersection. Bentley & Ottmann (1979) suggested perturbing the input slightly to avoid these kinds of numerical coincidences, but did not describe in detail how to perform these perturbations. de Berg et al. (2000) describe in more detail the following measures for handling special-position inputs:
Break ties between event points with the same x-coordinate by using the y-coordinate. Events with different y-coordinates are handled as before. This modification handles both the problem of multiple event points with the same x-coordinate, and the problem of vertical line segments: the left endpoint of a vertical segment is defined to be the one with the lower y-coordinate, and the steps needed to process such a segment are essentially the same as those needed to process a non-vertical segment with a very high slope.
Define a line segment to be a closed set, containing its endpoints. Therefore, two line segments that share an endpoint, or a line segment that contains an endpoint of another segment, both count as an intersection of two line segments.
When multiple line segments intersect at the same point, create and process a single event point for that intersection. The updates to the binary search tree caused by this event may involve removing any line segments for which this is the right endpoint, inserting new line segments for which this is the left endpoint, and reversing the order of the remaining line segments containing this event point. The output from the version of the algorithm described by de Berg et al. (2000) consists of the set of intersection points of line segments, labeled by the segments they belong to, rather than the set of pairs of line segments that intersect.
A similar approach to degeneracies was used in the LEDA implementation of the Bentley–Ottmann algorithm.


== Numerical precision issues ==
For the correctness of the algorithm, it is necessary to determine without approximation the above-below relations between a line segment endpoint and other line segments, and to correctly prioritize different event points. For this reason it is standard to use integer coordinates for the endpoints of the input line segments, and to represent the rational number coordinates of the intersection points of two segments exactly, using arbitrary-precision arithmetic. However, it may be possible to speed up the calculations and comparisons of these coordinates by using floating point calculations and testing whether the values calculated in this way are sufficiently far from zero that they may be used without any possibility of error. The exact arithmetic calculations required by a naïve implementation of the Bentley–Ottmann algorithm may require five times as many bits of precision as the input coordinates, but Boissonat & Preparata (2000) describe modifications to the algorithm that reduce the needed amount of precision to twice the number of bits as the input coordinates.


== Faster algorithms ==
The O(n log n) part of the time bound for the Bentley–Ottmann algorithm is necessary, as there are matching lower bounds for the problem of detecting intersecting line segments in algebraic decision tree models of computation. However, the dependence on k, the number of crossings, can be improved. Clarkson (1988) and Mulmuley (1988) both provided randomized algorithms for constructing the planar graph whose vertices are endpoints and crossings of line segments, and whose edges are the portions of the segments connecting these vertices, in expected time O(n log n + k), and this problem of arrangement construction was solved deterministically in the same O(n log n + k) time bound by Chazelle & Edelsbrunner (1992). However, constructing this arrangement as a whole requires space O(n + k), greater than the O(n) space bound of the Bentley–Ottmann algorithm; Balaban (1995) described a different algorithm that lists all intersections in time O(n log n + k) and space O(n).
If the input line segments and their endpoints form the edges and vertices of a connected graph (possibly with crossings), the O(n log n) part of the time bound for the Bentley–Ottmann algorithm may also be reduced. As Clarkson, Cole & Tarjan (1992) show, in this case there is a randomized algorithm for solving the problem in expected time O(n log* n + k), where log* denotes the iterated logarithm, a function much more slowly growing than the logarithm. A closely related randomized algorithm of Eppstein, Goodrich & Strash (2009) solves the same problem in time O(n + k log(i)n) for any constant i, where log(i) denotes the function obtained by iterating the logarithm function i times. The first of these algorithms takes linear time whenever k is larger than n by a log(i)n factor, for any constant i, while the second algorithm takes linear time whenever k is smaller than n by a log(i)n factor. Both of these algorithms involve applying the Bentley–Ottmann algorithm to small random samples of the input.


== Notes ==


== References ==
Balaban, I. J. (1995), ""An optimal algorithm for finding segments intersections"", Proc. 11th ACM Symp. Computational Geometry, pp. 211–219, doi:10.1145/220279.220302 .
Bartuschka, U.; Mehlhorn, K.; Näher, S. (1997), ""A robust and efficient implementation of a sweep line algorithm for the straight line segment intersection problem"", in Italiano, G. F.; Orlando, S., Proc. Worksh. Algorithm Engineering .
Bentley, J. L.; Ottmann, T. A. (1979), ""Algorithms for reporting and counting geometric intersections"", IEEE Transactions on Computers, C–28 (9): 643–647, doi:10.1109/TC.1979.1675432 .
de Berg, Mark; van Kreveld, Marc; Overmars, Mark; Schwarzkopf, Otfried (2000), ""Chapter 2: Line segment intersection"", Computational Geometry (2nd ed.), Springer-Verlag, pp. 19–44, ISBN 978-3-540-65620-3 .
Boissonat, J.-D.; Preparata, F. P. (2000), ""Robust plane sweep for intersecting segments"" (PDF), SIAM Journal on Computing, 29 (5): 1401–1421, doi:10.1137/S0097539797329373 .
Brown, K. Q. (1981), ""Comments on ""Algorithms for Reporting and Counting Geometric Intersections"""", IEEE Transactions on Computers, C–30 (2): 147, doi:10.1109/tc.1981.6312179 .
Chazelle, Bernard; Edelsbrunner, Herbert (1992), ""An optimal algorithm for intersecting line segments in the plane"", Journal of the ACM, 39 (1): 1–54, doi:10.1145/147508.147511 .
Chen, E. Y.; Chan, T. M. (2003), ""A space-efficient algorithm for segment intersection"", Proc. 15th Canadian Conference on Computational Geometry (PDF) .
Clarkson, K. L. (1988), ""Applications of random sampling in computational geometry, II"", Proc. 4th ACM Symp. Computational Geometry, pp. 1–11, doi:10.1145/73393.73394 .
Clarkson, K. L.; Cole, R.; Tarjan, R. E. (1992), ""Randomized parallel algorithms for trapezoidal diagrams"", International Journal of Computational Geometry and Applications, 2 (2): 117–133, doi:10.1142/S0218195992000081 . Corrigendum, 2 (3): 341–343.
Eppstein, D.; Goodrich, M.; Strash, D. (2009), ""Linear-time algorithms for geometric graphs with sublinearly many crossings"", Proc. 20th ACM-SIAM Symp. Discrete Algorithms (SODA 2009), pp. 150–159, arXiv:0812.0893  .
Mulmuley, K. (1988), ""A fast planar partition algorithm, I"", Proc. 29th IEEE Symp. Foundations of Computer Science (FOCS 1988), pp. 580–589, doi:10.1109/SFCS.1988.21974 .
O'Rourke, J. (1998), ""Section 7.7: Intersection of segments"", Computational Geometry in C (2nd ed.), Cambridge University Press, pp. 263–265, ISBN 978-0-521-64976-6 .
Preparata, F. P.; Shamos, M. I. (1985), ""Section 7.2.3: Intersection of line segments"", Computational Geometry: An Introduction, Springer-Verlag, pp. 278–287 .
Pach, J.; Sharir, M. (1991), ""On vertical visibility in arrangements of segments and the queue size in the Bentley–Ottmann line sweeping algorithm"", SIAM Journal on Computing, 20 (3): 460–470, doi:10.1137/0220029, MR 1094525 .
Shamos, M. I.; Hoey, Dan (1976), ""Geometric intersection problems"", 17th IEEE Conf. Foundations of Computer Science (FOCS 1976), pp. 208–215, doi:10.1109/SFCS.1976.16 .


== External links ==
Smid, Michiel (2003), Computing intersections in a set of line segments: the Bentley–Ottmann algorithm (PDF) ."
45,British Machine Vision Conference,42973483,21498,"The British Machine Vision Conference (BMVC) is the British Machine Vision Association (BMVA) annual conference on machine vision, image processing, and pattern recognition. It is one of the major international conferences on computer vision and related areas, held in UK. Particularly, BMVC is ranked as A2 by Qualis, and B by ERA. The upcoming 27th BMVC will be hosted by Imperial College London in September 2017.
BMVC is a successor of the older British Alvey Vision Conference (AVC), which had run in years 1985 (University of Sussex), 1987 (University of Cambridge), 1988 (University of Manchester) and 1989 (University of Reading). The British Machine Vision Conference has replaced AVC in 1990, when BMVA was founded. Despite starting as a national conference, it is now a prestigious major international venue with high level of foreign participation (in 2013, 84% of accepted papers were completely from outside the UK and another 4% with mixed authorships) and high stress on quality of publications (in 2013, the acceptance rate was only 30%). BMVC is a relatively small conference, with the number of accepted publications (and therefore number of talks and posters) around 100.


== Usual programme ==
BMVC is a single-track conference held usually over the course of one week in early September. On Monday, there are usually one or several tutorials, followed by the main conference in the following three days. A typical conference day consists of a keynote talk, two or three oral sessions and a poster session. Thursday's programme tends to be shorter. The conference usually includes a banquet and a reception. The main conference is followed by a one-day student workshop on Friday, which provides an opportunity for doctoral students to present their work and interact with their peers.


== Awards ==
At BMVC, there are several awards given. Besides the Best Scientific Paper Award (formerly known as Science Prize), there is Best Industrial Paper Award (formerly known as Industry Prize), Best Poster Award and others. The awards recipients are tabulated below. Additionally, other BMVA prizes such as BMVA Distinguished Fellowship or Sullivan Prize are awarded during BMVC.
Other awards
1998 Demonstration Prize. Active object recognition in parametric eigenspace. M. Prantl.
2000 Demonstration Prize. A hierarchical model of dynamics for tracking people with a single video camera. I.A. Karaulova, P.M. Hall and A.D. Marshall.
2001 Demonstration Prize. Video image enhancement for terrestrial, aerial and underwater environments. J. Oakley
2001 Best Model Based Vision Paper. An Information Theoretic Approach to Statistical Shape Modelling. R.H. Davies, T.F. Cootes, C.J. Twining and C.J. Taylor.
2001 Presentation Prize. Robust Registration of 2D and 3D Point Sets. A. Fitzgibbon.
2002 Demonstration Prize. Real time robust template matching. F. Jurie and M. Dhome.
2002 Best Model Based Vision Paper. Real time gesture recognition using deterministic boosting. R. Lockton and A. Fitzgibbon.
2002 Work that most deserves help with exploitation. Orientation correlation. A.J. Fitch, A. Kadyrov, W.J. Christmas and J. Kittler.
2003 Demonstration Prize. Visual golf club tracking for enhanced swing analysis. N. Gehrig, V. Lepetit and P. Fua.
2003 Best Model Based Vision Paper. Modelling talking head behaviour. C.A. Hack and C.J. Taylor.
2004 Demonstration Prize. Interactions between hand and wearable camera in 2D and 3D environments. A. Davison.
2004 Best Model Based Vision Paper. A Bayesian Occlusion Model for Sequential Object Matching. T. Tamminen and J. Lampinen.
2007 Best Security Paper Prize. Gender Classification using Shape from Shading. J. Wu, W.A.P. Smith and E.R. Hancock.
2008 Best Security Paper Prize. Crowd Detection from Still Images. O. Arandjelovic.
2008 Highly Commended Reviewers. J.-M. Geusebroek, B. Leibe, A. Shahrokni, J. Sivic, J. Starck.
2010 Best Student Paper Prize. Motion Coherent Tracking with Multi-label MRF optimization. D. Tsai, M. Flagg and J. Rehg.
2010 Best Supplementary Material Prize. Manifold Learning for ToF-based Human Body Tracking and Activity Recognition. L. Schwarz, D. Mateus, V. Castaneda and N. Navab.
2011 Best Impact Paper Prize. Branch and rank: non-linear object detection. A. Lehmann, P. Gehler and L. Van Gool.
2011 Best Supplementary Material Prize. Skeletal graph based human pose estimation in real-time. M. Straka, S. Hauswiesner, M. Rüther and H. Bischof.
2011 Student Workshop Prize. Model Constraints for Non-Rigid Structure from Motion. L. Tao, B. Matuszewski and S. Mein.
2012 Best Impact Paper Prize. PMBP: PatchMatch Belief Propagation for Correspondence Field Estimation. F. Besse, C. Rother, A. Fitzgibbon and J. Kautz.
2012 Mark Everingham Prize for Rigorous Evaluation. Tom-vs.-Pete Classifiers and Identity-Preserving Alignment for Face Verification. T. Berg and P. Belhumeu.
2012 Best Demonstration Prize. Online Feedback for Structure-from-Motion Image Acquisition. C. Hoppe, M. Klopschitz, M. Rumpler, A. Wendel, S. Kluckner, H. Bischof and G. Reitmayr.
2012 Best Video Prize. Automatic and Efficient Long Term Arm and Hand Tracking for Continuous Sign Language TV Broadcasts. Tomas Pfister, J. Charles, M. Everingham and A. Zisserman.
2013 Maria Petrou Prize for Invariance in Computer Vision. The Complete Rank Transform: A Tool for Accurate and Morphologically Invariant Matching of Structures. O. Demetz, D. Hafner and J. Weickert.
2014 Best Student Workshop Paper Prize. Gong Interactive Shadow Removal and Ground Truth for Variable Scene Categories. H. Gong and D. Cosker.


== See also ==
BMVA
BMVA Summer School
ICCV
CVPR
ECCV


== References =="
46,Integer factorization,15491,21146,"In number theory, integer factorization is the decomposition of a composite number into a product of smaller integers. If these integers are further restricted to prime numbers, the process is called prime factorization.
When the numbers are sufficiently large, no efficient, non-quantum integer factorization algorithm is known. An effort by several researchers, concluded in 2009, to factor a 232-digit number (RSA-768) utilizing hundreds of machines took two years and the researchers estimated that a 1024-bit RSA modulus would take about a thousand times as long. However, it has not been proven that no efficient algorithm exists. The presumed difficulty of this problem is at the heart of widely used algorithms in cryptography such as RSA. Many areas of mathematics and computer science have been brought to bear on the problem, including elliptic curves, algebraic number theory, and quantum computing.
Not all numbers of a given length are equally hard to factor. The hardest instances of these problems (for currently known techniques) are semiprimes, the product of two prime numbers. When they are both large, for instance more than two thousand bits long, randomly chosen, and about the same size (but not too close, e.g., to avoid efficient factorization by Fermat's factorization method), even the fastest prime factorization algorithms on the fastest computers can take enough time to make the search impractical; that is, as the number of digits of the primes being factored increases, the number of operations required to perform the factorization on any computer increases drastically.
Many cryptographic protocols are based on the difficulty of factoring large composite integers or a related problem—for example, the RSA problem. An algorithm that efficiently factors an arbitrary integer would render RSA-based public-key cryptography insecure.


== Prime decomposition ==

By the fundamental theorem of arithmetic, every positive integer has a unique prime factorization. (By convention 1 is the empty product.) If the integer is prime then it can be recognized as such in polynomial time. If composite however, the theorem gives no insight into how to obtain the factors.
Given a general algorithm for integer factorization, any integer can be factored down to its constituent prime factors simply by repeated application of this algorithm. The situation is more complicated with special-purpose factorization algorithms, whose benefits may not be realized as well or even at all with the factors produced during decomposition. For example, if N = 10 × p × q where p < q are very large primes, trial division will quickly produce the factors 2 and 5 but will take p divisions to find the next factor. As a contrasting example, if N is the product of the primes 13729, 1372933, and 18848997161, where 13729 × 1372933 = 18848997157, Fermat's factorization method will start out with a = ⌈√N⌉ = 18848997159 which immediately yields b = √a2 − N = √4 = 2 and hence the factors a − b = 18848997157 and a + b = 18848997161. While these are easily recognized as respectively composite and prime, Fermat's method will take much longer to factorize the composite one because the starting value of ⌈√18848997157⌉ = 137292 for a is nowhere near 1372933.


== Current state of the art ==

Among the b-bit numbers, the most difficult to factor in practice using existing algorithms are those that are products of two primes of similar size. For this reason, these are the integers used in cryptographic applications. The largest such semiprime yet factored was RSA-768, a 768-bit number with 232 decimal digits, on December 12, 2009. This factorization was a collaboration of several research institutions, spanning two years and taking the equivalent of almost 2000 years of computing on a single-core 2.2 GHz AMD Opteron. Like all recent factorization records, this factorization was completed with a highly optimized implementation of the general number field sieve run on hundreds of machines.


=== Difficulty and complexity ===
No algorithm has been published that can factor all integers in polynomial time, i.e., that can factor b-bit numbers in time O(bk) for some constant k. Neither the existence nor non-existence of such algorithms has been proved, but it is generally suspected that they do not exist and hence that the problem is not in class P. The problem is clearly in class NP but has not been proved to be or not be NP-complete. It is generally suspected not to be NP-complete.
There are published algorithms that are faster than O((1+ε)b) for all positive ε, i.e., sub-exponential. The best published asymptotic running time is for the general number field sieve (GNFS) algorithm, which, for a b-bit number n, is:

  
    
      
        O
        
          (
          
            exp
            ⁡
            
              
                
                  
                    
                      64
                      9
                    
                  
                  b
                  (
                  log
                  ⁡
                  b
                  
                    )
                    
                      2
                    
                  
                
                
                  3
                
              
            
          
          )
        
        .
      
    
    {\displaystyle O\left(\exp {\sqrt[{3}]{{\frac {64}{9}}b(\log b)^{2}}}\right).}
  
For current computers, GNFS is the best published algorithm for large n (more than about 100 digits). For a quantum computer, however, Peter Shor discovered an algorithm in 1994 that solves it in polynomial time. This will have significant implications for cryptography if quantum computation becomes scalable. Shor's algorithm takes only O(b3) time and O(b) space on b-bit number inputs. In 2001, the first seven-qubit quantum computer became the first to run Shor's algorithm. It factored the number 15.
When discussing what complexity classes the integer factorization problem falls into, it is necessary to distinguish two slightly different versions of the problem:
The function problem version: given an integer N, find an integer d with 1 < d < N that divides N (or conclude that N is prime). This problem is trivially in FNP and it's not known whether it lies in FP or not. This is the version solved by practical implementations.
The decision problem version: given an integer N and an integer M with 1 < M < N, does N have a factor d with 1 < d ≤ M? This version is useful because most well studied complexity classes are defined as classes of decision problems, not function problems.
For √N ≤ M < N, the decision problem is equivalent to asking if N is not prime.
An algorithm for either version provides one for the other. Repeated application of the function problem (applied to d and N/d, and their factors, if needed) will eventually provide either a factor of N no larger than M or a factorization into primes all greater than M. All known algorithms for the decision problem work in this way. Hence it is only of theoretical interest that, with at most log N queries using an algorithm for the decision problem, one would isolate a factor of N (or prove it prime) by binary search.
It is not known exactly which complexity classes contain the decision version of the integer factorization problem. It is known to be in both NP and co-NP. This is because both YES and NO answers can be verified in polynomial time. An answer of YES can be certified by exhibiting a factorization N = d(N/d) with d ≤ M. An answer of NO can be certified by exhibiting the factorization of N into distinct primes, all larger than M. We can verify their primality using the AKS primality test and that their product is N by multiplication. The fundamental theorem of arithmetic guarantees that there is only one possible string that will be accepted (providing the factors are required to be listed in order), which shows that the problem is in both UP and co-UP. It is known to be in BQP because of Shor's algorithm. It is suspected to be outside of all three of the complexity classes P, NP-complete, and co-NP-complete. It is therefore a candidate for the NP-intermediate complexity class. If it could be proved that it is in either NP-Complete or co-NP-Complete, that would imply NP = co-NP. That would be a very surprising result, and therefore integer factorization is widely suspected to be outside both of those classes. Many people have tried to find classical polynomial-time algorithms for it and failed, and therefore it is widely suspected to be outside P.
In contrast, the decision problem ""is N a composite number?"" (or equivalently: ""is N a prime number?"") appears to be much easier than the problem of actually finding the factors of N. Specifically, the former can be solved in polynomial time (in the number n of digits of N) with the AKS primality test. In addition, there are a number of probabilistic algorithms that can test primality very quickly in practice if one is willing to accept the vanishingly small possibility of error. The ease of primality testing is a crucial part of the RSA algorithm, as it is necessary to find large prime numbers to start with.


== Factoring algorithms ==


=== Special-purpose ===
A special-purpose factoring algorithm's running time depends on the properties of the number to be factored or on one of its unknown factors: size, special form, etc. Exactly what the running time depends on varies between algorithms.
An important subclass of special-purpose factoring algorithms is the Category 1 or First Category algorithms, whose running time depends on the size of smallest prime factor. Given an integer of unknown form, these methods are usually applied before general-purpose methods to remove small factors. For example, trial division is a Category 1 algorithm.
Trial division
Wheel factorization
Pollard's rho algorithm
Algebraic-group factorisation algorithms, among which are Pollard's p − 1 algorithm, Williams' p + 1 algorithm, and Lenstra elliptic curve factorization
Fermat's factorization method
Euler's factorization method
Special number field sieve


=== General-purpose ===
A general-purpose factoring algorithm, also known as a Category 2, Second Category, or Kraitchik family algorithm (after Maurice Kraitchik), has a running time which depends solely on the size of the integer to be factored. This is the type of algorithm used to factor RSA numbers. Most general-purpose factoring algorithms are based on the congruence of squares method.
Dixon's algorithm
Continued fraction factorization (CFRAC)
Quadratic sieve
Rational sieve
General number field sieve
Shanks' square forms factorization (SQUFOF)


=== Other notable algorithms ===
Shor's algorithm, for quantum computers


== Heuristic running time ==
In number theory, there are many integer factoring algorithms that heuristically have expected running time

  
    
      
        
          L
          
            n
          
        
        
          [
          
            
              
                
                  1
                  2
                
              
            
            ,
            1
            +
            o
            (
            1
            )
          
          ]
        
        =
        
          e
          
            (
            1
            +
            o
            (
            1
            )
            )
            
              
                (
                log
                ⁡
                n
                )
                (
                log
                ⁡
                log
                ⁡
                n
                )
              
            
          
        
      
    
    {\displaystyle L_{n}\left[{\tfrac {1}{2}},1+o(1)\right]=e^{(1+o(1)){\sqrt {(\log n)(\log \log n)}}}}
  
in big O and L-notation. Some examples of those algorithms are the elliptic curve method and the quadratic sieve. Another such algorithm is the class group relations method proposed by Schnorr, Seysen, and Lenstra, that is proved under the assumption of the Generalized Riemann Hypothesis (GRH).


== Rigorous running time ==
The Schnorr-Seysen-Lenstra probabilistic algorithm has been rigorously proven by Lenstra and Pomerance to have expected running time 
  
    
      
        
          L
          
            n
          
        
        
          [
          
            
              
                
                  1
                  2
                
              
            
            ,
            1
            +
            o
            (
            1
            )
          
          ]
        
      
    
    {\displaystyle L_{n}\left[{\tfrac {1}{2}},1+o(1)\right]}
   by replacing the GRH assumption with the use of multipliers. The algorithm uses the class group of positive binary quadratic forms of discriminant Δ denoted by GΔ. GΔ is the set of triples of integers (a, b, c) in which those integers are relative prime.


=== Schnorr-Seysen-Lenstra Algorithm ===
Given an integer n that will be factored, where n is an odd positive integer greater than a certain constant. In this factoring algorithm the discriminant Δ is chosen as a multiple of n, Δ = −dn, where d is some positive multiplier. The algorithm expects that for one d there exist enough smooth forms in GΔ. Lenstra and Pomerance show that the choice of d can be restricted to a small set to guarantee the smoothness result.
Denote by PΔ the set of all primes q with Kronecker symbol 
  
    
      
        
          (
          
            
              
                Δ
                q
              
            
          
          )
        
        =
        1
      
    
    {\displaystyle \left({\tfrac {\Delta }{q}}\right)=1}
  . By constructing a set of generators of GΔ and prime forms fq of GΔ with q in PΔ a sequence of relations between the set of generators and fq are produced. The size of q can be bounded by 
  
    
      
        
          c
          
            0
          
        
        (
        log
        ⁡
        
          |
        
        Δ
        
          |
        
        
          )
          
            2
          
        
      
    
    {\displaystyle c_{0}(\log |\Delta |)^{2}}
   for some constant 
  
    
      
        
          c
          
            0
          
        
      
    
    {\displaystyle c_{0}}
  .
The relation that will be used is a relation between the product of powers that is equal to the neutral element of GΔ. These relations will be used to construct a so-called ambiguous form of GΔ, which is an element of GΔ of order dividing 2. By calculating the corresponding factorization of Δ and by taking a gcd, this ambiguous form provides the complete prime factorization of n. This algorithm has these main steps:
Let n be the number to be factored.
Let Δ be a negative integer with Δ = −dn, where d is a multiplier and Δ is the negative discriminant of some quadratic form.
Take the t first primes 
  
    
      
        
          p
          
            1
          
        
        =
        2
        ,
        
          p
          
            2
          
        
        =
        3
        ,
        
          p
          
            3
          
        
        =
        5
        ,
        …
        ,
        
          p
          
            t
          
        
      
    
    {\displaystyle p_{1}=2,p_{2}=3,p_{3}=5,\dots ,p_{t}}
  , for some 
  
    
      
        t
        ∈
        
          
            N
          
        
      
    
    {\displaystyle t\in {\mathbb {N} }}
  .
Let 
  
    
      
        
          f
          
            q
          
        
      
    
    {\displaystyle f_{q}}
   be a random prime form of GΔ with 
  
    
      
        
          (
          
            
              
                Δ
                q
              
            
          
          )
        
        =
        1
      
    
    {\displaystyle \left({\tfrac {\Delta }{q}}\right)=1}
  .
Find a generating set X of GΔ
Collect a sequence of relations between set X and {fq : q ∈ PΔ} satisfying: 
  
    
      
        
          (
          
            
              ∏
              
                x
                ∈
                
                  X
                  

                  
                
              
            
            
              x
              
                r
                (
                x
                )
              
            
          
          )
        
        .
        
          (
          
            
              ∏
              
                q
                ∈
                
                  P
                  
                    Δ
                  
                
              
            
            
              f
              
                q
              
              
                t
                (
                q
                )
              
            
          
          )
        
        =
        1
      
    
    {\displaystyle \left(\prod _{x\in X_{}}x^{r(x)}\right).\left(\prod _{q\in P_{\Delta }}f_{q}^{t(q)}\right)=1}
  
Construct an ambiguous form 
  
    
      
        (
        a
        ,
        b
        ,
        c
        )
      
    
    {\displaystyle (a,b,c)}
   that is an element f ∈ GΔ of order dividing 2 to obtain a coprime factorization of the largest odd divisor of Δ in which 
  
    
      
        Δ
        =
        −
        4
        a
        c
        
           or 
        
        a
        (
        a
        −
        4
        c
        )
        
           or 
        
        (
        b
        −
        2
        a
        )
        (
        b
        +
        2
        a
        )
      
    
    {\displaystyle \Delta =-4ac{\text{ or }}a(a-4c){\text{ or }}(b-2a)(b+2a)}
  
If the ambiguous form provides a factorization of n then stop, otherwise find another ambiguous form until the factorization of n is found. In order to prevent useless ambiguous forms from generating, build up the 2-Sylow group Sll2(Δ) of G(Δ).
To obtain an algorithm for factoring any positive integer, it is necessary to add a few steps to this algorithm such as trial division, and the Jacobi sum test.


=== Expected running time ===
The algorithm as stated is a probabilistic algorithm as it makes random choices. Its expected running time is at most 
  
    
      
        
          L
          
            n
          
        
        
          [
          
            
              
                
                  1
                  2
                
              
            
            ,
            1
            +
            o
            (
            1
            )
          
          ]
        
      
    
    {\displaystyle L_{n}\left[{\tfrac {1}{2}},1+o(1)\right]}
  .


== See also ==
Canonical representation of a positive integer
Factorization
Multiplicative partition
Partition (number theory) – a way of writing a number as a sum of positive integers.


== Notes ==


== References ==
Richard Crandall and Carl Pomerance (2001). Prime Numbers: A Computational Perspective. Springer. ISBN 0-387-94777-9.  Chapter 5: Exponential Factoring Algorithms, pp. 191–226. Chapter 6: Subexponential Factoring Algorithms, pp. 227–284. Section 7.4: Elliptic curve method, pp. 301–313.
Donald Knuth. The Art of Computer Programming, Volume 2: Seminumerical Algorithms, Third Edition. Addison-Wesley, 1997. ISBN 0-201-89684-2. Section 4.5.4: Factoring into Primes, pp. 379–417.
Samuel S. Wagstaff, Jr. (2013). The Joy of Factoring. Providence, RI: American Mathematical Society. ISBN 978-1-4704-1048-3. .
Warren Jr., Henry S. (2013). Hacker's Delight (2 ed.). Addison Wesley - Pearson Education, Inc. ISBN 978-0-321-84268-8. 


== External links ==
msieve - SIQS and NFS - has helped complete some of the largest public factorizations known
Richard P. Brent, ""Recent Progress and Prospects for Integer Factorisation Algorithms"", Computing and Combinatorics"", 2000, pp. 3–22. download
Manindra Agrawal, Neeraj Kayal, Nitin Saxena, ""PRIMES is in P."" Annals of Mathematics 160(2): 781-793 (2004). August 2005 version PDF
Eric W. Weisstein, “RSA-640 Factored” MathWorld Headline News, November 8, 2005"
47,Computable topology,36075414,20748,"Computable topology is a discipline in mathematics that studies the topological and algebraic structure of computation. Computable topology is not to be confused with algorithmic or computational topology, which studies the application of computation to topology.


== Topology of lambda calculus ==
As shown by Alan Turing and Alonzo Church, the λ-calculus is strong enough to describe all mechanically computable functions (see Church–Turing thesis). Lambda-calculus is thus effectively a programming language, from which other languages can be built. For this reason when considering the topology of computation it is common to focus on the topology of λ-calculus. Note that this is not necessarily a complete description of the topology of computation, since functions which are equivalent in the Church-Turing sense may still have different topologies.
The topology of λ-calculus is the Scott topology, and when restricted to continuous functions the type free λ-calculus amounts to a topological space reliant on the tree topology. Both the Scott and Tree topologies exhibit continuity with respect to the binary operators of application ( f applied to a = fa ) and abstraction ((λx.t(x))a = t(a)) with a modular equivalence relation based on a congruency. The λ-algebra describing the algebraic structure of the lambda-calculus is found to be an extension of the combinatory algebra, with an element introduced to accommodate abstraction.
Type free λ-calculus treats functions as rules and does not differentiate functions and the objects which they are applied to, meaning λ-calculus is type free. A by-product of type free λ-calculus is an effective computability equivalent to general recursion and Turing machines. The set of λ-terms can be considered a functional topology in which a function space can be embedded, meaning λ mappings within the space X are such that λ:X → X. Introduced November 1969, Dana Scott's untyped set theoretic model constructed a proper topology for any λ-calculus model whose function space is limited to continuous functions. The result of a Scott continuous λ-calculus topology is a function space built upon a programming semantic allowing fixed point combinatorics, such as the Y combinator, and data types. By 1971, λ-calculus was equipped to define any sequential computation and could be easily adapted to parallel computations. The reducibility of all computations to λ-calculus allows these λ-topological properties to become adopted by all programming languages.


== Computational algebra from λ-calculus algebra ==
Based on the operators within lambda calculus, application and abstraction, it is possible to develop an algebra whose group structure uses application and abstraction as binary operators. Application is defined as an operation between lambda terms producing a λ-term, e.g. the application of λ onto the lambda term a produces the lambda term λa. Abstraction incorporates undefined variables by denoting λx.t(x) as the function assigning the variable a to the lambda term with value t(a) via the operation ((λ x.t(x))a = t(a)). Lastly, an equivalence relation emerges which identifies λ-terms modulo convertible terms, an example being beta normal form.


== Scott topology ==
The Scott topology is essential in understanding the topological structure of computation as expressed through the λ-calculus. Scott found that after constructing a function space using λ-calculus one obtains a Kolmogorov space, a 
  
    
      
        
          T
          
            o
          
        
      
    
    {\displaystyle T_{o}}
   topological space which exhibits pointwise convergence, in short the product topology. It is the ability of self homeomorphism as well as the ability to embed every space into such a space, denoted Scott continuous, as previously described which allows Scott's topology to be applicable to logic and recursive function theory. Scott approaches his derivation using a complete lattice, resulting in a topology dependent on the lattice structure. It is possible to generalise Scott's theory with the use of complete partial orders. For this reason a more general understanding of the computational topology is provided through complete partial orders. We will re-iterate to familiarize ourselves with the notation to be used during the discussion of Scott topology.
Complete partial orders are defined as follows:
First, given the partially ordered set D=(D,≤), a nonempty subset X ⊆ D is directed if ∀ x,y ∈ X ∃ z ∈ X where x≤ z & y ≤ z.
D is a complete partial order (cpo) if:

· Every directed X ⊆D has a supremum, and:

∃ bottom element ⊥ ∈ D such that ∀ x ∈ D ⊥ ≤ x.

We are now able to define the Scott topology over a cpo (D, ≤ ).
O ⊆ D is open if:

for x ∈ O, and x ≤ y, then y ∈ O, i.e. O is an upper set.
for a directed set X ⊆ D, and supremum(X) ∈ O, then X ∩ O ≠ ∅.

Using the Scott topological definition of open it is apparent that all topological properties are met.

·∅ and D, i.e. the empty set and whole space, are open.

·Arbitrary unions of open sets are open:

Proof: Assume 
  
    
      
        
          U
          
            i
          
        
      
    
    {\displaystyle U_{i}}
   is open where i ∈ I, I being the index set. We define U = ∪{ 
  
    
      
        
          U
          
            i
          
        
      
    
    {\displaystyle U_{i}}
   ; i ∈ I}. Take b as an element of the upper set of U, therefore a ≤ b for some a ∈ U It must be that a ∈ 
  
    
      
        
          U
          
            i
          
        
      
    
    {\displaystyle U_{i}}
   for some i, likewise b ∈ upset(
  
    
      
        
          U
          
            i
          
        
      
    
    {\displaystyle U_{i}}
  ). U must therefore be upper as well since 
  
    
      
        
          U
          
            i
          
        
      
    
    {\displaystyle U_{i}}
   ∈ U.

Likewise, if D is a directed set with a supremum in U, then by assumption sup(D) ∈ 
  
    
      
        
          U
          
            i
          
        
      
    
    {\displaystyle U_{i}}
  where 
  
    
      
        
          U
          
            i
          
        
      
    
    {\displaystyle U_{i}}
  is open. Thus there is a b ∈ D where b ∈ 
  
    
      
        
          U
          
            i
          
        
        ∩
        D
        ⊆
        U
        ∩
        D
      
    
    {\displaystyle U_{i}\cap D\subseteq U\cap D}
  . The union of open sets 
  
    
      
        
          U
          
            i
          
        
      
    
    {\displaystyle U_{i}}
  is therefore open.

·Open sets under intersection are open:

Proof: Given two open sets, U and V, we define W = U∩V. If W = ∅ then W is open. If non-empty say b ∈ upset(W) (the upper set of W), then for some a ∈ W, a ≤ b. Since a ∈ U∩V, and b an element of the upper set of both U and V, then b ∈ W.

Finally, if D is a directed set with a supremum in W, then by assumption sup(D) ∈ 
  
    
      
        U
        ∩
        V
      
    
    {\displaystyle U\cap V}
  . So there is a ∈ 
  
    
      
        U
        ∩
        D
      
    
    {\displaystyle U\cap D}
   and b ∈ 
  
    
      
        V
        ∩
        D
      
    
    {\displaystyle V\cap D}
  . Since D is directed there is c ∈ D with 
  
    
      
        a
        ≤
        c
        ,
        b
        ≤
        c
      
    
    {\displaystyle a\leq c,b\leq c}
  ; and since U and V are upper sets, c ∈ 
  
    
      
        U
        ∩
        V
      
    
    {\displaystyle U\cap V}
   as well.

Though not shown here, it is the case that the map 
  
    
      
        f
        :
        D
        →
        
          D
          
            
              
              ′
            
          
        
      
    
    {\displaystyle f:D\rightarrow D^{'}}
   is continuous if and only if f(sup(X)) = sup(f(X)) for all directed X⊆D, where f(X) = {f(x) | x ∈ X} and the second supremum in 
  
    
      
        
          D
          
            
              
              ′
            
          
        
      
    
    {\displaystyle D^{'}}
  .
Before we begin explaining that application as common to λ-calculus is continuous within the Scott topology we require a certain understanding of the behavior of supremums over continuous functions as well as the conditions necessary for the product of spaces to be continuous namely

With 
  
    
      
        
          
            
              f
              
                i
              
            
          
          
            i
          
        
        ⊆
        [
        D
        →
        
          D
          
            
              
              ′
            
          
        
        ]
      
    
    {\displaystyle {f_{i}}_{i}\subseteq [D\rightarrow D^{'}]}
   be a directed family of maps, then 
  
    
      
        f
        (
        x
        )
        =
        
          ∪
          
            i
          
        
        
          f
          
            i
          
        
        (
        x
        )
      
    
    {\displaystyle f(x)=\cup _{i}f_{i}(x)}
   if well defined and continuous.
If F 
  
    
      
        ⊆
        [
        D
        →
        
          D
          
            
              
              ′
            
          
        
        ]
      
    
    {\displaystyle \subseteq [D\rightarrow D^{'}]}
   is directed and cpo and 
  
    
      
        [
        D
        →
        
          D
          
            
              
              ′
            
          
        
        ]
      
    
    {\displaystyle [D\rightarrow D^{'}]}
   a cpo where sup({f(x) | f ∈ F}).

We now show the continuity of application. Using the definition of application as follows:

Ap: 
  
    
      
        [
        D
        →
        
          D
          
            
              
              ′
            
          
        
        ]
        ×
        D
        →
        
          D
          
            
              
              ′
            
          
        
      
    
    {\displaystyle [D\rightarrow D^{'}]\times D\rightarrow D^{'}}
   where Ap(f,x) = f(x).

Ap is continuous with respect to the Scott topology on the product (
  
    
      
        [
        D
        →
        
          D
          
            
              
              ′
            
          
        
        ]
        ×
        D
        →
        
          D
          
            
              
              ′
            
          
        
      
    
    {\displaystyle [D\rightarrow D^{'}]\times D\rightarrow D^{'}}
  ) :

Proof: λx.f(x) = f is continuous. Let h = λ f.f(x). For directed F
  
    
      
        ⊆
        [
        D
        →
        
          D
          
            
              
              ′
            
          
        
        ]
      
    
    {\displaystyle \subseteq [D\rightarrow D^{'}]}
  

h(sup(F)) = sup(F)(x)

= sup( {f(x) | f ∈ F} )

= sup( {h(f) | f ∈ F} )

= sup( h(F) )

By definition of Scott continuity h has been shown continuous. All that is now required to prove is that application is continuous when it's separate arguments are continuous, i.e. 
  
    
      
        [
        D
        →
        
          D
          
            
              
              ′
            
          
        
        ]
      
    
    {\displaystyle [D\rightarrow D^{'}]}
  and 
  
    
      
        D
        →
        
          D
          
            
              
              ′
            
          
        
      
    
    {\displaystyle D\rightarrow D^{'}}
  are continuous, in our case f and h.

Now abstracting our argument to show 
  
    
      
        f
        :
        D
        ×
        
          D
          
            
              
              ′
            
          
        
        →
        
          D
          
            
              
              ″
            
          
        
      
    
    {\displaystyle f:D\times D^{'}\rightarrow D^{''}}
   with 
  
    
      
        g
        =
        λ
        x
        .
        f
        (
        x
        ,
        
          x
          
            0
          
        
        )
      
    
    {\displaystyle g=\lambda x.f(x,x_{0})}
   and 
  
    
      
        d
        =
        λ
        
          x
          
            
              
              ′
            
          
        
        .
        f
        (
        
          x
          
            0
          
        
        ,
        
          x
          
            
              
              ′
            
          
        
        )
      
    
    {\displaystyle d=\lambda x^{'}.f(x_{0},x^{'})}
   as the arguments for D and 
  
    
      
        
          D
          
            
              
              ′
            
          
        
      
    
    {\displaystyle D^{'}}
   respectively, then for a directed X ⊆ D

  
    
      
        g
        (
        sup
        (
        X
        )
        )
        =
        f
        (
        sup
        (
        X
        )
        ,
        
          x
          
            0
          
          
            
              
              ′
            
          
        
        )
        )
      
    
    {\displaystyle g(\sup(X))=f(\sup(X),x_{0}^{'}))}
  

= f( sup( (x,
  
    
      
        
          x
          
            0
          
          
            
              
              ′
            
          
        
      
    
    {\displaystyle x_{0}^{'}}
  ) | x ∈ X} ))

(since f is continuous and {(x,
  
    
      
        
          x
          
            0
          
          
            
              
              ′
            
          
        
      
    
    {\displaystyle x_{0}^{'}}
  ) | x ∈ X}) is directed):

= sup( {f(x,
  
    
      
        
          x
          
            0
          
          
            
              
              ′
            
          
        
      
    
    {\displaystyle x_{0}^{'}}
  ) | x ∈ X} )

= sup(g(X))

g is therefore continuous. The same process can be taken to show d is continuous.
It has now been shown application is continuous under the Scott topology.

In order to demonstrate the Scott topology is a suitable fit for λ-calculus it is necessary to prove abstraction remains continuous over the Scott topology. Once completed it will have been shown that the mathematical foundation of λ-calculus is a well defined and suitable candidate functional paradigm for the Scott topology.
With 
  
    
      
        f
        ∈
        [
        D
        ×
        
          D
          
            
              
              ′
            
          
        
        →
        
          D
          
            
              
              ″
            
          
        
        ]
      
    
    {\displaystyle f\in [D\times D^{'}\rightarrow D^{''}]}
   we define 
  
    
      
        
          
            
              f
              ˇ
            
          
        
      
    
    {\displaystyle {\check {f}}}
   (x) =λ y ∈ 
  
    
      
        
          D
          
            
              
              ′
            
          
        
      
    
    {\displaystyle D^{'}}
  f(x,y)We will show:
(i) 
  
    
      
        
          
            
              f
              ˇ
            
          
        
      
    
    {\displaystyle {\check {f}}}
   is continuous, meaning 
  
    
      
        
          
            
              f
              ˇ
            
          
        
      
    
    {\displaystyle {\check {f}}}
   ∈ 
  
    
      
        [
        D
        →
        [
        
          D
          
            
              
              ′
            
          
        
        →
        
          D
          
            
              
              ″
            
          
        
        ]
      
    
    {\displaystyle [D\rightarrow [D^{'}\rightarrow D^{''}]}
  
(ii) λ 
  
    
      
        f
        .
        
          
            
              f
              ˇ
            
          
        
        :
        [
        D
        ×
        
          D
          
            
              
              ′
            
          
        
        →
        
          D
          
            
              
              ″
            
          
        
        ]
        →
        [
        D
        →
        [
        
          D
          
            
              
              ′
            
          
        
        →
        
          D
          
            
              
              ″
            
          
        
        ]
      
    
    {\displaystyle f.{\check {f}}:[D\times D^{'}\rightarrow D^{''}]\rightarrow [D\rightarrow [D^{'}\rightarrow D^{''}]}
   is continuous.

Proof (i): Let X ⊆ D be directed, then

  
    
      
        
          
            
              f
              ˇ
            
          
        
      
    
    {\displaystyle {\check {f}}}
  (sup(X)) = λ y.f( sup(X),y )

= λ y.
  
    
      
        
          sup
          
            x
            ∈
            X
          
        
      
    
    {\displaystyle \sup _{x\in X}}
  ( f(x,y) )

= 
  
    
      
        
          sup
          
            x
            ∈
            X
          
        
      
    
    {\displaystyle \sup _{x\in X}}
  ( λy.f(x,y) )

= sup(
  
    
      
        
          
            
              f
              ˇ
            
          
        
      
    
    {\displaystyle {\check {f}}}
  (X))

Proof (ii): Defining L = λ 
  
    
      
        f
        .
        
          
            
              f
              ˇ
            
          
        
      
    
    {\displaystyle f.{\check {f}}}
   then for F 
  
    
      
        ⊆
        [
        D
        ×
        
          D
          
            
              
              ′
            
          
        
        →
        
          D
          
            
              
              ″
            
          
        
        ]
      
    
    {\displaystyle \subseteq [D\times D^{'}\rightarrow D^{''}]}
   directed

L(sup(F)) = λ x λ y. (sup(F))(x,y))

= λ x λ y. 
  
    
      
        
          sup
          
            y
            ∈
            F
          
        
      
    
    {\displaystyle \sup _{y\in F}}
  f(x,y)

= 
  
    
      
        
          sup
          
            y
            ∈
            F
          
        
      
    
    {\displaystyle \sup _{y\in F}}
  λx λy.f(x,y)

= sup(L(F))

It has not been demonstrated how and why the λ-calculus defines the Scott topology.


== Böhm trees and computational topology ==
Böhm trees, easily represented graphically, express the computational behavior of a lambda term. It is possible to predict the functionality of a given lambda expression from reference to its correlating Böhm tree. Böhm trees can be seen somewhat analogous to 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
   where the Böhm tree of a given set is similar to the continued fraction of a real number, and what is more, the Böhm tree corresponding to a sequence in normal form is finite, similar to the rational subset of the Reals.
Böhm trees are defined by a mapping of elements within a sequence of numbers with ordering (≤, lh) and a binary operator * to a set of symbols. The Böhm tree is then a relation among a set of symbols through a partial mapping ψ.
Informally Böhm trees may be conceptualized as follows:
Given: Σ = 
  
    
      
        ⊥
        ∪
      
    
    {\displaystyle \perp \cup }
   { λ x_{1} 
  
    
      
        ⋯
      
    
    {\displaystyle \cdots }
  x_{n} . y | n ∈ 
  
    
      
        
          N
        
        ,
        
          x
          
            1
          
        
        .
        .
        .
        
          x
          
            n
          
        
      
    
    {\displaystyle \mathbb {N} ,x_{1}...x_{n}}
  y are variables and denoting BT(M) as the Böhm tree for a lambda term M we then have:
BT(M) = ⊥ if M is unsolvable (therefore a single node)

More formally:
Σ is defined as a set of symbols. The Böhm tree of a λ term M, denoted BT(M), is the Σ labelled tree defined as follows:

If M is unsolvable:

  
    
      
        B
        T
        (
        M
        )
        (
        ⟨
         
        ⟩
        )
        =⊥
        ,
      
    
    {\displaystyle BT(M)(\langle \ \rangle )=\perp ,}
  

BT(M)(
  
    
      
        ⟨
        k
        ⟩
        ∗
        α
      
    
    {\displaystyle \langle k\rangle *\alpha }
  ) is unsolvable 
  
    
      
        ∀
        k
        ,
        α
      
    
    {\displaystyle \forall k,\alpha }
  

If M is solvable, where M = λ x_{1}
  
    
      
        ⋯
        
          x
          
            n
          
        
        .
        y
        
          M
          
            0
          
        
        ⋯
        
          M
          
            m
            −
            1
          
        
      
    
    {\displaystyle \cdots x_{n}.yM_{0}\cdots M_{m-1}}
  :

BT(M)(< >) = λ x_{1} 
  
    
      
        ⋯
        
          x
          
            n
          
        
        .
        y
      
    
    {\displaystyle \cdots x_{n}.y}
  

BT(M)(
  
    
      
        ⟨
        k
        ⟩
        ∗
        α
      
    
    {\displaystyle \langle k\rangle *\alpha }
  ) = BT(M_k)(
  
    
      
        α
      
    
    {\displaystyle \alpha }
  ) 
  
    
      
        ∀
        α
      
    
    {\displaystyle \forall \alpha }
   and k < m

= undefined 
  
    
      
        ∀
        α
      
    
    {\displaystyle \forall \alpha }
   and k ≥ m

We may now move on to show that Böhm trees act as suitable mappings from the tree topology to the scott topology. Allowing one to see computational constructs, be it within the Scott or tree topology, as Böhm tree formations.


=== Böhm tree and tree topology ===
It is found that Böhm tree's allow for a continuous mapping from the tree topology to the Scott topology. More specifically:
We begin with the cpo B = (B,⊆) on the Scott topology, with ordering of Böhm tree's denoted M⊆ N, meaning M, N are trees and M results from N. The tree topology on the set Γ is the smallest set allowing for a continuous map

BT:
  
    
      
        Γ
        →
      
    
    {\displaystyle \Gamma \rightarrow }
  B.

An equivalent definition would be to say the open sets of Γ are the image of the inverse Böhm tree 
  
    
      
        B
        
          T
          
            −
            1
          
        
      
    
    {\displaystyle BT^{-1}}
   (O) where O is Scott open in B.
The applicability of the Bömh trees and the tree topology has many interesting consequences to λ-terms expressed topologically:
Normal forms are found to exist as isolated points.
Unsolvable λ-terms are compactification points.
Application and abstraction, similar to the Scott topology, are continuous on the tree topology.


== Algebraic structure of computation ==
New methods of interpretation of the λ-calculus are not only interesting in themselves but allow new modes of thought concerning the behaviors of computer science. The binary operator within the λ-algebra A is application. Application is denoted · and is said to give structure 
  
    
      
        A
        =
        (
        X
        ,
        ⋅
        )
      
    
    {\displaystyle A=(X,\cdot )}
  . A combinatory algebra allows for the application operator and acts as a useful starting point but remains insufficient for the λ-calculus in being unable to express abstraction. The λ algebra becomes a combinatory algebra M combined with a syntactic operator λ* that transforms a term B(x,y), with constants in M, into C(
  
    
      
        
          
            
              y
              ^
            
          
        
      
    
    {\displaystyle {\hat {y}}}
  )≡ λ* x.B(x,
  
    
      
        
          
            
              y
              ^
            
          
        
      
    
    {\displaystyle {\hat {y}}}
  ). It is also possible to define an extension model to circumvent the need of the λ* operator by allowing ∀x (fx =gx) ⇒ f =g . The construction of the λ-algebra through the introduction of an abstraction operator proceeds as follows:
We must construct an algebra which allows for solutions to equations such as axy = xyy such that a = λ xy.xyy there is need for the combinatory algebra. Relevant attributes of the combinatory algebra are:
Within combinatory algebra there exists applicative structures. An applicative structure W is a combinatory algebra provided:

·W is non-trival, meaning W has cardinality > 1
·W exhibits combinatory completeness (see completeness of the S-K basis). More specifically: for every term A ∈ the set of terms of W, and 
  
    
      
        
          x
          
            1
          
        
        ,
        .
        .
        .
        ,
        
          x
          
            n
          
        
      
    
    {\displaystyle x_{1},...,x_{n}}
   with the free variables of A within 
  
    
      
        
          
            x
            
              1
            
          
          ,
          .
          .
          .
          ,
          
            x
            
              n
            
          
        
      
    
    {\displaystyle {x_{1},...,x_{n}}}
   then:

  
    
      
        ∃
        f
        ∀
        
          x
          
            1
          
        
        ⋅
        ⋅
        ⋅
        
          x
          
            n
          
        
      
    
    {\displaystyle \exists f\forall x_{1}\cdot \cdot \cdot x_{n}}
   where 
  
    
      
        f
        
          x
          
            1
          
        
        ⋅
        ⋅
        ⋅
        
          x
          
            n
          
        
        =
        A
      
    
    {\displaystyle fx_{1}\cdot \cdot \cdot x_{n}=A}
  

The combinatory algebra is:
Never commutative
Not associative.
Never finite.
Never recursive.
Combinatory algebras remain unable to act as the algebraic structure for λ-calculus, the lack of recursion being a major disadvantage. However the existence of an applicative term 
  
    
      
        A
        (
        x
        ,
        
          
            
              y
              →
            
          
        
      
    
    {\displaystyle A(x,{\vec {y}}}
  ) provides a good starting point to build a λ-calculus algebra. What is needed is the introduction of a lambda term, i.e. include λx.A(x, 
  
    
      
        
          
            
              y
              →
            
          
        
      
    
    {\displaystyle {\vec {y}}}
  ).
We begin by exploiting the fact that within a combinatory algebra M, with A(x, 
  
    
      
        
          
            
              y
              →
            
          
        
      
    
    {\displaystyle {\vec {y}}}
  ) within the set of terms, then:

  
    
      
        ∀
        
          
            
              y
              →
            
          
        
      
    
    {\displaystyle \forall {\vec {y}}}
   
  
    
      
        ∃
        b
      
    
    {\displaystyle \exists b}
   s.t. bx = A(x, 
  
    
      
        
          
            
              y
              →
            
          
        
      
    
    {\displaystyle {\vec {y}}}
  ).

We then require b have a dependence on 
  
    
      
        
          
            
              y
              →
            
          
        
      
    
    {\displaystyle {\vec {y}}}
   resulting in:

  
    
      
        ∀
        x
      
    
    {\displaystyle \forall x}
   B(
  
    
      
        
          
            
              y
              →
            
          
        
      
    
    {\displaystyle {\vec {y}}}
  )x = A(x, 
  
    
      
        
          
            
              y
              →
            
          
        
      
    
    {\displaystyle {\vec {y}}}
  ).

B(
  
    
      
        
          
            
              y
              →
            
          
        
      
    
    {\displaystyle {\vec {y}}}
  ) becomes equivalent to a λ term, and is therefore suitably defined as follows: B(
  
    
      
        
          
            
              y
              →
            
          
        
        )
        ≡
      
    
    {\displaystyle {\vec {y}})\equiv }
   λ*.
A pre-λ-algebra (pλA) can now be defined.

pλA is an applicative structure W = (X,·) such that for each term A within the set of terms within W and for every x there is a term λ*x.A ∈ T(W) (T(W) ≡ the terms of W) where (the set of free variables of λ*x.A) = (the set of free variables of A) - {x}. W must also demonstrate:

  
    
      
        (
        β
        )
      
    
    {\displaystyle (\beta )}
   (λ*x.A)x = A

  
    
      
        
          α
          
            1
          
        
      
    
    {\displaystyle \alpha _{1}}
  λ*x.A≡ λ*x.A[x:=y] provided y is not a free variable of A

  
    
      
        
          α
          
            2
          
        
      
    
    {\displaystyle \alpha _{2}}
  (λ*x.A)[y:=z]≡λ*x.A[x:=y] provided y,z ≠ x and z is not a free variable of A

Before defining the full λ-algebra we must introduce the following definition for the set of λ-terms within W denoted 
  
    
      
        Γ
        (
        W
        )
      
    
    {\displaystyle \Gamma (W)}
   with the following requirements:

a ∈ W 
  
    
      
        ⇒
        
          c
          
            a
          
        
        ∈
        Γ
        (
        W
        )
      
    
    {\displaystyle \Rightarrow c_{a}\in \Gamma (W)}
  
x ∈ 
  
    
      
        Γ
        (
        W
        )
      
    
    {\displaystyle \Gamma (W)}
   for x ∈ (
  
    
      
        
          v
          
            0
          
        
        ,
        
          v
          
            1
          
        
        ,
        .
        .
        .
      
    
    {\displaystyle v_{0},v_{1},...}
  )
M,N ∈ 
  
    
      
        Γ
        (
        W
        )
        ⇒
      
    
    {\displaystyle \Gamma (W)\Rightarrow }
   (MN) ∈ 
  
    
      
        Γ
        (
        W
        )
      
    
    {\displaystyle \Gamma (W)}
  
M ∈ 
  
    
      
        Γ
        (
        W
        )
        ⇒
      
    
    {\displaystyle \Gamma (W)\Rightarrow }
   (λx.M) ∈ 
  
    
      
        Γ
        (
        W
        )
      
    
    {\displaystyle \Gamma (W)}
  

A mapping from the terms within 
  
    
      
        Γ
        (
        W
        )
      
    
    {\displaystyle \Gamma (W)}
   to all λ terms within W, denoted * : 
  
    
      
        Γ
        (
        W
        )
        →
        
          T
        
        (
        W
        )
      
    
    {\displaystyle \Gamma (W)\rightarrow \mathrm {T} (W)}
  , can then be designed as follows:

  
    
      
        
          v
          
            i
          
          
            ∗
          
        
        =
        
          w
          
            i
          
        
        ,
        
          c
          
            a
          
          
            ∗
          
        
        =
        
          c
          
            a
          
        
      
    
    {\displaystyle v_{i}^{*}=w_{i},c_{a}^{*}=c_{a}}
  
(MN)* = M* N*
(λx.M)* = λ* x*.M*

We now define λ(M) to denote the extension after evaluating the terms within 
  
    
      
        Γ
        (
        W
        )
      
    
    {\displaystyle \Gamma (W)}
  .

λx.(λy.yx)
  
    
      
        
          c
          
            a
          
        
      
    
    {\displaystyle c_{a}}
   = λx.
  
    
      
        
          c
          
            a
          
        
      
    
    {\displaystyle c_{a}}
  x in λ(W).

Finally we obtain the full λ-algebra through the following definition:

(1) A λ-algebra is a pλA W such that for M,N ∈ Γ(W):
λ(W) ⊢ M = N ⇒ W ⊨ M = N.

Though arduous, the foundation has been set for a proper algebraic framework for which the λ-calculus, and therefore computation, may be investigated in a group theoretic manner.


== References =="
48,Loebner Prize,238725,20692,"The Loebner Prize is an annual competition in artificial intelligence that awards prizes to the computer programs considered by the judges to be the most human-like. The format of the competition is that of a standard Turing test. In each round, a human judge simultaneously holds textual conversations with a computer program and a human being via computer. Based upon the responses, the judge must decide which is which.
The contest was launched in 1990 by Hugh Loebner in conjunction with the Cambridge Center for Behavioral Studies, Massachusetts, United States. Since 2014 it has been organised by the AISB at Bletchley Park. It has also been associated with Flinders University, Dartmouth College, the Science Museum in London, University of Reading and Ulster University, Magee Campus, Derry, UK City of Culture. In 2004 and 2005, it was held in Loebner's apartment in New York City. Within the field of artificial intelligence, the Loebner Prize is somewhat controversial; the most prominent critic, Marvin Minsky, called it a publicity stunt that does not help the field along.


== Prizes ==
Originally, $2,000 was awarded for the most human-seeming program in the competition. The prize was $3,000 in 2005 and $2,250 in 2006. In 2008, $3,000 was awarded.
In addition, there are two one-time-only prizes that have never been awarded. $25,000 is offered for the first program that judges cannot distinguish from a real human and which can convince judges that the human is the computer program. $100,000 is the reward for the first program that judges cannot distinguish from a real human in a Turing test that includes deciphering and understanding text, visual, and auditory input. Once this is achieved, the annual competition will end.


== Competition rules and restrictions ==
The rules have varied over the years and early competitions featured restricted conversation Turing tests but since 1995 the discussion has been unrestricted.
For the three entries in 2007, Robert Medeksza, Noah Duncan and Rollo Carpenter, some basic ""screening questions"" were used by the sponsor to evaluate the state of the technology. These included simple questions about the time, what round of the contest it is, etc.; general knowledge (""What is a hammer for?""); comparisons (""Which is faster, a train or a plane?""); and questions demonstrating memory for preceding parts of the same conversation. ""All nouns, adjectives and verbs will come from a dictionary suitable for children or adolescents under the age of 12."" Entries did not need to respond ""intelligently"" to the questions to be accepted.
For the first time in 2008 the sponsor allowed introduction of a preliminary phase to the contest opening up the competition to previously disallowed web-based entries judged by a variety of invited interrogators. The available rules do not state how interrogators are selected or instructed. Interrogators (who judge the systems) have limited time: 5 minutes per entity in the 2003 competition, 20+ per pair in 2004–2007 competitions, 5 minutes to conduct simultaneous conversations with a human and the program in 2008-2009, increased to 25 minutes of simultaneous conversation since 2010.


== Criticisms ==
The prize has long been scorned by experts in the field, for a variety of reasons.
It is regarded by many as a publicity stunt. Marvin Minsky scathingly offered a ""prize"" to anyone who could stop the competition. Loebner responded by jokingly observing that Minsky's offering a prize to stop the competition effectively made him a co-sponsor.
The rules of the competition have encouraged poorly qualified judges to make rapid judgements. Interactions between judges and competitors was originally very brief, for example effectively 2.5 mins of questioning, which permitted only a few questions. Questioning was initially restricted to ""whimsical conversation"", a domain suiting standard chatbot tricks.
Competition entrants do not aim at understanding or intelligence but resort to basic ELIZA style tricks, and successful entrants find deception and pretense is rewarded.
Reporting of the annual competition often confuses the imitation test with intelligence, a typical example being Brian Christian's introduction to his article ""Mind vs. Machine"" in The Atlantic, March 2011, stating that ""in the race to build computers that can think like humans, the proving ground is the Turing Test"".


== Contests ==


=== 2006 ===
In 2006, the contest was organised by Tim Child (CEO of Televirtual) and Huma Shah. On August 30, the four finalists were announced:
Rollo Carpenter
Richard Churchill and Marie-Claire Jenkins
Noah Duncan
Robert Medeksza
The contest was held on 17 September in the VR theatre, Torrington Place campus of University College London. The judges included the University of Reading's cybernetics professor, Kevin Warwick, a professor of artificial intelligence, John Barnden (specialist in metaphor research at the University of Birmingham), a barrister, Victoria Butler-Cole and a journalist, Graham Duncan-Rowe. The latter's experience of the event can be found in an article in Technology Review. The winner was 'Joan', based on Jabberwacky, both created by Rollo Carpenter.


=== 2007 ===
The 2007 competition was held on October 21 in New York City. The judges were: computer science professor Russ Abbott, philosophy professor Hartry Field, psychology assistant professor Clayton Curtis and English lecturer Scott Hutchins.
No bot passed the Turing test, but the judges ranked the three contestants as follows:
1st: Robert Medeksza from Zabaware, creator of Ultra Hal Assistant
2nd: Noah Duncan, a private entry, creator of Cletus
3rd: Rollo Carpenter from Icogno, creator of Jabberwacky
The winner received $2,250 and the annual medal. The runners-up received $250 each.


=== 2008 ===
The 2008 competition was organised by professor Kevin Warwick, coordinated by Huma Shah and held on October 12 at the University of Reading, UK. After testing by over one hundred judges during the preliminary phase, in June and July 2008, six finalists were selected from thirteen original entrants - artificial conversational entity (ACE). Five of those invited competed in the finals:
Brother Jerome, Peter Cole and Benji Adams
Elbot, Fred Roberts / Artificial Solutions
Eugene Goostman, Vladimir Veselov, Eugene Demchenko and Sergey Ulasen
Jabberwacky, Rollo Carpenter
Ultra Hal, Robert Medeksza
In the finals, each of the judges was given five minutes to conduct simultaneous, split-screen conversations with two hidden entities. Elbot of Artificial Solutions won the 2008 Loebner Prize bronze award, for most human-like artificial conversational entity, through fooling three of the twelve judges who interrogated it (in the human-parallel comparisons) into believing it was human. This is coming very close to the 30% traditionally required to consider that a program has actually passed the Turing test. Eugene Goostman and Ultra Hal both deceived one judge each that it was the human.
Will Pavia, a journalist for The Times, has written about his experience; a Loebner finals' judge, he was deceived by Elbot and Eugene. Kevin Warwick and Huma Shah have reported on the parallel-paired Turing tests.


=== 2009 ===
The 2009 Loebner Prize Competition was held September 6, 2009 at the Brighton Centre, Brighton UK in conjunction with the Interspeech 2009 conference. The prize amount for 2009 was $3,000.
Entrants were David Levy, Rollo Carpenter, and Mohan Embar, who finished in that order.
The writer Brian Christian participated in the 2009 Loebner Prize Competition as a human confederate, and described his experiences at the competition in his book The Most Human Human.


=== 2010 ===
The 2010 Loebner Prize Competition was held on October 23 at California State University, Los Angeles. The 2010 competition was the 20th running of the contest. The winner was Bruce Wilcox with Suzette.


=== 2011 ===
The 2011 Loebner Prize Competition was held on October 19 at the University of Exeter, Devon, United Kingdom. The prize amount for 2011 was $4,000.
The four finalists and their chatterbots were Bruce Wilcox (Rosette), Adeena Mignogna (Zoe), Mohan Embar (Chip Vivant) and Ron Lee (Tutor), who finished in that order.
That year there was an addition of a panel of junior judges, namely Jean-Paul Astal-Stain, William Dunne, Sam Keat and Kirill Jerdev. The results of the junior contest were markedly different from the main contest, with chatterbots Tutor and Zoe tying for first place and Chip Vivant and Rosette coming in third and fourth place, respectively.


=== 2012 ===
The 2012 Loebner Prize Competition was held on the 15th of May in Bletchley Park in Bletchley, Buckinghamshire, England, in honor of the Alan Turing centenary celebrations. The prize amount for 2012 was $5,000. The local arrangements organizer was David Levy, who won the Loebner Prize in 1997 and 2009.
The four finalists and their chatterbots were Mohan Embar (Chip Vivant), Bruce Wilcox (Angela), Daniel Burke (Adam), M. Allan (Linguo), who finished in that order.
That year, a team from the University of Exeter's computer science department (Ed Keedwell, Max Dupenois and Kent McClymont) conducted the first-ever live webcast of the conversations.


=== 2013 ===
The 2013 Loebner Prize Competition was held, for the first time on the Island of Ireland, on September 14 at the Ulster University, Magee College, Derry, Northern Ireland, UK.
The four finalists and their chatbots were Steve Worswick (Mitsuku), Dr. Ron C. Lee (Tutor), Bruce Wilcox (Rose) and Brian Rigsby (Izar), who finished in that order.
The judges were Professor Roger Schank (Socratic Arts), Professor Noel Sharkey (Sheffield University), Professor Minhua (Eunice) Ma (Huddersfield University, then University of Glasgow) and Professor Mike McTear (Ulster University).
For the 2013 Junior Loebner Prize Competition the chatbots Mitsuku and Tutor tied for first place with Rose and Izar in 3rd and 4th place respectively.


=== 2014 ===
The 2014 Loebner Prize Competition was held at Bletchley Park, England, on Saturday 15 November 2014. The event was filmed live by Sky News. The guest judge was television presenter and broadcaster James May.
After 2 hours of judging, 'Rose' by Bruce Wilcox was declared the winner. Bruce will receive a cheque for $4000 and a bronze medal. The ranks were as follows:
Rose - Rank 1 ($4000 & Bronze Medal); Izar - Rank 2.25 ($1500); Uberbot - Rank 3.25 ($1000); and Mitsuku - Rank 3.5 ($500).
The Judges were Dr Ian Hocking, Writer & Senior Lecturer in Psychology, Christ Church College, Canterbury; Dr Ghita Kouadri-Mostefaoui, Lecturer in Computer Science and Technology, University of Bedfordshire; Mr James May, Television Presenter and Broadcaster; and Dr Paul Sant, Dean of UCMK, University of Bedfordshire.


=== 2015 ===
The 2015 Loebner Prize Competition was again won by 'Rose' by Bruce Wilcox.
The judges were Jacob Aaron, Physical sciences reporter for New Scientist; Rory Cellan-Jones, Technology correspondent for the BBC; Brett Marty, Film Director and Photographer; Ariadne Tampion, Writer.


=== 2016 ===
The 2016 Loebner Prize was held at Bletchley Park on 17 September 2016. After 2 hours of judging the final results were announced. The ranks were as follows:
1st place: Mitsuku
2nd place: Tutor
3rd place: Rose


== Winners ==
Official list of winners.


== See also ==
Artificial intelligence
Glossary of artificial intelligence
Robot
Artificial general intelligence
Confederate effect
Computer game bot Turing Test


== References ==


== External links ==
Official website
Markoff, John (Jan 10, 1993). ""Cocktail-Party Conversation -- With a Computer"". New York Times. Conversation with the 1992 winner; topic: men and women 
Platt, Charles (April 1995). ""What's It Mean to be Human, Anyway?"". Wired. 
Shah, Huma (Oct 2008). ""2008 Loebner Prize: myths and misconceptions"". 
Christian, Brian (March 2011). ""Mind vs. Machine"". The Atlantic."
49,Biodiversity informatics,6900845,20428,"Biodiversity Informatics is the application of informatics techniques to biodiversity information for improved management, presentation, discovery, exploration and analysis. It typically builds on a foundation of taxonomic, biogeographic, or ecological information stored in digital form, which, with the application of modern computer techniques, can yield new ways to view and analyse existing information, as well as predictive models for information that does not yet exist (see niche modelling). Biodiversity informatics is a relatively young discipline (the term was coined in or around 1992) but has hundreds of practitioners worldwide, including the numerous individuals involved with the design and construction of taxonomic databases. The term ""Biodiversity Informatics"" is generally used in the broad sense to apply to computerized handling of any biodiversity information; the somewhat broader term ""bioinformatics"" is often used synonymously with the computerized handling of data in the specialized area of molecular biology.


== Overview ==
Biodiversity informatics (different but linked to bioinformatics) is the application of information technology methods to the problems of organizing, accessing, visualizing and analyzing primary biodiversity data. Primary biodiversity data is composed of names, observations and records of specimens, and genetic and morphological data associated to a specimen. Biodiversity informatics may also have to cope with managing information from unnamed taxa such as that produced by environmental sampling and sequencing of mixed-field samples. The term biodiversity informatics is also used to cover the computational problems specific to the names of biological entities, such as the development of algorithms to cope with variant representations of identifiers such as species names and authorities, and the multiple classification schemes within which these entities may reside according to the preferences of different workers in the field, as well as the syntax and semantics by which the content in taxonomic databases can be made machine queryable and interoperable for biodiversity informatics purposes...


== History of the discipline ==
Biodiversity Informatics can be considered to have commenced with the construction of the first computerized taxonomic databases in the early 1970s, and progressed through subsequent developing of distributed search tools towards the late 1990s including the Species Analyst from Kansas University, the North American Biodiversity Information Network NABIN, CONABIO in Mexico, and others, the establishment of the Global Biodiversity Information Facility in 2001, and the parallel development of a variety of niche modelling and other tools to operate on digitized biodiversity data from the mid-1980s onwards (e.g. see ). In September 2000, the U.S. journal Science devoted a special issue to ""Bioinformatics for Biodiversity"", the journal ""Biodiversity Informatics"" commenced publication in 2004, and several international conferences through the 2000s have brought together Biodiversity Informatics practitioners, including the London e-Biosphere conference in June 2009. A supplement to the journal BMC Bioinformatics (Volume 10 Suppl 14) published in November 2009 also deals with Biodiversity Informatics.


== History of the term ==
According to correspondence reproduced by Walter Berendsohn, the term ""Biodiversity Informatics"" was coined by John Whiting in 1992 to cover the activities of an entity known as the Canadian Biodiversity Informatics Consortium, a group involved with fusing basic biodiversity information with environmental economics and geospatial information in the form of GPS and GIS. Subsequently, it appears to have lost any obligate connection with the GPS/GIS world and be associated with the computerized management of any aspects of biodiversity information (e.g. see )


== Current Biodiversity Informatics issues ==


=== Global list of all species ===
One major issue for biodiversity informatics at a global scale is the current absence of a complete master list of currently recognised species of the world, although this is an aim of the Catalogue of Life project which has ca. 1.65 million species of an estimated 1.9 million described species in its 2016 Annual Checklist. A similar effort for fossil taxa, the Paleobiology Database documents some 100,000+ names for fossil species, out of an unknown total number.


=== Genus and species scientific names as unique identifiers ===
Application of the Linnaean system of binomial nomenclature for species, and uninomials for genera and higher ranks, has led to many advantages but also problems with homonyms (the same name being used for multiple taxa, either inadvertently or legitimately across multiple kingdoms), synonyms (multiple names for the same taxon), as well as variant representations of the same name due to orthographic differences, minor spelling errors, variation in the manner of citation of author names and dates, and more. In addition, names can change through time on account of changing taxonomic opinions (for example, the correct generic placement of a species, or the elevation of a subspecies to species rank or vice versa), and also the circumscription of a taxon can change according to different authors' taxonomic concepts. One proposed solution to this problem is the usage of Life Science Identifiers (LSIDs) for machine-machine communication purposes, although there are both proponents and opponents of this approach.


=== A consensus classification of organisms ===
Organisms can be classified in a multitude of ways (see main page Biological classification), which can create design problems for Biodiversity Informatics systems aimed at incorporating either a single or multiple classification to suit the needs of users, or to guide them towards a single ""preferred"" system. Whether a single consensus classification system can ever be achieved is probably an open question, however the Catalogue of Life has commissioned activity in this area which has been succeeded by a published system proposed in 2015 by M. Ruggiero and co-workers.


== Mobilizing primary biodiversity information ==
""Primary"" biodiversity information can be considered the basic data on the occurrence and diversity of species (or indeed, any recognizable taxa), commonly in association with information regarding their distribution in either space, time, or both. Such information may be in the form of retained specimens and associated information, for example as assembled in the natural history collections of museums and herbaria, or as observational records, for example either from formal faunal or floristic surveys undertaken by professional biologists and students, or as amateur and other planned or unplanned observations including those increasingly coming under the scope of citizen science. Providing online, coherent digital access to this vast collection of disparate primary data is a core Biodiversity Informatics function that is at the heart of regional and global biodiversity data networks, examples of the latter including OBIS and GBIF.
As a secondary source of biodiversity data, relevant scientific literature can be parsed either by humans or (potentially) by specialized information retrieval algorithms to extract the relevant primary biodiversity information that is reported therein, sometimes in aggregated / summary form but frequently as primary observations in narrative or tabular form. Elements of such activity (such as extracting key taxonomic identifiers, keywording / index terms, etc.) have been practiced for many years at a higher level by selected academic databases and search engines. However, for the maximum Biodiversity Informatics value, the actual primary occurrence data should ideally be retrieved and then made available in a standardized form or forms; for example both the Plazi and INOTAXA projects are transforming taxonomic literature into XML formats that can then be read by client applications, the former using TaxonX-XML and the latter using the taXMLit format. The Biodiversity Heritage Library is also making significant progress in its aim to digitize substantial portions of the out-of-copyright taxonomic literature, which is then subjected to OCR (optical character recognition) so as to be amenable to further processing using Biodiversity Informatics tools.


== Standards and protocols ==
In common with other data-related disciplines, Biodiversity Informatics benefits from the adoption of appropriate standards and protocols in order to support machine-machine transmission and interoperability of information within its particular domain. Examples of relevant standards include the Darwin Core XML schema for specimen- and observation-based biodiversity data developed from 1998 onwards, plus extensions of the same, Taxonomic Concept Transfer Schema, plus standards for Structured Descriptive Data and Access to Biological Collection Data (ABCD); while data retrieval and transfer protocols include DiGIR (now mostly superseded) and TAPIR (TDWG Access Protocol for Information Retrieval). Many of these standards and protocols are currently maintained, and their development overseen, by the Taxonomic Databases Working Group (TDWG).


== Current activities ==
At the 2009 e-Biosphere conference in the U.K., the following themes were adopted, which is indicative of a broad range of current Biodiversity Informatics activities and how they might be categorized:
Application: Conservation / Agriculture / Fisheries / Industry / Forestry
Application: Invasive Alien Species
Application: Systematic and Evolutionary Biology
Application: Taxonomy and Identification Systems
New Tools, Services and Standards for Data Management and Access
New Modeling Tools
New Tools for Data Integration
New Approaches to Biodiversity Infrastructure
New Approaches to Species Identification
New Approaches to Mapping Biodiversity

National and Regional Biodiversity Databases and Networks
A post-conference workshop of key persons with current significant Biodiversity Informatics roles also resulted in a Workshop Resolution that stressed, among other aspects, the need to create durable, global registries for the resources that are basic to biodiversity informatics (e.g., repositories, collections); complete the construction of a solid taxonomic infrastructure; and create ontologies for biodiversity data.


== Example Biodiversity Informatics projects ==
Global:
The Global Biodiversity Information Facility (GBIF), and the Ocean Biogeographic Information System (OBIS) (for marine species)
The Species 2000, ITIS (Integrated Taxonomic Information System), and Catalogue of Life projects
Global Names
EOL, The Encyclopedia of Life project
The Consortium for the Barcode of Life project
The Map of Life project
The uBio Universal Biological Indexer and Organizer, from the Woods Hole Marine Biological Laboratory
The Index to Organism Names (ION) from Thomson Reuters, providing access to scientific names of taxa from numerous journals as indexed in the Zoological Record
ZooBank, the registry for nomenclatural acts and relevant systematic literature in zoology
The Index Nominum Genericorum, compilation of generic names published for organisms covered by the International Code of Botanical Nomenclature, maintained at the Smithsonian Institution in the U.S.A.
The International Plant Names Index
MycoBank, documenting new names and combinations for fungi
The List of Prokaryotic names with Standing in Nomenclature (LPSN) - Official register of valid names for bacteria and archaea, as governed by the International Code of Nomenclature of Bacteria
The Biodiversity Heritage Library project - digitising biodiversity literature
Wikispecies, open source (community-editable) compilation of taxonomic information, companion project to Wikipedia
TaxonConcept.org, a Linked Data project that connects disparate species databases
Instituto de Ciencias Naturales. Universidad Nacional de Colombia. Virtual Collections and Biodiversity Informatics Unit
ANTABIF. The Antarctic Biodiversity Information Facility gives free and open access to Antarctic Biodiversity data, in the spirit of the Antarctic Treaty.
Genesys (website), database of plant genetic resources maintained in national, regional and international gene banks
VertNet, Access to vertebrate primary occurrence data from data sets worldwide.
Regional / national projects:
Fauna Europaea
Atlas of Living Australia
A Pan-European Species-directories Infrastructure (PESI)
Symbiota
i4Life project
Sistema de Información sobre Biodiversidad de Colombia
India Biodiversity Portal (IBP)
Bhutan Biodiversity Portal (BBP)
Weed Identification and Knowledge in the Western Indian Ocean (WIKWIO)
LifeWatch is proposed by ESFRI as a pan-European research (e-)infrastructure to support Biodiversity research and policy-making.
A listing of over 600 current biodiversity informatics related activities can be found at the TDWG ""Biodiversity Information Projects of the World"" database.


== See also ==
Biodiversity
Global biodiversity
Taxonomic database
Web-based taxonomy
List of biodiversity databases


== References ==


== Further reading ==
OECD Megascience Forum Working Group on Biological Informatics (1999). Final Report of the OECD Megascience Forum Working Group on Biological Informatics, January 1999. pp. 1–74. 
Canhos, V.P.; Souza, S.; Giovanni, R. & Canhos, D.A.L. (2004). ""Global biodiversity informatics: setting the scene for a ""new world"" of ecological modeling"". Biodiversity Informatics. 1: 1–13. 
Soberón, J. & Peterson, A.T. (2004). ""Biodiversity informatics: managing and applying primary biodiversity data"". Phil. Trans. R. Soc. Lond. B359: 689–698. doi:10.1098/rstb.2003.1439. 
Chapman, A.D. (2005). Uses of Primary Species-Occurrence Data (PDF). Copenhagen: Global Biodiversity Information Facility. pp. 1–106. 
Johnson, N.F. (2007). ""Biodiversity informatics"". Annual Review of Entomology. 52: 421–438. doi:10.1146/annurev.ento.52.110405.091259. PMID 16956323. 
Sarkar, I.N. (2007). ""Biodiversity informatics: organizing and linking information across the spectrum of life"". Briefings in Bioinformatics. 8 (5): 347–357. doi:10.1093/bib/bbm037. PMID 17704120. 
Guralnick, R.P.; Hill, A (2009). ""Biodiversity Informatics: Automated Approaches for Documenting Global Biodiversity Patterns and Processes"". Bioinformatics. 25 (4): 421–428. doi:10.1093/bioinformatics/btn659. PMID 19129210. 


== External links ==
Biodiversity Informatics (journal)
Website of the 2009 e-Biosphere International Conference on Biodiversity Informatics"
50,Carlos Osuna,50963179,20225,"Carlos Osuna (born November 22, 1970) is a Mexican computer programmer, software architect and entrepreneur best known as being one of the founders of Espacios Business Media during its inception days. He installed one of the first commercial Linux web hosting servers in Latin America, using Red Hat in 1997. He later left that company in 1998 to pursue other interests, joining Consiss , an up and coming consulting group, created in 1999 as one of its first Java specialist leaving 6 years later to pursue a career as IT architect and Data Integration Specialist. He was hired by Xerox—formerly the Intellinex e-learning division of Ernst & Young  —in 2009 after 3 years as independent consultant.
Ironically after only four years the division was disbanded so Carlos reverted as consultant but swiftly was poached by BSD Enterprise, a mayor player in the nearshore IT market to become its Data Integration lead. There he began work with DevOps towards and Agile and Lean Manufacturing approach for Software development. Those concepts became key, when BSD successfully bid and won the gob.mx contract to assist the Enrique Peña Nieto administration fulfill it's Digital National Government strategy, akin to the GOV.UK portal.
Currently he works for Accenture as DevOps Specialist working Jenkins, Docker, Kubernetes and Rancher technologies.
He was a past collaborator for the Java Bindings for KHTML and collaborator for the Mono and Apache Tomcat projects, mostly in documentation and bug hunting. He's also currently spearheading Google Go language as well as Apple's Swift as both are the future for computing.


== Early life ==
Carlos Osuna was born and grew up in a mixed jewish/catholic household on Northern Mexico just 200 miles south of the Texas border, in the small town of Monterrey, which surge in development during the late 70's and 80's, where his family had a construction business. His father was a civil engineer coming from neighbor state of Tamaulipas and his mother was a housewife. They had three sons of which Carlos was the youngest. His mom's younger sister was to be Monterrey's Tec first head of Computer engineering division, hosting the Electronic Systems and Computer Systems degrees. As such, she had access to all the universities resources and promptly help his nephew grow in a computer savvy environment.
At age 9, Carlos was enrolled into a children's computer workshop led by the Arturo Rosenblueth Foundation, where he learnt LOGO using Commodore VIC-20. This would be followed by a Summer Camp at Monterrey Tech's focused on Apple II programming using BASIC and Pascal.
At age 18, Carlos attended Monterrey's Tec per his aunt's desire, but had to drop out due to poor grades. After several unsuccessful attempts, Carlos finally completed the BSE in Industrial and Systems Engineer at age 24 at the Universidad Regiomontana.


== Business career ==


=== Tequila Effect ===
Late 1994, Mexico entered into a severe crisis due steep currency devaluation product of a failed monetary strategy. This phenomenon was called the Tequila Effect which was chronicled accurately by Jorge Castañeda in his bestseller The Mexican Shock (1995).
This kept Carlos away from the labor market as this was a time of uncertainty and severe recession. After many failed job interviews, Carlos decided to start his own business in association with a successful young salesman named Raul Garcia. They formed Espacios de México which would later transform into the leading social media and community management service Espacios Business Media, led by Abelardo Leal.


=== Espacios de México ===
In 1996, after thorough discussions, Carlos and Raul decided to settle in Home Page creation service company in a 10 ft-square office near Monterrey's downtown, in a worn down neighborhood, next to the Alameda a 1940 art deco open plaza that had seen better days. They had moderate success giving HTML and Internet Marketing seminars in a local hotel called Hotel Rio.
Things were rough at the first days, as people had no idea what Internet meant and those days online access was accomplished by slow land line based modems which averaged 2.4 kbits/sec. Elegant graphics and elaborated designs were out of the question as either CSS or HTML4 hadn't been created yet. Rudimentary designs, but well thought out designs  with sometimes awful results .
As time went by, the need for an in-house host was needed and Espacios bought a brand new Pentium Pro system in 1998. It needed an OS and Carlos selected Red Hat Linux 5.0 dubbed ""Hurricane"". This would be one of the first Linux implementations used on ISP throughout Mexico and one of the first implementations in Latin America. Since none of the team knew how to create dynamic database driven sites, all data was housed inside the server and no tiers were needed for the design.
In late 1998, internal politics drew Carlos out and a new CTO was named. He would follow Carlos' legacy and implement FoxWeb to integrate with applications created in FoxPro.


=== Consiss, S.A. de C.V. ===
The year 1999 will mean a profound change on the way web page and website design were done. Microsoft's Active Server Pages—later known as Classic ASP or ASP Classic—would revolutionize server-side computing, creating the Web Platform ideal to migrate current client-server solutions into a more transparent and user friendly environments.
As such, Carlos was hired by Consiss as leading programmer for their B2B development strategies. Since their current client Vitro had established Java as their reference platform, Carlos' team had to find an alternative to ASP in order to interact with both their data warehouse solution, created with Cognos and an Oracle back-end as well as interact with their current ERP JD Edwards WorldSoftware which was hosted in AS/400 minicomputers. Carlos' team found the solution in a fledgling web server called the Java Web Server (JWS) which could handle Java servlet and Java Server Pages in one package. IBM's solution at that time, which was also recommended to Vitro's staff, WebSphere was limited to servlet execution.
After 4 years of successful implementations using the JWS pattern exclusively based on Server Side Includes and JSP, Carlos moved on to more meaningful projects at Consiss, including a complex middleware setup for convenience store manager Oxxo. The solution required a complex interaction between the current Point-of-Sale written in Clipper and the central ERP hosted by Oracle and exposed using WebMethods. This proved challenging as was one of the first uses of FTP based Web Services in a SOAP-like protocol of messages and protocols in an Enterprise application integration solution using the Business to Business Integration pattern.
Three more years passed and in 2007, Carlos decides to move on and following a brief stint at JackBe Mexico doing AJAX and mashup web development using EXT JS, it became clear that he needed a formal architect role as opposed to a mixed software developer/architect position.
With that in mind, Carlos decided to go freelance and offered enterprise integration and UML tutoring on his own, in association with Kernel Technologies (now an ACL official distributor). Several world class implementations were done during that year and dozens of people were trained on those seminars.


=== Xerox Learning Services ===
In 2009, Carlos was hired by Affiliated Computer Services to be one of technical architects for Learning Services division at the Mexican site which supported a SumTotal based LMS solution. This was a challenging job, as the LMS was mix of traditional ASP, which Carlos had learnt in the way, and .NET technologies. Carlos became Vertical lead for Integrations, one year after being hired.
In 2010, just two years after Carlos was hired, ACS was acquired by Xerox which transformed company into a fully owned subsidiary, which changed names into Xerox Learning Services, managed by the Human Resources, part of the Xerox Business Services division. During that time, new clients Deloitte, University of Miami and Beckman Coulter were acquired, while present ones were migrated into Cornerstone OnDemand Software, hosted on the cloud.
Part of Carlos' job was to care and maintain the DataFeed and DataFlow services which allowed other systems to integrate with the LMS seamlessly. A legacy Visual Basic application was used prior to Carlos tenure, which was replaced with several Windows Communication Foundation and Windows Workflow Foundation solutions which allowed stateful ETL transfers of live data.
At that time, Carlos became involved with the seminal work of François Ragnet  on the Future of Documents.     He envisioned a world where Live Documents would be exchanged by systems which would keep both transport metadata and live dynamic data allowing the seamless transport of information without a constant process. His team at the Xerox Research Centre Europe led by Frederique Segond, created wonderful systems using Semantic Parsing . Sadly no actual collaboration was done at the time, due to budget constraints and the physical impossibility of the group working together across divisions. Only some minimal Proof-of-Concepts were creating using Google as basis as RDF as semantic query language.
In 2012, the Xerox Learning Services division was streamlined and all the team working at the Dallas, TX and Monterrey, Mexico offices were effectively downsized and the full team was laid-off. Carlos had to move on leaving behind plenty of ideas that were kept unimplemented at the time.


=== BSD Enterprise ===
After several months of soul searching and failed travel to San Francisco via a Startup, Carlos was hired by BSD Enterprise, a nearshore companies with a specialty for Software quality assurance and offices in the US, Mexico, Spain and Chile. There, he began working Sales team as Sales Engineer ultimately landing the Home Depot account.
After just one year there, Carlos returned to his role as Data Integration Specialist which were in awful need by the Mexican, Brazilian and American branches of ESAB using EDI X12 documents over an AS2 encrypted lines using Microsoft BizTalk, as broker. The first system implemented was the Mexican side, which had a BPCS back-end; later came the Brazilian implementation, using SAP. Finally, USA-Europe operations had to be maintained. Carlos trained a staff specifically for this task during a two-year period.
He also collaborated briefly with the Mobile Development team, which splits their time between Ionic Framework cross-platform developments and native iOS and Android solutions, doing serious experiments on Apple TV with tvOS and Apple Watch using watchOS. Swift also became a chief interest for the group, both on the client and on the server side using Vapor Framework.
Ultimately he became part of the team in charge of the gob.mx portal, which was created to unite and integrate on a single portal the full breath of paperwork and formalities both federal, state and municipal-wide. With that in mind, it needed a multi disciplinary team knowledgeable in Data Integration, Enterprise Service Bus, Continuous Deployment, Continuous Integration and Containerization. Collectively this technologies later became known as DevOps and in the end became a fundamental part of the Digital National Strategy.
With that in mind, the next 3 years became key in learning the DevOps playbook and trying to adapt and use it for other customers.


=== Accenture ===
In January 2017, Carlos joined Accenture as lead DevOps Engineer for the Delivery Center, Monterrey.


== Personal life ==
Carlos is a practicing Jew close to his community. He loves baseball and American Football. In 1994, he made his reglementary social service before graduation at the Museum of Contemporary Art (MARCO) where he was tasked on finding works of art from a particular artist, scatters throughout the world. He also worked that same year, and the year before on the local newspaper El Norte where he published several editorials regarding life and youth, which were published on a special supplement for youngsters.
He has worked in several inclusion initiatives specially for people with Autism and Asperger syndrome helping others understand the complexities of the mind of children inside the Autism spectrum. As a computer programmer and a specialist in computer science, he knows the connections and touch points between computer communications and human interaction. His goal is to one day resolve the Autism Question by using computer models simulating this interaction. He also wants to someday specialize in Bioinformatics so as to use help research in silicio that will create new DNA molecules which could later be tested in vitro, thus speeding the process of genetic manipulation.
He currently lives with his fathers caring for them at their old age.


== References =="
51,List of books in computational geometry,8701085,20127,"This is a list of books in computational geometry. There are two major, largely nonoverlapping categories:
Combinatorial computational geometry, which deals with collections of discrete objects or defined in discrete terms: points, lines, polygons, polytopes, etc., and algorithms of discrete/combinatorial character are used
Numerical computational geometry, also known as geometric modeling and computer-aided geometric design (CAGD), which deals with modelling of shapes of real-life objects in terms of curves and surfaces with algebraic representation.


== Combinatorial computational geometry ==


=== General-purpose textbooks ===
Franco P. Preparata and Michael Ian Shamos (1985). Computational Geometry - An Introduction. Springer-Verlag. 1st edition: ISBN 0-387-96131-3; 2nd printing, corrected and expanded, 1988: ISBN 3-540-96131-3; Russian translation, 1989: ISBN 5-03-001041-6. CS1 maint: Uses authors parameter (link)
The book is the first comprehensive monograph on the level of a graduate textbook to systematically cover the fundamental aspects of the emerging discipline of computational geometry. It is written by founders of the field and the first edition covered all major developments in the preceding 10 years. In the aspect of comprehensiveness it was preceded only by the 1984 survey paper, Lee, D, T., Preparata, F. P. : ""Computational geometry - a survey"". IEEE Trans. on Computers. Vol. 33, No. 12, pp. 1072–1101 (1984). It is focused on two-dimensional problems, but also has digressions into higher dimensions.
The initial core of the book was M.I.Shamos' doctoral dissertation, which was suggested to turn into a book by a yet another pioneer in the field, Ronald Graham.
The introduction covers the history of the field, basic data structures, and necessary notions from the theory of computation and geometry.
The subsequent sections cover geometric searching (point location, range searching), convex hull computation, proximity-related problems (closest points, computation and applications of the Voronoi diagram, Euclidean minimum spanning tree, triangulations, etc.), geometric intersection problems, algorithms for sets of isothetic rectangles

Herbert Edelsbrunner (1987). Algorithms in Combinatorial Geometry. Springer-Verlag. ISBN 0-89791-517-8. 
The monograph is a rather advanced exposition of problems and approaches in computational geometry focused on the role of hyperplane arrangements, which are shown to constitute a basic underlying combinatorial-geometric structure in certain areas of the field. The primary target audience are active theoretical researchers in the field, rather than application developers. Unlike most of books in computational geometry focused on 2- and 3-dimensional problems (where most applications of computational geometry are), the book aims to treat its subject in the general multi-dimensional setting.

Mark de Berg, Otfried Cheong, Marc van Kreveld, and Mark Overmars (2008). Computational Geometry (3rd revised ed.). Springer-Verlag. ISBN 3-540-77973-6. 1st edition (1997): ISBN 3-540-61270-X. CS1 maint: Uses authors parameter (link)
The textbook provides an introduction to computation geometry from the point of view of practical applications. Starting with an introduction chapter, each of the 15 remaining ones formulates a real application problem, formulates an underlying geometrical problem, and discusses techniques of computational geometry useful for its solution, with algorithms provided in pseudocode. The book treats mostly 2- and 3-dimensional geometry. The goal of the book is to provide a comprehensive introduction into methods and approached, rather than the cutting edge of the research in the field: the presented algorithms provide transparent and reasonably efficient solutions based on fundamental ""building blocks"" of computational geometry.
The book consists of the following chapters (which provide both solutions for the topic of the title and its applications): ""Computational Geometry (Introduction)"" ""Line Segment Intersection"", ""Polygon Triangulation"", ""Linear Programming"", ""Orthogonal Range Searching"", ""Point Location"", ""Voronoi Diagrams"", ""Arrangements and Duality"", ""Delaunay Triangulations"", ""More Geometric Data Structures"", ""Convex Hulls"", ""Binary Space Partitions"", ""Robot Motion Planning"", ""Quadtrees"", ""Visibility Graphs"", ""Simplex Range Searching"".

Jean-Daniel Boissonnat, Mariette Yvinec (1998). Algorithmic Geometry. Cambridge University Press. ISBN 0-521-56529-4. Translation of a 1995 French edition. CS1 maint: Uses authors parameter (link)
Joseph O'Rourke (1998). Computational Geometry in C (2nd ed.). Cambridge University Press. ISBN 0-521-64976-5. 
Satyan Devadoss, Joseph O'Rourke (2011). Discrete and Computational Geometry. Princeton University Press. ISBN 978-0-691-14553-2. CS1 maint: Uses authors parameter (link)
Jim Arlow (2014). Interactive Computational Geometry - A taxonomic approach. Mountain Way Limited. 1st edition: ISBN 978-0-9572928-2-6. 
This book is an interactive introduction to the fundamental algorithms of computational geometry, formatted as an interactive document viewable using software based on Mathematica.


=== Specialized textbooks and monographs ===
Efi Fogel, Dan Halperin, and Ron Wein (2012). CGAL Arrangements and Their Applications, A Step-by-Step Guide. Springer-Verlag. ISBN 978-3-642-17283-0. CS1 maint: Uses authors parameter (link)
Fajie Li and Reinhard Klette (2011). Euclidean Shortest Paths. Springer-Verlag. ISBN 978-1-4471-2255-5. CS1 maint: Uses authors parameter (link)
Philip J. Schneider and David H. Eberly (2002). Geometric Tools for Computer Graphics. Morgan Kaufmann. CS1 maint: Uses authors parameter (link)
Kurt Mehlhorn (1984). Data Structures and Efficient Algorithms 3: Multi-dimensional Searching and Computational Geometry. Springer-Verlag. 
Ketan Mulmuley (1994). Computational Geometry: An Introduction Through Randomized Algorithms. Prentice-Hall. ISBN 0-13-336363-5. 
János Pach and Pankaj K. Agarwal (1995). Combinatorial Geometry. John Wiley and Sons. ISBN 0-471-58890-3. CS1 maint: Uses authors parameter (link)
Micha Sharir and Pankaj K. Agarwal (1995). Davenport–Schinzel Sequences and Their Geometric Applications. Cambridge University Press. ISBN 0-521-47025-0. CS1 maint: Uses authors parameter (link)
Kurt Mehlhorn and Stefan Naeher (1999). LEDA, A Platform for Combinatorial and Geometric Computing. Cambridge University Press. ISBN 0-521-56329-1. CS1 maint: Uses authors parameter (link)
Selim G. Akl and Kelly A. Lyons (1993). Parallel Computational Geometry. Prentice-Hall. ISBN 0-13-652017-0. CS1 maint: Uses authors parameter (link)
The books discusses parallel algorithms for basic problems in computational geometry in various models of parallel computation.

Joseph O'Rourke (1987). Art Gallery Theorems and Algorithms. Oxford University Press. 
Hanan Samet (1990). The Design and Analysis of Spatial Data Structures. Addison-Wesley. 
Clara I. Grima & Alberto Márquez (1990). Computational Geometry on Surfaces: Performing Computational Geometry on the Cylinder, the Sphere, the Torus, and the Cone. Kluwer Academic Publishers. ISBN 1-4020-0202-5. 
The book shows how classical problems of computational geometry and algorithms for their solutions may be adapted or redesigned to work on surfaces other than plane. After defining notations and ways of positioning on these surfaces, the book considers the problems of the construction of convex hulls, Voronoi diagrams, and triangulations, proximity problems, and visibility problems.

Ghosh, Subir Kumar (2007). Visibility Algorithms in the Plane. Cambridge University Press. ISBN 0-521-87574-9. 
Contents: Preface; 1. Background; 2. Point visibility; 3. Weak visibility and shortest paths; 4. L-R visibility and shortest paths; 5. Visibility graphs; 6. Visibility graph theory; 7. Visibility and link paths; 8. Visibility and path queries

Giri Narasimhan; Michiel Smid (2007). Geometric Spanner Networks. Cambridge University Press. ISBN 0-521-81513-4. 
Contents:
Part I. Introduction: 1. Introduction; 2. Algorithms and graphs; 3. The algebraic computation-tree model;
Part II. Spanners Based on Simplical Cones: 4. Spanners based on the Q-graph; 5. Cones in higher dimensional space and Q-graphs; 6. Geometric analysis: the gap property; 7. The gap-greedy algorithm; 8. Enumerating distances using spanners of bounded degree;
Part III. The Well Separated Pair Decomposition and its Applications: 9. The well-separated pair decomposition; 10. Applications of well-separated pairs; 11. The Dumbbell theorem; 12. Shortcutting trees and spanners with low spanner diameter; 13. Approximating the stretch factor of Euclidean graphs;
Part IV. The Path Greedy Algorithm: 14. Geometric analysis: the leapfrog property; 15. The path-greedy algorithm; Part V. Further Results and Applications: 16. The distance range hierarchy; 17. Approximating shortest paths in spanners; 18. Fault-tolerant spanners; 19. Designing approximation algorithms with spanners; 20. Further results and open problems.

Erik D. Demaine; Joseph O'Rourke (2007). Geometric Folding Algorithms: Linkages, Origami, Polyhedra. Cambridge University Press. ISBN 978-0-521-85757-4. 


=== References ===
Jacob E. Goodman; Joseph O'Rourke, eds. (2004) [1997]. Handbook of Discrete and Computational Geometry. North-Holland. 1st edition: ISBN 0-8493-8524-5, 2nd edition: ISBN 1-58488-301-4. 
In its organization, the book resembles the classical handbook in algorithms, Introduction to Algorithms, in its comprehensiveness, only restricted to discrete and computational geometry, computational topology, as well as a broad range of their applications. The second edition expands the book by half, with 14 chapters added and old chapters brought up to date. Its 65 chapters (in over 1,500 pages) are written by a large team of active researchers in the field.

Jörg-Rudiger Sack; Jorge Urrutia (1998). Handbook of Computational Geometry. North-Holland. 1st edition: ISBN 0-444-82537-1, 2nd edition (2000): 1-584-88301-4. 
The handbook contains survey chapters in classical and new studies in geometric algorithms: hyperplane arrangements, Voronoi diagrams, geometric and spatial data structures, polygon decomposition, randomized algorithms, derandomization, parallel computational geometry (deterministic and randomized), visibility, Art Gallery and Illumination Problems, closest point problems, link distance problems, similarity of geometric objects, Davenport–Schinzel sequences, spanning trees and spanners for geometric graphs, robustness and numerical issues for geometric algorithms, animation, and graph drawing.
In addition, the book surveys applications of geometric algorithms in such areas as geographic information systems, geometric shortest path and network optimization and mesh generation.

Ding-Zhu Du; Frank Hwang (1995). Computing in Euclidean Geometry. Lectures Notes Series on Computing. 4 (2nd ed.). World Scientific. ISBN 981-02-1876-1. 
""This book is a collection of surveys and exploratory articles about recent developments in the field of computational Euclidean geometry."" Its 11 chapters cover quantitative geometry, a history of computational geometry, mesh generation, automated generation of geometric proofs, randomized geometric algorithms, Steiner tree problems, Voronoi diagrams and Delaunay triangulations, constraint solving, spline surfaces, network design, and numerical primitives for geometric computing.


== Numerical computational geometry (geometric modelling, computer-aided geometric design) ==


=== Monographs ===
I. D. Faux; Michael J. Pratt (1980). Computational Geometry for Design and Manufacture (Mathematics & Its Applications). Prentice Hall. ISBN 0-470-27069-1. 
Alan Davies; Philip Samuels (1996). An Introduction to Computational Geometry for Curves and Surfaces. Oxford University Press. ISBN 0-19-853695-X. 
Jean-Daniel Boissonnat; Monique Teillaud (2006). Effective Computational Geometry for Curves and Surfaces (Mathematics and Visualization Series ed.). Springer Verlag. ISBN 3-540-33258-8. 
Gerald Farin (1988). Curves and Surfaces for Computer Aided Geometric Design. Academic Press. ISBN 0-12-249050-9. 
Richard H. Bartels, John C Beatty, and Brian A. Barsky (1987). Splines for Use in Computer Graphics and Geometric Modeling. Morgan Kaufmann. ISBN 0-934613-27-3. CS1 maint: Uses authors parameter (link)
Christoph M. Hoffmann (1989). Geometric and Solid Modeling: An Introduction. Morgan Kaufmann. ISBN 1-55860-067-1.  The book is out of print. Its main chapters are:
Basic Concepts
Boolean Operations on Boundary Representation
Robust and Error-Free Geometric Operations
Representation of Curved Edges and Faces
Surface Intersections
Gröbner Bases Techniques


== Other ==
Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Introduction to Algorithms, Second Edition. MIT Press and McGraw-Hill, 1990. ISBN 0-262-03293-7. — This book has a chapter on geometric algorithms.
Frank Nielsen. Visual Computing: Graphics, Vision, and Geometry, Charles River Media, 2005. ISBN 1-58450-427-7 — This book combines graphics, vision and geometric computing and targets advanced undergraduates and professionals in game development and graphics. Includes some concise C++ code for common tasks.
Jeffrey Ullman, Computational Aspects of VLSI, Computer Science Press, 1984, ISBN 0-914894-95-1 — Chapter 9: ""Algorithms for VLSI Design Tools"" describes algorithms for polygon operations involved in electronic design automation (design rule checking, circuit extraction, placement and routing).
D.T. Lee, Franco P. Preparata, ""Computational Geometry - A Survey"", IEEE Trans. Computers, vol 33 no. 12, 1984, 1072-1101. (Errata: IEEE Tr. C. vol.34, no.6, 1985) Although not a book, this 30-page paper is of historical interest, because it was the first comprehensive coverage, the 1984 snapshot of the emerging discipline, with 354-item bibliography.
George T. Heineman; Gary Pollice & Stanley Selkow (2008). ""Chapter 9:Computational Geometry"". Algorithms in a Nutshell. Oreilly Media. pp. 251–298. ISBN 978-0-596-51624-6.  — This book has associated code repository with full Java implementations


== Conferences ==
Annual Symposium on Computational Geometry (SoCG)
Canadian Conference on Computational Geometry (CCCG)
Japanese Conference on Discrete and Computational Geometry (JCDCG)
The conferences below, of broad scope, published many seminal papers in the domain.
ACM-SIAM Symposium on Discrete Algorithms (SODA)
Annual ACM Symposium on Theory of Computing (STOC)
Annual IEEE Symposium on Foundations of Computer Science (FOCS)
Annual Allerton Conference on Communications, Control and Computing (ACCC)


== Paper collections ==
""Combinatorial and Computational Geometry"", eds. Jacob E. Goodman, János Pach, Emo Welzl (MSRI Publications – Volume 52), 2005, ISBN 0-521-84862-8.
32 papers, including surveys and research articles on geometric arrangements, polytopes, packing, covering, discrete convexity, geometric algorithms and their computational complexity, and the combinatorial complexity of geometric objects.

""Surveys on Discrete and Computational Geometry: Twenty Years Later"" (""Contemporary Mathematics"" series), American Mathematical Society, 2008, ISBN 0-8218-4239-0


== See also ==
List of important publications in mathematics


== References ==


== External links ==
Computational Geometry Pages"
52,Computational neurogenetic modeling,8402086,19376,"Computational neurogenetic modeling (CNGM) is concerned with the study and development of dynamic neuronal models for modeling brain functions with respect to genes and dynamic interactions between genes. These include neural network models and their integration with gene network models. This area brings together knowledge from various scientific disciplines, such as computer and information science, neuroscience and cognitive science, genetics and molecular biology, as well as engineering.


== Levels of processing ==


=== Molecular kinetics ===
Models of the kinetics of proteins and ion channels associated with neuron activity represent the lowest level of modeling in a computational neurogenetic model. The altered activity of proteins in some diseases, such as the amyloid beta protein in Alzheimer's disease, must be modeled at the molecular level to accurately predict the effect on cognition. Ion channels, which are vital to the propagation of action potentials, are another molecule that may be modeled to more accurately reflect biological processes. For instance, to accurately model synaptic plasticity (the strengthening or weakening of synapses) and memory, it is necessary to model the activity of the NMDA receptor (NMDAR). The speed at which the NMDA receptor lets Calcium ions into the cell in response to Glutamate is an important determinant of Long-term potentiation via the insertion of AMPA receptors (AMPAR) into the plasma membrane at the synapse of the postsynaptic cell (the cell that receives the neurotransmitters from the presynaptic cell).


=== Genetic regulatory network ===

In most models of neural systems neurons are the most basic unit modeled. In computational neurogenetic modeling, to better simulate processes that are responsible for synaptic activity and connectivity, the genes responsible are modeled for each neuron.
A gene regulatory network, protein regulatory network, or gene/protein regulatory network, is the level of processing in a computational neurogenetic model that models the interactions of genes and proteins relevant to synaptic activity and general cell functions. Genes and proteins are modeled as individual nodes, and the interactions that influence a gene are modeled as excitatory (increases gene/protein expression) or inhibitory (decreases gene/protein expression) inputs that are weighted to reflect the effect a gene or protein is having on another gene or protein. Gene regulatory networks are typically designed using data from microarrays.
Modeling of genes and proteins allows individual responses of neurons in an artificial neural network that mimic responses in biological nervous systems, such as division (adding new neurons to the artificial neural network), creation of proteins to expand their cell membrane and foster neurite outgrowth (and thus stronger connections with other neurons), up-regulate or down-regulate receptors at synapses (increasing or decreasing the weight (strength) of synaptic inputs), uptake more neurotransmitters, change into different types of neurons, or die due to necrosis or apoptosis. The creation and analysis of these networks can be divided into two sub-areas of research: the gene up-regulation that is involved in the normal functions of a neuron, such as growth, metabolism, and synapsing; and the effects of mutated genes on neurons and cognitive functions.


=== Artificial neural network ===

An artificial neural network generally refers to any computational model that mimics the central nervous system, with capabilities such as learning and pattern recognition. With regards to computational neurogenetic modeling, however, it is often used to refer to those specifically designed for biological accuracy rather than computational efficiency. Individual neurons are the basic unit of an artificial neural network, with each neuron acting as a node. Each node receives weighted signals from other nodes that are either excitatory or inhibitory. To determine the output, a transfer function (or activation function) evaluates the sum of the weighted signals and, in some artificial neural networks, their input rate. Signal weights are strengthened (long-term potentiation) or weakened (long-term depression) depending on how synchronous the presynaptic and postsynaptic activation rates are (Hebbian theory).
The synaptic activity of individual neurons is modeled using equations to determine the temporal (and in some cases, spatial) summation of synaptic signals, membrane potential, threshold for action potential generation, the absolute and relative refractory period, and optionally ion receptor channel kinetics and Gaussian noise (to increase biological accuracy by incorporation of random elements). In addition to connectivity, some types of artificial neural networks, such as spiking neural networks, also model the distance between neurons, and its effect on the synaptic weight (the strength of a synaptic transmission).


=== Combining gene regulatory networks and artificial neural networks ===
For the parameters in the gene regulatory network to affect the neurons in the artificial neural network as intended there must be some connection between them. In an organizational context, each node (neuron) in the artificial neural network has its own gene regulatory network associated with it. The weights (and in some networks, frequencies of synaptic transmission to the node), and the resulting membrane potential of the node (including whether an action potential is produced or not), affect the expression of different genes in the gene regulatory network. Factors affecting connections between neurons, such as synaptic plasticity, can be modeled by inputting the values of synaptic activity-associated genes and proteins to a function that re-evaluates the weight of an input from a particular neuron in the artificial neural network.


=== Incorporation of other cell types ===
Other cell types besides neurons can be modeled as well. Glial cells, such as astroglia and microglia, as well as endothelial cells, could be included in an artificial neural network. This would enable modeling of diseases where pathological effects may occur from sources other than neurons, such as Alzheimer's disease.


== Factors affecting choice of artificial neural network ==
While the term artificial neural network is usually used in computational neurogenetic modeling to refer to models of the central nervous system meant to possess biological accuracy, the general use of the term can be applied to many gene regulatory networks as well.


=== Time variance ===
Artificial neural networks, depending on type, may or may not take into account the timing of inputs. Those that do, such as spiking neural networks, fire only when the pooled inputs reach a membrane potential is reached. Because this mimics the firing of biological neurons, spiking neural networks are viewed as a more biologically accurate model of synaptic activity.


=== Growth and shrinkage ===
To accurately model the central nervous system, creation and death of neurons should be modeled as well. To accomplish this, constructive artificial neural networks that are able to grow or shrink to adapt to inputs are often used. Evolving connectionist systems are a subtype of constructive artificial neural networks (evolving in this case referring to changing the structure of its neural network rather than by mutation and natural selection).


=== Randomness ===
Both synaptic transmission and gene-protein interactions are stochastic in nature. To model biological nervous systems with greater fidelity some form of randomness is often introduced into the network. Artificial neural networks modified in this manner are often labeled as probabilistic versions of their neural network sub-type (e.g., pSNN).


=== Incorporation of fuzzy logic ===
Fuzzy logic is a system of reasoning that enables an artificial neural network to deal in non-binary and linguistic variables. Biological data is often unable to be processed using Boolean logic, and moreover accurate modeling of the capabilities of biological nervous systems requires fuzzy logic. Therefore, artificial neural networks that incorporate it, such as evolving fuzzy neural networks (EFuNN) or Dynamic Evolving Neural-Fuzzy Inference Systems (DENFIS), are often used in computational neurogenetic modeling. The use of fuzzy logic is especially relevant in gene regulatory networks, as the modeling of protein binding strength often requires non-binary variables.


=== Types of learning ===
Artificial Neural Networks designed to simulate of the human brain require an ability to learn a variety of tasks that is not required by those designed to accomplish a specific task. Supervised learning is a mechanism by which an artificial neural network can learn by receiving a number of inputs with a correct output already known. An example of an artificial neural network that uses supervised learning is a multilayer perceptron (MLP). In unsupervised learning, an artificial neural network is trained using only inputs. Unsupervised learning is the learning mechanism by which a type of artificial neural network known as a self-organizing map (SOM) learns. Some types of artificial neural network, such as evolving connectionist systems, can learn in both a supervised and unsupervised manner.


== Improvement ==
Both gene regulatory networks and artificial neural networks have two main strategies for improving their accuracy. In both cases the output of the network is measured against known biological data using some function, and subsequent improvements are made by altering the structure of the network. A common test of accuracy for artificial neural networks is to compare some parameter of the model to data acquired from biological neural systems, such as from an EEG. In the case of EEG recordings, the local field potential (LFP) of the artificial neural network is taken and compared to EEG data acquired from human patients. The relative intensity ratio (RIRs) and fast Fourier transform (FFT) of the EEG are compared with those generated by the artificial neural networks to determine the accuracy of the model.


=== Genetic algorithm ===

Because the amount of data on the interplay of genes and neurons and their effects is not enough to construct a rigorous model, evolutionary computation is used to optimize artificial neural networks and gene regulatory networks, a common technique being the genetic algorithm. A genetic algorithm is a process that can be used to refine models by mimicking the process of natural selection observed in biological ecosystems. The primary advantages are that, due to not requiring derivative information, it can be applied to black box problems and multimodal optimization. The typical process for using genetic algorithms to refine a gene regulatory network is: first, create a population; next, to create offspring via a crossover operation and evaluate their fitness; then, on a group chosen for high fitness, simulate mutation via a mutation operator; finally, taking the now mutated group, repeat this process until a desired level of fitness is demonstrated. 


=== Evolving systems ===
Methods by which artificial neural networks may alter their structure without simulated mutation and fitness selection have been developed. A dynamically evolving neural network is one approach, as the creation of new connections and new neurons can be modeled as the system adapts to new data. This enables the network to evolve in modeling accuracy without simulated natural selection. One method by which dynamically evolving networks may be optimized, called evolving layer neuron aggregation, combines neurons with sufficiently similar input weights into one neuron. This can take place during the training of the network, referred to as online aggregation, or between periods of training, referred to as offline aggregation. Experiments have suggested that offline aggregation is more efficient.


== Potential applications ==
A variety of potential applications have been suggested for accurate computational neurogenetic models, such as simulating genetic diseases, examining the impact of potential treatments, better understanding of learning and cognition, and development of hardware able to interface with neurons.
The simulation of disease states is of particular interest, as modeling both the neurons and their genes and proteins allows linking genetic mutations and protein abnormalities to pathological effects in the central nervous system. Among those diseases suggested as being possible targets of computational neurogenetic modeling based analysis are epilepsy, schizophrenia, mental retardation, brain aging and Alzheimer's disease, and Parkinson's disease.


== See also ==
Memristor


== References ==


== External links ==
http://ecos.watts.net.nz/Algorithms/"
53,Technical Committee on VLSI,47044276,18651,"Technical Committee on VLSI (TCVLSI) is a constituency of IEEE Computer Society (IEEE-CS) that oversees various technical activities related to computer hardware, integrated circuit design, software for computer hardware design. TCVLSI is one of the 26 technical committees/councils of IEEE-CS that covers various specializations of computer science and computer engineering discipline. IEEE-CS is the largest of the 39 societies of Institute of Electrical and Electronics Engineers (IEEE). The technical scope of TCVLSI covers the Computer-aided design (CAD) or electronic design automation (EDA) techniques to facilitate the VLSI design process. The VLSI may include various types of circuits and systems, such as digital circuits and systems, analog circuits, as well as mixed-signal circuits and systems. The emphasis of TCVLSI widely covers the integrating the design, Computer-aided design (CAD), fabrication, application, and business aspects of Very-large-scale integration (VLSI) while encompassing both hardware and software.


== Membership ==
Membership in TCVLSI is open and free of charge to researchers, practitioners and students, and general prospective members are not required to be members of IEEE or IEEE Computer Society. However, to serve on the executive committee, a member needs to belong to the IEEE Computer Society. The Chair of the TCVLSI is elected by the voting members of TCVLSI. Other executive members of TCVLSI are appointed by the Chair.


== Technical Activities ==
The TCVLSI sponsors conferences, special sessions, and workshops for the IEEE-CS. TCVLSI also runs VLSI Circuits and Systems Letter, three times a year, which has many components including a very selective dissemination of quick papers, TCVLSI member news, upcoming conferences, workshops, call for papers, and funding opportunities of interest to members of TCVLSI. TCVLSI provides several student travel grants for the TCVLSI sponsored conferences. TCVLSI also sponsors best paper awards for the sponsored conferences.


== TCVLSI Awards ==
TCVLSI, IEEE-CS introduces the following awards from 2018. More information of each award is available at the following link: 
IEEE-CS TCVLSI Technical Excellence Award
IEEE-CS TCVLSI Distinguished Leadership Award
IEEE-CS TCVLSI Distinguished Research Award
IEEE-CS TCVLSI Mid-Career Research Achievement Award
IEEE-CS TCVLSI Outstanding Editor Award


== IEEE VLSI Circuits and Systems Letter ==
The VLSI Circuits and Systems Letter is freely available to the global audience online. At present, VLSI Circuits and Systems Letter (VCAL) is published twice a year which provides timely updates on science, engineering, and technologies as well as educations and opportunities related to VLSI circuits and systems. At this point, the letter contains the following sections: Features, Opinion, Updates, and Outreach and Community. The published issues has been made available below for quick access to the readers. The editorial team of the letter is also presented.


=== Published Volumes and Issues ===


==== Volume 4 ====
VLSI Circuits and Systems Letter: Volume 4, Issue 1, Feb 2018: pdf version


==== Volume 3 ====
VLSI Circuits and Systems Letter: Volume 3, Issue 3, Oct 2017: pdf version
VLSI Circuits and Systems Letter: Volume 3, Issue 2, June 2017: pdf version
VLSI Circuits and Systems Letter: Volume 3, Issue 1, Feb 2017: pdf version


==== Volume 2 ====
VLSI Circuits and Systems Letter: Volume 2, Issue 2, Oct 2016: pdf version
VLSI Circuits and Systems Letter: Volume 2, Issue 1, April 2016: pdf version


==== Volume 1 ====
VLSI Circuits and Systems Letter: Volume 1, Issue 2, October 2015: pdf version
VLSI Circuits and Systems Letter: Volume 1, Issue 1, April 2015: pdf version


=== Editor-in-Chiefs (EiCs) ===
Saraju Mohanty, University of North Texas, USA
Anirban Sengupta, Indian Institute of Technology Indore, India


=== Deputy Editor-in-Chief ===
Yiyu Shi, University of Notre Dame, USA


=== Associate Editors ===
Helen Li, Duke University, USA
Hideharu Amano, Keio University, Japan
Himanshu Thapliyal, University of Kentucky, USA
Jun Tao, Fudan University, China
Michael Hübner, Ruhr University Bochum, Germany
Mike Borowczak, University of Wyoming, USA
Qi Zhu, University of California, Riverside, USA
Saket Srivastava, University of Lincoln, UK
Shiyan Hu, Michigan Technological University, USA
Yasuhiro Takahashi, Gifu University, Japan
Sergio Saponara, University of Pisa, Italy


==== Emeritus Editor-in-Chief (EiC) ====
Xin Li, Duke University, USA


==== Past Associate Editors ====
Prasun Ghosal, Indian Institute of Engineering Science and Technology (IIEST), India
Jawar Singh, Indian Institute of Information Technology, Design and Manufacturing, Jabalpur, India


== Key People ==


=== Executive Committee ===
Chair - Saraju Mohanty, University of North Texas, USA
Vice Chair for Conferences - Jia Di, University of Arkansas, USA
Treasurer - Hai (Helen) Li, Duke University, USA
Vice Chair for Membership - Dhruva Ghai, Oriental University Indore, India
Vice Chair for Liaison - Nagi Naganathan, Broadcom Limited, USA
Vice Chair for Outreach and Webmaster - Mike Borowczak, University of Wyoming, USA
IEEE VCAL, Editor-in-Chiefs (EiCs)–
Saraju Mohanty, University of North Texas, USA
Anirban Sengupta, Indian Institute of Technology Indore


=== Past Chairs ===
2002-2014: Joseph Cavallaro, Rice University.
2000-2002: Vijaykrishnan Narayanan, Pennsylvania State University.
1996-2000: Nagarajan Ranganathan, University of South Florida.
1984-1986: Amar Mukherjee, University of Central Florida.


== TCVLSI Sister Conferences ==


=== Sponsored Conferences ===
ARITH, IEEE Symposium on Computer Arithmetic
ARITH 25 (2018): July 25–27, 2017, Amherst, MA, USA
ARITH 24 (2017): July 24–26, 2017, London, UK
ARITH 23 (2016): July 10–13, 2016, Santa Clara, California, USA
ARITH 22 (2015): June 22–24, 2015, Lyon, France
Steering Committee:
Earl Swartzlander, University of Texas at Austin, Chair
Elisardo Antelo, University of Santiago de Compostela, Spain
Jean-Claude Bajard, Université Pierre et Marie Curie, Paris, France
Javier Bruguera, ARM Cambridge, UK
Neil Burgess, ARM, USA
Marius Cornea, Intel, USA
Debjit DasSarma, AMD, USA
Milos Ercegovac, University of California at Los Angeles, USA
David Hough, Oracle, USA
Paolo Ienne, EPFL, Switzerland
Israel Koren, University of Massachusetts, USA
Peter Kornerup, University of Southern Denmark, Denmark
David Matula, Southern Methodist University, USA
Paolo Montuschi, Politecnico di Torino, Italy
Jean-Michel Muller, CNRS, France
Alberto Nannarelli, Technical University of Denmark, Denmark
Vojin G. Oklobdzija, ACSEL, USA
Michael Schulte, AMD, USA
Eric Schwarz, IBM, USA
Peter-Michael Seidel, AMD, USA
Naofumi Takagi, Kyoto University, Japan
Ping Tak Peter Tang, Intel, USA
Arnaud Tisserand, CNRS, IRISA, France
Julio Villalba, University of Malaga, Spain

ASAP, IEEE International Conference on Application-specific Systems, Architectures and Processors
ASAP 2017: July 10–12, 2017, Seattle, WA. Website: http://www.asapconference.org/
ASAP 2016: July 6–8, 2016, London, England. Website: http://www.asap2016.org/
ASAP 2015: July 27–29, 2015, Toronto, Canada. Website: http://www.eecg.toronto.edu/asap2015/
Steering Committee :
José A.B. Fortes, University of Florida, USA
Sun-Yuan Kung, Princeton University, USA
Wayne Luk, Imperial College London
Michael J. Schulte, University of Wisconsin Madison, USA
Earl Swartzlander, The University of Texas at Austin, USA

ASYNC, IEEE International Symposium on Asynchronous Circuits and Systems
ASYNC 2019: May 12–15, 2019, Aomori, Japan
ASYNC 2018: May 13–16, 2018, Vienna, Austria, Website: http://www.async2018.wien/
ASYNC 2017: May 21–24, 2017, San Diego, CA, USA, Website: http://www.async2017.org/
ASYNC 2016: May 8–11, 2016, Porto Alegre, Brazil, Website: http://www.inf.pucrs.br/async2016/
ASYNC 2015: May 4–6, 2015, Mountain View, CA, USA, Website: http://ee.usc.edu/async2015/
Steering Committee:
Montek Singh, University of North Carolina at Chapel Hill, Chair
Peter A. Beerel, University of Southern California
Edith Beigné, CEA-LETI, France
Erik Brunvand, University of Utah
Ney Laert Vilar Calazans, PUCRS, Brazil
Hong Chen, Tsinghua University, China
Mark Greenstreet, University of British Columbia, Canada
Ian Jones, Oracle Corporation
Joycee Mekie, Indian Institute of Technology Gandhinagar, India
Milos Krstic, IHP, Germany
Jens Sparsø, Technical University of Denmark
Andreas Steininger, Vienna University of Technology
Pascal Vivet, CEA-LETI, France
Eslam Yahya, Benha University, Egypt
Tomohiro Yoneda, NII, Japan
Jordi Cortadella, UPC, Spain

iSES, IEEE International Symposium on Smart Electronic Systems (formerly IEEE International Symposium on Nanoelectronic and Information Systems)
iSES 2018: December 17–19, 2018, Hyderabad, India
iNIS 2017: December 18–20, 2017, Bhopal, India
iNIS 2016: December 19–21, 2016, Gwalior, India
iNIS 2015: December 21–23, 2015, Indore, India
Steering Committee:
Saraju P. Mohanty, University of North Texas, USA, Chair
Dhruva Ghai, Oriental University, India, Vice Chair
Aida Todri-Sanial, CNRS-LIRMM, France
Ashok Srivastava, Louisiana State University, USA
Anirban Sengupta, Indian Institute of Technology Indore, India
Hai (Helen) Li, University of Pittsburgh, USA
Himanshu Thapliyal, University of Kentucky, USA
Jia Di, University of Arkansas, USA
Nabanita Das, Indian Statistical Institute, India
Prasun Ghosal, Indian Institute of Engineering Science and Technology, Shibpur, India
Sudeep Pasricha, Colorado State University, USA
Xin Li, Carnegie Mellon University, USA

ISVLSI, IEEE Computer Society Symposium on VLSI
ISVLSI 2020: July 6–8, 2020, Cyprus
ISVLSI 2019: July 8–10, 2019, Miami, FL, USA
ISVLSI 2018: July 9–11, 2018, Hong Kong
ISVLSI 2017: July 3–5, 2017, Bochum, Germany
ISVLSI 2016: July 11–13, 2016, Pittsburgh, Pennsylvania, USA
ISVLSI 2015: July 10–12, 2015, Montpellier, France
ISVLSI 2014: July 9–11, 2014, Tampa, Florida, USA
ISVLSI 2013: August 5–7, 2013, Natal, Brazil
ISVLSI 2012: August 19–21, 2012, Amherst, USA
Steering Committee:
Jürgen Becker, Karlsruhe Institute of Technology, Germany, Chair
Saraju P. Mohanty, University of North Texas, USA, Vice-Chair
Hai (Helen) Li, University of Pittsburgh, USA
Lionel Torres, University of Montpellier, France
Michael Hübner, Ruhr-University of Bochum, Germany
Nikolaos Voros, Technological Educational Institute of Messolonghi, Greece
Ricardo Reis, Universidade Federal do Rio Grande do Sul, Brazil
Sandip Kundu, University of Massachusetts, Amherst, USA
Sanjukta Bhanja, University of South Florida, USA
Susmita Sur-Kolay, Indian Statistical Institute, Kolkata, India
Vijaykrishnan Narayanan, Pennsylvanian State University, USA

IWLS, IEEE International Workshop on Logic & Synthesis
IWLS 2017: June 17 – 18, 2017, Thompson Conference Center — Austin, TX
IWLS 2016: June 10–11, 2016, Thompson Conference Center — Austin, TX
IWLS 2015: June 12–13, 2015, Computer History Museum — Mountain View, CA
Steering Committee:
Dirk Stroobandt, Ghent University, Belgium
Andre Reis, UFRGS, Brazil
Ilya Wagner, Intel, USA
Valeria Bertacco, University of Michigan, USA
Philip Brisk, University of California Riverside, USA
Stephen A. Edwards, Columbia University, USA
Alan Mishchenko, University of California Berkeley, USA

MSE, IEEE International Conference on Microelectronic Systems Education
MSE 2017: May 11–12, 2017, Banff, Canada
MSE 2015: May 20–21, 2015, Pittsburgh, PA, USA
Steering Committee:
Don Bouldin, University of Tennessee Knoxville, Chair

SLIP, ACM/IEEE System Level Interconnect Prediction
SLIP 2017: June 17, 2017, Austin Convention Center, Austin, TX, USA
SLIP 2016: June 4, 2016, Austin Convention Center, Austin, TX, USA
SLIP 2015: June 6, 2015, Moscone Center, San Francisco, CA, USA
Steering Committee :
Chuck Alpert, Cadence Inc., USA
Deming Chen, University of Illinois at Urbana-Champaign, USA
Chung-Kuan Cheng, University of California, San Diego, USA
Andrew B. Kahng, University of California, San Diego, USA
Michael Kishinevsky, Intel Corp., USA
Rasit O. Topaloglu, IBM Corp., USA

ECMSM, IEEE International Workshop of Electronics, Control, Measurement, Signals and their application to Mechatronics
ECMSM 2017: June 24–26, 2017, Mondragon, Spain, Website: http://ecmsm2017.mondragon.edu/en
ECMSM 2015: June 22–24, 2015, Liberec, Czech Republic, Website: http://ecmsm2015.tul.cz/
Steering Committee:
Francois Pigache, University of Toulouse, France, Chair
Yannick DEVILLE, France
Philippe JOLY, France
Zbynek KOLDOVSKÝ, Czechoslovakia
Jiri MALEK, Czechoslovakia
J. Carlos MUGARZA, Spain
Jaroslav NOSEK, Czechoslovakia
Zdenek PLIVA, Czechoslovakia


=== Technically Co-Sponsored Conferences ===
ACSD, International Conference on Application of Concurrency to System Design
ACSD 2017: June 25–30, 2017, Zaragoza, Spain, Website: http://pn2017.unizar.es/
ACSD 2016: June 19–24, 2016, Toruń, Poland, Website: http://pn2016.mat.umk.pl/
ACSD 2015: June 21–26, 2015, Brussels, Belgium, Website: http://www.ulb.ac.be/di/verif/pn2015acsd2015/
Steering Committee:
Alex Yakovlev, Newcastle University, Chair

VLSID, International Conference on VLSI Design
VLSID 2018: January 6–10, 2018, Pune, India
VLSID 2017: January 7–11, 2017, Hyderabad, India
VLSID 2016: January 4–8, 2016, Kolkata, India
VLSID 2015: January 3–7, 2016, Bangalore, India
Steering Committee:
Vishwani D. Agrawal, Auburn University, Chair


== References =="
54,Computational social choice,51245349,18188,"Computational social choice is a field at the intersection of social choice theory, theoretical computer science, and the analysis of multi-agent systems. It consists of the analysis of problems arising from the aggregation of preferences of a group of agents from a computational perspective. In particular, computational social choice is concerned with the efficient computation of outcomes of voting rules, with the computational complexity of various forms of manipulation, and issues arising from the problem of representing and eliciting preferences in combinatorial settings.


== Winner determination ==
The usefulness of a particular voting system can be severely limited if it takes a very long time to calculate the winner of an election. Therefore, it is important to design fast algorithms that can evaluate a voting rule when given ballots as input. As is common in computational complexity theory, an algorithm is thought to be efficient if it takes polynomial time. Many popular voting rules can be evaluated in polynomial time in a straightforward way (i.e., counting), such as the Borda count, approval voting, or the plurality rule. For rules such as the Schulze method or ranked pairs, more sophisticated algorithms can be used to show polynomial runtime. Certain voting systems, however, are computationally difficult to evaluate. In particular, winner determination for the Kemeny-Young method, Dodgson's method, and Young's method are all NP-hard problems. This has led to the development of approximation algorithms and fixed-parameter tractable algorithms to improve the theoretical calculation of such problems.


== Hardness of manipulation ==
By the Gibbard-Satterthwaite theorem, all non-trivial voting rules can be manipulated in the sense that voters can sometimes achieve a better outcome by misrepresenting their preferences, that is, they submit a non-truthful ballot to the voting system. Social choice theorists have long considered ways to circumvent this issue, such as the proposition by Bartholdi, Tovey, and Trick in 1989 based on computational complexity theory. They considered the second-order Copeland rule (which can be evaluated in polynomial time), and proved that it is NP-complete for a voter to decide, given knowledge of how everyone else has voted, whether it is possible to manipulate in such a way as to make some favored candidate the winner. The same property holds for single transferable vote.
Hence, assuming the widely believed hypothesis that P ≠ NP, there are instances where polynomial time is not enough to establish whether a beneficial manipulation is possible. Because of this, the voting rules that come with an NP-hard manipulation problem are ""resistant"" to manipulation. One should note that these results only concern the worst-case: it might well be possible that a manipulation problem is usually easy to solve, and only requires superpolynomial time on very unusual inputs.


== Other topics ==
Tournament solutions. A tournament solution is a rule that assigns to every tournament a set of winners. Since a preference profile induces a tournament through its majority relation, every tournament solution can also be seen as a voting rule which only uses information about the outcomes of pairwise majority contests. Many tournament solutions have been proposed, and computational social choice theorists have studied the complexity of the associated winner determination problems.
Preference restrictions. Restricted preference domains, such as single-peaked or single-crossing preferences, are an important area of study in social choice theory, since preferences from these domains avoid the Condorcet paradox and thus can circumvent impossibility results like Arrow's theorem and the Gibbard-Satterthwaite theorem. From a computational perspective, such domain restrictions are useful to speed up winner determination problems, both computationally hard single-winner and multi-winner rules can be computed in polynomial time when preferences are structured appropriately. On the other hand, manipulation problem also tend to be easy on these domains, so complexity shields against manipulation are less effective. Another computational problem associated with preference restrictions is that of recognizing when a given preference profile belongs to some restricted domain. This task is polynomial time solvable in many cases, including for single-peaked and single-crossing preferences, but can be hard for more general classes.
Multiwinner elections. While most traditional voting rules focus on selecting a single winner, many situations require selecting multiple winners. This is the case when a fixed-size parliament or a committee is to be elected, though multiwinner voting rules can also be used to select a set of recommendations or facilities or a shared bundle of items. Work in computational social choice has focused on defining such voting rules, understanding their properties, and studying the complexity of the associated winner determination problems.


== See also ==
Algorithmic game theory
Algorithmic mechanism design
Hedonic games
Cake-cutting
Fair division


== References ==


== External links ==
The COMSOC website, offering a collection of materials related to computational social choice, such as academic workshops, PhD theses, and a mailing list."
55,Paris Kanellakis Award,2977079,17867,"The Paris Kanellakis Theory and Practice Award is granted yearly by the Association for Computing Machinery (ACM) to honor ""specific theoretical accomplishments that have had a significant and demonstrable effect on the practice of computing"". It was instituted in 1996, in memory of Paris C. Kanellakis, a computer scientist who died with his immediate family in an airplane crash in South America in 1995 (American Airlines Flight 965). The award is accompanied by a prize of $10,000 and is endowed by contributions from Kanellakis's parents, with additional financial support provided by four ACM Special Interest Groups (SIGACT, SIGDA, SIGMOD, and SIGPLAN), the ACM SIG Projects Fund, and individual contributions.


== Winners ==


== Notes ==


== External links ==
Paris Kanellakis Theory and Practice Award on the ACM website.
The Paris Kanellakis Theory and Practice Award Committee on the ACM website."
56,International Solid-State Circuits Conference,1464404,17628,"International Solid-State Circuits Conference is a global forum for presentation of advances in solid-state circuits and Systems-on-a-Chip. The Conference offers a unique opportunity for engineers working at the cutting edge of IC design to maintain technical currency, and to network with leading experts. It is held every year in February at the San Francisco Marriott hotel in downtown San Francisco. ISSCC is sponsored by IEEE Solid-State Circuits Society.
According to The Register, ""The ISSCC event is the second event of each new year, following the Consumer Electronics Show, where new PC processors and sundry other computing gadgets are brought to market.""


== History of ISSCC ==
Early participants in the inaugural conference in 1954 belonged to the Institute of Radio Engineers (IRE) Circuit Theory Group and the IRE subcommittee of Transistor Circuits. The conference was held in Philadelphia and local chapters of IRE and American Institute of Electrical Engineers (AIEE) were in attendance. Later on AIEE and IRE would merge to become the present-day IEEE.
The first conference consisted of papers from six organizations: Bell Telephone Laboratories, General Electric, RCA, Philco, Massachusetts Institute of Technology and the University of Pennsylvania. The registration was $4 (early registration was $3) and 601 people registered. International attendees arrived from Canada, England and Japan. With subsequent conferences came many more international participants with the first international presentation in 1958. By 1965, the number of overseas program committee members increased to 8 and in 1970 the overseas members began meeting separately in both Europe and Japan. Selected members of these regional program committees would attend the final program meeting in America.
The name of the 1954 Conference appears in various publications and documents as: ""The Transistor Conference"", ""The Conference on Transistor Circuits"", ""The Philadelphia Conference"", or ""The National Conference on Transistor Circuits"". The current name ""International Solid-State Circuits Conference"" was settled by the organizers in 1960.
While ISSCC was founded in Philadelphia, in the mid-1960s the center of semiconductor development in the United States was shifting west. In 1978, the conference was held on alternate coasts with New York soon substituting for Philadelphia. In 1990, San Francisco became the Conference’s permanent home.
In 2013 ISSCC is celebrating its 60th anniversary and will have several special programs to celebrate 60 years of circuit and SoC innovation.


== Evening/Education Program ==
During its evolution, ISSCC has strengthened its educational by adding the Short course in 1993 and Tutorials in 1995. The Short Course is directed towards engineers facing significant new knowledge demands. The purpose of tutorials is to provide engineers new to the topic to quickly ramp up and gain ""instantbackground"". Forums give a deep-dive look into topics/applications of interest to the general audience. By 2011, ISSCC had evolved to a five-day format with up to five simultaneous events.


== Technical Program Committee ==
The Technical Program Committee (TPC) in early years was extremely fluid in order to deal with the constantly changing topics in the industry. By 1968 the list of subcommittees had settled to Digital, Analog (Linear), Microwave and Other, where the subcommittee members in Other would address the one-of-a-kind papers. In the 80’s, the Microwave Subcommittee was dropped from the program as the overlap between the topics and attendees was diminishing. In addition, Digital split into Digital, Memory and Signal Processing subcommittees. In 1992, Emerging Technologies was launched and chartered to seek out the one-of-a-kind applications which may find a home in ISSCC. Today there are 10 subcommittees: Analog, Data Converters, Energy Efficient Digital (EED), High-Performance Digital (HPD), Imagers, MEMs, Medical and Displays (IMMD), Memory, RF, Technology Directions (formerly Emerging Technologies), Wireless and Wireline.


== TPC chairs ==


== European Committee Chairs ==


== Far East Committee Chairs ==


== Executive Committee ==
ISSCC is a strictly non-profit organization whose vision and finances is run by the Executive Committee. From formative years through 1980 the Conference chair was usually filled by the previous year’s Program Chair. To provide needed continuity, the term of Conference Chair was extended to at least 5 years


== Conference Chairs ==


== See also ==
Custom Integrated Circuits Conference
Hot Chips
International Electron Devices Meeting
Symposia on VLSI Technology and Circuits


== References ==

W. David Pricer, ""ISSCC: Sixty Years of Innovative Evolution"", 2013 Commemorative Supplement to the Digest of Technical Papers, ISSCC 2013 (to be published)


== External links ==
Official website"
57,Agent-based computational economics,10941831,17610,"Agent-based computational economics (ACE) is the area of computational economics that studies economic processes, including whole economies, as dynamic systems of interacting agents. As such, it falls in the paradigm of complex adaptive systems. In corresponding agent-based models, the ""agents"" are ""computational objects modeled as interacting according to rules"" over space and time, not real people. The rules are formulated to model behavior and social interactions based on incentives and information. Such rules could also be the result of optimization, realized through use of AI methods (such as Q-learning and other reinforcement learning techniques).
The theoretical assumption of mathematical optimization by agents in equilibrium is replaced by the less restrictive postulate of agents with bounded rationality adapting to market forces. ACE models apply numerical methods of analysis to computer-based simulations of complex dynamic problems for which more conventional methods, such as theorem formulation, may not find ready use. Starting from initial conditions specified by the modeler, the computational economy evolves over time as its constituent agents repeatedly interact with each other, including learning from interactions. In these respects, ACE has been characterized as a bottom-up culture-dish approach to the study of economic systems.
ACE has a similarity to, and overlap with, game theory as an agent-based method for modeling social interactions. But practitioners have also noted differences from standard methods, for example in ACE events modeled being driven solely by initial conditions, whether or not equilibria exist or are computationally tractable, and in the modeling facilitation of agent autonomy and learning.
The method has benefited from continuing improvements in modeling techniques of computer science and increased computer capabilities. The ultimate scientific objective of the method is to ""test theoretical findings against real-world data in ways that permit empirically supported theories to cumulate over time, with each researcher’s work building appropriately on the work that has gone before."" The subject has been applied to research areas like asset pricing, competition and collaboration, transaction costs, market structure and industrial organization and dynamics, welfare economics, and mechanism design, information and uncertainty, macroeconomics, and Marxist economics.


== Overview ==
The ""agents"" in ACE models can represent individuals (e.g. people), social groupings (e.g. firms), biological entities (e.g. growing crops), and/or physical systems (e.g. transport systems). The ACE modeler provides the initial configuration of a computational economic system comprising multiple interacting agents. The modeler then steps back to observe the development of the system over time without further intervention. In particular, system events should be driven by agent interactions without external imposition of equilibrium conditions. Issues include those common to experimental economics in general and development of a common framework for empirical validation and resolving open questions in agent-based modeling.
ACE is an officially designated special interest group (SIG) of the Society for Computational Economics. Researchers at the Santa Fe Institute have contributed to the development of ACE.


== Example: finance ==
One area where ACE methodology has frequently been applied is asset pricing. W. Brian Arthur, Eric Baum, William Brock, Cars Hommes, and Blake LeBaron, among others, have developed computational models in which many agents choose from a set of possible forecasting strategies in order to predict stock prices, which affects their asset demands and thus affects stock prices. These models assume that agents are more likely to choose forecasting strategies which have recently been successful. The success of any strategy will depend on market conditions and also on the set of strategies that are currently being used. These models frequently find that large booms and busts in asset prices may occur as agents switch across forecasting strategies. More recently, Brock, Hommes, and Wagener (2009) have used a model of this type to argue that the introduction of new hedging instruments may destabilize the market, and some papers have suggested that ACE might be a useful methodology for understanding the recent financial crisis.


== See also ==
ACEGES
Agent-based social simulation
Artificial economics
Computational economics
Econophysics
Macroeconomic model
Multi-agent system
Statistical finance


== References =="
58,International Conference on Business Process Management,52038701,17271,"The International Conference on Business Process Management is an academic conference organized annually by the BPM community. The conference was first organized in 2003 Eindhoven, Netherlands. Since then the conference has been organized annually. The conference is the premium forum for researchers, practitioners and developers in the field of Business Process Management (BPM). The conference typically attracts over 300 participants from all over the world.
The BPM Steering Committee is responsible for the conference, including selection of organizers, invited speakers, workshops, etc.


== Topics ==

Topics covered by the conference include:
Business process modeling
BPM/WFM systems
Process mining
Business process intelligence
Workflow automation
Process change management
Reference process models
Process modeling languages
Case management
Process variability and configuration
Operations research for business processes
Collaborative business process management
Qualitative and quantitative process analysis (e.g. process simulation)
Management aspects of BPM
Decision management
Process discovery
Process compliance
Process innovation
Process execution architectures


== History ==
The first conference was organized by Wil van der Aalst and was held in conjunction with the 24th International Conference on Applications and Theory of Petri Nets in Eindhoven. Since BPM 2005 in Nancy, the conference has co-located workshops in different subfields of BPM.
BPM 2018 in Sydney, Australia
PC Co-Chairs: Mathias Weske, Marco Montali, Ingo Weber, Jan vom Brocke
General Chair: Boualem Benatallah and Jian Yang
Workshop Chairs: Florian Daniel, Hamid Motahari, Michael Sheng

BPM 2017 in Barcelona, Spain PC Co-Chairs: Josep Carmona, Gregor Engels, Akhil Kumar
General Chair: Josep Carmona
Workshop Chairs: Matthias Weidlich, Ernest Teniente

BPM 2016 in Rio de Janeiro, Brazil PC Co-Chairs: Marcello La Rosa, Peter Loos, Oscar Pastor
General Chair: Flávia Santoro
Workshop Chairs: Marlon Dumas, Marcelo Fantinato

BPM 2015 in Innsbruck, Austria PC Co-Chairs: Hamid Reza Motahari-Nezhad, Jan Recker, Matthias Weidlich
General Chair: Barbara Weber

BPM 2014 in Eindhoven, Netherlands (relocated from Haifa, Israel) PC Co-Chairs: Shazia Sadiq, Pnina Soffer, Hagen Völzer
General Co-Chairs: Avigdor Gal, Mor Peleg
Local Chair: Wil van der Aalst

BPM 2013 in Beijing, China PC Co-Chairs: Florian Daniel, Jianmin Wang, Barbara Weber
General Chair: Jianmin Wang

BPM 2012 in Tallinn, Estonia PC Co-Chairs: Alistair Barros, Avi Gal, Ekkart Kindler
General Chair: Marlon Dumas

BPM 2011 in Clermont-Ferrand, France PC Co-Chairs: Stefanie Rinderle-Ma, Farouk Toumani, Karsten Wolf
General Chair: Farouk Toumani, Mohand-Said Hacid

BPM 2010 in Hoboken (NJ), United States PC Co-Chairs: Richard Hull, Jan Mendling, Stefan Tai
General Chair: Michael zur Mühlen

BPM 2009 in Ulm, Germany PC Co-Chairs: Umeshwar Dayal, Johann Eder, Hajo A. Reijers
General Chairs: Peter Dadam, Manfred Reichert

BPM 2008 in Milan, Italy PC Co-Chairs: Marlon Dumas, Manfred Reichert, Ming-Chien Shan
General Chair: Barbara Pernici

BPM 2007 in Brisbane, Australia PC Co-Chairs: Gustavo Alonso, Peter Dadam, Michael Rosemann
General Chairs: Marlon Dumas, Michael Rosemann

BPM 2006 in Vienna, Austria PC Co-Chairs: Schahram Dustdar, José Luiz Fiadeiro, Amit P. Sheth
General Chair: Schahram Dustdar

BPM 2005 in Nancy, France PC Co-Chairs: Wil M. P. van der Aalst, Boualem Benatallah, Fabio Casati
General Chair: Claude Godart

BPM 2004 in Potsdam, Germany PC Co-Chairs: Jörg Desel, Barbara Pernici, Mathias Weske
General Chair: Mathias Weske

BPM 2003 in Eindhoven, Netherlands PC Co-Chairs: Wil van der Aalst, Arthur H. M. ter Hofstede, Mathias Weske
General Chair: Wil van der Aalst


== See also ==
The list of computer science conferences contains other academic conferences in computer science.
The topics of the conference cover the field of computer science.
Process mining is a process management technique that allows for the analysis of business processes based on event logs.
Business Process Management (BPM) includes methods, techniques, and tools to support the design, enactment, management, and analysis of operational business processes. It can be considered as an extension of classical Workflow Management (WFM) systems and approaches.


== References ==


== External links ==
Website of the BPM Conference Series.
BPM Steering Committee
Website of the 15th International Conference on Business Process Management, Barcelona, September 2017.
DBLP entry listing all BPM conference proceedings (including workshops)
BPM Newsletters (two issues per year)"
59,Computational archaeology,171644,16563,"Computational archaeology describes computer-based analytical methods for the study of long-term human behaviour and behavioural evolution. As with other sub-disciplines that have prefixed 'computational' to their name (e.g. computational biology, computational physics and computational sociology), the term is reserved for (generally mathematical) methods that could not realistically be performed without the aid of a computer.
Computational archaeology may include the use of geographical information systems (GIS), especially when applied to spatial analyses such as viewshed analysis and least-cost path analysis as these approaches are sufficiently computationally complex that they are extremely difficult if not impossible to implement without the processing power of a computer. Likewise, some forms of statistical and mathematical modelling, and the computer simulation of human behaviour and behavioural evolution using software tools such as Swarm or Repast would also be impossible to calculate without computational aid. The application of a variety of other forms of complex and bespoke software to solve archaeological problems, such as human perception and movement within built environments using software such as University College London's Space Syntax program, also falls under the term 'computational archaeology'.
Computational archaeology is also known as archaeological informatics (Burenhult 2002, Huggett and Ross 2004) or archaeoinformatics (sometimes abbreviated as ""AI"", but not to be confused with artificial intelligence).


== Origins and objectives ==
In recent years, it has become clear that archaeologists will only be able to harvest the full potential of quantitative methods and computer technology if they become aware of the specific pitfalls and potentials inherent in the archaeological data and research process. AI science is an emerging discipline that attempts to uncover, quantitatively represent and explore specific properties and patterns of archaeological information. Fundamental research on data and methods for a self-sufficient archaeological approach to information processing produces quantitative methods and computer software specifically geared towards archaeological problem solving and understanding.
AI science is capable of complementing and enhancing almost any area of scientific archaeological research. It incorporates a large part of the methods and theories developed in quantitative archaeology since the 1960s but goes beyond former attempts at quantifying archaeology by exploring ways to represent general archaeological information and problem structures as computer algorithms and data structures. This opens archaeological analysis to a wide range of computer-based information processing methods fit to solve problems of great complexity. It also promotes a formalized understanding of the discipline's research objects and creates links between archaeology and other quantitative disciplines, both in methods and software technology. Its agenda can be split up in two major research themes that complement each other:
Fundamental research (theoretical AI science) on the structure, properties and possibilities of archaeological data, inference and knowledge building. This includes modeling and managing fuzziness and uncertainty in archaeological data, scale effects, optimal sampling strategies and spatio-temporal effects.
Development of computer algorithms and software (applied AI science) that make this theoretical knowledge available to the user.
There is already a large body of literature on the use of quantitative methods and computer-based analysis in archaeology. The development of methods and applications is best reflected in the annual publications of the CAA conference (see external links section at bottom). At least two journals, the Italian Archeologia e Calcolatori and the British Archaeological Computing Newsletter, are dedicated to archaeological computing methods. AI Science contributes to many fundamental research topics, including but not limited to:
advanced statistics in archaeology, spatial and temporal archaeological data analysis
bayesian analysis and advanced probability models, fuzziness and uncertainty in archaeological data
scale-related phenomena and scale transgressions
intrasite analysis (representations of stratigraphy, 3D analysis, artefact distributions)
landscape analysis (territorial modeling, visibility analysis)
optimal survey and sampling strategies
process-based modeling and simulation models
archaeological predictive modeling and heritage management applications
supervised and unsupervised classification and typology, artificial intelligence applications
digital excavations and virtual reality
computational reproducibility of archaeological research
archaeological software development, electronic data sharing and publishing
AI science advocates a formalized approach to archaeological inference and knowledge building. It is interdisciplinary in nature, borrowing, adapting and enhancing method and theory from numerous other disciplines such as computer science (e.g. algorithm and software design, database design and theory), geoinformation science (spatial statistics and modeling, geographic information systems), artificial intelligence research (supervised classification, fuzzy logic), ecology (point pattern analysis), applied mathematics (graph theory, probability theory) and statistics.


== Training and research ==
Scientific progress in archaeology, as in any other discipline, requires building abstract, generalized and transferable knowledge about the processes that underlie past human actions and their manifestations. Quantification provides the ultimate known way of abstracting and extending our scientific abilities past the limits of intuitive cognition. Quantitative approaches to archaeological information handling and inference constitute a critical body of scientific methods in archaeological research. They provide the tools, algebra, statistics and computer algorithms, to process information too voluminous or complex for purely cognitive, informal inference. They also build a bridge between archaeology and numerous quantitative sciences such as geophysics, geoinformation sciences and applied statistics. And they allow archaeological scientists to design and carry out research in a formal, transparent and comprehensible way.
Being an emerging field of research, AI science is currently a rather dispersed discipline in need of stronger, well-funded and institutionalized embedding, especially in academic teaching. Despite its evident progress and usefulness, today's quantitative archaeology is often inadequately represented in archaeological training and education. Part of this problem may be misconceptions about the seeming conflict between mathematics and humanistic archaeology.
Nevertheless, digital excavation technology, modern heritage management and complex research issues require skilled students and researchers to develop new, efficient and reliable means of processing an ever-growing mass of untackled archaeological data and research problems. Thus, providing students of archaeology with a solid background in quantitative sciences such as mathematics, statistics and computer sciences seems today more important than ever.
Currently, universities based in the UK provide the largest share of study programmes for prospective quantitative archaeologists, with more institutes in Italy, Germany and the Netherlands developing a strong profile quickly. In Germany, the country's first lecturer's position in AI science (""Archäoinformatik"") was established in 2005 at the University of Kiel (Benjamin Ducke, now at Oxford Archaeology), while currently there is only one regular junior professorship in Archaeoinformatics in the field of Classical Archaeology at Freie Universität Berlin. From April 2016 a new full professorship in Archaeoinformatics will be established at the University of Cologne (Institute of Archaeology).
The most important platform for students and researchers in quantitative archaeology and AI science is the international conference on Computer Applications and Quantitative Methods in Archaeology (CAA) which has been in existence for more than 30 years now and is held in a different city of Europe each year. Vienna's city archaeology unit also hosts an annual event that is quickly growing in international importance (see links at bottom).


== Further reading ==
Roosevelt, Cobb, Moss, Olson, and Ünlüsoy 2015: ""Excavation is Destruction Digitization: Advances in Archaeological Practice,"" Journal of Field Archaeology, Volume 40, Issue 3 (June 2015), pp. 325-346.
Burenhult 2002: Burenhult, G. (ed.): Archaeological Informatics: Pushing The Envelope. CAA2001. Computer Applications and Quantitative Methods in Archaeology. BAR International Series 1016, Archaeopress, Oxford.
Falser, Michael; Juneja, Monica (Eds.): 'Archaeologizing' Heritage? Transcultural Entanglements between Local Social Practices and Global Virtual Realities (Series: Transcultural Research – Heidelberg Studies on Asia and Europe in a Global Context). Springer: Heidelberg/New York, 2013, VIII, 287 p. 200 illus., 90 illus. in color.
Huggett and Ross 2004: J. Huggett, S. Ross (eds.): Archaeological Informatics. Beyond Technology. Internet Archaeology 15. http://intarch.ac.uk/journal/issue15/
Marwick, Ben (2016). ""Computational Reproducibility in Archaeological Research: Basic Principles and a Case Study of Their Implementation"". Journal of Archaeological Method and Theory. doi:10.1007/s10816-015-9272-9. 
Schlapke 2000: Schlapke, M. Die ""Archäoinformatik"" am Thüringischen Landesamt für Archäologische Denkmalpflege, Ausgrabungen und Funde im Freistaat Thüringen, 5, 2000, S. 1-5.
Zemanek 2004: Zemanek, H.: Archaeological Information - An information scientist looks on archaeology. In: Ausserer, K.F., Börner, w., Goriany, M. & Karlhuber-Vöckl, L. (eds) 2004. Enter the Past. The E-way into the four Dimensions of Cultural Heritage. CAA 2003, Computer Applications and Quantitative Methods in Archaeology. BAR International Series 1227, Archaeopress, Oxford, 16-26.
Archeologia e Calcolatori journal homepage
Archaeological Computing Newsletter homepage, now a supplement to Archeologia e Calcolatori
Computational archaeology
Computational Archaeology Blog


== References ==


== External links ==


=== Studying Computational Archaeology ===
University College London: M.Sc. GIS and Spatial Analysis in Archaeology
University of York: MSc Archaeological Information Systems
University of Birmingham: MA/ PG Dip Landscape Archaeology, GIS and Virtual Environments
University of Southampton: MSc in Archaeological Computing (Spatial Technologies) and MSc in Archaeological Computing (Virtual Pasts)
Archaeoinformatics at Siena University (Italian page)
Archaeoinformation science at CAU Kiel
University of the Aegean M.Sc. in Cultural Informatics
University of Washington Digital Archaeology Research Lab
The Computational Archaeology Lab at San Diego State University focuses on Open-Science and Open-Source approaches to GIS, Agent Based Modeling, Imagery Analysis, and Computation in archaeology and coupled human-natural systems science.


=== Research groups and institutions ===
University College London: Material Culture and Data Science Research Group
University of York: Archaeological Information Systems Research Group
University of Southampton: Archaeological Computing Research Group
University of Birmingham: HP Visual and Spatial Technology Centre Archaeological Computing Division
Foundation for Research and Technology Hellas (FORTH), Center for Cultural Informatics
Alexandria Archive Institute (AAI)
Internet and Open Source for Archaeology (2004-2013) was a portal dedicated to the collection and creation of resources to help archaeologists eveluate open source alternatives to proprietary software.
Cultural and Educational Technology Institute is a research institute which constitutes an integrated research environment with continuous interaction with the academic community, in particular with the Democritus University of Thrace, the national and European educational and cultural technology industry, the international scientific community and the public sector.
Michigan State University Cultural Heritage Informatics Initiative is a platform for interdisciplinary scholarly collaboration and communication in the domain of Cultural Heritage Informatics at Michigan State University. In addition, the initiative strives to equip students (both graduate and undergraduate) with the practical and analytical skills necessary creatively to apply information, communication, and computing technologies to cultural heritage materials.
ISAAK (Initiative for Statistical Analysis in Archaeology Kiel)


=== Conferences ===
""Computer Applications and Quantitative Methods in Archaeology (CAA"")
""International Conference on Cultural Heritage and New Technologies"" (formerly: ""Workshop Archäologie und Computer"" at Vienna)


=== Archaeological IT service providers ===
L - P : Archaeology
Archaeovision
Oxford Archaeology Digital
Archaeology Data Service
Intrasis GIS
ArcTron (in German)
Open Context: experimental system for archaeological data-sharing"
60,Conference on Computer Vision and Pattern Recognition,24073428,16395,"The Conference on Computer Vision and Pattern Recognition is an annual conference on computer vision and pattern recognition, by several measures regarded as the top conference in computer vision. 


== Official Affiliations ==
CVPR was first held in Washington DC in 1983 by Takeo Kanade and Dana Ballard (previously the conference was named Pattern Recognition and Image Processing). From 1985-2010 it was sponsored by the IEEE Computer Society. In 2011 it was co-sponsored by the IEEE Computer Society and by University of Colorado Colorado Springs. Since 2012 it has been co-sponsored by IEEE Computer Society and the Computer Vision Foundation (CVF). CVF now provides open access to the conference papers.


== Scope ==
CVPR considers a wide range of topics related to computer vision and pattern recognition—basically any topic that is extracting structures or answers from images or video or applying mathematical methods to data to extract or recognize patterns. Each year the conference has an explicit list of topics for that year. The conference event also includes a wide range of workshops and tutorials. Each year multiple company also donate funds to support the conference and many of those also exhibit at the conference.
The conference is highly selective with generally <30% acceptance rates for all papers and <5% for oral presentations. The conference is managed by a rotating group of volunteers who are chosen in a public election at the PAMI-TC meeting four years before the meeting. CVPR uses a multi-tier double-blind review process. The Program Chairs (who cannot submit papers), select Area Chairs who manage the reviewers for their subset of reviewers. There are generally three or more reviewers per paper. The Area Chairs discuss the paper with the reviewers then among the ACs and finally produce a meta-review and make a recommendation the Program Chairs, who make the final decisions. Due to growth of the conference and the number of accompanying paper submissions, the number of Area Chairs has grown to just over 100 in 2018.


== Location ==
The conference is usually held in June and rotates around the U.S. generally West, Central and East. In 2013 the conference was in Portland, OR. In 2014 the conference was held in Columbus, OH with over 1900 attendees. In 2015 it was in Boston, MA. In 2016 it was in Las Vegas, NV. In 2017 it was held in Honolulu, HI.
Future meetings are planned as follows:
2018 in Salt Lake City, UT
2019 in Long Beach, CA
2020 in Seattle, WA


== Awards ==


=== CVPR Best Paper Award ===
These awards are picked by committees delegated by the program chairs of the conference.


==== CVPR Best Paper Award recipients ====
Awarded at CVPR 2017:
Best Paper: Densely Connected Convolutional Networks by Gao Huang, Zhuang Liu, Laurens van der Maaten, & Kilian Q. Weinberger
Best Paper: Learning from Simulated and Unsupervised Images through Adversarial Training by Ashish Shrivastava, Tomas Pfister, Oncel Tuzel, Joshua Susskind, Wenda Wang, & Russell Webb
Best Paper Honorable Mention: Annotating Object Instances with a Polygon-RNN by Lluís Castrejón, Kaustav Kundu, Raquel Urtasun, & Sanja Fidler
Best Paper Honorable Mention: YOLO9000: Better, Faster, Stronger by Joseph Redmon & Ali Farhadi
Best Student Paper: Computational Imaging on the Electric Grid by Mark Sheinin, Yoav Y. Schechner, & Kiriakos N. Kutulakos
Longuet-Higgins Prize: Object Retrieval with Large Vocabularies and Fast Spatial Matching by James Philbin, Ondrej Chum, Michael Isard, Josef Sivic & Andrew Zisserman

Awarded at CVPR 2016:
Best Paper: Deep Residual Learning for Image Recognition, Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun
Best Student Paper: Structural-RNN: Deep Learning on Spatio-Temporal Graphs, Ashesh Jain, Amir R. Zamir, Silvio Savarese, Ashutosh Saxena
Best Paper Honorable Mention: Sublabel-Accurate Relaxation of Nonconvex Energies., Thomas Möllenhoff, Emanuel Laude, Michael Moeller, Jan Lellmann, Daniel Cremers

Awarded at CVPR 2015:
Best Paper: DynamicFusion: Reconstruction and Tracking of Non-rigid Scenes in Real-Time Richard, A. Newcombe, Dieter Fox, Steven M. Seitz
Best Student Paper: Category-Specific Object Reconstruction from a Single Image, Abhishek Kar, Shubham Tulsiani, João Carreira, Jitendra Malik
Best Paper Honorable Mention: Efficient Globally Optimal Consensus Maximisation with Tree Search, Tat-Jun Chin, Pulak Purkait, Anders Eriksson, David Suter
Best Paper Honorable Mention: Fully Convolutional Networks for Semantic Segmentation, Jonathan Long, Evan Shelhamer, Trevor Darrell
Best Paper Honorable Mention: Picture: A Probabilistic Programming Language for Scene Perception, Tejas D Kulkarni, Pushmeet Kohli, Joshua B Tenenbaum, Vikash Mansinghka

Awarded at CVPR 2014:
Best Paper: What Camera Motion Reveals About Shape with Unknown BRDF, Manmohan Chandraker
Best Paper Runner-Up: 3D Shape and Indirect Appearance by Structured Light Transport, Matthew O'Toole, John Mather, Kyros Kutulakos
Best Student Paper: Partial Optimality by Pruning for MAP-inference with General Graphical Models, Paul Swoboda, Bogdan Savchynskyy, Joerg Kappes, Christoph Schnörr
Outstanding Demo: Real-Time Video Magnification, Neal Wadhwa, Michael Rubinstein, Frédo Durand, William T. Freeman
Honorable Mention Demo: Learning to be a Depth Camera, Sean Ryan Fanello, Cem Keskin, Shahram Izadi, Pushmeet Kohli, David Kim, David Sweeney, Antonio Criminisi, Jamie Shotton, Sing Bing Kang, Tim Paek

Awarded at CVPR 2013:
Best Paper: Fast, Accurate Detection of 100,000 Object Classes on a Single Machine, Thomas Dean, Jay Yagnik, Mark Ruzon, Mark Segal, Jonathon Shlens, and Sudheendra Vijayanarasimhan
Best Paper Runner-Up: Lost! Leveraging the Crowd for Probabilistic Visual Self-Localization, Marcus Brubaker, Andreas Geiger, and Raquel Urtasun
Best Student Paper: Discriminative Non-blind Deblurring, Uwe Schmidt, Carsten Rother, Sebastian Nowozin, Jeremy Jancsary, and Stefan Roth

Awarded at CVPR 2012:
Best Paper: A Simple Prior-free Method for Non-Rigid Structure-from-Motion Factorization, Yuchao Dai, Hongdong Li, Mingyi He
Best Student Paper: Max-Margin Early Event Detectors, Minh Hoai, Fernando De la Torre

Awarded at CVPR 2011:
Best Paper: Real-time Human Pose Recognition in Parts from Single Depth Images, Jamie Shotton, Andrew Fitzgibbon, Mat Cook, Toby Sharp, Mark Finocchio, Richard Moore, Alex Kipman, Andrew Blake
Best Paper Honorable Mention: Discrete-Continuous Optimization for Large-scale Structure from Motion, David Crandall, Andrew Owens, Noah Snavely, Daniel Huttenlocher
Best Student Paper: Recognition Using Visual Phrases, Ali Farhadi, Mohammad Amin Sadeghi
Best Student Paper Honorable Mention: Separating Reflective and Fluorescent Components of An Image, Cherry Zhang, Imari Sato

Awarded at CVPR 2010:
Best Paper: Efficient Computation of Robust Low-Rank Matrix Approximations in the Presence of Missing Data using the L1 Norm, Anders Eriksson and Anton van den Hengel
Best Student Paper: Visual Event Recognition in Videos by Learning from Web Data, Lixin Duan, Dong Xu, Wai-Hung Tsang, and Jiebo Luo
Best Student Paper Honorable Mention: Modeling Mutual Context of Object and Human Pose in Human-Object Interaction Activities, Bangpeng Yao and Li Fei-Fei

Awarded at CVPR 2009:
Best Paper: Single Image Haze Removal Using Dark Channel Prior, Kaiming He, Jian Sun, Xiaoou Tang
Best Paper Honorable Mention: Understanding and evaluating blind deconvolution algorithms, Anat Levin, Yair Weiss, Fredo Durand, Bill Freeman
Best Student Paper: Nonparametric Scene Parsing: Label Transfer via Dense Scene Alignment, Ce Liu, Jenny Yuen, Antonio Torralba
Best Student Paper Honorable Mention: A Tensor-Based Algorithm for High-Order Graph Matching, Olivier Duchenne, Francis Bach, In So Kweon, Jean Ponce

Awarded at CVPR 2008
Best Paper: Beyond Sliding Windows: Object Localization by Efficient Subwindow Search, Christoph H. Lampert, Matthew B.Blaschko,Thomas Hofmann
Best Paper: Global Stereo Reconstruction under Second Order Smoothness Priors, Oliver Woodford, Ian Reid, Philip Torr, Andrew Fitzgibbon
Best Student Paper: Fast Image Search for Learned Metrics, Prateek Jain, Brian Kulis, Kristen Grauman

Awarded at CVPR 2007
Best Paper: Dynamic 3D Scene Analysis from a Moving Vehicles, Bastian Leibe, Nico Cornelis, Kurt Cornelis, and Luc Van Gool
Best Paper Honorable Mention: Spectral Matting, Anat Levin, Alex Rav-Acha, and Dani Lischinski
Best Paper Honorable Mention: Human Detection via Classification on Riemannian Manifolds, Oncel Tuzel, Fatih Porikli, and Peter Meer
Best Student Paper: Tracking in Low Frame Rate Video: A Cascade Particle Filter with Discriminative Observers of Different Life Spans, Yuan Li, Haizhou Ai, Takayoshi Yamashita, Shihong Lao, and Masato Kawade

Awarded at CVPR 2006
Best Paper: Putting Objects in Perspective, Derek Hoiem, Alexei A. Efros, Martial Hebert
Best Paper Honorable Mention:Incremental learning of object detectors using a visual shape alphabet, Andreas Opelt, Axel Pinz, Andrew Zisserman

Awarded at CVPR 2005
Best Paper: Real-Time Non-Rigid Surface Detection, Julien Pilet, Vincent Lepetit, Pascal Fua
Best Paper Honorable Mention: A Non-Local Algorithm for Image Denoising, Antoni Buades, Bartomeu Coll, Jean-Michel Morel
Bi-Layer Segmentation of Binocular Stereo Video, Vladimir Kolmogorov, Antonio Criminisi, Andrew Blake, Geoffrey Cross, Carsten Rother
Video Epitomes, Vincent Cheung, Brendan J. Frey, Nebojsa Jojic

Awarded at CVPR 2004
Best Paper: Programmable Imaging using a Digital Micromirror Array, Shree K Nayar, Vlad Branzoi, Terrance E Boult
Best Student Paper:Unsupervised learning of image manifolds by semidefinite programming Killeen Q. Weinberger and Lawrence K. Saul.

Awarded at CVPR 2003
Best Paper: Object Class Recognition by Unsupervised Scale-Invariant Learning, Rob Fergus, Pietro Perona, and Andrew Zisserman
Best Paper Honorable Mention: Constraint on Five Points in Two Images, Tomas Werner
Best Student Paper: Vector-Valued Image Regularization with PDE's: A Common Framework for Different Applications, David Tschumperle and Rashid Deriche

Awarded at CVPR 2001
Best Paper: Morphable 3D models from video, Matthew Brand
Best Paper Runner Up: Robust on-line appearance models for visual tracking, A. Jepson, D. Fleet, T.F. El-Maraghi
Best Student Paper: Tracking and modeling non-rigid objects with rank constraints, L. Torresani, D. Yang, E. Alexander, C. Bregler
Outstanding Student Paper: Dense image matching with global and local statistical criteria: a variational approach, G. Hermosillo, O. Faugeras
Outstanding Student Paper: JPDAF based HMM for real-time contour tracking, Y. Chen, Y. Rui, T. Huang
Outstanding Student Paper: Model-based curve evolution techniques for image segmentation, A. Tsai, A. Yezzi, W. Wells, C. Tempany, D. Tucker, A. Fan, E. Grimson, A. Willsky

Awarded at CVPR 2000
Best Paper: Real-Time Tracking of Non-Rigid Objects using Mean Shift, D. Comaniciu, V. Ramesh, P. Meer
Best Paper Runner Up: A New Algorithm for Non-Rigid Point Matching, H. Chui, A. Rangarajan
Best Student Paper: Statistical Shape Influence in Geodesic Active Contours, M. Leventon, E. Grimson, O. Faugeras


=== Longuet-Higgins Prize ===
The Longuet-Higgins Prize recognizes CVPR papers from ten years ago that have made a significant impact on computer vision research.


=== PAMI Young Researcher Award ===
The Pattern Analysis and Machine Intelligence (PAMI) Young Researcher Award is an award given by the Technical Committee on Pattern Analysis and Machine Intelligence (TCPAMI) of the IEEE Computer Society at the CVPR to a researcher within 7 years of completing their Ph.D. for outstanding early career research contributions. Candidates are nominated by the computer vision community, with winners selected by a committee of senior researchers in the field. This award was originally instituted in 2012 by the Elsevier journal Image and Vision Computing, also presented at the CVPR, and the ICV continues to sponsor the award.


==== PAMI Young Researcher Award recipients ====
2017: Ross Girshick and Julien Mairal
2016: Abhinav Gupta and Ce Liu
2015: John Wright
2014: Derek Hoiem and Jamie Shotton
2013: Anat Levin and Kristen Grauman
2012 (as the IVC Outstanding Young Researcher Award): Deva Ramanan


== See also ==
International Conference on Computer Vision


== References ==


== External links ==
2009 conference website
2010 conference website
2011 conference website
2012 conference website
2013 conference website
2014 conference website
2015 conference website
2016 conference website
2017 conference website"
61,Unique games conjecture,3000842,16360,"In computational complexity theory, the unique games conjecture is a conjecture made by Subhash Khot in 2002. The conjecture postulates that the problem of determining the approximate value of a certain type of game, known as a unique game, has NP-hard algorithmic complexity. It has broad applications in the theory of hardness of approximation. If it is true, then for many important problems it is not only impossible to get an exact solution in polynomial time (as postulated by the P versus NP problem), but also impossible to get a good polynomial-time approximation. The problems for which such an inapproximability result would hold include constraint satisfaction problems, which crop up in a wide variety of disciplines.
The conjecture is unusual in that the academic world seems about evenly divided on whether it is true or not.


== Formulations ==
The unique games conjecture can be stated in a number of equivalent ways.


=== Unique label cover ===
The following formulation of the unique games conjecture is often used in hardness of approximation. The conjecture postulates the NP-hardness of the following promise problem known as label cover with unique constraints. For each edge, the colors on the two vertices are restricted to some particular ordered pairs. Unique constraints means that for each edge none of the ordered pairs have the same color for the same node.
This means that an instance of label cover with unique constraints over an alphabet of size k can be represented as a directed graph together with a collection of permutations πe: [k] → [k], one for each edge e of the graph. An assignment to a label cover instance gives to each vertex of G a value in the set [k] = {1, 2, ... k}, often called “colours.”

Such instances are strongly constrained in the sense that the colour of a vertex uniquely defines the colours of its neighbours, and hence for its entire connected component. Thus, if the input instance admits a valid assignment, then such an assignment can be found efficiently by iterating over all colours of a single node. In particular, the problem of deciding if a given instance admits a satisfying assignment can be solved in polynomial time.

The value of a unique label cover instance is the fraction of constraints that can be satisfied by any assignment. For satisfiable instances, this value is 1 and is easy to find. On the other hand, it seems to be very difficult to determine the value of an unsatisfiable game, even approximately. The unique games conjecture formalises this difficulty.
More formally, the (c, s)-gap label-cover problem with unique constraints is the following promise problem (Lyes, Lno):
Lyes = {G: Some assignment satisfies at least a c-fraction of constraints in G}
Lno = {G: Every assignment satisfies at most an s-fraction of constraints in G}
where G is an instance of the label cover problem with unique constraints.
The unique games conjecture states that for every sufficiently small pair of constants ε, δ > 0, there exists a constant k such that the (1 - δ, ε)-gap label-cover problem with unique constraints over alphabet of size k is NP-hard.
Instead of graphs, the label cover problem can be formulated in terms of linear equations. For example, suppose that we have a system of linear equations over the integers modulo 7:

  
    
      
        
          
            
              
                
                  x
                  
                    1
                  
                
              
              
                
                ≡
                2
                ⋅
                
                  x
                  
                    2
                  
                
                
                  
                  (
                  mod
                  
                  7
                  )
                
              
            
            
              
                
                  x
                  
                    2
                  
                
              
              
                
                ≡
                4
                ⋅
                
                  x
                  
                    5
                  
                
                
                  
                  (
                  mod
                  
                  7
                  )
                
              
            
            
              
              
                

                
                 
                 
                ⋮
              
            
            
              
                
                  x
                  
                    1
                  
                
              
              
                
                ≡
                2
                ⋅
                
                  x
                  
                    7
                  
                
                
                  
                  (
                  mod
                  
                  7
                  )
                
                .
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}x_{1}&\equiv 2\cdot x_{2}{\pmod {7}}\\x_{2}&\equiv 4\cdot x_{5}{\pmod {7}}\\&{}\ \ \vdots \\x_{1}&\equiv 2\cdot x_{7}{\pmod {7}}.\end{aligned}}}
  
This is an instance of the label cover problem with unique constraints. For example, the first equation corresponds to the permutation π(1, 2) where π(1, 2)(x1) = 2x2 modulo 7.


=== Two-prover proof systems ===
A unique game is a special case of a two-prover one-round (2P1R) game. A two-prover one-round game has two players (also known as provers) and a referee. The referee sends each player a question drawn from a known probability distribution, and the players each have to send an answer. The answers come from a set of fixed size. The game is specified by a predicate that depends on the questions sent to the players and the answers provided by them.
The players may decide on a strategy beforehand, although they cannot communicate with each other during the game. The players win if the predicate is satisfied by their questions and their answers.
A two-prover one-round game is called a unique game if for every pair of questions and every answer to the first question, there is exactly one answer to the second question that results in a win for the players, and vice versa. The value of a game is the maximum winning probability for the players over all strategies.
The unique games conjecture states that for every sufficiently small pair of constants ε, δ > 0, there exists a constant k such that the following promise problem (Lyes, Lno) is NP-hard:
Lyes = {G: the value of G is at least 1 − δ}
Lno = {G: the value of G is at most ε}
where G is a unique game whose answers come from a set of size k.


=== Probabilistically checkable proofs ===
Alternatively, the unique games conjecture postulates the existence of a certain type of probabilistically checkable proof for problems in NP.
A unique game can be viewed as a special kind of nonadaptive probabilistically checkable proof with query complexity 2, where for each pair of possible queries of the verifier and each possible answer to the first query, there is exactly one possible answer to the second query that makes the verifier accept, and vice versa.
The unique games conjecture states that for every sufficiently small pair of constants ε, δ > 0 there is a constant K such that every problem in NP has a probabilistically checkable proof over an alphabet of size K with completeness 1 - δ, soundness ε and randomness complexity O(log(n)) which is a unique game.


== Relevance ==
The unique games conjecture was introduced by Subhash Khot in 2002 in order to make progress on certain questions in the theory of hardness of approximation.
The truth of the unique games conjecture would imply the optimality of many known approximation algorithms (assuming P ≠ NP). For example, the approximation ratio achieved by the algorithm of Goemans and Williamson for approximating the maximum cut in a graph is optimal to within any additive constant assuming the unique games conjecture and P ≠ NP.
A list of results that the unique games conjecture is known to imply is shown in the adjacent table together with the corresponding best results for the weaker assumption P ≠ NP. A constant of c + ε or c − ε means that the result holds for every constant (with respect to the problem size) strictly greater than or less than c, respectively.


== Discussion and alternatives ==
Currently there is no consensus regarding the truth of the unique games conjecture. Certain stronger forms of the conjecture have been disproved.
A different form of the conjecture postulates that distinguishing the case when the value of a unique game is at least 1 − δ from the case when the value is at most ε is impossible for polynomial-time algorithms (but perhaps not NP-hard). This form of the conjecture would still be useful for applications in hardness of approximation. On the other hand, distinguishing instances with value at most 3/8 + δ from instances with value at least 1/2 is known to be NP-hard.
The constant δ > 0 in the above formulations of the conjecture is necessary unless P = NP. If the uniqueness requirement is removed the corresponding statement is known to be true by the parallel repetition theorem, even when δ = 0.
Karpinski and Schudy constructed linear time approximation schemes for dense instances of unique games problem.
In 2010, Arora, Barak and Steurer found a subexponential time approximation algorithm for the unique games problem.


== Notes ==


== References ==
Khot, Subhash (2010), ""On the Unique Games Conjecture"", Proc. 25th IEEE Conference on Computational Complexity (PDF), pp. 99–121, doi:10.1109/CCC.2010.19 ."
62,National Centre for Text Mining,10795520,16123,"The National Centre for Text Mining (NaCTeM) is a publicly funded text mining (TM) centre. It was established to provide support, advice, and information on TM technologies and to disseminate information from the larger TM community, while also providing tailored services and tools in response to the requirements of the United Kingdom academic community.
The software tools and services which NaCTeM supplies allow researchers to apply text mining techniques to problems within their specific areas of interest – examples of these tools are highlighted below. In addition to providing services, the Centre is also involved in, and makes significant contributions to, the text mining research community both nationally and internationally in initiatives such as Europe PubMed Central.
The Centre is located in the Manchester Institute of Biotechnology and is operated and organized by the University of Manchester School of Computer Science. NaCTeM contributes expertise in natural language processing and information extraction, including named-entity recognition and extractions of complex relationships (or events) that hold between named entitites, along with parallel and distributed data mining systems in biomedical and clinical applications.


== Services ==
TerMine is a domain independent method for automatic term recognition which can be used to help locate the most important terms in a document and automatically ranks them.
AcroMine finds all known expanded forms of acronyms as they have appeared in Medline entries or conversely, it can be used to find possible acronyms of expanded forms as they have previously appeared in Medline and disambiguates them.
Medie is an intelligent search engine, for semantic retrieval of sentences containing biomedical correlations from Medline abstracts 
Facta+ is a Medline search engine for finding associations between biomedical concepts.
Facta+ Visualizer is a web application that aids in understanding FACTA+ search results through intuitive graphical visualisation.
KLEIO is a faceted semantic information retrieval system over Medline abstracts.
Europe PMC EvidenceFinder helps users to explore facts that involve entities of interest within the full text articles of the Europe PubMed Central database.
EUPMC Evidence Finder for Anatomical entities with meta-knowledge – similar to the Europe PMC EvidenceFinder, allowing exploration of facts involving anatomical entities within the full text articles of the Europe PubMed Central database. Facts can be filtered according to various aspects of their interpretation (e.g., negation, certainly level, novelty).
Info-PubMed provides information and graphical representation of biomedical interactions extracted from Medline using deep semantic parsing technology. This is supplemented with a term dictionary consisting of over 200,000 protein/gene names and identification of disease types and organisms.
Clinical Trial Protocols (ASCOT) is an efficient, semantically-enhanced search application, customised for clinical trial documents.
History of Medicine (HOM) is a semantic search system over historical medical document archives


== Resources ==
BioLexicon – a large-scale terminological resource for the biomedical domain.
GENIA – a collection of reference materials for the development of biomedical text mining systems.
GREC – a semantically annotated corpus of Medline abstracts intended for training IE systems and/or resources which are used to extract events from biomedical literature.
Metabolite and Enzyme Corpus – a corpus of Medline abstracts annotated by experts with metabolite and enzyme names.
Anatomy Corpora – A collection of corpora manually annotated with fine-grained, species-independent anatomical entities, to facilitate the development of text mining systems that can carry out detailed and comprehensive analyses of biomedical scientific text. 
Meta-knowledge corpus – an enrichment of the GENIA Event corpus, in which events are enriched with various levels of information pertaining to their interpretation. The aim is to allow systems to be trained that can distinguish between events that factual information or experimental analyses, definite information from speculated information, etc.


== Projects ==
Argo – The objective of the Argo project is to develop a workbench for analysing (primarily annotating) textual data. The workbench, which is accessed as a web application, supports the combination of elementary text-processing components to form comprehensive processing workflows. It provides functionality to manually intervene in the otherwise automatic process of annotation by correcting or creating new annotations, and facilitates user collaboration by providing sharing capabilities for user-owned resources. Argo benefits users such as text-analysis designers by providing an integrated environment for the development of processing workflows; annotators/curators by providing manual annotation functionalities supported by automatic pre-processing and post-processing; and developers by providing a workbench for testing and evaluating text analytics.
Big Mechanism – Big mechanisms are large, explanatory models of complicated systems in which interactions have important causal effects. Whilst the collection of big data is increasingly automated, the creation of big mechanisms remains a largely human effort, which is becoming made increasingly challenging, according to the fragmentation and distribution of knowledge. The ability to automate the construction of big mechanisms could have a major impact on scientific research. As one of a number of different projects that make up the big mechanism programme, funded by DARPA, the aim is to assemble an overarching big mechanism from the literature and prior experiments and to utilise this for the probabilistic interpretation of new patient panomics data. We will integrate machine reading of the cancer literature with probabilistic reasoning across cancer claims using specially-designed ontologies, computational modeling of cancer mechanisms (pathways), automated hypothesis generation to extend knowledge of the mechanisms and a 'Robot Scientist' that performs experiments to test the hypotheses. A repetitive cycle of text mining, modelling, experimental testing, and worldview updating is intended to lead to increased knowledge about cancer mechanisms.
COPIOUS – This project aims to produce a knowledge repository of Philippine biodiversity by combining the domain-relevant expertise and resources of Philippine partners with the text mining-based big data analytics of the University of Manchester's National Centre for Text Mining. The repository will be a synergy of different types of information, e.g., taxonomic, occurrence, ecological, biomolecular, biochemical, thus providing users with a comprehensive view on species of interest that will allow them to (1) carry out predictive analysis on species distributions, and (2) investigate potential medicinal applications of natural products derived from Philippine species.
Europe PMC Project – This is a collaboration with the Text-Mining group at the European Bioinformatics Institute (EBI) and Mimas (data centre), forming a work package in the Europe PubMed Central project (formerly UKPMC) hosted and coordinated by the British Library. Europe PMC, as a whole, forms a European version of the PubMed Central paper repository, in collaboration with the National Institutes of Health (NIH) in the United States. Europe PMC is funded by a consortium of key funding bodies from the biomedical research funders. The contribution to this major project is in the application of text mining solutions to enhance information retrieval and knowledge discovery. As such this is an application of technology developed in other NaCTeM projects on a large scale and in a prominent resource for the Biomedicine community.
Mining Biodiversity – This project aims to transform the Biodiversity Heritage Library (BHL) into a next-generation social digital library resource to facilitate the study and discussion (via social media integration) of legacy science documents on biodiversity by a worldwide community and to raise awareness of the changes in biodiversity over time in the general public. The project integrates novel text mining methods, visualisation, crowdsourcing and social media into the BHL. The resulting digital resource will provide fully interlinked and indexed access to the full content of BHL library documents, via semantically enhanced and interactive browsing and searching capabilities, allowing users to locate precisely the information of interest to them in an easy and efficient manner.
Mining for Public Health – This project aims to conduct novel research in text mining and machine learning to transform the way in which evidence-based public health (EBPH) reviews are conducted. The aims of the project are to develop new text mining unsupervised methods for deriving term similarities, to support screening while searching in EBPH reviews and to develop new algorithms for ranking and visualising meaningful associations of multiple types in a dynamic and iterative manner. These newly developed methods will be evaluated in EBPH reviews, based on implementation of a pilot, to ascertain the level of transformation in EBPH reviewing.


== References ==


== External links ==
http://www.nactem.ac.uk"
63,Polygon covering,43043289,16054,"A covering of a polygon is a set of primitive units (e.g. squares) whose union equals the polygon. A polygon covering problem is a problem of finding a covering with a smallest number of units for a given polygon. This is an important class of problems in computational geometry. There are many different polygon covering problems, depending on the type of polygon being covered and on the types of units allowed in the covering. An example polygon covering problem is: given a rectilinear polygon, find a smallest set of squares whose union equals the polygon.
In some scenarios, it is not required to cover the entire polygon but only its edges (this is called polygon edge covering) or its vertices (this is called polygon vertex covering).
In a covering problem, the units in the covering are allowed to overlap, as long as their union is exactly equal to the target polygon. This is in contrast to a packing problem, in which the units must be disjoint and their union may be smaller than the target polygon, and to a polygon partition problem, in which the units must be disjoint and their union must be equal to the target polygon.
A polygon covering problem is a special case of the set cover problem. In general, the problem of finding a smallest set covering is NP-complete, but for special classes of polygons, a smallest polygon covering can be found in polynomial time.


== Basic concepts ==
A unit u contained in a polygon P is called maximal if it is not contained in any other unit in P. When looking for a polygon covering, it is sufficient to consider maximal units, since every unit which is not maximal can be replaced with a maximal unit containing it without affecting the size of the covering.
A covering of a polygon P is a collection of maximal units, possibly overlapping, whose union equals P.
A minimal covering is a covering that does not contain any other covering (i.e. it is a local minimum).
A minimum covering is a covering with a smallest number of units (i.e. a global minimum). Every minimum covering is minimal, but not vice versa.


== Covering a rectilinear polygon with squares ==
A rectilinear polygon can always be covered with a finite number of squares.
For hole-free polygons, a minimum covering by squares can be found in time O(n), where n is the number of vertices of the polygon. The algorithm uses a local optimization approach: it builds the covering by iteratively selecting maximal squares that are essential to the cover (- contain uncovered points not covered by other maximal squares) and then deleting from the polygon the points that become unnecessary (- unneeded to support future squares). Here is a simplified pseudo-code of the algorithm:
While the polygon P is not empty:
Select a continuator square s in P.
If the balcony of s is not yet covered, then add s to the covering.
Remove the balcony of s from P.
If what remains of s is a one-knob continuator, then remove from P a certain rectangle adjacent to the knob, taking care to leave a sufficient security distance for future squares.

For polygons which may contain holes, finding a minimum such covering is NP-hard. This sharp difference between hole-free and general polygons can be intuitively explained based on the following analogy between maximal squares in a rectilinear polygon and nodes in an undirected graph:
Some maximal squares have a continuous intersection with the boundary of the polygon; when they are removed, the remaining polygon remains connected. Such squares are called ""continuators"" and are analogous to leaf nodes – nodes with a single edge – that can be removed without disconnecting the graph.
Other maximal squares are ""separators"": when they are removed, the polygon splits into two disconnected polygons. They are analogous to nodes with two or more edges that, when removed, leave a disconnected remainder.
In a hole-free rectilinear polygon, all maximal squares are either continuators or separators; thus, such a polygon is analogous to a tree graph. A general polygon is analogous to a general graph. Just like the Vertex cover problem is polynomial for tree graphs but NP-hard for general graphs, the square covering problem is linear for hole-free polygons but NP-hard for general polygons.
It is possible to use the linear algorithm to get a 2-approximation – i.e., a covering with at most 2⋅OPT squares, where OPT is the number of squares in a minimum covering:
For each hole, find a square s connecting the hole to the external boundary.
Cut s from the polygon, then glue back two overlapping copies of s (see figure). The resulting polygon is not planar, but it still 2-dimensional, and now it has no holes.
Now use the original algorithm to find a minimum covering.
The number of squares in the resulting covering is at most OPT+HOLES, where HOLES is the number of holes. It is possible to prove that OPT≥HOLES. Hence the number of squares in the covering is at most 2⋅OPT.


== Covering a rectilinear polygon with rectangles ==
For general rectilinear polygons, the problem of finding a minimum rectangle covering is NP-hard, even when the target polygon is hole-free. Several partial solutions have been suggested to this problem:
1. In orthogonally convex polygons, the number of rectangles in a minimum covering is equal to the number of blocks in an anti rectangle, and this fact can be used to build a polynomial time algorithm for finding a minimum covering by rectangles.
2. Even when the target polygon is only half-orthogonally convex (i.e. only in the y direction), a minimum covering by rectangles can be found in time O(n2), where n is the number of vertices of the polygon.
3. An approximation algorithm which gives good empirical results on real-life data is presented by.
4. For rectilinear polygons which may contain holes, there is an O(√log n) factor approximation algorithm.


== Covering a rectilinear polygon with orthogonally convex polygons ==
For a rectilinear polygon which is half-orthogonally convex (i.e. only in the x direction), a minimum covering by orthogonally convex polygons can be found in time O(n^2), where n is the number of vertices of the polygon. The same is true for a covering by rectilinear star polygons.
The number of orthogonally-convex components in a minimum covering can, in some cases, be found without finding the covering itself, in time O(n).


== Covering a rectilinear polygon with star polygons ==
A rectilinear star polygon is a polygon P containing a point p, such that for every point q in P, there is an orthogonally convex polygon containing p and q.
The problem of covering a polygon with star polygons is a variant of the art gallery problem.
The visibility graph for the problem of minimally covering hole-free rectilinear polygons with star polygons is a perfect graph. This perfectness property implies a polynomial algorithm for finding a minimum covering of any rectilinear polygon with rectilinear star polygons.


== Covering a polygon without acute angles with squares or rectangles ==
The most general class of polygons for which coverings by squares or rectangles can be found is the class of polygons without acute interior angles. This is because an acute angle cannot be covered by a finite number of rectangles. This problem is NP-hard, but several approximation algorithms exist.


== Covering a polygon with rectangles of a finite family ==
In some cases, a polygon has to be covered not with arbitrary rectangles but with rectangles from a finite family.


== Covering a polygon with triangles ==
Finding the smallest set of triangles covering a given polygon is NP-hard. It is also hard to approximate - every polynomial-time algorithm might find a covering with size (1+1/19151) times the minimal covering.
If the polygon is in general position (i.e. no two edges are collinear), then every triangle can cover at most 3 polygon edges. Hence every Polygon triangulation is a 3-approximation.
If the covering is restricted to triangles whose vertices are vertices of the polygon (i.e. Steiner points are not allowed), then the problem is NP-complete.
If Steiner points are not allowed and the polygon is in general position (i.e. no two edges are collinear), then every minimal covering without Steiner points is also a minimal partitioning of the polygon to triangles (i.e., the triangles in the minimal covering to not overlap). Hence, the minimum covering problem is identical to the Polygon triangulation problem, which can be solved in time O(nlogn). Note that if we drop the general position assumption, there are polygons in which the triangles in the optimal covering overlap. Think of the Star of David for example.
The problem of covering only the boundary of a polygon with triangles is NP-complete, but there is an efficient 2-approximation.


== Covering a polygon with convex polygons ==
Covering a polygon (which may contain holes) with convex polygons is NP-hard. There is an O(logn) approximation algorithm.
Covering a polygon with convex polygons is NP-hard even when the target polygon is hole-free. It is also APX-hard. The problem is NP-complete when the covering must not introduce new vertices (i.e. Steiner points are not allowed).


== Covering a polygon with star polygons ==

Covering a polygon (which may contain holes) with star polygons is NP-hard.
Covering a general (non-rectilinear) polygon with star polygons is NP-hard even when the target polygon is hole-free.


== Other combinations ==
Covering a polygon (which may contain holes) with spirals is NP-hard.
Covering a polygon with Pseudotriangles has also been studied.
Additional information can be found in.


== See also ==
Covering problems
Art gallery problem
Tessellation


== References =="
64,Coding bootcamp,49858916,16015,"Coding bootcamps are intensive programs (from one weekend up to a three months) of software development tuition in an immersive learning environment. They arose due to the demand for trained software professionals exceeding the number who were qualifying through traditional education routes. While coding bootcamps can be part-time or online, designed to be undertaken while in full-time employment, many are full-time and funded by employers or qualify for student loans.
Like coding dojos, coding bootcamps are immersive environments where students spend relatively little time in lectures, instead spending most of the day coding software that demonstrates their skills. While there has been some controversy in the fees some organisations have charged, reports of graduates gaining significant increases in pay rates still encourage them to go down this route.


== History ==
Coding bootcamps made their debut in 2011 with the Code Academy (now Starter League) with many others following.
Growth has been rapid, with the number of developers graduating from coding bootcamps nearly doubling from 2014 to 2015, and doubling again to over 22,000 by 2017. As a point of comparison, it is estimated that there were 79,650 undergraduate computer science graduates from accredited US universities in 2016.
As of July, 2017 there are 95 full-time coding bootcamp courses in the US, but there are concerns that the bubble is bursting, with too many organisations rushing to deliver coding bootcamps with not enough focus on quality or outcomes for the students.


== Job placement and outcomes ==
In a job outcomes study conducted by Course Report researchers published on Dec 19, 2017, the following trends were found:
The average first salary after a coding bootcamp is $70,698 with a 50.5% increase.
85% of graduates are placed in a full-time job within 120 days after bootcamp.
There's an average satisfaction rating of 8.3/10.
The average age of a coding bootcamp graduate is 30 years old.
Graduates in California show the highest salaries of $100,482.


== Reception by U.S. government ==
On August 16, 2016, the US Department of Education announced up to $17 million in loans or grants for students to study with nontraditional training providers, including coding bootcamps. These grants or loans will be administered through the pilot program, EQUIP which stands for Educational Quality through Innovation Partnerships. This will allow students, especially those who are low-income, to access federal student financial aid. Eight entities were selected to participate in the pilot program including four coding bootcamps - Flatiron School, MakerSquare, Epicodus, and Zip Code Wilmington. Two of the bodies selected to oversee EQUIP partnerships are industry lobby groups, which has raised eyebrows among skeptics. The groups are the American Council on Education and the Council for Higher Education Accreditation. Programs must partner with an accredited college and third-party quality assurance entity (QAE) in order to receive federal financial aid.


== Collaboration with higher education ==
Universities have started to take note of the coding bootcamp model, and these partnerships are on the rise. Traditional colleges are trying to meet the demand for people with coding and data analytic skills. They are doing this either by starting their own intensive coding programs, like Northeastern’s Level, or by partnering with an existing private coding bootcamp. Examples of these partnerships include General Assembly and Lynn University, Trilogy Education with Northwestern, UT Austin, and Case Western Reserve University and Coding Dojo and Bellevue – all of which are not part of EQUIP.


== Online coding bootcamps ==
There are various online coding bootcamp options to provide students with flexibility in their learning. These remote programs offer convenience and structure, usually by matching students with a mentor. These online coding bootcamp options are also more likely to be cheaper and more accommodating to specific student needs.


== Data science bootcamps and fellowships ==
The demand for skilled data scientists and data engineers remains strong in 2016 and will continue to grow. Big data analysis is becoming a necessity for companies to prosper in all industries from agriculture to finance. The requirements for enrollment are more stringent than those of coding bootcamps. For example, The Data Incubator's acceptance rate is widely reported to be lower than that of Harvard's  


== Tuition ==
According to a 2017 market research report, tuition ranges from free to $21,000 for a course, with an average tuition of $11,874. Courses range from 8 to 36 weeks, but most courses are in the 10- to 12-week range with an average of 12.9 weeks.
Schools like App Academy and The Grace Hopper Program can offer “Deferred Tuition."" Deferred Tuition refers to a payment model in which students pay the school a percentage (18%-22.5%) of their salary for 1–3 years after graduation, instead of upfront tuition.
In Europe, coding bootcamps tend to be 'more reasonably priced'. One reason for this is because university tuition can be free or a couple thousand euros per program. In contrast to formal university education, private offerings for training appear expensive. As of 2016, the most 'affordable' bootcamp offered in Europe in English is, Elium Academy according to an article on Switch Up, a leading programming bootcamp review source. Elium Academy is based in Brussels and offers programs as low as 1,800 Euros, or free with their Learn and Train model.


== Acquisition ==
The first coding bootcamp acquisition was in June 2014 where Dev Bootcamp was acquired by Kaplan Test Prep. With rapid market growth in the bootcamp industry, large for-profit education companies and universities are stepping in to acquire more coding bootcamps. Additional acquisitions include, but are not limited to:
Apollo Education acquiring The Iron Yard
Strayer Education acquiring New York Code & Design
Capella Education acquiring Hakcbright Academy and DevMountain


== Controversy ==
Experts are worried that partnering private coding bootcamps with federal financial aid could attract less reputable organizations to create coding bootcamp programs. Some believe that “integrating bootcamps into the established system, might just saddle them with all the established system’s problems.” Barriers to entry and exit mean established schools face less competition than in a free market, which can lead to deterioration of quality, and increase in prices. Also, problems within traditional university models could easily transfer to the university/bootcamp partnerships. On the other hand, others believe that enhancing policy around financial aid will help lower income prospective students attend. There are several sentiments of coding bootcamps being accessible only for the rich.


== References =="
65,Symposia on VLSI Technology and Circuits,33963534,16015,"The Symposia on VLSI Technology and Circuits are two closely connected international conferences on semiconductor technology and circuits, thereby offering an opportunity to interact and synergize on topics of joint interest, spanning the range from process technology to systems-on-chip.          The Symposia take place once a year around the middle of June at locations alternating between Kyoto, Japan and Honolulu, USA. They bring together managers, engineers, and scientists from industry and academia around the world to discuss challenges in manufacturing and design of Very-large-scale integration (VLSI) circuits. The Symposium on VLSI Technology started in 1981 while the Symposium on VLSI Circuits was established in 1987. Beside regular presentations of technical papers, the Symposia comprise short courses, panel sessions, and invited talks conducted by experts in the field from both Industry and Academia.


== Sponsors ==
The Symposium on VLSI Technology is sponsored by the IEEE Electron Devices Society and the Japan Society of Applied Physics in cooperation with the IEEE Solid-State Circuits Society. The Symposium on VLSI Circuits is sponsored by the IEEE Solid-State Circuits Society and the Japan Society of Applied Physics in cooperation with the Institute of Electronics, Information and Communication Engineers and the IEEE Electron Devices Society. Extended versions of selected papers from the Symposium on VLSI Circuits are regularly published once a year in a special Issue of the IEEE Journal of Solid-State Circuits. 


== Symposia 2018 ==
The 2018 Symposia on VLSI Technology and Circuits will take place June 18-22, 2018. Themed ""Technology, Circuits and Systems for Smart Living,"" they will be held at the Hilton Hawaiian Village in beautiful Honolulu, Hawaii.
The Symposia Call for Papers have been posted for both Technology and Circuits. The first day of the conference, June 18th, will feature several Short Courses, followed by the technical program June 19-21. On Friday June 22nd, a Forum on ""Deep Learning and Artificial Intelligence"" will be held. There will also be significant opportunity for networking at evening panels and social events.


== Symposia 2017 ==
The 2017 Symposia on VLSI Technology and Circuits were held at the RIHGA Royal Hotel, Kyoto, Japan between Monday, 5-Jun-2017 and Thursday, 8-Jun-2017.
For the first time, the Technology and Circuits Symposia fully overlapped, maximizing the opportunity to learn from and interact with experts from both sides of the conference.
The advance technical program for both Symposia are available online.
Short courses covered 5 nm technology enablers, machine learning, and circuits for autonomous vehicles.
VLSI Circuits Symposium 2017 hosted a new Demo Session, which will be held during the Symposium Joint Circuits/Technology Reception. At the demo session, authors of selected papers employed posters to augment their demonstrations.
The Circuits plenary session included talks by Google on Waymo's self-driving car and from Panasonic on the topic of an interconnected IoT world.
The Technology plenary session was headlined by talks from SoftBank and NXP Semiconductors.
Several evening panels were held, with a chance to hear from a variety of industrial and academic thought leaders.
A new International Forum on Singularity: Exponential X was held on Friday, 9-Jun-2017, and was free for all paid registrants of the VLSI Symposia.


== Symposia 2016 ==
The 2016 Symposia on VLSI Technology and Circuits was held at the Hilton Hawaiian Village, Honolulu, Hawaii between Monday, 13-Jun-2016 and Friday, 17-Jun-2016.
IEEE SSCS Magazine provides a summary report
The conference theme for 2016 was ""VLSI Inflection for a Smart Society,"" reflecting the transition from More Moore to More Than Moore technologies in support of the increasingly connected society enabled by the Internet of Things.
The Circuits plenary session highlighted talks from Google on machine learning and Sony covering advances in and impact of image sensors.
The Technology plenary session featured the impact of MEMS sensors in a talk from InvenSense, as well as a presentation on intelligent mobility from Nissan.
The Executive Panel, moderated by Dan Hutcheson of VLSI Research Inc., included industry leaders discussing the ""Semiconductor Business: Inflections Beyond Scaling.""
Technical Program


== Symposia 2015 ==
The 2015 Symposia on VLSI Technology and Circuits were held at the RIHGA Royal Hotel, Kyoto, Japan between Monday, 15-Jun-2015 and Friday, 19-Jun-2015.
Technical Program
Plenary sessions addressed robotics, technology needs for future consumer devices, commercialization of the Internet of Things (IoT), and autonomous driving.
Joint technology and circuits sessions covered design in scaled technologies, design enablement, memory technologies, and 3D-integration (TSV).
For the first time, the circuits symposium encouraged submissions of papers beyond the chip context in areas related to IoT sensing, industrial electronics, big data processing/storage systems, and robotics and smart car.


== Symposia 2014 ==
The 2014 Symposia on VLSI Technology and Circuits were held at the Hilton Hawaiian Village, Honolulu, Hawaii between Monday, 09-Jun-2014 and Friday, 13-Jun-2014.
Advance Program
Joint circuit and technology focus sessions were offered in the following special topics of mutual interest: 3D Circuits & Applications, 3D Systems & Packaging, Design/Technology Co-Optimization, Non-volatile & Emerging Memory, and SRAM & DRAM.
This year, a Luncheon & Executive Panel did debate on ""Emerging Semiconductor Industry Trends and Implications"" (Link)
A joint technology-circuits panel session addressed the question: ""Who Gives Up on Scaling First: Device & Process Technology Engineers, Circuit Designers or Company Executives? (Link)
The 2014 Symposia on VLSI Technology and Circuits were preceded by three full-day short courses: ""High Performance Mobile SoCs Enabled by 10nm SoC Technology"" (Link), ""Advanced Data Converter & Mixed-Signal Circuit Design"", and ""Advanced Energy-Efficient Digital Design"" (Link).
Also, the Symposia were accompanied by two satellite workshops, the 2014 Silicon Nanoelectronics Workshop (June 8–9) and the 2014 Spintronics Workshop on LSI (June 9) at the same location.


== Symposia 2013 ==
The 2013 Symposia on VLSI Technology and Circuits were held in Kyoto, Japan between 2013-06-11 and 2013-06-14 with more than 1000 microelectronics engineers, managers and researchers attending.
IEEE SSCS Magazine provides a summary report
The Technical Programs are available on the Symposia's web site (Circuits / Technology).
Highlights of the technical program are provided in Chinese, Englisch, Japanese and Korean (Editor Press Center)
The Symposia were accompanied by two satellite workshops, the 2013 Silicon Nanoelectronics Workshop (June 9–10) and the 2013 Spintronics Workshop on LSI (June 10) at the same location.


== References ==


== Additional information ==
Press Release Web-Site with information in Chinese, Englisch, Japanese, and Korean
IEEE Conference Server
IEEE Xplore Digital Library
LinkedIn Discussion Group


== Related Conferences ==
Custom Integrated Circuits Conference (CICC)
Design Automation Conference (DAC)
FD-SOI Workshop (succeeding the VLSI Symposia)
International Electron Devices Meeting (IEDM)
International Interconnect Technology Conference (IITC - adjacent to the VLSI Symposia)
International Solid-State Circuits Conference (ISSCC)
Silicon Nanoelectronics Workshop (SNW - preceding the VLSI Symposia)
Spintronics Workshop on LSI (preceding the VLSI Symposia)"
66,Raspberry Pi Foundation,32264875,15833,"The Raspberry Pi Foundation is a charity founded in 2009 to promote the study of basic computer science in schools, and is responsible for developing a single-board computer called the Raspberry Pi, the UK's best-selling PC of all time.


== Foundation ==

The Raspberry Pi Foundation is a charitable organization registered with the Charity Commission for England and Wales. The board of trustees was assembled by 2008 and the Raspberry Pi Foundation was founded as a registered charity in May 2009 in Caldecote, Cambridgeshire, UK. In 2016, The Foundation moved its headquarters to Station Road, Cambridge, Cambridge. The Foundation is supported by the University of Cambridge Computer Laboratory and Broadcom. Its aim is to ""promote the study of computer science and related topics, especially at school level, and to put the fun back into learning computing."" Project co-founder Eben Upton is a former academic, currently employed by Broadcom as a system-on-chip architect and associate technical director. Components, albeit in small numbers, were able to be sourced from suppliers, due to the charitable status of the organization.


=== History ===
When the decline in numbers and skills of students applying for Computer Science became a concern for a team that included Eben Upton, Rob Mullins, Jack Lang and Alan Mycroft at the University of Cambridge’s Computer Laboratory in 2006, a need for a tiny and affordable computer came to their minds. Several versions of the early Raspberry Pi prototypes were designed but were very limited by the high cost and low power processors for mobile devices at that time.
In 2008, the team started a collaboration with Pete Lomas, MD of Norcott Technologies and David Braben, the co-author of the seminal BBC micro game Elite, and formed the Raspberry Pi Foundation. Three years later, the Raspberry Pi Model B was born and it had sold over two million units within two years of mass production.


==== Founders and current leadership ====

The original founders of the organization includes
Eben Upton
Rob Mullins: a Senior Lecturer in the Computer Laboratory at the University of Cambridge
Jack Lang: an affiliated Lecturer at the Computer Laboratory and the founder of Electronic Share Information Ltd
Alan Mycroft: professor of Computing in the Computer Laboratory and co-founded the European Association for Programming Languages and Systems
Pete Lomas: director of Engineering at Norcott Technologies
David Braben: CEO of Frontier Developments and co-writer of the seminal Elite
In early 2013 the organization split into two parts: Raspberry Pi Foundation which is responsible for the charitable and educational activities; and Raspberry Pi (Trading) Ltd responsible for the engineering and trading activities. Raspberry Pi (Trading) Ltd is a wholly owned subsidiary of Raspberry Pi Foundation, with the money earned from sales of Raspberry Pi products being used to fund the charitable work of the Foundation. Eben Upton was initially CEO of both divisions, but in September 2013 Lance Howarth became CEO of the Raspberry Pi Foundation, with Eben Upton remaining as CEO of Raspberry Pi (Trading) Ltd. Philip Colligan took over from Lance Howarth as CEO of Raspberry Pi Foundation in July 2015.


=== Trustees and Patron ===
As of 31 December 2015, the foundation has 7 Trustees:
Jack Lang (trustee and company secretary)
David Braben
David Cleevely (Chairman)
Sherry Coutu (angel investor, Canadian but now Cambridge-based)
Louis Glass (corporate lawyer; partner at Olswang)
Pete Lomas
Chris Mairs (chief scientist at Metaswitch Networks)
The Board of Trustees is elected by and supported by the Members of the Foundation, with Members serving in a voluntary role and coming from a range of backgrounds.
Prince Andrew, Duke of York serves as Patron of the Raspberry Pi Foundation.


=== Early expectations ===
The Foundation expected that children would program using Scratch and that the input/output functionality would be used to control external devices. Additionally, the low power requirement facilitates battery-powered usage in robots, while the video capabilities have led to interest in use as a home media centre.


=== Education fund ===
In April 2014, the foundation announced a £1 million education fund to support projects that enhance the understanding of computing and to promote the use of technology in other subjects, particularly STEM and creative arts for children. They offer to provide up to 50% of the total projected costs to successful applicants.


=== Logo ===
In October 2011, the logo was selected from a number submitted from open competition. A shortlist of six was drawn up, with the final judging taking several days. The chosen design was created by Paul Beech and based on a buckyball.


== Raspberry Pi ==

In 2011, the Raspberry Pi Foundation developed a single-board computer named the Raspberry Pi. The Foundation's goal was to offer two versions, priced at US$25 and $35 (plus local taxes). The Foundation started accepting orders for the higher priced model on 29 February 2012. The Raspberry Pi is intended to stimulate the teaching of computer science in schools.


== Raspberry Pi Zero ==
In 2015 the foundation unveiled the Raspberry Pi Zero. This version of the microcomputer had a significantly reduced form factor and a lower price, launching at £4/$5. The new model features a 1Ghz, Single-core CPU; 512MB RAM, Mini HDMI and USB ports, Micro USB power, HAT-compatible 40-pin header as well as Composite video and reset headers [1]. As a fully functioning Linux system the Raspberry Pi Zero's 1 GHz processor is comparable to the middle of the road for the Intel Pentium 3 architecture (450 MHz to 1.4 GHz), a standard in 2000. The reduced price and smaller form factor encourages use in smaller and embedded projects.


== References ==


== External links ==
Official website"
67,Large-scale Complex IT Systems,33676779,15792,"The UK Large-Scale Complex IT Systems (LSCITS) Initiative is a research and graduate education programme focusing on the problems of developing large-scale, complex IT systems (also referred to as Ultra-large-scale systems or ULSS). The initiative is funded by the EPSRC, with more than ten million pounds of funding awarded between 2006 and 2013.


== Background ==
The initial motivation for the establishment of a research programme in large-scale complex IT systems was the publication of a 2004 report by the Royal Academy of Engineering and the British Computer Society. This report examined the causes of failure of a number of large software projects and made several recommendations for research to address some of these problems.
A second report, authored by Seth Bullock & Dave Cliff and also published in 2004, was commissioned by the UK Government's Department of Trade and Industry (DTI) Office of Science and Technology and carried the title Complexity and Emergent Behaviour in ICT Systems The main conclusions of this report were that the primary challenges needing to be addressed in the UK are institutional and cultural obstacles to appropriate interdisciplinary research and that there was an urgent need to address omissions in UK undergraduate computer science education.
In October 2005, Dave Cliff was appointed Director of the LSCITS initiative by the EPSRC and was asked to consult extensively with industry on their problems in this area and, on the basis of this consultation, to form a consortium to tackle these problems. The results of the consultation were that the key concerns of industry were socio-technical issues arising from the interactions between organisations, people and systems and in high-integrity systems engineering. On the basis of this, a consortium was formed with two partners (York, Oxford) focusing on formal methods and high-integrity systems and two partners (Leeds, St Andrews) focusing on socio-technical systems. Subsequently, a further project focusing on cloud computing was approved with Bristol, St Andrews and Aston Universities as partners.
The five-year research project started in October 2007 with the associated EngD program starting in October 2009.


== Partners ==
University of Bristol. Department of Computer Science (Prof Dave Cliff)
University of Leeds. Institute of Health Sciences (Prof Justin Keen). Dr Andreas Hild and Mr Kanwar Adeel Waheed Khan also worked on the LSCITS team along from 2008-2011, examining complexity in organisations aspects.
University of Oxford. Department of Computer Science (Prof Marta Kwiatkowska)
University of St Andrews. School of Computer Science (Prof Ian Sommerville)
University of York. Department of Computer Science (Prof John McDermid, Prof Tim Kelly, Prof Richard Paige, Dr Radu Calinescu)


== Research ==
The aim of the LSCITS research project is:
""to improve existing technical approaches to complex systems engineering and to develop new socio-technical approaches that help us understand the complex interactions between organisations, processes and systems"".
The LSCITS stack (Figure 1) shows the research areas that are particularly relevant to LSCITS.

The focus of the work of the project was initially in the following areas:
Complexity in organisations (led by University of Leeds)
Socio-technical systems engineering (led by University of St Andrews).
Predictable software systems(led by University of Oxford)
High-integrity systems (led by University of York)
Work on mathematical foundations was not included as these were funded in a separate research programme by the EPSRC with complexity science research centres at the University of Bristol and the University of Warwick. A further centre on complex systems simulation was funded later at the University of Southampton.
The work on novel computational approaches was superseded by work on cloud computing as the significance of this area emerged during the project.
The Key Publications below describe the work of the project in more detail.


== The LSCITS EngD programme ==
The LSCITS EngD programme  is an Engineering Doctorate scheme, coordinated by the University of York that focuses on training and research in complex IT systems. Students on the programme take a range of core and optional taught modules and carry out research in conjunction with an industrial sponsor. The key difference between this programme and a conventional PhD is that students spend the majority of their time working with the industrial sponsor and may submit a portfolio thesis, describing several related research projects on a common theme, rather than a single topic.
Core modules on the scheme include
Empirical Methods for LSCITS
High-Integrity Systems Engineering
Predictable Software Systems
Socio-Technical Systems
Systems Engineering for LSCITS
Technology Innovation
Students take a number of optional modules in addition to these core topics from computer science, mathematics and management. Overall, the core and optional modules are intended to provide EngD students with breadth as well as depth in LSCITS topics.
In parallel with the taught part of the programme, students carry out research; research projects span LSCITS topics, including socio-technical systems, high-performance computing, cloud computing, systems and software engineering, safety critical systems, interactive and accessible systems, and advanced decision making. EngD industrial sponsors include leading multi-national corporations, through to small-to-medium-sized enterprises who wish to build research capability and capacity.


== Management and governance ==
Operational management of the entire LSCITS Initiative is the responsibility of the Director (Dave Cliff), and the two Initiative Co-Directors (Ian Sommerville and John McDermid). Ian Sommerville manages integration across the various LSCITS work-packages and activities. John McDermid works with Richard Paige, the LSCITS EngD Centre Director, to manage the York-based LSCITS Engineering Doctorate programme.
The Director reports to the chair of the LSCITS International Scientific Advisory Board, and to the chair of the LSCITS National Stakeholder Board. These two boards provide their guidance and advice on the LSCITS research and training programmes.


== Key publications ==
The papers below, organised according to the LSCITS stack shown in Figure 1, describe the work of the project. A full list of publications is available on the LSCITS web site.


=== LSCITS in general ===
R. Calinescu & M. Kwiatkowska (2010). Software Engineering Techniques for the Development of Systems of Systems. In C. Choppy & O. Sokolski (editors), Foundations of Computer Software. Future Trends and Techniques for Development, vol. 6026 of LNCS, pp. 59–82, Springer. Preprint available online.
D. Cliff & L. Northrop (2011). The Global Financial Markets: An Ultra-Large Scale Systems Perspective. Briefing paper for UK Government Office for Science Foresight project on The Future of Computer Trading in the Financial Markets. September 2011.
I. Sommerville, D. Cliff, R. Calinescu, J. Keen, T. Kelly, M. Kwiatkowska, J. McDermid, and R. Paige. (2011) Large Scale Complex IT Systems.


=== Complexity in organisations ===
J. Rooksby and I. Sommerville. (2012) The Management and Use of Social Network Sites in a Government Department. Computer-supported Cooperative Work - The Journal of Collaborative Computing.
J. Keen. (2011) The Governance of Privacy and Confidentiality. Paper prepared for IRSPM XV, Dublin, 11–13 April 2011.
J. Keen. (2009) Integration At Any Price: The Case of the NHS National Programme for IT. In: H Margetts, C Hood and 6 P (eds) Paradoxes of Modernization. Oxford, Oxford University Press.


=== Socio-technical systems engineering ===
Baxter, G.; Sommerville, I. (2010). ""Socio-Technical Systems: From Design Methods to Systems Engineering"". Interacting With Computers. 23: 4–17. doi:10.1016/j.intcom.2010.07.003. 
I. Sommerville (editor). The Socio-technical Systems Engineering Handbook. (2011). University of St Andrews.


=== High-integrity systems ===
X. Ge, R.F. Paige, J. McDermid: Probabilistic Failure Propagation and Transformation Analysis. SAFECOMP 2009: 215-228.
Paige, R.F.; Galloway, A.; Charalambous, R.; Ge, X.; Brooke, P.J. (2011). ""High-integrity agile processes for the development of safety critical software"". IJCCBS. 2 (2): 181–216. doi:10.1504/IJCCBS.2011.041259. 
Williams, J.R.; Poulding, S.M.; Rose, L.M.; Paige, R.F.; Polack, F.A. C. ""Identifying Desirable Game Character Behaviours through the Application of Evolutionary Algorithms to Model-Driven Engineering Metamodels"". SSBSE. 2011: 112–126. doi:10.1007/978-3-642-23716-4_13. 


=== Predictable software systems ===
R. Calinescu, L. Grunske, M. Kwiatkowska, R. Mirandola, G. Tamburrelli (2011). Dynamic QoS Management and Optimisation in Service-Based Systems. In: IEEE Transactions on Software Engineering.
L. Feng, M. Kwiatkowska and D. Parker. (2011) Automated Learning of Probabilistic Assumptions for Compositional Reasoning. Proc. 14th International Conference on Fundamental Approaches to Software Engineering (FASE'11), volume 6603 of LNCS, pages 2–17, Springer.
M. Kwiatkowska. (2007) Quantitative Verification: Models, Techniques and Tools. Proc. 6th joint meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering (ESEC/FSE), pages 449-458, ACM Press.
M. Kwiatkowska, G. Norman and D. Parker. (2009) PRISM: Probabilistic Model Checking for Performance and Reliability Analysis. ACM SIGMETRICS Performance Evaluation Review, 36(4), pages 40–45, ACM.


=== Cloud computing ===
John Cartlidge and Ilango Sriram (2011). Modelling resilience in cloud-scale data centres. Proceedings of 23rd European Modeling and Simulation Symposium, Rome. September 2011
A. Khajeh-Hosseini, D. Greenwood, J. W. Smith & I. Sommerville (2011). The Cloud Adoption Toolkit: Supporting Cloud Adoption Decisions in the Enterprise. Software: Practice and Experience - Special Issue on Software Architectures and Application Development Environments for Cloud Computing.


== Publications by similar groups ==
Ultra-large scale systems: Overview. Software Engineering Institute, Carnegie Mellon University.
Goth, G. (2008). ""Ultralarge Systems: Redefining Software Engineering?"". IEEE Software. 25 (3): 91–94. doi:10.1109/MS.2008.82. 
H. Sillitto, (2010) ""Design Principles for Ultra-Large-Scale Systems"". Proc. 20th Annual International Council on Systems Engineering (INCOSE) International Symposium, July, 2010, Chicago, IL, USA.
Northrop, L. et al. (2006). Ultra-Large-Scale Systems: The Software Challenge of the Future. Software Engineering Institute, Carnegie Mellon University. (6.5MB download)


== See also ==
Cloud computing
Sociotechnical systems
System of systems


== References =="
68,Fulkerson Prize,1425916,15559,"The Fulkerson Prize for outstanding papers in the area of discrete mathematics is sponsored jointly by the Mathematical Optimization Society (MOS) and the American Mathematical Society (AMS). Up to three awards of $1500 each are presented at each (triennial) International Symposium of the MOS. Originally, the prizes were paid out of a memorial fund administered by the AMS that was established by friends of the late Delbert Ray Fulkerson to encourage mathematical excellence in the fields of research exemplified by his work. The prizes are now funded by an endowment administered by MPS.


== Winners ==
1979:
Richard M. Karp for classifying many important NP-complete problems.
Kenneth Appel and Wolfgang Haken for the four color theorem.
Paul Seymour for generalizing the max-flow min-cut theorem to matroids.

1982:
D.B. Judin, Arkadi Nemirovski, Leonid Khachiyan, Martin Grötschel, László Lovász and Alexander Schrijver for the ellipsoid method in linear programming and combinatorial optimization.
G. P. Egorychev and D. I. Falikman for proving van der Waerden's conjecture that the matrix with all entries equal has the smallest permanent of any doubly stochastic matrix.

1985:
Jozsef Beck for tight bounds on the discrepancy of arithmetic progressions.
H. W. Lenstra, Jr. for using the geometry of numbers to solve integer programs with few variables in time polynomial in the number of constraints.
Eugene M. Luks for a polynomial time graph isomorphism algorithm for graphs of bounded maximum degree.

1988:
Éva Tardos for finding minimum cost circulations in strongly polynomial time.
Narendra Karmarkar for Karmarkar's algorithm for linear programming.

1991:
Martin E. Dyer, Alan M. Frieze and Ravindran Kannan for random-walk-based approximation algorithms for the volume of convex bodies.
Alfred Lehman for 0,1-matrix analogues of the theory of perfect graphs.
Nikolai E. Mnev for Mnev's universality theorem, that every semialgebraic set is equivalent to the space of realizations of an oriented matroid.

1994:
Louis Billera for finding bases of piecewise-polynomial function spaces over triangulations of space.
Gil Kalai for making progress on the Hirsch conjecture by proving subexponential bounds on the diameter of d-dimensional polytopes with n facets.
Neil Robertson, Paul Seymour and Robin Thomas for the six-color case of Hadwiger's conjecture.

1997:
Jeong Han Kim for finding the asymptotic growth rate of the Ramsey numbers R(3,t).

2000:
Michel X. Goemans and David P. Williamson for approximation algorithms based on semidefinite programming.
Michele Conforti, Gérard Cornuéjols, and M. R. Rao for recognizing balanced 0-1 matrices in polynomial time.

2003:
J. F. Geelen, A. M. H. Gerards and A. Kapoor for the GF(4) case of Rota's conjecture on matroid minors.
Bertrand Guenin for a forbidden minor characterization of the weakly bipartite graphs (graphs whose bipartite subgraph polytope is 0-1).
Satoru Iwata, Lisa Fleischer, Satoru Fujishige, and Alexander Schrijver for showing submodular minimization to be strongly polynomial.

2006:
Manindra Agrawal, Neeraj Kayal and Nitin Saxena, for the AKS primality test.
Mark Jerrum, Alistair Sinclair and Eric Vigoda, for approximating the permanent.
Neil Robertson and Paul Seymour, for the Robertson–Seymour theorem showing that graph minors form a well-quasi-ordering.

2009:
Maria Chudnovsky, Neil Robertson, Paul Seymour, and Robin Thomas, for the strong perfect graph theorem.
Daniel A. Spielman and Shang-Hua Teng, for smoothed analysis of linear programming algorithms.
Thomas C. Hales and Samuel P. Ferguson, for proving the Kepler conjecture on the densest possible sphere packings.

2012:
Sanjeev Arora, Satish Rao, and Umesh Vazirani for improving the approximation ratio for graph separators and related problems from 
  
    
      
        O
        (
        log
        ⁡
        n
        )
      
    
    {\displaystyle O(\log n)}
   to 
  
    
      
        O
        (
        
          
            log
            ⁡
            n
          
        
        )
      
    
    {\displaystyle O({\sqrt {\log n}})}
  .
Anders Johansson, Jeff Kahn, and Van H. Vu for determining the threshold of edge density above which a random graph can be covered by disjoint copies of a given smaller graph.
László Lovász and Balázs Szegedy for characterizing subgraph multiplicity in sequences of dense graphs.

2015 :
Francisco Santos Leal for a counter-example of the Hirsch conjecture.


== References ==


== External links ==
Official web page (MOS)
Official site with award details (AMS website)
AMS archive of past prize winners"
69,String operations,10803719,15551,"In computer science, in the area of formal language theory, frequent use is made of a variety of string functions; however, the notation used is different from that used for computer programming, and some commonly used functions in the theoretical realm are rarely used when programming. This article defines some of these basic terms.


== Strings and languages ==
A string is a finite sequence of characters. The empty string is denoted by 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
  . The concatenation of two string 
  
    
      
        s
      
    
    {\displaystyle s}
   and 
  
    
      
        t
      
    
    {\displaystyle t}
   is denoted by 
  
    
      
        s
        ⋅
        t
      
    
    {\displaystyle s\cdot t}
  , or shorter by 
  
    
      
        s
        t
      
    
    {\displaystyle st}
  . Concatenating with the empty string makes no difference: 
  
    
      
        s
        ⋅
        ε
        =
        s
        =
        ε
        ⋅
        s
      
    
    {\displaystyle s\cdot \varepsilon =s=\varepsilon \cdot s}
  . Concatenation of strings is associative: 
  
    
      
        s
        ⋅
        (
        t
        ⋅
        u
        )
        =
        (
        s
        ⋅
        t
        )
        ⋅
        u
      
    
    {\displaystyle s\cdot (t\cdot u)=(s\cdot t)\cdot u}
  .
For example, 
  
    
      
        (
        ⟨
        b
        ⟩
        ⋅
        ⟨
        l
        ⟩
        )
        ⋅
        (
        ε
        ⋅
        ⟨
        a
        h
        ⟩
        )
        =
        ⟨
        b
        l
        ⟩
        ⋅
        ⟨
        a
        h
        ⟩
        =
        ⟨
        b
        l
        a
        h
        ⟩
      
    
    {\displaystyle (\langle b\rangle \cdot \langle l\rangle )\cdot (\varepsilon \cdot \langle ah\rangle )=\langle bl\rangle \cdot \langle ah\rangle =\langle blah\rangle }
  .
A language is a finite or infinite set of strings. Besides the usual set operations like union, intersection etc., concatenation can be applied to languages: if both 
  
    
      
        S
      
    
    {\displaystyle S}
   and 
  
    
      
        T
      
    
    {\displaystyle T}
   are languages, their concatenation 
  
    
      
        S
        ⋅
        T
      
    
    {\displaystyle S\cdot T}
   is defined as the set of concatenations of any string from 
  
    
      
        S
      
    
    {\displaystyle S}
   and any string from 
  
    
      
        T
      
    
    {\displaystyle T}
  , formally 
  
    
      
        S
        ⋅
        T
        =
        {
        s
        ⋅
        t
        ∣
        s
        ∈
        S
        ∧
        t
        ∈
        T
        }
      
    
    {\displaystyle S\cdot T=\{s\cdot t\mid s\in S\land t\in T\}}
  . Again, the concatenation dot 
  
    
      
        ⋅
      
    
    {\displaystyle \cdot }
   is often omitted for brevity.
The language 
  
    
      
        {
        ε
        }
      
    
    {\displaystyle \{\varepsilon \}}
   consisting of just the empty string is to be distinguished from the empty language 
  
    
      
        {
        }
      
    
    {\displaystyle \{\}}
  . Concatenating any language with the former doesn't make any change: 
  
    
      
        S
        ⋅
        {
        ε
        }
        =
        S
        =
        {
        ε
        }
        ⋅
        S
      
    
    {\displaystyle S\cdot \{\varepsilon \}=S=\{\varepsilon \}\cdot S}
  , while concatenating with the latter always yields the empty language: 
  
    
      
        S
        ⋅
        {
        }
        =
        {
        }
        =
        {
        }
        ⋅
        S
      
    
    {\displaystyle S\cdot \{\}=\{\}=\{\}\cdot S}
  . Concatenation of languages is associative: 
  
    
      
        S
        ⋅
        (
        T
        ⋅
        U
        )
        =
        (
        S
        ⋅
        T
        )
        ⋅
        U
      
    
    {\displaystyle S\cdot (T\cdot U)=(S\cdot T)\cdot U}
  .
For example, abbreviating 
  
    
      
        D
        =
        {
        ⟨
        0
        ⟩
        ,
        ⟨
        1
        ⟩
        ,
        ⟨
        2
        ⟩
        ,
        ⟨
        3
        ⟩
        ,
        ⟨
        4
        ⟩
        ,
        ⟨
        5
        ⟩
        ,
        ⟨
        6
        ⟩
        ,
        ⟨
        7
        ⟩
        ,
        ⟨
        8
        ⟩
        ,
        ⟨
        9
        ⟩
        }
      
    
    {\displaystyle D=\{\langle 0\rangle ,\langle 1\rangle ,\langle 2\rangle ,\langle 3\rangle ,\langle 4\rangle ,\langle 5\rangle ,\langle 6\rangle ,\langle 7\rangle ,\langle 8\rangle ,\langle 9\rangle \}}
  , the set of all three-digit decimal numbers is obtained as 
  
    
      
        D
        ⋅
        D
        ⋅
        D
      
    
    {\displaystyle D\cdot D\cdot D}
  . The set of all decimal numbers of arbitrary length is an example for an infinite language.


== Alphabet of a string ==
The alphabet of a string is the set of all of the characters that occur in a particular string. If s is a string, its alphabet is denoted by

  
    
      
        Alph
        ⁡
        (
        s
        )
      
    
    {\displaystyle \operatorname {Alph} (s)}
  
The alphabet of a language 
  
    
      
        S
      
    
    {\displaystyle S}
   is the set of all characters that occur in any string of 
  
    
      
        S
      
    
    {\displaystyle S}
  , formally: 
  
    
      
        Alph
        ⁡
        (
        S
        )
        =
        
          ⋃
          
            s
            ∈
            S
          
        
        Alph
        ⁡
        (
        s
        )
      
    
    {\displaystyle \operatorname {Alph} (S)=\bigcup _{s\in S}\operatorname {Alph} (s)}
  .
For example, the set 
  
    
      
        {
        ⟨
        a
        ⟩
        ,
        ⟨
        c
        ⟩
        ,
        ⟨
        o
        ⟩
        }
      
    
    {\displaystyle \{\langle a\rangle ,\langle c\rangle ,\langle o\rangle \}}
   is the alphabet of the string 
  
    
      
        ⟨
        c
        a
        c
        a
        o
        ⟩
      
    
    {\displaystyle \langle cacao\rangle }
  , and the above 
  
    
      
        D
      
    
    {\displaystyle D}
   is the alphabet of the above language 
  
    
      
        D
        ⋅
        D
        ⋅
        D
      
    
    {\displaystyle D\cdot D\cdot D}
   as well as of the language of all decimal numbers.


== String substitution ==
Let L be a language, and let Σ be its alphabet. A string substitution or simply a substitution is a mapping f that maps characters in Σ to languages (possibly in a different alphabet). Thus, for example, given a character a ∈ Σ, one has f(a)=La where La ⊆ Δ* is some language whose alphabet is Δ. This mapping may be extended to strings as
f(ε)=ε
for the empty string ε, and
f(sa)=f(s)f(a)
for string s ∈ L and character a ∈ Σ. String substitutions may be extended to entire languages as 

  
    
      
        f
        (
        L
        )
        =
        
          ⋃
          
            s
            ∈
            L
          
        
        f
        (
        s
        )
      
    
    {\displaystyle f(L)=\bigcup _{s\in L}f(s)}
  
Regular languages are closed under string substitution. That is, if each character in the alphabet of a regular language is substituted by another regular language, the result is still a regular language. Similarly, context-free languages are closed under string substitution.
A simple example is the conversion fuc(.) to upper case, which may be defined e.g. as follows:
For the extension of fuc to strings, we have e.g.
fuc(‹Straße›) = {‹S›} ⋅ {‹T›} ⋅ {‹R›} ⋅ {‹A›} ⋅ {‹SS›} ⋅ {‹E›} = {‹STRASSE›},
fuc(‹u2›) = {‹U›} ⋅ {ε} = {‹U›}, and
fuc(‹Go!›) = {‹G›} ⋅ {‹O›} ⋅ {} = {}.
For the extension of fuc to languages, we have e.g.
fuc({ ‹Straße›, ‹u2›, ‹Go!› }) = { ‹STRASSE› } ∪ { ‹U› } ∪ { } = { ‹STRASSE›, ‹U› }.


== String homomorphism ==
A string homomorphism (often referred to simply as a homomorphism in formal language theory) is a string substitution such that each character is replaced by a single string. That is, 
  
    
      
        f
        (
        a
        )
        =
        s
      
    
    {\displaystyle f(a)=s}
  , where 
  
    
      
        s
      
    
    {\displaystyle s}
   is a string, for each character 
  
    
      
        a
      
    
    {\displaystyle a}
  .
String homomorphisms are monoid morphisms on the free monoid, preserving the empty string and the binary operation of string concatenation. Given a language 
  
    
      
        L
      
    
    {\displaystyle L}
  , the set 
  
    
      
        f
        (
        L
        )
      
    
    {\displaystyle f(L)}
   is called the homomorphic image of 
  
    
      
        L
      
    
    {\displaystyle L}
  . The inverse homomorphic image of a string 
  
    
      
        s
      
    
    {\displaystyle s}
   is defined as

  
    
      
        
          f
          
            −
            1
          
        
        (
        s
        )
        =
        {
        w
        
          |
        
        f
        (
        w
        )
        =
        s
        }
      
    
    {\displaystyle f^{-1}(s)=\{w|f(w)=s\}}
  
while the inverse homomorphic image of a language 
  
    
      
        L
      
    
    {\displaystyle L}
   is defined as

  
    
      
        
          f
          
            −
            1
          
        
        (
        L
        )
        =
        {
        s
        
          |
        
        f
        (
        s
        )
        ∈
        L
        }
      
    
    {\displaystyle f^{-1}(L)=\{s|f(s)\in L\}}
  
In general, 
  
    
      
        f
        (
        
          f
          
            −
            1
          
        
        (
        L
        )
        )
        ≠
        L
      
    
    {\displaystyle f(f^{-1}(L))\neq L}
  , while one does have

  
    
      
        f
        (
        
          f
          
            −
            1
          
        
        (
        L
        )
        )
        ⊆
        L
      
    
    {\displaystyle f(f^{-1}(L))\subseteq L}
  
and

  
    
      
        L
        ⊆
        
          f
          
            −
            1
          
        
        (
        f
        (
        L
        )
        )
      
    
    {\displaystyle L\subseteq f^{-1}(f(L))}
  
for any language 
  
    
      
        L
      
    
    {\displaystyle L}
  .
The class of regular languages is closed under homomorphisms and inverse homomorphisms. Similarly, the context-free languages are closed under homomorphisms and inverse homomorphisms.
A string homomorphism is said to be ε-free (or e-free) if 
  
    
      
        f
        (
        a
        )
        ≠
        ε
      
    
    {\displaystyle f(a)\neq \varepsilon }
   for all a in the alphabet 
  
    
      
        Σ
      
    
    {\displaystyle \Sigma }
  . Simple single-letter substitution ciphers are examples of (ε-free) string homomorphisms.
An example string homomorphism guc can also be obtained by defining similar to the above substitution: guc(‹a›) = ‹A›, ..., guc(‹0›) = ε, but letting guc undefined on punctuation chars. Examples for inverse homomorphic images are
guc−1({ ‹SSS› }) = { ‹sss›, ‹sß›, ‹ßs› }, since guc(‹sss›) = guc(‹sß›) = guc(‹ßs›) = ‹SSS›, and
guc−1({ ‹A›, ‹bb› }) = { ‹a› }, since guc(‹a›) = ‹A›, while ‹bb› cannot be reached by guc.
For the latter language, guc(guc−1({ ‹A›, ‹bb› })) = guc({ ‹a› }) = { ‹A› } ≠ { ‹A›, ‹bb› }. The homomorphism guc is not ε-free, since it maps e.g. ‹0› to ε.
A very simple string homomorphism example that maps each character to just a character is the conversion of an EBCDIC-encoded string to ASCII.


== String projection ==
If s is a string, and 
  
    
      
        Σ
      
    
    {\displaystyle \Sigma }
   is an alphabet, the string projection of s is the string that results by removing all characters that are not in 
  
    
      
        Σ
      
    
    {\displaystyle \Sigma }
  . It is written as 
  
    
      
        
          π
          
            Σ
          
        
        (
        s
        )
        
      
    
    {\displaystyle \pi _{\Sigma }(s)\,}
  . It is formally defined by removal of characters from the right hand side:

  
    
      
        
          π
          
            Σ
          
        
        (
        s
        )
        =
        
          
            {
            
              
                
                  ε
                
                
                  
                    
                      if 
                    
                  
                  s
                  =
                  ε
                  
                    
                       the empty string
                    
                  
                
              
              
                
                  
                    π
                    
                      Σ
                    
                  
                  (
                  t
                  )
                
                
                  
                    
                      if 
                    
                  
                  s
                  =
                  t
                  a
                  
                    
                       and 
                    
                  
                  a
                  ∉
                  Σ
                
              
              
                
                  
                    π
                    
                      Σ
                    
                  
                  (
                  t
                  )
                  a
                
                
                  
                    
                      if 
                    
                  
                  s
                  =
                  t
                  a
                  
                    
                       and 
                    
                  
                  a
                  ∈
                  Σ
                
              
            
            
          
        
      
    
    {\displaystyle \pi _{\Sigma }(s)={\begin{cases}\varepsilon &{\mbox{if }}s=\varepsilon {\mbox{ the empty string}}\\\pi _{\Sigma }(t)&{\mbox{if }}s=ta{\mbox{ and }}a\notin \Sigma \\\pi _{\Sigma }(t)a&{\mbox{if }}s=ta{\mbox{ and }}a\in \Sigma \end{cases}}}
  
Here 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
   denotes the empty string. The projection of a string is essentially the same as a projection in relational algebra.
String projection may be promoted to the projection of a language. Given a formal language L, its projection is given by

  
    
      
        
          π
          
            Σ
          
        
        (
        L
        )
        =
        {
        
          π
          
            Σ
          
        
        (
        s
        )
         
        |
         
        s
        ∈
        L
        }
      
    
    {\displaystyle \pi _{\Sigma }(L)=\{\pi _{\Sigma }(s)\ \vert \ s\in L\}}
  


== Right quotient ==
The right quotient of a character a from a string s is the truncation of the character a in the string s, from the right hand side. It is denoted as 
  
    
      
        s
        
          /
        
        a
      
    
    {\displaystyle s/a}
  . If the string does not have a on the right hand side, the result is the empty string. Thus:

  
    
      
        (
        s
        a
        )
        
          /
        
        b
        =
        
          
            {
            
              
                
                  s
                
                
                  
                    
                      if 
                    
                  
                  a
                  =
                  b
                
              
              
                
                  ε
                
                
                  
                    
                      if 
                    
                  
                  a
                  ≠
                  b
                
              
            
            
          
        
      
    
    {\displaystyle (sa)/b={\begin{cases}s&{\mbox{if }}a=b\\\varepsilon &{\mbox{if }}a\neq b\end{cases}}}
  
The quotient of the empty string may be taken:

  
    
      
        ε
        
          /
        
        a
        =
        ε
      
    
    {\displaystyle \varepsilon /a=\varepsilon }
  
Similarly, given a subset 
  
    
      
        S
        ⊂
        M
      
    
    {\displaystyle S\subset M}
   of a monoid 
  
    
      
        M
      
    
    {\displaystyle M}
  , one may define the quotient subset as

  
    
      
        S
        
          /
        
        a
        =
        {
        s
        ∈
        M
         
        |
         
        s
        a
        ∈
        S
        }
      
    
    {\displaystyle S/a=\{s\in M\ \vert \ sa\in S\}}
  
Left quotients may be defined similarly, with operations taking place on the left of a string.
Hopcroft and Ullman (1979) define the quotient L1/L2 of the languages L1 and L2 over the same alphabet as L1/L2 = { s | ∃t∈L2. st∈L1 }. This is not a generalization of the above definition, since, for a string s and distinct characters a, b, Hopcroft's and Ullman's definition implies {sa} / {b} yielding {}, rather than { ε }.
The left quotient (when definied similar to Hopcroft and Ullman 1979) of a singleton language L1 and an arbitrary language L2 is known as Brzozowski derivative; if L2 is represented by a regular expression, so can be the left quotient.


== Syntactic relation ==
The right quotient of a subset 
  
    
      
        S
        ⊂
        M
      
    
    {\displaystyle S\subset M}
   of a monoid 
  
    
      
        M
      
    
    {\displaystyle M}
   defines an equivalence relation, called the right syntactic relation of S. It is given by

  
    
      
        
          ∼
          
            S
          
        
        
        
        =
        
        {
        (
        s
        ,
        t
        )
        ∈
        M
        ×
        M
         
        |
         
        S
        
          /
        
        s
        =
        S
        
          /
        
        t
        }
      
    
    {\displaystyle \sim _{S}\;\,=\,\{(s,t)\in M\times M\ \vert \ S/s=S/t\}}
  
The relation is clearly of finite index (has a finite number of equivalence classes) if and only if the family right quotients is finite; that is, if

  
    
      
        {
        S
        
          /
        
        m
         
        |
         
        m
        ∈
        M
        }
      
    
    {\displaystyle \{S/m\ \vert \ m\in M\}}
  
is finite. In the case that M is the monoid of words over some alphabet, S is then a regular language, that is, a language that can be recognized by a finite state automaton. This is discussed in greater detail in the article on syntactic monoids.


== Right cancellation ==
The right cancellation of a character a from a string s is the removal of the first occurrence of the character a in the string s, starting from the right hand side. It is denoted as 
  
    
      
        s
        ÷
        a
      
    
    {\displaystyle s\div a}
   and is recursively defined as

  
    
      
        (
        s
        a
        )
        ÷
        b
        =
        
          
            {
            
              
                
                  s
                
                
                  
                    
                      if 
                    
                  
                  a
                  =
                  b
                
              
              
                
                  (
                  s
                  ÷
                  b
                  )
                  a
                
                
                  
                    
                      if 
                    
                  
                  a
                  ≠
                  b
                
              
            
            
          
        
      
    
    {\displaystyle (sa)\div b={\begin{cases}s&{\mbox{if }}a=b\\(s\div b)a&{\mbox{if }}a\neq b\end{cases}}}
  
The empty string is always cancellable:

  
    
      
        ε
        ÷
        a
        =
        ε
      
    
    {\displaystyle \varepsilon \div a=\varepsilon }
  
Clearly, right cancellation and projection commute:

  
    
      
        
          π
          
            Σ
          
        
        (
        s
        )
        ÷
        a
        =
        
          π
          
            Σ
          
        
        (
        s
        ÷
        a
        )
      
    
    {\displaystyle \pi _{\Sigma }(s)\div a=\pi _{\Sigma }(s\div a)}
  


== Prefixes ==
The prefixes of a string is the set of all prefixes to a string, with respect to a given language:

  
    
      
        
          Pref
          
            L
          
        
        ⁡
        (
        s
        )
        =
        {
        t
         
        |
         
        s
        =
        t
        u
        
          
             for 
          
        
        t
        ,
        u
        ∈
        Alph
        ⁡
        (
        L
        
          )
          
            ∗
          
        
        }
      
    
    {\displaystyle \operatorname {Pref} _{L}(s)=\{t\ \vert \ s=tu{\mbox{ for }}t,u\in \operatorname {Alph} (L)^{*}\}}
  
where 
  
    
      
        s
        ∈
        L
      
    
    {\displaystyle s\in L}
  .
The prefix closure of a language is

  
    
      
        Pref
        ⁡
        (
        L
        )
        =
        
          ⋃
          
            s
            ∈
            L
          
        
        
          Pref
          
            L
          
        
        ⁡
        (
        s
        )
        =
        
          {
          
            t
             
            |
             
            s
            =
            t
            u
            ;
            s
            ∈
            L
            ;
            t
            ,
            u
            ∈
            Alph
            ⁡
            (
            L
            
              )
              
                ∗
              
            
          
          }
        
      
    
    {\displaystyle \operatorname {Pref} (L)=\bigcup _{s\in L}\operatorname {Pref} _{L}(s)=\left\{t\ \vert \ s=tu;s\in L;t,u\in \operatorname {Alph} (L)^{*}\right\}}
  
Example:
  
    
      
        L
        =
        
          {
          
            a
            b
            c
          
          }
        
        
          
             then 
          
        
        Pref
        ⁡
        (
        L
        )
        =
        
          {
          
            ε
            ,
            a
            ,
            a
            b
            ,
            a
            b
            c
          
          }
        
      
    
    {\displaystyle L=\left\{abc\right\}{\mbox{ then }}\operatorname {Pref} (L)=\left\{\varepsilon ,a,ab,abc\right\}}
  
A language is called prefix closed if 
  
    
      
        Pref
        ⁡
        (
        L
        )
        =
        L
      
    
    {\displaystyle \operatorname {Pref} (L)=L}
  .
The prefix closure operator is idempotent:

  
    
      
        Pref
        ⁡
        (
        Pref
        ⁡
        (
        L
        )
        )
        =
        Pref
        ⁡
        (
        L
        )
      
    
    {\displaystyle \operatorname {Pref} (\operatorname {Pref} (L))=\operatorname {Pref} (L)}
  
The prefix relation is a binary relation 
  
    
      
        ⊑
      
    
    {\displaystyle \sqsubseteq }
   such that 
  
    
      
        s
        ⊑
        t
      
    
    {\displaystyle s\sqsubseteq t}
   if and only if 
  
    
      
        s
        ∈
        
          Pref
          
            L
          
        
        ⁡
        (
        t
        )
      
    
    {\displaystyle s\in \operatorname {Pref} _{L}(t)}
  . This relation is a particular example of a prefix order.


== See also ==
Comparison of programming languages (string functions)
Levi's lemma
String (computer science) — definition and implementation of more basic operations on strings


== Notes ==


== References ==
Hopcroft, John E.; Ullman, Jeffrey D. (1979). Introduction to Automata Theory, Languages and Computation. Reading, Massachusetts: Addison-Wesley Publishing. ISBN 0-201-02988-X. Zbl 0426.68001.  (See chapter 3.)"
70,3SUM,506330,15463,"In computational complexity theory, the 3SUM problem asks if a given set of 
  
    
      
        n
      
    
    {\displaystyle n}
   real numbers contains three elements that sum to zero. A generalized version, k-SUM, asks the same question on k numbers. 3SUM can be easily solved in 
  
    
      
        O
        (
        
          n
          
            2
          
        
        )
      
    
    {\displaystyle O(n^{2})}
   time, and matching 
  
    
      
        Ω
        (
        
          n
          
            ⌈
            k
            
              /
            
            2
            ⌉
          
        
        )
      
    
    {\displaystyle \Omega (n^{\lceil k/2\rceil })}
   lower bounds are known in some specialized models of computation (Erickson 1999).
It was widely conjectured that any deterministic algorithm for the 3SUM requires 
  
    
      
        Ω
        (
        
          n
          
            2
          
        
        )
      
    
    {\displaystyle \Omega (n^{2})}
   time. In 2014, the original 3SUM conjecture was refuted by Allan Grønlund and Seth Pettie who gave a deterministic algorithm that solves 3SUM in 
  
    
      
        O
        (
        
          n
          
            2
          
        
        
          /
        
        (
        
          log
          ⁡
          n
        
        
          /
        
        
          log
          ⁡
          log
          ⁡
          n
        
        
          )
          
            2
            
              /
            
            3
          
        
        )
      
    
    {\displaystyle O(n^{2}/({\log n}/{\log \log n})^{2/3})}
   time . Additionally, Grønlund and Pettie showed that the 4-linear decision tree complexity of 3SUM is 
  
    
      
        O
        (
        
          n
          
            3
            
              /
            
            2
          
        
        
          
            log
            ⁡
            n
          
        
        )
      
    
    {\displaystyle O(n^{3/2}{\sqrt {\log n}})}
  . These bounds were subsequently improved; the current best known algorithm for 3SUM runs in 
  
    
      
        O
        (
        
          n
          
            2
          
        
        
          /
        
        (
        
          log
          ⁡
          n
        
        
          /
        
        
          log
          ⁡
          log
          ⁡
          n
        
        )
        )
      
    
    {\displaystyle O(n^{2}/({\log n}/{\log \log n}))}
   time,  and the randomized 4-linear decision tree complexity of 3SUM is 
  
    
      
        O
        (
        
          n
          
            3
            
              /
            
            2
          
        
        )
      
    
    {\displaystyle O(n^{3/2})}
   . It is still conjectured that 3SUM is unsolvable in 
  
    
      
        O
        (
        
          n
          
            2
            −
            Ω
            (
            1
            )
          
        
        )
      
    
    {\displaystyle O(n^{2-\Omega (1)})}
   expected time.
When the elements are integers in the range 
  
    
      
        [
        −
        N
        ,
        …
        ,
        N
        ]
      
    
    {\displaystyle [-N,\dots ,N]}
  , 3SUM can be solved in 
  
    
      
        O
        (
        n
        +
        N
        log
        ⁡
        N
        )
      
    
    {\displaystyle O(n+N\log N)}
   time by representing the input set 
  
    
      
        S
      
    
    {\displaystyle S}
   as a bit vector, computing the set 
  
    
      
        S
        +
        S
      
    
    {\displaystyle S+S}
   of all pairwise sums as a discrete convolution using the Fast Fourier transform, and finally comparing this set to 
  
    
      
        −
        S
      
    
    {\displaystyle -S}
  .


== Quadratic algorithm ==
Suppose the input array is 
  
    
      
        S
        [
        0..
        n
        −
        1
        ]
      
    
    {\displaystyle S[0..n-1]}
  . In integer (word RAM) models of computing, 3SUM can be solved in 
  
    
      
        O
        (
        
          n
          
            2
          
        
        )
      
    
    {\displaystyle O(n^{2})}
   time on average by inserting each number 
  
    
      
        S
        [
        i
        ]
      
    
    {\displaystyle S[i]}
   into a hash table, and then for each index 
  
    
      
        i
      
    
    {\displaystyle i}
   and 
  
    
      
        j
      
    
    {\displaystyle j}
  , checking whether the hash table contains the integer 
  
    
      
        −
        (
        S
        [
        i
        ]
        +
        S
        [
        j
        ]
        )
      
    
    {\displaystyle -(S[i]+S[j])}
  .
It is also possible to solve the problem in the same time in a comparison-based model of computing or real RAM, for which hashing is not allowed. The algorithm below first sorts the input array and then tests all possible pairs in a careful order that avoids the need to binary search for the pairs in the sorted list, achieving worst-case 
  
    
      
        O
        (
        
          n
          
            2
          
        
        )
      
    
    {\displaystyle O(n^{2})}
   time, as follows.

 sort(S);
 for i=0 to n-2 do
    a = S[i];
    start = i+1;
    end = n-1;
    while (start < end) do
       b = S[start]
       c = S[end];
       if (a+b+c == 0) then
          output a, b, c;
          // Continue search for all triplet combinations summing to zero.
          if (b == S[start + 1]) then
             start = start + 1;
          else
             end = end - 1;
       else if (a+b+c > 0) then
          end = end - 1;
       else
          start = start + 1;
       end
    end
 end

The following example shows this algorithm's execution on a small sorted array. Current values of a are shown in green, values of b and c are shown in blue.

 -25 -10 -7 -3 2 4 8 10  (a+b+c==-25)
 -25 -10 -7 -3 2 4 8 10  (a+b+c==-22)
 . . .
 -25 -10 -7 -3 2 4 8 10  (a+b+c==-7)
 -25 -10 -7 -3 2 4 8 10  (a+b+c==-7)
 -25 -10 -7 -3 2 4 8 10  (a+b+c==-3)
 -25 -10 -7 -3 2 4 8 10  (a+b+c==2)
 -25 -10 -7 -3 2 4 8 10  (a+b+c==0)

The correctness of the algorithm can be seen as follows. Suppose we have a solution a + b + c = 0. Since the pointers only move in one direction, we can run the algorithm until the leftmost pointer points to a. Run the algorithm until either one of the remaining pointers points to b or c, whichever occurs first. Then the algorithm will run until the last pointer points to the remaining term, giving the affirmative solution.


== Variants ==


=== Non-zero sum ===
Instead of looking for numbers whose sum is 0, it is possible to look for numbers whose sum is any constant C in the following way:
Subtract C/3 from all elements of the input array.
In the modified array, find 3 elements whose sum is 0.
For e.g., if A=[1,2,3,4] and if you are asked to find 3sum for C=4, then subtract all the elements of A by 4/3 and solve it in the usual 3sum way, i.e., (a-C/3) + (b-C/3) + (c-C/3) = 0


=== 3 different arrays ===
Instead of searching for the 3 numbers in a single array, we can search for them in 3 different arrays. I.e., given three arrays X, Y and Z, find three numbers a∈X, b∈Y, c∈Z, such that 
  
    
      
        a
        +
        b
        +
        c
        =
        0
      
    
    {\displaystyle a+b+c=0}
  . Call the 1-array variant 3SUM×1 and the 3-array variant 3SUM×3.
Given a solver for 3SUM×1, the 3SUM×3 problem can be solved in the following way (assuming all elements are integers):
For every element in X, Y and Z, set: 
  
    
      
        X
        [
        i
        ]
        ←
        X
        [
        i
        ]
        ∗
        10
        +
        1
      
    
    {\displaystyle X[i]\gets X[i]*10+1}
  , 
  
    
      
        Y
        [
        i
        ]
        ←
        Y
        [
        i
        ]
        ∗
        10
        +
        2
      
    
    {\displaystyle Y[i]\gets Y[i]*10+2}
  , 
  
    
      
        Z
        [
        i
        ]
        ←
        Z
        [
        i
        ]
        ∗
        10
        −
        3
      
    
    {\displaystyle Z[i]\gets Z[i]*10-3}
  .
Let S be a concatenation of the arrays X, Y and Z.
Use the 3SUM×1 oracle to find three elements 
  
    
      
        
          a
          ′
        
        ∈
        S
        ,
         
        
          b
          ′
        
        ∈
        S
        ,
         
        
          c
          ′
        
        ∈
        S
      
    
    {\displaystyle a'\in S,\ b'\in S,\ c'\in S}
   such that 
  
    
      
        
          a
          ′
        
        +
        
          b
          ′
        
        +
        
          c
          ′
        
        =
        0
      
    
    {\displaystyle a'+b'+c'=0}
  .
Return 
  
    
      
        a
        ←
        (
        
          a
          ′
        
        −
        1
        )
        
          /
        
        10
        ,
         
        b
        ←
        (
        
          b
          ′
        
        −
        2
        )
        
          /
        
        10
        ,
         
        c
        ←
        (
        
          c
          ′
        
        +
        3
        )
        
          /
        
        10
      
    
    {\displaystyle a\gets (a'-1)/10,\ b\gets (b'-2)/10,\ c\gets (c'+3)/10}
  .
By the way we transformed the arrays, it is guaranteed that a∈X, b∈Y, c∈Z.


=== Convolution sum ===
Instead of looking for arbitrary elements of the array such that:

  
    
      
        S
        [
        k
        ]
        =
        S
        [
        i
        ]
        +
        S
        [
        j
        ]
      
    
    {\displaystyle S[k]=S[i]+S[j]}
  
the convolution 3sum problem (Conv3SUM) looks for elements in specific locations:

  
    
      
        S
        [
        i
        +
        j
        ]
        =
        S
        [
        i
        ]
        +
        S
        [
        j
        ]
      
    
    {\displaystyle S[i+j]=S[i]+S[j]}
  


==== Reduction from Conv3SUM to 3SUM ====
Given a solver for 3SUM, the Conv3SUM problem can be solved in the following way.
Define a new array T, such that for every index i: 
  
    
      
        T
        [
        i
        ]
        =
        2
        n
        S
        [
        i
        ]
        +
        i
      
    
    {\displaystyle T[i]=2nS[i]+i}
   (where n is the number of elements in the array, and the indices run from 0 to n-1).
Solve 3SUM on the array T.
Correctness proof:
If in the original array there is a triple with 
  
    
      
        S
        [
        i
        +
        j
        ]
        =
        S
        [
        i
        ]
        +
        S
        [
        j
        ]
      
    
    {\displaystyle S[i+j]=S[i]+S[j]}
  , then 
  
    
      
        T
        [
        i
        +
        j
        ]
        =
        2
        n
        S
        [
        i
        +
        j
        ]
        +
        i
        +
        j
        =
        (
        2
        n
        S
        [
        i
        ]
        +
        i
        )
        +
        (
        2
        n
        S
        [
        j
        ]
        +
        j
        )
        =
        T
        [
        i
        ]
        +
        T
        [
        j
        ]
      
    
    {\displaystyle T[i+j]=2nS[i+j]+i+j=(2nS[i]+i)+(2nS[j]+j)=T[i]+T[j]}
  , so this solution will be found by 3SUM on T.
Conversely, if in the new array there is a triple with 
  
    
      
        T
        [
        k
        ]
        =
        T
        [
        i
        ]
        +
        T
        [
        j
        ]
      
    
    {\displaystyle T[k]=T[i]+T[j]}
  , then 
  
    
      
        2
        n
        S
        [
        k
        ]
        +
        k
        =
        2
        n
        (
        S
        [
        i
        ]
        +
        S
        [
        j
        ]
        )
        +
        (
        i
        +
        j
        )
      
    
    {\displaystyle 2nS[k]+k=2n(S[i]+S[j])+(i+j)}
  . Because 
  
    
      
        i
        +
        j
        <
        2
        n
      
    
    {\displaystyle i+j<2n}
  , necessarily 
  
    
      
        S
        [
        k
        ]
        =
        S
        [
        i
        ]
        +
        S
        [
        j
        ]
      
    
    {\displaystyle S[k]=S[i]+S[j]}
   and 
  
    
      
        k
        =
        i
        +
        j
      
    
    {\displaystyle k=i+j}
  , so this is a valid solution for Conv3SUM on S.


==== Reduction from 3SUM to Conv3SUM ====
Given a solver for Conv3SUM, the 3SUM problem can be solved in the following way.
The reduction uses a hash function. As a first approximation, assume that we have a linear hash function, i.e. a function h such that:

  
    
      
        h
        (
        x
        +
        y
        )
        =
        h
        (
        x
        )
        +
        h
        (
        y
        )
      
    
    {\displaystyle h(x+y)=h(x)+h(y)}
  
Suppose that all elements are integers in the range: 0...N-1, and that the function h maps each element to an element in the smaller range of indices: 0...n-1. Create a new array T and send each element of S to its hash value in T, i.e., for every x in S:

  
    
      
        T
        [
        h
        (
        x
        )
        ]
        =
        x
      
    
    {\displaystyle T[h(x)]=x}
  
Initially, suppose that the mappings are unique (i.e. each cell in T accepts only a single element from S). Solve Conv3SUM on T. Now:
If there is a solution for 3SUM: 
  
    
      
        z
        =
        x
        +
        y
      
    
    {\displaystyle z=x+y}
  , then: 
  
    
      
        T
        [
        h
        (
        z
        )
        ]
        =
        T
        [
        h
        (
        x
        )
        ]
        +
        T
        [
        h
        (
        y
        )
        ]
      
    
    {\displaystyle T[h(z)]=T[h(x)]+T[h(y)]}
   and 
  
    
      
        h
        (
        z
        )
        =
        h
        (
        x
        )
        +
        h
        (
        y
        )
      
    
    {\displaystyle h(z)=h(x)+h(y)}
  , so this solution will be found by the Conv3SUM solver on T.
Conversely, if a Conv3SUM is found on T, then obviously it corresponds to a 3SUM solution on S since T is just a permutation of S.
This idealized solution doesn't work, because any hash function might map several distinct elements of S to the same cell of T. The trick is to create an array T* by selecting a single random element from each cell of T, and run Conv3SUM on T*. If a solution is found, then it is a correct solution for 3SUM on S. If no solution is found, then create a different random T* and try again. Suppose there are at most R elements in each cell of T. Then the probability of finding a solution (if a solution exists) is the probability that the random selection will select the correct element from each cell, which is 
  
    
      
        (
        1
        
          /
        
        R
        
          )
          
            3
          
        
      
    
    {\displaystyle (1/R)^{3}}
  . By running Conv3SUM 
  
    
      
        
          R
          
            3
          
        
      
    
    {\displaystyle R^{3}}
   times, the solution will be found with a high probability.
Unfortunately, we do not have linear perfect hashing, so we have to use an almost linear hash function, i.e. a function h such that:

  
    
      
        h
        (
        x
        +
        y
        )
        =
        h
        (
        x
        )
        +
        h
        (
        y
        )
      
    
    {\displaystyle h(x+y)=h(x)+h(y)}
   or

  
    
      
        h
        (
        x
        +
        y
        )
        =
        h
        (
        x
        )
        +
        h
        (
        y
        )
        +
        1
      
    
    {\displaystyle h(x+y)=h(x)+h(y)+1}
  
This requires to duplicate the elements of S when copying them into T, i.e., put every element 
  
    
      
        x
        ∈
        S
      
    
    {\displaystyle x\in S}
   both in 
  
    
      
        T
        [
        h
        (
        x
        )
        ]
      
    
    {\displaystyle T[h(x)]}
   (as before) and in 
  
    
      
        T
        [
        h
        (
        x
        )
        ]
        −
        1
      
    
    {\displaystyle T[h(x)]-1}
  . So each cell will have 2R elements, and we will have to run Conv3SUM 
  
    
      
        (
        2
        R
        
          )
          
            3
          
        
      
    
    {\displaystyle (2R)^{3}}
   times.


== 3SUM-hardness ==
A problem is called 3SUM-hard if solving it in subquadratic time implies a subquadratic-time algorithm for 3SUM. The concept of 3SUM-hardness was introduced by Gajentaan & Overmars (1995). They proved that a large class of problems in computational geometry are 3SUM-hard, including the following ones. (The authors acknowledge that many of these problems are contributed by other researchers.)
Given a set of lines in the plane, are there three that meet in a point?
Given a set of non-intersecting axis-parallel line segments, is there a line that separates them into two non-empty subsets?
Given a set of infinite strips in the plane, do they fully cover a given rectangle?
Given a set of triangles in the plane, compute their measure.
Given a set of triangles in the plane, does their union have a hole?
A number of visibility and motion planning problems, e.g.,
Given a set of horizontal triangles in space, can a particular triangle be seen from a particular point?
Given a set of non-intersecting axis-parallel line segment obstacles in the plane, can a given rod be moved by translations and rotations between a start and finish positions without colliding with the obstacles?

By now there are a multitude of other problems that fall into this category. An example is the decision version of X + Y sorting: given sets of numbers X and Y of n elements each, are there n² distinct x + y for x ∈ X, y ∈ Y?


== See also ==
Subset sum problem


== Notes ==


== References ==
Grønlund, A.; Pettie, S. (2014), Threesomes, Degenerates, and Love Triangles, p. 621, doi:10.1109/FOCS.2014.72, ISBN 978-1-4799-6517-5 
Freund, Ari (2017), ""Improved Subquadratic 3SUM"", Algorithmica, 44 (2): 440–458, doi:10.1007/s00453-015-0079-6 .
Gold, Omer; Sharir, Micha (2015), ""Improved Bounds for 3SUM, $k$-SUM, and Linear Degeneracy"", CoRR, abs/1512.05279 
Baran, Ilya; Demaine, Erik D.; Pătraşcu, Mihai (2008), ""Subquadratic algorithms for 3SUM"", Algorithmica, 50 (4): 584–596, doi:10.1007/s00453-007-9036-3 .
Demaine, Erik D.; Mitchell, Joseph S. B.; O'Rourke, Joseph (July 2005), ""Problem 11: 3SUM Hard Problems"", The Open Problems Project .
Erickson, Jeff (1999), ""Lower bounds for linear satisfiability problems"", Chicago Journal of Theoretical Computer Science, MIT Press, 1999 .
Gajentaan, Anka; Overmars, Mark H. (1995), ""On a class of O(n2) problems in computational geometry"", Computational Geometry: Theory and Applications, 5 (3): 165–185, doi:10.1016/0925-7721(95)00022-2 .
King, James (2004), A survey of 3SUM-hard problems (PDF) ."
71,Art gallery problem,1448859,15390,"The art gallery problem or museum problem is a well-studied visibility problem in computational geometry. It originates from a real-world problem of guarding an art gallery with the minimum number of guards who together can observe the whole gallery. In the geometric version of the problem, the layout of the art gallery is represented by a simple polygon and each guard is represented by a point in the polygon. A set 
  
    
      
        S
      
    
    {\displaystyle S}
   of points is said to guard a polygon if, for every point 
  
    
      
        p
      
    
    {\displaystyle p}
   in the polygon, there is some 
  
    
      
        q
        ∈
        S
      
    
    {\displaystyle q\in S}
   such that the line segment between 
  
    
      
        p
      
    
    {\displaystyle p}
   and 
  
    
      
        q
      
    
    {\displaystyle q}
   does not leave the polygon.


== Two dimensions ==

There are numerous variations of the original problem that are also referred to as the art gallery problem. In some versions guards are restricted to the perimeter, or even to the vertices of the polygon. Some versions require only the perimeter or a subset of the perimeter to be guarded.
Solving the version in which guards must be placed on vertices and only vertices need to be guarded is equivalent to solving the dominating set problem on the visibility graph of the polygon.


=== Chvátal's art gallery theorem ===
Chvátal's art gallery theorem, named after Václav Chvátal, gives an upper bound on the minimal number of guards. It states that 
  
    
      
        
          ⌊
          
            n
            
              /
            
            3
          
          ⌋
        
      
    
    {\displaystyle \left\lfloor n/3\right\rfloor }
   guards are always sufficient and sometimes necessary to guard a simple polygon with 
  
    
      
        n
      
    
    {\displaystyle n}
   vertices.
The question about how many vertices/watchmen/guards were needed was posed to Chvátal by Victor Klee in 1973. Chvátal proved it shortly thereafter. Chvátal's proof was later simplified by Steve Fisk, via a 3-coloring argument.


=== Fisk's short proof ===

Steve Fisk's proof  is so short and elegant that it was chosen for inclusion in Proofs from THE BOOK. The proof goes as follows:
First, the polygon is triangulated (without adding extra vertices). It is known that the vertices of the resulting triangulation graph may be 3-colored. Clearly, under a 3-coloring, every triangle must have all three colors. The vertices with any one color form a valid guard set, because every triangle of the polygon is guarded by its vertex with that color. Since the three colors partition the n vertices of the polygon, the color with the fewest vertices defines a valid guard set with at most 
  
    
      
        ⌊
        n
        
          /
        
        3
        ⌋
      
    
    {\displaystyle \lfloor n/3\rfloor }
   guards.


=== Generalizations ===
Chvátal's upper bound remains valid if the restriction to guards at corners is loosened to guards at any point not exterior to the polygon.
There are a number of other generalizations and specializations of the original art-gallery theorem. For instance, for orthogonal polygons, those whose edges/walls meet at right angles, only 
  
    
      
        ⌊
        n
        
          /
        
        4
        ⌋
      
    
    {\displaystyle \lfloor n/4\rfloor }
   guards are needed. There are at least three distinct proofs of this result, none of them simple: by Kahn, Klawe, and Kleitman; by Lubiw; and by Sack and Toussaint.
A related problem asks for the number of guards to cover the exterior of an arbitrary polygon (the ""Fortress Problem""): 
  
    
      
        ⌈
        n
        
          /
        
        2
        ⌉
      
    
    {\displaystyle \lceil n/2\rceil }
   are sometimes necessary and always sufficient. In other words, the infinite exterior is more challenging to cover than the finite interior.


=== Computational complexity ===
In decision problem versions of the art gallery problem, one is given as input both a polygon and a number k, and must determine whether the polygon can be guarded with k or fewer guards. This problem is 
  
    
      
        ∃
        
          R
        
      
    
    {\displaystyle \exists \mathbb {R} }
  -complete and all of its standard variations (such as restricting the guard locations to vertices or edges of the polygon) are NP-hard. Regarding approximation algorithms for the minimum number of guards, Eidenbenz, Stamm & Widmayer (2001) proved the problem to be APX-hard, implying that it is unlikely that any approximation ratio better than some fixed constant can be achieved by a polynomial time approximation algorithm. However, a constant approximation ratio is not known. Instead, a logarithmic approximation may be achieved for the minimum number of vertex guards by reducing the problem to a set cover problem. As Valtr (1998) showed, the set system derived from an art gallery problem has bounded VC dimension, allowing the application of set cover algorithms based on ε-nets whose approximation ratio is the logarithm of the optimal number of guards rather than of the number of polygon vertices. For unrestricted guards, the infinite number of potential guard positions makes the problem even more difficult.  However by restricting the guards to lie on a fine grid, a more complicated logarithmic approximation algorithm can be derived, under some mild extra assumptions. 
However, efficient algorithms are known for finding a set of at most 
  
    
      
        
          ⌊
          
            n
            
              /
            
            3
          
          ⌋
        
      
    
    {\displaystyle \left\lfloor n/3\right\rfloor }
   vertex guards, matching Chvátal's upper bound. David Avis and Godfried Toussaint (1981) proved that a placement for these guards may be computed in O(n log n) time in the worst case, via a divide and conquer algorithm. Kooshesh & Moret (1992) gave a linear time algorithm by using Fisk's short proof and Bernard Chazelle's linear time plane triangulation algorithm.
An exact algorithm was proposed by Couto, de Rezende & de Souza (2011) for vertex guards. The authors conducted extensive computational experiments with several classes of polygons showing that optimal solutions can be found in relatively small computation times even for instances associated to thousands of vertices. The input data and the optimal solutions for these instances are available for download.


== Three dimensions ==

If a museum is represented in three dimensions as a polyhedron, then putting a guard at each vertex will not ensure that all of the museum is under observation. Although all of the surface of the polyhedron would be surveyed, for some polyhedra there are points in the interior which might not be under surveillance.


== See also ==
Polygon covering#Covering a rectilinear polygon with star polygons


== Notes ==


== References ==
Aggarwal, A. (1984), The art gallery theorem: Its variations, applications, and algorithmic aspects, Ph.D. thesis, Johns Hopkins University .
Avis, D.; Toussaint, G. T. (1981), ""An efficient algorithm for decomposing a polygon into star-shaped polygons"" (PDF), Pattern Recognition, 13 (6): 395–398, doi:10.1016/0031-3203(81)90002-9 .
Brönnimann, H.; Goodrich, M. T. (1995), ""Almost optimal set covers in finite VC-dimension"", Discrete and Computational Geometry, 14 (1): 463–479, doi:10.1007/BF02570718 .
Chvátal, V. (1975), ""A combinatorial theorem in plane geometry"", Journal of Combinatorial Theory, Series B, 18: 39–41, doi:10.1016/0095-8956(75)90061-1 .
Couto, M.; de Rezende, P.; de Souza, C. (2011), ""An exact algorithm for minimizing vertex guards on art galleries"", International Transactions in Operational Research: no–no, doi:10.1111/j.1475-3995.2011.00804.x .
Couto, M.; de Rezende, P.; de Souza, C. (2011), Benchmark instances for the art gallery problem with vertex guards .
Deshpande, Ajay; Kim, Taejung; Demaine, Erik D.; Sarma, Sanjay E. (2007), ""A Pseudopolynomial Time O(logn)-Approximation Algorithm for Art Gallery Problems"", Proc. Worksh. Algorithms and Data Structures, Lecture Notes in Computer Science, 4619, Springer-Verlag, pp. 163–174, doi:10.1007/978-3-540-73951-7_15, ISBN 978-3-540-73948-7 .
Eidenbenz, S.; Stamm, C.; Widmayer, P. (2001), ""Inapproximability results for guarding polygons and terrains"" (PDF), Algorithmica, 31 (1): 79–113, doi:10.1007/s00453-001-0040-8, archived from the original (PDF) on 2003-06-24 .
Fisk, S. (1978), ""A short proof of Chvátal's watchman theorem"", Journal of Combinatorial Theory, Series B, 24 (3): 374, doi:10.1016/0095-8956(78)90059-X .
Ghosh, S. K. (1987), ""Approximation algorithms for art gallery problems"", Proc. Canadian Information Processing Society Congress, pp. 429–434 .
Kahn, J.; Klawe, M.; Kleitman, D. (1983), ""Traditional galleries require fewer watchmen"", SIAM J. Alg. Disc. Meth., 4 (2): 194–206, doi:10.1137/0604020 .
Kooshesh, A. A.; Moret, B. M. E. (1992), ""Three-coloring the vertices of a triangulated simple polygon"", Pattern Recognition, 25 (4): 443, doi:10.1016/0031-3203(92)90093-X .
Lee, D. T.; Lin, A. K. (1986), ""Computational complexity of art gallery problems"", IEEE Transactions on Information Theory, 32 (2): 276–282, doi:10.1109/TIT.1986.1057165 .
Lubiw, A. (1985), ""Decomposing polygonal regions into convex quadrilaterals"", Proc. 1st ACM Symposium on Computational Geometry, pp. 97–106, doi:10.1145/323233.323247, ISBN 0-89791-163-6 .
O'Rourke, Joseph (1987), Art Gallery Theorems and Algorithms, Oxford University Press, ISBN 0-19-503965-3 .
Sack, J. R.; Toussaint, G. T. (1988), ""Guard placement in rectilinear polygons"", in Toussaint, G. T., Computational Morphology, North-Holland, pp. 153–176 .
Shermer, Thomas (1992), ""Recent Results in Art Galleries"" (PDF), Proceedings of the IEEE, 80 (9): 1384–1399, doi:10.1109/5.163407 .
Valtr, P. (1998), ""Guarding galleries where no point sees a small area"", Israel J. Math., 104 (1): 1–16, doi:10.1007/BF02897056 ."
72,Computational aeroacoustics,4248526,15347,"Computational aeroacoustics is a branch of aeroacoustics that aims to analyze the generation of noise by turbulent flows through numerical methods.


== History ==
The origin of Computational Aeroacoustics can only very likely be dated back to the middle of the 1980s, with a publication of Hardin and Lamkin who claimed, that

""[...] the field of computational fluid mechanics has been advancing rapidly in the past few years and now offers the hope that ""computational aeroacoustics,"" where noise is computed directly from a first principles determination of continuous velocity and vorticity fields, might be possible, [...]""

Later in a publication 1986 the same authors introduced the abbreviation CAA. The term was initially used for a low Mach number approach (Expansion of the acoustic perturbation field about an incompressible flow) as it is described under EIF. Later in the beginning 1990s the growing CAA community picked up the term and extensively used it for any kind of numerical method describing the noise radiation from an aeroacoustic source or the propagation of sound waves in an inhomogeneous flow field. Such numerical methods can be far field integration methods (e.g. FW-H) as well as direct numerical methods optimized for the solutions (e.g.) of a mathematical model describing the aerodynamic noise generation and/or propagation. With the rapid development of the computational resources this field has undergone spectacular progress during the last three decades.


== Methods ==


=== Direct numerical simulation (DNS) Approach to CAA ===
The compressible Navier-Stokes equation describes both the flow field, and the aerodynamically generated acoustic field. Thus both may be solved for directly. This requires very high numerical resolution due to the large differences in the length scale present between the acoustic variables and the flow variables. It is computationally very demanding and unsuitable for any commercial use.


=== Hybrid Approach ===
In this approach the computational domain is split into different regions, such that the governing acoustic or flow field can be solved with different equations and numerical techniques. This would involve using two different numerical solvers, first a dedicated Computational fluid dynamics (CFD) tool and secondly an acoustic solver. The flow field is then used to calculate the acoustical sources. Both steady state (RANS, SNGR (Stochastic Noise Generation and Radiation), ...) and transient (DNS, LES, DES, URANS, ...) fluid field solutions can be used. These acoustical sources are provided to the second solver which calculates the acoustical propagation. Acoustic propagation can be calculated using one of the following methods :
Integral Methods
Lighthill's analogy
Kirchhoff integral
FW-H

LEE
Pseudospectral
EIF
APE


==== Integral methods ====
There are multiple methods, which are based on a known solution of the acoustic wave equation to compute the acoustic far field of a sound source. Because a general solution for wave propagation in the free space can be written as an integral over all sources, these solutions are summarized as integral methods. The acoustic sources have to be known from some different source (e.g. a Finite Element simulation of a moving mechanical system or a fluid dynamic CFD simulation of the sources in a moving medium). The integral is taken over all sources at the retarded time (source time), which is the time at that the source is sent out the signal, which arrives now at a given observer position. Common to all integral methods is, that they cannot account for changes in the speed of sound or the average flow speed between source and observer position as they use a theoretical solution of the wave equation. When applying Lighthill's theory  to the Navier Stokes equations of Fluid mechanics, one obtains volumetric sources, whereas the other two analogies provide the far field information based on a surface integral. Acoustic analogies can be very efficient and fast, as the known solution of the wave equation is used. One far away observer takes as long as one very close observer. Common for the application of all analogies is the integration over a large number of contributions, which can lead to additional numerical problems (addition/subtraction of many large numbers with result close to zero.) Furthermore, when applying an integral method, usually the source domain is limited somehow. While in theory the sources outside have to be zero, the application can not always fulfill this condition. Especially in connection with CFD simulations, this leads to large cut-off errors. By damping the source gradually to zero at the exit of the domain or adding some additional terms to correct this end-effect, these cut-off errors can be minimized.


===== Lighthill's analogy =====
Also called 'Acoustic Analogy'. To obtain Lighthill's aeroacoustic analogy the governing Navier-Stokes equations are rearranged. The left hand side is a wave operator, which is applied to the density perturbation or pressure perturbation respectively. The right hand side is identified as the acoustic sources in a fluid flow, then. As Lighthill's analogy follows directly from the Navier-Stokes equations without simplification, all sources are present. Some of the sources are then identified as turbulent or laminar noise. The far-field sound pressure is then given in terms of a volume integral over the domain containing the sound source. The source term always includes physical sources and such sources, which describe the propagation in an inhomogeneous medium.
The wave operator of Lighthill's analogy is limited to constant flow conditions outside the source zone. No variation of density, speed of sound and Mach number is allowed. Different mean flow conditions are identified as strong sources with opposite sign by the analogy, once an acoustic wave passes it. Part of the acoustic wave is removed by one source and a new wave is radiated to fix the different wave speed. This often leads very large volumes with strong sources. Several modifications to Lighthill's original theory have been proposed to account for the sound-flow interaction or other effects. To improve Lighthill's analogy different quantities inside the wave operator as well as different wave operators are considered by following analogies. All of them obtain modified source terms, which sometimes allow a more clear sight on the ""real"" sources. The acoustic analogies of Lilley, Pierce, Howe and Möhring are only some examples for aeroacoustic analogies based on Lighthill's ideas. All acoustic analogies require a volume integration over a source term.
The major difficulty with the acoustic analogy, however, is that the sound source is not compact in supersonic flow. Errors could be encountered in calculating the sound field, unless the computational domain could be extended in the downstream direction beyond the location where the sound source has completely decayed. Furthermore, an accurate account of the retarded time-effect requires keeping a long record of the time-history of the converged solutions of the sound source, which again represents a storage problem. For realistic problems, the required storage can reach the order of 1 terabyte of data.


===== Kirchhoff integral =====
Kirchhoff and Helmholtz showed, that the radiation of sound from a limited source region can be described by enclosing this source region by a control surface - the so-called Kichhoff surface. Then the sound field inside or outside the surface, where no sources are allowed and the wave operator on the left hand side applies, can be produced as a superposition of monopoles and dipoles on the surface. The theory follows directly from the wave equation. The source strength of monopoles and dipoles on the surface can be calculated if the normal velocity (for monopoles) and the pressure (for dipoles) on the surface are known respectively. A modification of the method allows even to calculate the pressure on the surface based on the normal velocity only. The normal velocity could be given by a FE-simulation of a moving structure for instance. However, the modification to avid the acoustic pressure on the surface to be known leads to problems, when considering an enclosed volume at its resonant frequencies, which is a major issue of the implementations of their method. The Kirchhoff integral method finds for instance application in Boundary element methods (BEM). A non-zero flow velocity is accounted by considering a moving frame of reference with the outer flow speed, in which the acoustic wave propagation takes place. Repetitive applications of the method can account for obstacles. First the sound field on the surface of the obstacle is calculated and then the obstacle is introduced by adding sources on its surface to cancel the normal velocity on the surface of the obstacle. Variations of the average flow field (speed of sound, density and velocity) can be taken into account by a similar method (e.g. dual reciprocity BEM).


===== FW-H =====
The integration method of Ffowcs Williams and Hawkings is based on Lighthill's acoustic analogy. However, by some mathematical modifications under the assumption of a limited source region, which is enclosed by a control surface (FW-H surface), the volume integral is avoided. Surface integrals over monopole and dipole sources remain. Different from the Kirchhoff method, these sources follow directly from the Navier-Stokes equations through Lighthill's analogy. Sources outside the FW-H surface can be accounted by an additional volume integral over quadrupole sources following from the Lighthill Tensor. However, when considering the same assumptions as Kirchhoffs linear theory, the FW-H method equals the Kirchhoff method.


==== Linearized Euler Equations ====
Considering small disturbances superimposed on a uniform mean flow of density 
  
    
      
        
          ρ
          
            0
          
        
      
    
    {\displaystyle \rho _{0}}
  , pressure 
  
    
      
        
          p
          
            0
          
        
      
    
    {\displaystyle p_{0}}
   and velocity on x-axis 
  
    
      
        
          u
          
            0
          
        
      
    
    {\displaystyle u_{0}}
  , the Euler equations for a two dimensional model is presented as:

  
    
      
        
          
            
              ∂
              
                U
              
            
            
              ∂
              t
            
          
        
        +
        
          
            
              ∂
              
                F
              
            
            
              ∂
              x
            
          
        
        +
        
          
            
              ∂
              
                G
              
            
            
              ∂
              y
            
          
        
        =
        
          S
        
      
    
    {\displaystyle {\frac {\partial \mathbf {U} }{\partial t}}+{\frac {\partial \mathbf {F} }{\partial x}}+{\frac {\partial \mathbf {G} }{\partial y}}=\mathbf {S} }
  ,
where

  
    
      
        
          U
        
        =
        
          
            [
            
              
                
                  ρ
                
              
              
                
                  u
                
              
              
                
                  v
                
              
              
                
                  p
                
              
            
            ]
          
        
         
        ,
         
        
          F
        
        =
        
          
            [
            
              
                
                  
                    ρ
                    
                      0
                    
                  
                  u
                  +
                  ρ
                  
                    u
                    
                      0
                    
                  
                
              
              
                
                  
                    u
                    
                      0
                    
                  
                  u
                  +
                  p
                  
                    /
                  
                  
                    ρ
                    
                      0
                    
                  
                
              
              
                
                  
                    u
                    
                      0
                    
                  
                  v
                
              
              
                
                  
                    u
                    
                      0
                    
                  
                  p
                  +
                  γ
                  
                    p
                    
                      0
                    
                  
                  u
                
              
            
            ]
          
        
         
        ,
         
        
          G
        
        =
        
          
            [
            
              
                
                  
                    ρ
                    
                      0
                    
                  
                  v
                
              
              
                
                  0
                
              
              
                
                  p
                  
                    /
                  
                  
                    ρ
                    
                      0
                    
                  
                
              
              
                
                  γ
                  
                    p
                    
                      0
                    
                  
                  v
                
              
            
            ]
          
        
        ,
      
    
    {\displaystyle \mathbf {U} ={\begin{bmatrix}\rho \\u\\v\\p\\\end{bmatrix}}\ ,\ \mathbf {F} ={\begin{bmatrix}\rho _{0}u+\rho u_{0}\\u_{0}u+p/\rho _{0}\\u_{0}v\\u_{0}p+\gamma p_{0}u\\\end{bmatrix}}\ ,\ \mathbf {G} ={\begin{bmatrix}\rho _{0}v\\0\\p/\rho _{0}\\\gamma p_{0}v\\\end{bmatrix}},}
  
where 
  
    
      
        ρ
      
    
    {\displaystyle \rho }
  , 
  
    
      
        u
      
    
    {\displaystyle u}
  , 
  
    
      
        v
      
    
    {\displaystyle v}
   and 
  
    
      
        p
      
    
    {\displaystyle p}
   are the acoustic field variables, 
  
    
      
        γ
      
    
    {\displaystyle \gamma }
   the ratio of specific heats 
  
    
      
        
          c
          
            p
          
        
        
          /
        
        
          c
          
            v
          
        
      
    
    {\displaystyle c_{p}/c_{v}}
  , for air at 20 °C 
  
    
      
        
          c
          
            p
          
        
        
          /
        
        
          c
          
            v
          
        
        =
        1.4
      
    
    {\displaystyle c_{p}/c_{v}=1.4}
  , and the source term 
  
    
      
        
          S
        
      
    
    {\displaystyle \mathbf {S} }
   on the right-side represents distributed unsteady sources. The application of LEE can be found in engine noise studies.
For high Mach number flows in compressible regimes, the acoustic propagation may be influenced by non-linearities and the LEE may no longer be the appropriate mathematical model.


==== Pseudospectral ====
A Fourier pseudospectral time-domain method can be applied to wave propagation problems pertinent to computational aeroacoustics. The original algorithm of the Fourier pseudo spectral time domain method works for periodical problems without the interaction with physical boundaries. A slip wall boundary condition, combined with buffer zone technique to solve some non-periodical aeroacoustic problems has been proposed. Compared to other computational methods, pseudospectral method is preferred for its high-order accuracy.


==== EIF ====
Expansion about Incompressible Flow


==== APE ====
Acoustic Perturbation Equations
Refer to the paper ""Acoustic Perturbation Equations Based on Flow Decomposition via Source Filtering"" by R.Ewert and W.Schroder.


== See also ==
Aeroacoustics
Acoustic theory


== External links ==
Examples in Aeroacoustics from NASA
Computational Aeroacoustics at the Ecole Centrale de Lyon
Computational Aeroacoustics at the University of Leuven
Computational Aeroacoustics at Technische Universität Berlin
A CAA lecture script of Technische Universität Berlin


== References ==

Lighthill, M. J., ""A General Introduction to Aeroacoustics and Atmospheric Sounds"", ICASE Report 92-52, NASA Langley Research Centre, Hampton, VA, 1992"
73,Business software,1037763,15197,"Business software or a business application is any software or set of computer programs used by business users to perform various business functions. These business applications are used to increase productivity, to measure productivity and to perform other business functions accurately.
By and large, business software is likely to be developed to meet the needs of a specific business, and therefore is not easily transferable to a different business environment, unless its nature and operation is identical. Due to the unique requirements of each business, off-the-shelf software is unlikely to completely address a company's needs. However, where an on-the-shelf solution is necessary, due to time or monetary considerations, some level of customization is likely to be required. Exceptions do exist, depending on the business in question, and thorough research is always required before committing to bespoke or off-the-shelf solutions.
Some business applications are interactive, i.e., they have a graphical user interface or user interface and users can query/modify/input data and view results instantaneously. They can also run reports instantaneously. Some business applications run in batch mode: they are set up to run based on a predetermined event/time and a business user does not need to initiate them or monitor them.
Some business applications are built in-house and some are bought from vendors (off the shelf software products). These business applications are installed on either desktops or big servers. Prior to the introduction of COBOL (a universal compiler) in 1965, businesses developed their own unique machine language. RCA's language consisted of a 12-position instruction. For example, to read a record into memory, the first two digits would be the instruction (action) code. The next four positions of the instruction (an 'A' address) would be the exact leftmost memory location where you want the readable character to be placed. Four positions (a 'B' address) of the instruction would note the very rightmost memory location where you want the last character of the record to be located. A two digit 'B' address also allows a modification of any instruction. Instruction codes and memory designations excluded the use of 8's or 9's. The first RCA business application was implemented in 1962 on a 4k RCA 301. The RCA 301, mid frame 501, and large frame 601 began their marketing in early 1960.
Many kinds of users are found within the business environment, and can be categorized by using a small, medium and large matrix:
The small business market generally consists of home accounting software, and office suites such as OpenOffice.org or Microsoft Office.
The medium size, or small and medium-sized enterprise (SME), has a broader range of software applications, ranging from accounting, groupware, customer relationship management, human resource management systems, outsourcing relationship management, loan origination software, shopping cart software, field service software, and other productivity enhancing applications.
The last segment covers enterprise level software applications, such as those in the fields of enterprise resource planning, enterprise content management (ECM), business process management (BPM) and product lifecycle management. These applications are extensive in scope, and often come with modules that either add native functions, or incorporate the functionality of third-party computer programs.
Technologies that previously only existed in peer-to-peer software applications, like Kazaa and Napster, are starting to appear within business applications.


== Types of business tools ==
Enterprise application software (EAS)
Resource Management
Digital dashboards, also known as business intelligence dashboards, enterprise dashboards, or executive dashboards. These are visually based summaries of business data that show at-a-glance understanding of conditions through metrics and key performance indicators (KPIs). Dashboards are a very popular tools that have arisen in the last few years.
Online analytical processing (OLAP), (which include HOLAP, ROLAP and MOLAP) - are a capability of some management, decision support, and executive information systems that support interactive examination of large amounts of data from many perspectives.
Reporting software generates aggregated views of data to keep the management informed about the state of their business.
Procurement software is business software that helps to automate the purchasing function of organizations.
Data mining is the extraction of consumer information from a database by utilizing software that can isolate and identify previously unknown patterns or trends in large amounts of data. There is a variety of data mining techniques that reveal different types of patterns. Some of the techniques that belong here are statistical methods (particularly business statistics) and neural networks, as very advanced means of analyzing data.
Business performance management (BPM)
Document management software is made for organizing and managing multiple documents of various types. Some of them have storage functions for security and back-up of valuable business information.
Employee scheduling software- used for creating and distributing employee schedules, as well as for tracking employee hours.


== Brief history ==
The essential motivation for business software is to increase profits by cutting costs or speeding the productive cycle. In the earliest days of white-collar business automation, large mainframe computers were used to tackle the most tedious jobs, like bank cheque clearing and factory accounting.
Factory accounting software was among the most popular of early business software tools, and included the automation of general ledgers, fixed assets inventory ledgers, cost accounting ledgers, accounts receivable ledgers, and accounts payable ledgers (including payroll, life insurance, health insurance, federal and state insurance and retirement).
The early use of software to replace manual white-collar labor was extremely profitable, and caused a radical shift in white-collar labor. One computer might easily replace 100 white-collar 'pencil pushers', and the computer would not require any health or retirement benefits.
Building on these early successes with IBM, Hewlett-Packard and other early suppliers of business software solutions, corporate consumers demanded business software to replace the old-fashioned drafting board. CAD-CAM software (or computer-aided drafting for computer-aided manufacturing) arrived in the early 1980s. Also, project management software was so valued in the early 1980s that it might cost as much as $500,000 per copy (although such software typically had far fewer capabilities than modern project management software such as Microsoft Project, which one might purchase today for under $500 per copy.)
In the early days, perhaps the most noticeable, widespread change in business software was the word processor. Because of its rapid rise, the ubiquitous IBM typewriter suddenly vanished in the 1980s as millions of companies worldwide shifted to the use of Word Perfect business software, and later, Microsoft Word software. Another vastly popular computer program for business were mathematical spreadsheet programs such as Lotus 1-2-3, and later Microsoft Excel.
In the 1990s business shifted massively towards globalism with the appearance of SAP software which coordinates a supply-chain of vendors, potentially worldwide, for the most efficient, streamlined operation of factory manufacture.
Yet nothing in the history of business software has had the global impact of the Internet, with its email and websites that now serve commercial interests worldwide. Globalism in business fully arrived when the Internet became a household word.
The next phase in the evolution of business software is being led by the emergance of Robotic Process Automation (RPA), which involves identifying and automating highly repetitive tasks and processes, with an aim to drive operational efficiency, reduce costs and limit human error. Industries that have been in the forefront of RPA adoption include the Insurance industry, Banking and Financial Services, the Legal industry and the Healthcare industry.


== Application support ==
Business applications are built based on the requirements from the business users. Also, these business applications are built to use certain kind of Business transactions or data items. These business applications run flawlessly until there are no new business requirements or there is no change in underlying Business transactions. Also, the business applications run flawlessly if there are no issues with computer hardware, computer networks (Intenet/intranet), computer disks, power supplies, and various software components (middleware, database, computer programs, etc.).
Business applications can fail when an unexpected error occurs. This error could occur due to a data error (an unexpected data input or a wrong data input), an environment error (an in frastructure related error), a programming error, a human error or a work flow error. When a business application fails one needs to fix the business application error as soon as possible so that the business users can resume their work. This work of resolving business application errors is known as business application support.


=== Reporting errors ===
The Business User calls the business application support team phone number or sends an e-mail to the business application support team. The business application support team gets all the details of the error from the business user on the phone or from the e-mail. These details are then entered in a tracking software. The tracking software creates a request number and this request number is given to the business user. This request number is used to track the progress on the support issue. The request is assigned to a support team member.


=== Notification of errors ===
For critical business application errors (such as an application not available or an application not working correctly), an e-mail is sent to the entire organization or impacted teams so that they are aware of the issue. They are also provided with an estimated time for application availability.


=== Investigation or analysis of application errors ===
The business application support team member collects all the necessary information about the business software error. This information is then recorded in the support request. All of the data used by the business user is also used in the investigation. The application program is reviewed for any possible programming errors.


=== Error resolution ===
If any similar business application errors occurred in the past then the issue resolution steps are retrieved from the support knowledge base and the error is resolved using those steps. If it is a new support error, then new issue resolution steps are created and the error is resolved. The new support error resolution steps are recorded in the knowledge base for future use. For major business application errors (critical infrastructure or application failures), a phone conference call is initiated and all required support persons/teams join the call and they all work together to resolve the error.


=== Code correction ===
If the business application error occurred due to programming errors, then a request is created for the application development team to correct programming errors. If the business user needs new features or functions in the business application, then the required analysis/design/programming/testing/release is planned and a new version of the business software is deployed.


=== Business process correction ===
If the business application error occurred due to a work flow issue or human errors during data input, then the business users are notified. Business users then review their work flow and revise it if necessary. They also modify the user guide or user instructions to avoid such an error in the future.


=== Infrastructure issue correction ===
If the business application error occurred due to infrastructure issues, then the specific infrastructure team is notified. The infrastructure team then implements permanent fixes for the issue and monitors the infrastructure to avoid the re-occurrence of the same error.


== Support follow up and internal reporting ==
The business application error tracking system is used to review all issues periodically (daily, weekly and monthly) and reports are generated to monitor the resolved issues, repeating issues, and pending issues. Reports are also generated for the IT/IS management for improvement and management of business applications.


== See also ==


== References ==


== External links =="
74,Machine learning in bioinformatics,53970843,15121,"Machine learning, a subfield of computer science involving the development of algorithms that learn how to make predictions based on data, has a number of emerging applications in the field of bioinformatics. Bioinformatics deals with computational and mathematical approaches for understanding and processing biological data.
Prior to the emergence of machine learning algorithms, bioinformatics algorithms had to be explicitly programmed by hand which, for problems such as protein structure prediction, proves extremely difficult. Machine learning techniques such as deep learning enable the algorithm to make use of automatic feature learning which means that based on the dataset alone, the algorithm can learn how to combine multiple features of the input data into a more abstract set of features from which to conduct further learning. This multi-layered approach to learning patterns in the input data allows such systems to make quite complex predictions when trained on large datasets. In recent years, the size and number of available biological datasets have skyrocketed, enabling bioinformatics researchers to make use of these machine learning systems. Machine learning has been applied to six main subfields of bioinformatics: genomics, proteomics, microarrays, systems biology, evolution, and text mining.


== Applications ==


=== Genomics ===

Genomics involves the study of the genome, the complete DNA sequence, of organisms. While genomic sequence data has historically been sparse due to the technical difficulty in sequencing a piece of DNA, the number of available sequences is growing exponentially. However, while raw data is becoming increasingly available and accessible, the biological interpretation of this data is occurring at a much slower pace. Therefore, there is an increasing need for the development of machine learning systems that can automatically determine the location of protein-encoding genes within a given DNA sequence. This is a problem in computational biology known as gene prediction.
Gene prediction is commonly performed through a combination of what are known as extrinsic and intrinsic searches. For the extrinsic search, the input DNA sequence is run through a large database of sequences whose genes have been previously discovered and their locations annotated. A number of the sequence's genes can be identified by determining which strings of bases within the sequence are homologous to known gene sequences. However, given the limitation in size of the database of known and annotated gene sequences, not all the genes in a given input sequence can be identified through homology alone. Therefore, an intrinsic search is needed where a gene prediction program attempts to identify the remaining genes from the DNA sequence alone.
Machine learning is also been used for the problem of multiple sequence alignment which involves aligning many DNA or amino acid sequences in order to determine regions of similarity that could indicate a shared evolutionary history. It can also be used to detect and visualize genome rearrangements.


=== Proteomics ===

Proteins, strings of amino acids, gain much of their function from protein folding in which they conform into a three-dimensional structure. This structure is composed of a number of layers of folding, including the primary structure (i.e. the flat string of amino acids), the secondary structure (alpha helices and beta sheets), the tertiary structure, and the quartenary structure.
Protein secondary structure prediction is a main focus of this subfield as the further protein foldings (tertiary and quartenary structures) are determined based on the secondary structure. Solving the true structure of a protein is an incredibly expensive and time-intensive process, furthering the need for systems that can accurately predict the structure of a protein by analyzing the amino acid sequence directly. Prior to machine learning, researchers needed to conduct this prediction manually. This trend began in 1951 when Pauling and Corey released their work on predicting the hydrogen bond configurations of a protein from a polypeptide chain. Today, through the use of automatic feature learning, the best machine learning techniques are able to achieve an accuracy of 82-84%. The current state-of-the-art in secondary structure prediction uses a system called DeepCNF (deep convolutional neural fields) which relies on the machine learning model of artificial neural networks to achieve an accuracy of approximately 84% when tasked to classify the amino acids of a protein sequence into one of three structural classes (helix, sheet, or coil). The theoretical limit for three-state protein secondary structure is 88–90%.
Machine learning has also been applied to proteomics problems such as protein side-chain prediction, protein loop modeling, and protein contact map prediction.


=== Microarrays ===
Microarrays, a type of lab-on-a-chip, are used for automatically collecting data about large amounts of biological material. Machine learning can aid in the analysis of this data, and it has been applied to expression pattern identification, classification, and genetic network induction.

This technology is especially useful for monitoring the expression of genes within a genome, aiding in diagnosing different types of cancer based on which genes are expressed. One of the main problems in this field is identifying which genes are expressed based on the collected data. In addition, due to the huge number of genes on which data is collected by the microarray, there is a large amount of irrelevant data to the task of expressed gene identification, further complicating this problem. Machine learning presents a potential solution to this problem as various classification methods can be used to perform this identification. The most commonly used methods are radial basis function networks, deep learning, Bayesian classification, decision trees, and random forest.


=== Systems biology ===
Systems biology focuses on the study of the emergent behaviors from complex interactions of simple biological components in a system. Such components can include molecules such as DNA, RNA, proteins, and metabolites.
Machine learning has been used to aid in the modelling of these complex interactions in biological systems in domains such as genetic networks, signal transduction networks, and metabolic pathways. Probabilistic graphical models, a machine learning technique for determining the structure between different variables, are one of the most commonly used methods for modeling genetic networks. In addition, machine learning has been applied to systems biology problems such as identifying transcription factor binding sites using a technique known as Markov chain optimization. Genetic algorithms, machine learning techniques which are based on the natural process of evolution, have been used to model genetic networks and regulatory structures.
Other systems biology applications of machine learning include the task of enzyme function prediction, high throughput microarray data analysis, analysis of genome-wide association studies to better understand markers of Multiple Sclerosis, protein function prediction, and identification of NCR-sensitivity of genes in yeast.


=== Text mining ===
The increase in available biological publications led to the issue of the increase in difficulty in searching through and compiling all the relevant available information on a given topic across all sources. This task is known as knowledge extraction. This is necessary for biological data collection which can then in turn be fed into machine learning algorithms to generate new biological knowledge. Machine learning can be used for this knowledge extraction task using techniques such as natural language processing to extract the useful information from human-generated reports in a database. Text Nailing, an alternative approach to machine learning, capable of extracting features from clinical narrative notes was introduced in 2017.
This technique has been applied to the search for novel drug targets, as this task requires the examination of information stored in biological databases and journals. Annotations of proteins in protein databases often do not reflect the complete known set of knowledge of each protein, so additional information must be extracted from biomedical literature. Machine learning has been applied to automatic annotation of the function of genes and proteins, determination of the subcellular localization of a protein, analysis of DNA-expression arrays, large-scale protein interaction analysis, and molecule interaction analysis.
Another application of text mining is the detection and visualization of distinct DNA regions given sufficient reference data.


== References =="
75,List of computer science conferences,2979338,15107,"This is a list of academic conferences in computer science. Only conferences with separate articles are included; within each field, the conferences are listed alphabetically by their short names.


== General ==
FCRC – Federated Computing Research Conference


== Algorithms and theory ==

Conferences accepting a broad range of topics from theoretical computer science, including algorithms, data structures, computability, computational complexity, automata theory and formal languages:
FCT – International Symposium on Fundamentals of Computation Theory
FOCS – IEEE Symposium on Foundations of Computer Science
ICALP – International Colloquium on Automata, Languages and Programming
ISAAC – International Symposium on Algorithms and Computation
MFCS – International Symposium on Mathematical Foundations of Computer Science
STACS – Symposium on Theoretical Aspects of Computer Science
STOC – ACM Symposium on Theory of Computing
WoLLIC – Workshop on Logic, Language, Information and Computation


=== Algorithms ===
Conferences whose topic is algorithms and data structures considered broadly, but that do not include other areas of theoretical computer science such as computational complexity theory:
ESA – European Symposium on Algorithms
SODA – ACM–SIAM Symposium on Discrete Algorithms
SWAT – Scandinavian Symposium and Workshops on Algorithm Theory
WADS – Algorithms and Data Structures Symposium
WAOA – Workshop on Approximation and Online Algorithms


=== Geometric algorithms ===
Conferences on computational geometry, graph drawing, and other application areas of geometric computing:
GD – International Symposium on Graph Drawing
SoCG – ACM Symposium on Computational Geometry


=== Logic ===

LICS – ACM–IEEE Symposium on Logic in Computer Science
RTA – International Conference on Rewriting Techniques and Applications


=== Other specialized subtopics ===
CIAA – International Conference on Implementation and Application of Automata
CCC – Computational Complexity Conference
DCFS – International Workshop on Descriptional Complexity of Formal Systems
DLT – International Conference on Developments in Language Theory
ISSAC – International Symposium on Symbolic and Algebraic Computation
Petri Nets - International Conference on Applications and Theory of Petri Nets and Concurrency


== Languages and software ==


=== Programming languages ===
Conferences on programming languages, programming language theory and compilers:
CC – ETAPS International Conference on Compiler Construction
ECOOP – AITO European Conference on Object-Oriented Programming
ESOP – ETAPS European Symposium on Programming
HOPL – ACM SIGPLAN History of Programming Languages Conference
ICFP – ACM SIGPLAN International Conference on Functional Programming
ICLP – ALP International Conference on Logic Programming
ISMM – ACM SIGPLAN International Symposium on Memory Management
OOPSLA – ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications
POPL – ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages
PLDI – ACM SIGPLAN Conference on Programming Language Design and Implementation


=== Software engineering ===
Conferences on software engineering:
ASE – IEEE/ACM International Conference on Automated Software Engineering
ICSE – International Conference on Software Engineering
ICSR – International Conference on Software Reuse
MODELS – IEEE/ACM International Conference on Model Driven Engineering Languages and Systems


=== Formal methods ===

Conferences on formal methods in software engineering, including formal specification, formal verification, and static code analysis:
CAV – Computer Aided Verification
FORTE – IFIP International Conference on Formal Techniques for Networked and Distributed Systems


== Concurrent, distributed and parallel computing ==

Conferences on concurrent, distributed, and parallel computing, fault-tolerant systems, and dependable systems:
DEBS - ACM International Conference on Distributed Event-Based Systems
DISC - International Symposium on Distributed Computing
DSN - International Conference on Dependable Systems and Networks
ICDCS - IEEE International Conference on Distributed Computing Systems
ICPADS - IEEE International Conference on Parallel and Distributed Systems
IPDPS - IEEE International Parallel and Distributed Processing Symposium
PODC - ACM Symposium on Principles of Distributed Computing
PPoPP - ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming
SIROCCO - International Colloquium on Structural Information and Communication Complexity
SPAA - ACM Symposium on Parallelism in Algorithms and Architectures
SRDS - IEEE International Symposium on Reliable Distributed Systems


=== High-performance computing ===
Conferences on high-performance computing, cluster computing, and grid computing:
HiPC - International Conference on High Performance Computing
SC - ACM/IEEE Supercomputing Conference


== Operating systems ==

Conferences on operating systems, storage systems and middleware:
ATC - USENIX Annual Technical Conference
FAST - USENIX Conference on File and Storage Technologies
Middleware - ACM/IFIP/USENIX International Middleware Conference
SOSP - ACM Symposium on Operating Systems Principles
SYSTOR - ACM International Systems and Storage Conference


== Computer architecture ==
Conferences on computer architecture:
ASPLOS - International Conference on Architectural Support for Programming Languages and Operating Systems
ISCA - International Symposium on Computer Architecture
MICRO - IEEE/ACM International Symposium on Microarchitecture


=== Computer hardware ===
Conferences on computer hardware:
ISCAS - IEEE International Symposium on Circuits and Systems


=== Computer-aided design ===
Conferences on computer-aided design and electronic design automation:
ASP-DAC - Asia and South Pacific Design Automation Conference
DAC - Design Automation Conference
DATE - Design, Automation, and Test in Europe
ICCAD - International Conference on Computer-Aided Design
ISPD - International Symposium on Physical Design


== Computer networking ==

Conferences on computer networking:
GlobeCom - IEEE Global Communications Conference
ICC - IEEE International Conference on Communications
ICSOC - International Conference on Service Oriented Computing
INFOCOM - IEEE Conference on Computer Communications
SIGCOMM - ACM SIGCOMM Conference
WINE - The Workshop on Internet & Network Economics


=== Wireless networks and mobile computing ===
Wireless networks and mobile computing, including ubiquitous and pervasive computing, wireless ad hoc networks and wireless sensor networks:
EWSN - European Conference on Wireless Sensor Networks
IPSN - ACM/IEEE International Conference on Information Processing in Sensor Networks
ISWC - International Symposium on Wearable Computers
MobiHoc - ACM International Symposium on Mobile Ad Hoc Networking and Computing
SenSys - ACM Conference on Embedded Networked Sensor Systems


== Security and privacy ==
Conferences on computer security and privacy:
DSN - International Conference on Dependable Systems and Networks
USENIX Security Symposium
SOUPS - Symposium on Usable Privacy and Security


=== Cryptography ===

Cryptography conferences:
ACNS - Applied Cryptography and Network Security
ANTS - Algorithmic Number Theory Symposium
ASIACRYPT - International Conference on the Theory and Application of Cryptology and Information Security
CHES - Workshop on Cryptographic Hardware and Embedded Systems
CRYPTO - International Cryptology Conference
EUROCRYPT - International Conference on the Theory and Applications of Cryptographic Techniques
FSE - Fast Software Encryption Workshop
INDOCRYPT - International Conference on Cryptology in India
PKC - International Workshop on Practice and Theory in Public Key Cryptography
RSA - RSA Conference
TCC - Theory of Cryptography Conference


== Data management ==
Conferences on databases, information systems, information retrieval, data mining and the world wide web:
CIDR - Conference on Innovative Data Systems Research
CIKM - ACM Conference on Information and Knowledge Management
ECIR - European Conference on Information Retrieval
ECIS - European Conference on Information Systems
ER - International Conference on Conceptual Modeling
ICDT - International Conference on Database Theory
ICIS - International Conference on Information Systems
ISWC - International Semantic Web Conference
JCDL - ACM/IEEE Joint Conference on Digital Libraries
KDD - ACM SIGKDD Conference on Knowledge Discovery and Data Mining
PODS - ACM Symposium on Principles of Database Systems
SIGIR - Annual International ACM SIGIR Conference
SIGMOD - ACM SIGMOD Conference
VLDB - International Conference on Very Large Data Bases
WWW - World Wide Web Conference


== Artificial intelligence ==
Conferences on artificial intelligence and machine learning:
AAAI - AAAI Conference on Artificial Intelligence
AAMAS - International Conference on Autonomous Agents and Multiagent Systems
ECAI - European Conference on Artificial Intelligence
ECML PKDD - European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases
ICML - International Conference on Machine Learning
IJCAI - International Joint Conference on Artificial Intelligence
ISWC - International Semantic Web Conference
NIPS - Conference on Neural Information Processing Systems
RuleML - RuleML Symposium


=== Evolutionary computation ===
Conferences on Evolutionary computation.
CEC - IEEE Congress on Evolutionary Computation
GECCO - Genetic and Evolutionary Computation Conference


=== Automated reasoning ===

Conferences on automated reasoning:
IJCAR - International Joint Conference on Automated Reasoning
LPAR - International Conference on Logic for Programming, Artificial Intelligence and Reasoning
RuleML - RuleML Symposium
TABLEAUX - International Conference on Automated Reasoning with Analytic Tableaux and Related Methods
WoLLIC - Workshop on Logic, Language, Information and Computation


=== Computer vision ===

Conferences on computer vision (including also image analysis) and pattern recognition:
BMVC - British Machine Vision Conference
CVPR - Conference on Computer Vision and Pattern Recognition
ECCV - European Conference on Computer Vision
ICCV - International Conference on Computer Vision
PSIVT - Pacific Rim Symposium on Image and Video Technology
SCIA - Scandinavian Conference on Image Analysis
SSIAI - IEEE Southwest Symposium on Image Analysis and Interpretation


=== Natural language processing ===
Conferences on computational linguistics and natural language processing:
ACL - Annual Meeting of the Association for Computational Linguistics
CICLing - International Conference on Intelligent Text Processing and Computational Linguistics
NAACL - Annual Conference of the North American Chapter of the Association for Computational Linguistics
TSD - Text, Speech and Dialogue


== Computer graphics ==

Conferences on computer graphics, geometry processing, image processing, and multimedia:
Eurographics - Annual Conference of the European Association for Computer Graphics
MM - ACM International Conference on Multimedia
SGP - Symposium on Geometry Processing
SIGGRAPH - International Conference on Computer Graphics and Interactive Techniques


=== Visualization ===
Conferences on scientific visualization and information visualization:
VIS - IEEE Visualization Conference


== Human–computer interaction ==
Conferences on human–computer interaction and user interfaces:
ASSETS - International ACM SIGACCESS Conference on Computers and Accessibility
CHI - ACM Conference on Human Factors in Computing Systems
GI - Graphics Interface
MobileHCI - Conference on Human-Computer Interaction with Mobile Devices and Services
SIGDOC - ACM International Conference on Design of Communication
UIST - ACM Symposium on User Interface Software and Technology
UMAP - ACM International Conference on User Modeling, Adaptation, and Personalization


== Computational biology ==
Conferences on bioinformatics and computational biology:
CIBB - International Meeting on Computational Intelligence Methods for Bioinformatics and Biostatistics
ISMB - Intelligent Systems for Molecular Biology
PSB - Pacific Symposium on Biocomputing
RECOMB - Research in Computational Molecular Biology
WABI - Workshop on Algorithms in Bioinformatics


== Education ==
Conferences on computer science education and electronic learning:
SIGCSE - ACM Technical Symposium on Computer Science Education


== See also ==
List of computer science conference acronyms
List of computer science journals
List of publications in computer science
Outline of computer science


== External links ==
DBLP database with conferences and workshops
The Informatics Europe's Computer Science Event List"
76,Seymour Cray Computer Engineering Award,26603341,14935,"The Seymour Cray Computer Engineering Award, also known as the Seymour Cray Award, is an award given by the IEEE Computer Society, to recognize significant and innovative contributions in the field of high-performance computing. The award honors scientists who exhibit the creativity demonstrated by Seymour Cray, founder of Cray Research, Inc., and an early pioneer of supercomputing. Cray was an American electrical engineer and supercomputer architect who designed a series of computers that were the fastest in the world for decades, and founded Cray Research which built many of these machines. Called ""the father of supercomputing,"" Cray has been credited with creating the supercomputer industry. He played a key role in the invention and design of the UNIVAC 1103, a landmark high-speed computer and the first computer available for commercial use.
In 1972 the IEEE presented Cray with the Harry H. Goode Memorial Award for his contributions to large-scale computer design and the development of multiprocessing systems. One year after Cray's death in 1996, IEEE created the Seymour Cray Computer Engineering Award in honor of his creative spirit.  The award is one of the 12 technical awards sponsored by the IEEE computer society as recognition given to pioneers in the field of computer science and engineering.  The winner receives a crystal memento, certificate, and US$10,000 honorarium.
The first recipient, in 1999, was John Cocke.


== Nomination and Ceremony ==
The following criteria are considered when selecting a recipient:
Leadership in field
Breadth of work
Achievement in other fields
Inventive value (patents)
Individual vs. group contribution
Publications (articles, etc.)
Originality of contribution
Quality of nomination
IEEE Society activities and honors
Quality of endorsements
The annual nomination deadline is July 1st. Anyone may nominate a candidate, although self-nomination is not allowed. A candidate must receive at least three nominations to be considered by the award committee. Nominations should be prepared and submitted through the IEEE official website. 
The Seymour Cray Computer Engineering Award presentation and reception are held at the SC conference, the international conference for high-performance computing networks, storage, and analysis. The conference is sponsored by the ACM (Association for Computing Machinery) and the IEEE Computer Society. It is held annually in mid-November. Several other IEEE sponsored awards are presented at the same event, including the ACM Gordon Bell Prize, the ACM/IEEE-CS Ken Kennedy Award, the ACM/IEEE-CS George Michael Memorial HPC Fellowship, the ACM SIGHPC / Intel Computational & Data Science Fellowships, the IEEE-CS Seymour Cray Computer Engineering Award, and the IEEE-CS Sidney Fernbach Memorial Award.


== Recipients ==


== See also ==
List of prizes, medals and awards
List of prizes named after people
IEEE John von Neumann Medal
ACM Gordon Bell Prize
ACM/IEEE-CS Ken Kennedy Award
ACM/IEEE-CS George Michael Memorial HPC Fellowship
ACM SIGHPC / Intel Computational & Data Science Fellowships
IEEE-CS Seymour Cray Computer Engineering Award
IEEE-CS Sidney Fernbach Memorial Award


== References ==


== External links ==
Official web page"
77,Computational auditory scene analysis,8953842,14933,"Computational auditory scene analysis (CASA) is the study of auditory scene analysis by computational means. In essence, CASA systems are ""machine listening"" systems that aim to separate mixtures of sound sources in the same way that human listeners do. CASA differs from the field of blind signal separation in that it is (at least to some extent) based on the mechanisms of the human auditory system, and thus uses no more than two microphone recordings of an acoustic environment. It is related to the cocktail party problem.


== Principles ==
Since CASA serves to model functionality parts of the auditory system, it is necessary to view parts of the biological auditory system in terms of known physical models. Consisting of three areas, the outer, middle and inner ear, the auditory periphery acts as a complex transducer that converts sound vibrations into action potentials in the auditory nerve. The outer ear consists of the external ear, ear canal and the ear drum. The outer ear, like an acoustic funnel, helps locating the sound source. The ear canal acts as a resonant tube (like an organ pipe) to amplify frequencies between 2–5.5 kHz with a maximum amplification of about 11 dB occurring around 4 kHz. As the organ of hearing, the cochlea consists of two membranes, Reissner’s and the basilar membrane. The basilar membrane moves to audio stimuli through the specific stimulus frequency matches the resonant frequency of a particular region of the basilar membrane. The movement the basilar membrane displaces the inner hair cells in one direction, which encodes a half-wave rectified signal of action potentials in the spiral ganglion cells. The axons of these cells make up the auditory nerve, encoding the rectified stimulus. The auditory nerve responses select certain frequencies, similar to the basilar membrane. For lower frequencies, the fibers exhibit ""phase locking"". Neurons in higher auditory pathway centers are tuned to specific stimuli features, such as periodicity, sound intensity, amplitude and frequency modulation. There are also neuroanatomical associations of ASA through the posterior cortical areas, including the posterior superior temporal lobes and the posterior cingulate. Studies have found that impairments in ASA and segregation and grouping operations are affected in patients with Alzheimer's disease.


== System Architecture ==


=== Cochleagram ===
As the first stage of CASA processing, the cochleagram creates a time-frequency representation of the input signal. By mimicking the components of the outer and middle ear, the signal is broken up into different frequencies that are naturally selected by the cochlea and hair cells. Because of the frequency selectivity of the basilar membrane, a filter bank is used to model the membrane, with each filter associated with a specific point on the basilar membrane.
Since the hair cells produce spike patterns, each filter of the model should also produce a similar spike in the impulse response. The use of a gammatone filter provides an impulse response as the product of a gamma function and a tone. The output of the gammatone filter can be regarded as a measurement of the basilar membrane displacement. Most CASA systems represent the firing rate in the auditory nerve rather than a spike-based. To obtain this, the filter bank outputs are half-wave rectified followed by a square root. (Other models, such as automatic gain controllers have been implemented). The half-rectified wave is similar to the displacement model of the hair cells. Additional models of the hair cells include the Meddis hair cell model which pairs with the gammatone filter bank, by modeling the hair cell transduction. Based on the assumption that there are three reservoirs of transmitter substance within each hair cell, and the transmitters are released in proportion to the degree of displacement to the basilar membrane, the release is equated with the probability of a spike generated in the nerve fiber. This model replicates many of the nerve responses in the CASA systems such as rectification, compression, spontaneous firing, and adaptation.


=== Correlogram ===
Important model of pitch perception by unifying 2 schools of pitch theory:
Place theories (emphasizing the role of resolved harmonics)
Temporal theories (emphasizing the role of unresolved harmonics)
The correlogram is generally computed in the time domain by autocorrelating the simulated auditory nerve firing activity to the output of each filter channel. By pooling the autocorrelation across frequency, the position of peaks in the summary correlogram corresponds to the perceived pitch.


=== Cross-Correlogram ===
Because the ears receive audio signals at different times, the sound source can be determined by using the delays retrieved from the two ears. By cross-correlating the delays from the left and right channels (of the model), the coincided peaks can be categorized as the same localized sound, despite their temporal location in the input signal. The use of interaural cross-correlation mechanism has been supported through physiological studies, paralleling the arrangement of neurons in the auditory midbrain.


=== Time-Frequency Masks ===
To segregate the sound source, CASA systems mask the cochleagram. This mask, sometimes a Wiener filter, weighs the target source regions and suppresses the rest. The physiological motivation behind the mask results from the auditory perception where sound is rendered inaudible by a louder sound.


=== Resynthesis ===
A resynthesis pathway reconstructs an audio signal from a group of segments. Achieved by inverting the cochleagram, high quality resynthesized speech signals can be obtained.


== Applications ==


=== Monaural CASA ===
Monaural sound separation first began with separating voices based on frequency. There were many early developments based on segmenting different speech signals through frequency. Other models followed on this process, by the addition of adaption through state space models, batch processing, and prediction-driven architecture. The use of CASA has improved the robustness of ASR and speech separation systems.


=== Binaural CASA ===
Since CASA is modeling human auditory pathways, binaural CASA systems better the human model by providing sound localization, auditory grouping and robustness to reverberation by including 2 spatially separated microphones. With methods similar to cross-correlation, systems are able to extract the target signal from both input microphones.


=== Neural CASA Models ===
Since the biological auditory system is deeply connected with the actions of neurons, CASA systems also incorporated neural models within the design. Two different models provide the basis for this area. Malsburg and Schneider proposed a neural network model with oscillators to represent features of different streams (synchronized and desynchronized). Wang also presented a model using a network of excitatory units with a global inhibitor with delay lines to represent the auditory scene within the time-frequency.


=== Analysis of Musical Audio Signals ===
Typical approaches in CASA systems starts with segmenting sound-sources into individual constituents, in its attempts to mimic the physical auditory system. However, there is evidence that the brain does not necessarily process audio input separately, but rather as a mixture. Instead of breaking the audio signal down to individual constituents, the input is broken down of by higher level descriptors, such as chords, bass and melody, beat structure, and chorus and phrase repetitions. These descriptors run into difficulties in real-world scenarios, with monaural and binaural signals. Also, the estimation of these descriptors is highly dependent on the cultural influence of the musical input. For example, within Western music, the melody and bass influences the identity of the piece, with the core formed by the melody. By distinguishing the frequency responses of melody and bass, a fundamental frequency can be estimated and filtered for distinction. Chord detection can be implemented through pattern recognition, by extracting low-level features describing harmonic content. The techniques utilized in music scene analysis can also be applied to speech recognition, and other environmental sounds. Future bodies of work include a top-down integration of audio signal processing, such as a real-time beat-tracking system and expanding out of the signal processing realm with the incorporation of auditory psychology and physiology.


=== Neural Perceptual Modeling ===
While many models consider the audio signal as a complex combination of different frequencies, modeling the auditory system can also require consideration for the neural components. By taking a holistic process, where a stream (of feature-based sounds) correspond to neuronal activity distributed in many brain areas, the perception of the sound could be mapped and modeled. Two different solutions have been proposed to the binding of the audio perception and the area in the brain. Hierarchical coding models many cells to encode all possible combinations of features and objects in the auditory scene. Temporal or oscillatory correlation addressing the binding problem by focusing on the synchrony and desynchrony between neural oscillations to encode the state of binding among the auditory features. These two solutions are very similar to the debacle between place coding and temporal coding. While drawing from modeling neural components, another phenomenon of ASA comes into play with CASA systems: the extent of modeling neural mechanisms. The studies of CASA systems have involved modeling some known mechanisms, such as the bandpass nature of cochlear filtering and random auditory nerve firing patterns, however, these models may not lead to finding new mechanisms, but rather give an understanding of purpose to the known mechanisms.


== See also ==
auditory scene analysis
blind signal separation
cocktail party problem
machine vision
speech recognition


== Further reading ==
D. F. Rosenthal and H. G. Okuno (1998) Computational auditory scene analysis. Mahwah, NJ: Lawrence Erlbaum


== References =="
78,International Conference on Computational Linguistics and Intelligent Text Processing,29246999,14829,"CICLing (International Conference on Computational Linguistics and Intelligent Text Processing; before 2017 known under the name International Conference on Intelligent Text Processing and Computational Linguistics) is an annual conference on computational linguistics (CL) and natural language processing (NLP). The first CICLing conference was held in 2000 in Mexico City. The conference is attended by one to two hundred of NLP and CL researchers and students every year. As of 2017, it is ranked within top 20 sources (conferences and journals) on computational linguistics by Google Scholar. Past CICLing conferences have been held in Mexico, Korea, Israel, Romania, Japan, India, Greece, Nepal, Egypt, Turkey, and Hungary; the 2018 event will be held in Vietnam.


== Overview ==
CICLing is a series of annual international conferences devoted to computational linguistics (CL), natural language processing (NLP), human language technologies (HLT), natural-language human-computer interaction (HCI), as well as speech processing and speech recognition (SR).
Their topics of interest include, but are not limited to: text processing, computational morphology, tagging, stemming, syntactic analysis, parsing and shallow parsing, chunking, recognizing textual entailment, ambiguity resolution, semantic analysis, pragmatics, lexicon, lexical resources, dictionaries and machine-readable dictionaries (MRD), grammar, anaphora resolution, word sense disambiguation (WSD), machine translation (MT), information retrieval (IR), information extraction (IE), document handling, document classification and text classification, text summarization, text mining (TM), opinion mining, sentiment analysis, plagiarism detection, and spell checking (spelling).
CICLing series was founded in 2000 by Alexander Gelbukh. The acronym ""CICLing"" refers to ""Conference on Intelligent text processing and Computational Linguistics"", the name used before 2017.
Almost all CICLing events have been endorsed by the Association for Computational Linguistics.
Unlike some other conferences on computational linguistics and natural language processing, such as those run by the Association for Computational Linguistics, CICLing does not release its main proceedings as Open Access, publishing them instead with Springer; however, most of its complementary proceedings, published as special issues of journals, are released as Open Access; in addition, Springer allows the authors to make their papers available via their own webpages.


== Specific CICLing Conferences ==
In the table below, the figures for the number of accepted papers and acceptance rate refer to the main proceedings volume and do not include supplemental proceedings volumes. The number of countries corresponds to submissions, not to accepted papers.


== Past Keynote Speakers and Local Organizing Committee Chairs ==
The table lists, by year, experts that have given keynote addresses at past CICLing conferences, as well as the chairs of the Local Organizing Committee.


== See also ==
The list of computer science conferences contains other academic conferences in computer science.
The list of linguistics conferences contains other academic conferences in linguistics.


== References ==


== External links ==
CICLing series website"
79,Quantum image processing,54074489,14826,"Quantum image processing (QIP) is primarily devoted to using quantum computing and quantum information processing to create and work with quantum images.. Due to some of the astounding properties inherent to quantum computation, notably entanglement and parallelism, it is anticipated that QIP technologies will offer capabilities and performances that are, as yet, unrivaled by their traditional equivalents. These improvements could be in terms of computing speed, guaranteed security, and minimal storage requirements, etc., 


== Background ==
Vlasov’s work in 1997 focused on the use of a quantum system to recognize orthogonal images. This was followed by efforts using quantum algorithms to search specific patterns in binary images and detect the posture of certain targets. Notably, more optics-based interpretation for quantum imaging were initially experimentally demonstrated in  and formalized in  after seven years. Venegas-Andraca and Bose’s Qubit Lattice describes quantum images in 2003. Simultaneously, Lattorre proposed another kind of representation, called the Real Ket, whose purpose was to encode quantum images as a basis for further applications in QIMP.
Technically, these pioneering efforts with the subsequent studies related to them can be classified into three main groups:
Quantum-assisted digital image processing (QDIP): These applications aim at improving digital or classical image processing tasks and applications.
Optics-based quantum imaging (OQI)
Classically-inspired quantum image processing (QIP)


== Quantum image manipulations ==
A lot of the effort in QIP has been focused on designing algorithms to manipulate the position and color information encoded using the FRQI and its many variants. For instance, FRQI-based fast geometric transformations including (two-point) swapping, flip, (orthogonal) rotations and restricted geometric transformations to constrain these operations to a specified area of an image were initially proposed. Recently, NEQR-based quantum image translation to map the position of each picture element in an input image into a new position in an output image and quantum image scaling to resize a quantum image were discussed. While FRQI-based general form of color transformations were first proposed by means of the single qubit gates such as X, Z, and H gates. Later, MCQI-based channel of interest (CoI) operator to entail shifting the grayscale value of the preselected color channel and the channel swapping (CS) operator to swap the grayscale values between two channels were fully discussed in.
To illustrate the feasibility and capability of QIP algorithms and application, researchers always prefer to simulate the digital image processing tasks on the basis of the QIRs that we already have. By using the basic quantum gates and the aforementioned operations, so far, researchers have contributed to quantum image feature extraction, quantum image segmentation, quantum image morphology, quantum image comparison, quantum image filtering, quantum image classification, quantum image stabilization, among others. In particular, QIMP-based security technologies have attracted extensive interest of researchers as presented in the ensuing discussions. Similarly, these advancements have led to many applications in the areas of watermarking, encryption, and steganography etc., which form the core security technologies highlighted in this area.
In general, the work pursued by the researchers in this area are focused on expanding the applicability of QIP to realize more classical-like digital image processing algorithms; propose technologies to physically realize the QIMP hardware; or simply to note the likely challenges that could impede the realization of some QIMP protocols.


== Quantum image transform ==
By encoding and processing the image information in quantum-mechanical systems, a framework of quantum image processing is presented, where a pure quantum state encodes the image information: to encode the pixel values in the probability amplitudes and the pixel positions in the computational basis states. Given a image 
  
    
      
        F
        =
        (
        
          F
          
            i
            ,
            j
          
        
        
          )
          
            M
            ×
            L
          
        
      
    
    {\displaystyle F=(F_{i,j})_{M\times L}}
  , where 
  
    
      
        
          F
          
            i
            ,
            j
          
        
      
    
    {\displaystyle F_{i,j}}
   represents the pixel value at position 
  
    
      
        (
        i
        ,
        j
        )
      
    
    {\displaystyle (i,j)}
   with 
  
    
      
        i
        =
        1
        ,
        …
        ,
        M
      
    
    {\displaystyle i=1,\dots ,M}
   and 
  
    
      
        j
        =
        1
        ,
        …
        ,
        L
      
    
    {\displaystyle j=1,\dots ,L}
  , a vector 
  
    
      
        
          
            
              f
              →
            
          
        
      
    
    {\displaystyle {\vec {f}}}
   with 
  
    
      
        M
        L
      
    
    {\displaystyle ML}
   elements can be formed by letting the first 
  
    
      
        M
      
    
    {\displaystyle M}
   elements of 
  
    
      
        
          
            
              f
              →
            
          
        
      
    
    {\displaystyle {\vec {f}}}
   be the first column of 
  
    
      
        F
      
    
    {\displaystyle F}
  , the next 
  
    
      
        M
      
    
    {\displaystyle M}
   elements the second column, etc.
A large class of image operations is linear, e.g., unitary transformations, convolutions, and linear filtering. In the quantum computing, the linear transformation can be represented as 
  
    
      
        
          |
        
        g
        ⟩
        =
        
          
            
              U
              ^
            
          
        
        
          |
        
        f
        ⟩
      
    
    {\displaystyle |g\rangle ={\hat {U}}|f\rangle }
   with the input image state 
  
    
      
        
          |
        
        f
        ⟩
      
    
    {\displaystyle |f\rangle }
   and the output image state 
  
    
      
        
          |
        
        g
        ⟩
      
    
    {\displaystyle |g\rangle }
  . A unitary transformation can be implemented as a unitary evolution. Some basic and commonly used image transforms (e.g., the Fourier, Hadamard, and Haar wavelet transforms) can be expressed in the form 
  
    
      
        G
        =
        P
        F
        Q
      
    
    {\displaystyle G=PFQ}
  , with the resulting image 
  
    
      
        G
      
    
    {\displaystyle G}
   and a row (column) transform matrix 
  
    
      
        P
        (
        Q
        )
      
    
    {\displaystyle P(Q)}
  . The corresponding unitary operator 
  
    
      
        
          
            
              U
              ^
            
          
        
      
    
    {\displaystyle {\hat {U}}}
   can then be written as 
  
    
      
        
          
            
              U
              ^
            
          
        
        =
        
          
            Q
          
          
            T
          
        
        ⊗
        
          P
        
      
    
    {\displaystyle {\hat {U}}={Q}^{T}\otimes {P}}
  . Several commonly used two-dimensional image transforms, such as the Haar wavelet, Fourier, and Hadamard transforms, are experimentally demonstrated on a quantum computer, with exponential speedup over their classical counterparts. In addition, a novel highly efficient quantum algorithm is proposed and experimentally implemented for detecting the boundary between different regions of a picture: It requires only one single-qubit gate in the processing stage, independent of the size of the picture.


== References =="
80,Geometric separator,41620938,14659,"A geometric separator is a line (or other shape) that partitions a collection of geometric shapes into two subsets, such that proportion of shapes in each subset is bounded, and the number of shapes that do not belong to any subset (i.e. the shapes intersected by the separator itself) is small.
When a geometric separator exists, it can be used for building divide-and-conquer algorithms for solving various problems in computational geometry.


== Separators that are closed shapes ==
A simple case in which a separator is guaranteed to exist is the following:

Given a set of n disjoint axis-parallel squares in the plane, there is a rectangle R such that, at most 2n/3 of the squares are inside R, at most 2n/3 of the squares are outside R, and at most O(sqrt(n)) of the squares are not inside and not outside R (i.e. intersect the boundary of R).

Thus, R is a geometric separator that separates the n squares into two subset (""inside R"" and ""outside R""), with a relatively small ""loss"" (the squares intersected by R are considered ""lost"" because they do not belong to any of the two subsets).


=== Proof ===
Define a 2-fat rectangle as an axis-parallel rectangle with an aspect ratio of at most 2.
Let R0 be a minimal-area 2-fat rectangle that contains the centers of at least n/3 squares. Thus every 2-fat rectangle smaller than R0 contains fewer than n/3 squares.
For every t in [0,1), let Rt be a 2-fat rectangle with the same center as R0, inflated by 1 + t.
Rt contains R0, so it contains the centers of at least n/3 squares.
Rt is less than twice as large as R0, so it can be covered by two 2-fat rectangles that are smaller than R0. Each of these 2-fat rectangles contains the centers of less than n/3 squares. Therefore Rt contains the centers of less than 2n/3 squares.
Now it remains to show that there is a t for which Rt intersects at most O(sqrt(n)) squares.
First, consider all the ""large squares"" – the squares whose side-length is at least 
  
    
      
        width
        ⁡
        (
        
          R
          
            0
          
        
        )
        
          /
        
        2
        
          
            n
          
        
      
    
    {\displaystyle \operatorname {width} (R_{0})/2{\sqrt {n}}}
  . For every t, the perimeter of Rt is at most 2·perimeter(R0) which is at most 6·width(R0), so it can intersect at most 
  
    
      
        12
        
          
            n
          
        
      
    
    {\displaystyle 12{\sqrt {n}}}
   large squares.
Next, consider all the ""small squares"" – the squares whose side-length is less than 
  
    
      
        width
        ⁡
        (
        
          R
          
            0
          
        
        )
        
          /
        
        2
        
          
            n
          
        
      
    
    {\displaystyle \operatorname {width} (R_{0})/2{\sqrt {n}}}
  .
For every t, define: intersect(t) as the set of small squares intersected by the boundary of Rt. For every t1 and t2, if 
  
    
      
        
          |
        
        
          t
          
            1
          
        
        −
        
          t
          
            2
          
        
        
          |
        
        ≥
        1
        
          /
        
        
          
            n
          
        
      
    
    {\displaystyle |t_{1}-t_{2}|\geq 1/{\sqrt {n}}}
  , then 
  
    
      
        
          |
        
        width
        ⁡
        (
        
          R
          
            
              t
              
                1
              
            
          
        
        )
        −
        width
        ⁡
        (
        
          R
          
            
              t
              
                2
              
            
          
        
        )
        
          |
        
        ≥
        width
        ⁡
        (
        
          R
          
            0
          
        
        )
        
          /
        
        
          
            n
          
        
      
    
    {\displaystyle |\operatorname {width} (R_{t_{1}})-\operatorname {width} (R_{t_{2}})|\geq \operatorname {width} (R_{0})/{\sqrt {n}}}
  . Therefore there is a gap of at least 
  
    
      
        width
        ⁡
        (
        
          R
          
            0
          
        
        )
        
          /
        
        2
        
          
            n
          
        
      
    
    {\displaystyle \operatorname {width} (R_{0})/2{\sqrt {n}}}
   between the boundary of Rt1 and the boundary of Rt2. Therefore, intersect(t1) and intersect(t2) are disjoint. Therefore:

  
    
      
        
          ∑
          
            j
            =
            0
          
          
            
              
                n
              
            
            −
            1
          
        
        
          
            |
          
          intersect
          ⁡
          (
          j
          
            /
          
          
            
              n
            
          
          )
          
            |
          
        
        ≤
        n
      
    
    {\displaystyle \sum _{j=0}^{{\sqrt {n}}-1}{|\operatorname {intersect} (j/{\sqrt {n}})|}\leq n}
  
Therefore by the pigeonhole principle there is a certain j0 for which:

  
    
      
        
          |
        
        intersect
        ⁡
        (
        
          j
          
            0
          
        
        
          /
        
        
          
            n
          
        
        )
        
          |
        
        ≤
        
          
            n
          
        
      
    
    {\displaystyle |\operatorname {intersect} (j_{0}/{\sqrt {n}})|\leq {\sqrt {n}}}
  
The separator we look for is the rectangle Rt, where 
  
    
      
        t
        =
        
          j
          
            0
          
        
        
          /
        
        
          
            n
          
        
      
    
    {\displaystyle t=j_{0}/{\sqrt {n}}}
  .


=== Application example ===
Using this separator theorem, we can solve certain problems in computational geometry in the following way:
Separate the input set of squares to two disjoint subsets;
Solve the problem on each subset separately;
Combine the solutions to the two sub-problems and get an approximate solution to the original problem.


=== Generalizations ===
The above theorem can be generalized in many different ways, with possibly different constants. For example:
Instead of squares, the input collection can contain arbitrary fat objects, such as: circles, rectangles with a bounded aspect ratio, etc.
Instead of two-dimensional shapes in a plane, the input collection can contain objects of any dimension, and they can be situated in a d-dimensional torus.
Instead of requiring that the shapes in the input collection be disjoint, we can put a weaker requirement, that the collection is:k-thick, i.e., each point is covered by at most k different shapes.
l-k-thick, i.e., each point is covered by at most k different shapes with a size ratio (size of largest shape divided by size of smallest shape) at most l.
k-overloaded, i.e., for any subcollection of shapes, the sum of their individual measures is at most k times the measure of their union.

Instead of a rectangle separator, the separator can be any shape that can be covered by smaller copies of itself.
Instead of bounding the number of shapes in each side of the separator, it is possible to bound any measure which satisfies certain axioms.


=== Optimality ===
The ratio of 1:2, in the square separator theorem above, is the best that can be guaranteed: there are collections of shapes that cannot be separated in a better ratio using a separator that crosses only O(sqrt(n)) shapes. Here is one such collection (from theorem 34 of ):
Consider an equilateral triangle. At each of its 3 vertices, put N/3 shapes arranged in an exponential spiral, such that the diameter increases by a constant factor every turn of the spiral, and each shape touches its neighbours in the spiral ordering. For example, start with a 1-by-Φ rectangle, where Φ is the golden ratio. Add an adjacent Φ-by-Φ square and get another golden rectangle. Add an adjacent (1+Φ)-by-(1+Φ) square and get a larger golden rectangle, and so on.
Now, in order to separate more than 1/3 of the shapes, the separator must separate O(N) shapes from two different vertices. But to do this, the separator must intersect O(N) shapes.


== Separators that are hyperplanes ==

Given a set of N=4k disjoint axis-parallel rectangles in the plane, there is a line, either horizontal or vertical, such that at least N/4 rectangles lie entirely to each side of it (thus at most N/2 rectangles are intersected by the separator line).


=== Proof ===
Define W as the most western vertical line with at least N/4 rectangles entirely to its west. There are two cases:
If there are at least N/4 rectangles entirely to the east of W, then W is a vertical separator.
Otherwise, by moving W slightly to the west, we get a vertical line that intersects more than N/2 rectangles. Find a point on this line that has at least N/4 rectangles above and N/4 rectangles below it, and draw a horizontal separator through it.


=== Optimality ===

The number of intersected shapes, guaranteed by the above theorem, is O(N). This upper bound is asymptotically tight even when the shapes are squares, as illustrated in the figure to the right. This is in sharp contrast to the upper bound of O(√N) intersected shapes, which is guaranteed when the separator is a closed shape (see previous section).

Moreover, when the shapes are arbitrary rectangles, there are cases in which no line that separates more than a single rectangle can cross less than N/4 rectangles, as illustrated in the figure to the right.


=== Generalizations ===
The above theorem can be generalized from disjoint rectangles to k-thick rectangles. Additionally, by induction on d, it is possible to generalize the above theorem to d dimensions and get the following theorem:

Given N axis-parallel d-boxes whose interiors are k-thick, there exists an axis-parallel hyperplane such that at least:

  
    
      
        ⌊
        (
        N
        +
        1
        −
        k
        )
        
          /
        
        (
        2
        d
        )
        ⌋
      
    
    {\displaystyle \lfloor (N+1-k)/(2d)\rfloor }
  

of the d-box interiors lie to each side of the hyperplane.

For the special case when k = N − 1 (i.e. each point is contained in at most N − 1 boxes), the following theorem holds:

Given N axis-parallel d-boxes whose interiors are (N − 1)-thick, there exists an axis-parallel hyperplane that separates two of them.

The objects need not be boxes, and the separators need not be axis-parallel:

Let C be a collection of possible orientations of hyperplanes (i.e. C = {horizontal,vertical}). Given N d-objects, such that every two disjoint object are separated by a hyperplane with an orientation from C, whose interiors are k-thick, there exists a hyperplane with an orientation from C such that at least: (N + 1 − k)/O(C) of the d-objects interiors lie entirely to each side of the hyperplane.


=== Algorithmic versions ===
It is possible to find the hyperplanes guaranteed by the above theorems in O(Nd) steps. Also, if the 2d lists of the lower and upper endpoints of the intervals defining the boxes's ith coordinates are pre-sorted, then the best such hyperplane (according to a wide variety of optimality measures) may be found in O(Nd) steps.


== Separators that are width-bounded strips between parallel hyperplanes ==

Let Q be a set of n points in the plane such that the minimal distance between points is d. Let a>0 be a constant.
There is a pair of parallel lines of distance a, such that at most 2n/3 points lie to each side of the strip, and at most 
  
    
      
        1.3
        
          
            a
            d
          
        
        
          
            n
          
        
      
    
    {\displaystyle 1.3{a \over d}{\sqrt {n}}}
   points lie inside the strip.
Equivalently: there is a line such that at most 2n/3 points lie to each side of it and at most 
  
    
      
        1.3
        
          
            a
            d
          
        
        
          
            n
          
        
      
    
    {\displaystyle 1.3{a \over d}{\sqrt {n}}}
   points lie at a distance of less than a/2 from it.


=== Proof sketch ===
Define the centerpoint of Q as a point o such that every line through it has at most 2n/3 points of Q in each side of it. The existence of a centerpoint can be proved using Helly's theorem.
For a given point p and constant a>0, define Pr(a,p,o) as the probability that a random line through o lies at a distance of less than a from p. The idea is to bound this probability and thus bound the expected number of points at a distance less than a from a random line through o. Then, by the pigeonhole principle, at least one line through o is the desired separator.


=== Applications ===
Bounded-width separators can be used for approximately solving the protein folding problem. It can also be used for an exact sub-exponential algorithm to find a maximum independent set, as well as several related covering problems, in geometric graphs.


== Geometric separators and planar graph separators ==
The planar separator theorem may be proven by using the circle packing theorem to represent a planar graph as the contact graph of a system of disks in the plane, and then by finding a circle that forms a geometric separator for those disks.


== See also ==
Ham sandwich theorem: given n measurable objects in n-dimensional space, it is possible to divide all of them in half (with respect to their measure, i.e. volume) with a single (n − 1)-dimensional hyperplane.
Other Separation theorems.
Simultaneous separator: a separator that simultaneously separates the shapes in several collections, while simultaneously intersecting a small number of shapes in each collection, may not always exist.


== Notes =="
81,Viola–Jones object detection framework,14669989,14595,"The Viola–Jones object detection framework is the first object detection framework to provide competitive object detection rates in real-time proposed in 2001 by Paul Viola and Michael Jones. Although it can be trained to detect a variety of object classes, it was motivated primarily by the problem of face detection.


== Problem description ==
The problem to be solved is detection of faces in an image. A human can do this easily, but a computer needs precise instructions and constraints. To make the task more manageable, Viola–Jones requires full view frontal upright faces. Thus in order to be detected, the entire face must point towards the camera and should not be tilted to either side. While it seems these constraints could diminish the algorithm's utility somewhat, because the detection step is most often followed by a recognition step, in practice these limits on pose are quite acceptable.


== Components of the framework ==


=== Feature types and evaluation ===
The characteristics of Viola–Jones algorithm which make it a good detection algorithm are:
Robust – very high detection rate (true-positive rate) & very low false-positive rate always.
Real time – For practical applications at least 2 frames per second must be processed.
Face detection only (not recognition) - The goal is to distinguish faces from non-faces (detection is the first step in the recognition process).
The algorithm has four stages:
Haar Feature Selection
Creating an Integral Image
Adaboost Training
Cascading Classifiers
The features sought by the detection framework universally involve the sums of image pixels within rectangular areas. As such, they bear some resemblance to Haar basis functions, which have been used previously in the realm of image-based object detection. However, since the features used by Viola and Jones all rely on more than one rectangular area, they are generally more complex. The figure on the right illustrates the four different types of features used in the framework. The value of any given feature is the sum of the pixels within clear rectangles subtracted from the sum of the pixels within shaded rectangles. Rectangular features of this sort are primitive when compared to alternatives such as steerable filters. Although they are sensitive to vertical and horizontal features, their feedback is considerably coarser.


==== Haar Features ====
All human faces share some similar properties. These regularities may be matched using Haar Features.
A few properties common to human faces:
The eye region is darker than the upper-cheeks.
The nose bridge region is brighter than the eyes.
Composition of properties forming matchable facial features:
Location and size: eyes, mouth, bridge of nose
Value: oriented gradients of pixel intensities
The four features matched by this algorithm are then sought in the image of a face (shown at right).
Rectangle features:
Value = Σ (pixels in black area) - Σ (pixels in white area)
Three types: two-, three-, four-rectangles, Viola & Jones used two-rectangle features
For example: the difference in brightness between the white &black rectangles over a specific area
Each feature is related to a special location in the sub-window


==== Summed area table ====
An image representation called the integral image evaluates rectangular features in constant time, which gives them a considerable speed advantage over more sophisticated alternative features. Because each feature's rectangular area is always adjacent to at least one other rectangle, it follows that any two-rectangle feature can be computed in six array references, any three-rectangle feature in eight, and any four-rectangle feature in nine.


=== Learning algorithm ===
The speed with which features may be evaluated does not adequately compensate for their number, however. For example, in a standard 24x24 pixel sub-window, there are a total of M = 162,336 possible features, and it would be prohibitively expensive to evaluate them all when testing an image. Thus, the object detection framework employs a variant of the learning algorithm AdaBoost to both select the best features and to train classifiers that use them. This algorithm constructs a “strong” classifier as a linear combination of weighted simple “weak” classifiers.

  
    
      
        h
        (
        
          x
        
        )
        =
        sgn
        ⁡
        
          (
          
            
              ∑
              
                j
                =
                1
              
              
                M
              
            
            
              α
              
                j
              
            
            
              h
              
                j
              
            
            (
            
              x
            
            )
          
          )
        
      
    
    {\displaystyle h(\mathbf {x} )=\operatorname {sgn} \left(\sum _{j=1}^{M}\alpha _{j}h_{j}(\mathbf {x} )\right)}
  
Each weak classifier is a threshold function based on the feature 
  
    
      
        
          f
          
            j
          
        
      
    
    {\displaystyle f_{j}}
  .

  
    
      
        
          h
          
            j
          
        
        (
        
          x
        
        )
        =
        
          
            {
            
              
                
                  −
                  
                    s
                    
                      j
                    
                  
                
                
                  
                    if 
                  
                  
                    f
                    
                      j
                    
                  
                  <
                  
                    θ
                    
                      j
                    
                  
                
              
              
                
                  
                    s
                    
                      j
                    
                  
                
                
                  
                    otherwise
                  
                
              
            
            
          
        
      
    
    {\displaystyle h_{j}(\mathbf {x} )={\begin{cases}-s_{j}&{\text{if }}f_{j}<\theta _{j}\\s_{j}&{\text{otherwise}}\end{cases}}}
  
The threshold value 
  
    
      
        
          θ
          
            j
          
        
      
    
    {\displaystyle \theta _{j}}
   and the polarity 
  
    
      
        
          s
          
            j
          
        
        ∈
        ±
        1
      
    
    {\displaystyle s_{j}\in \pm 1}
   are determined in the training, as well as the coefficients 
  
    
      
        
          α
          
            j
          
        
      
    
    {\displaystyle \alpha _{j}}
  .
Here a simplified version of the learning algorithm is reported:
Input: Set of N positive and negative training images with their labels 
  
    
      
        
          (
          
            
              x
            
            
              i
            
          
          ,
          
            y
            
              i
            
          
          )
        
      
    
    {\displaystyle {(\mathbf {x} ^{i},y^{i})}}
  . If image i is a face 
  
    
      
        
          y
          
            i
          
        
        =
        1
      
    
    {\displaystyle y^{i}=1}
  , if not 
  
    
      
        
          y
          
            i
          
        
        =
        −
        1
      
    
    {\displaystyle y^{i}=-1}
  .
Initialization: assign a weight 
  
    
      
        
          w
          
            1
          
          
            i
          
        
        =
        
          
            1
            N
          
        
      
    
    {\displaystyle w_{1}^{i}={\frac {1}{N}}}
   to each image i.
For each feature 
  
    
      
        
          f
          
            j
          
        
      
    
    {\displaystyle f_{j}}
   with 
  
    
      
        j
        =
        1
        ,
        .
        .
        .
        ,
        M
      
    
    {\displaystyle j=1,...,M}
  
Renormalize the weights such that they sum to one.
Apply the feature to each image in the training set, then find the optimal threshold and polarity 
  
    
      
        
          θ
          
            j
          
        
        ,
        
          s
          
            j
          
        
      
    
    {\displaystyle \theta _{j},s_{j}}
   that minimizes the weighted classification error. That is 
  
    
      
        
          θ
          
            j
          
        
        ,
        
          s
          
            j
          
        
        =
        arg
        ⁡
        
          min
          
            θ
            ,
            s
          
        
        
        
          ∑
          
            i
            =
            1
          
          
            N
          
        
        
          w
          
            j
          
          
            i
          
        
        
          ε
          
            j
          
          
            i
          
        
      
    
    {\displaystyle \theta _{j},s_{j}=\arg \min _{\theta ,s}\;\sum _{i=1}^{N}w_{j}^{i}\varepsilon _{j}^{i}}
   where 
  
    
      
        
          ε
          
            j
          
          
            i
          
        
        =
        
          
            {
            
              
                
                  0
                
                
                  
                    if 
                  
                  
                    y
                    
                      i
                    
                  
                  =
                  
                    h
                    
                      j
                    
                  
                  (
                  
                    
                      x
                    
                    
                      i
                    
                  
                  ,
                  
                    θ
                    
                      j
                    
                  
                  ,
                  
                    s
                    
                      j
                    
                  
                  )
                
              
              
                
                  1
                
                
                  
                    otherwise
                  
                
              
            
            
          
        
      
    
    {\displaystyle \varepsilon _{j}^{i}={\begin{cases}0&{\text{if }}y^{i}=h_{j}(\mathbf {x} ^{i},\theta _{j},s_{j})\\1&{\text{otherwise}}\end{cases}}}
  
Assign a weight 
  
    
      
        
          α
          
            j
          
        
      
    
    {\displaystyle \alpha _{j}}
   to 
  
    
      
        
          h
          
            j
          
        
      
    
    {\displaystyle h_{j}}
   that is inversely proportional to the error rate. In this way best classifiers are considered more.
The weights for the next iteration, i.e. 
  
    
      
        
          w
          
            j
            +
            1
          
          
            i
          
        
      
    
    {\displaystyle w_{j+1}^{i}}
  , are reduced for the images i that were correctly classified.

Set the final classifier to 
  
    
      
        h
        (
        
          x
        
        )
        =
        sgn
        ⁡
        
          (
          
            
              ∑
              
                j
                =
                1
              
              
                M
              
            
            
              α
              
                j
              
            
            
              h
              
                j
              
            
            (
            
              x
            
            )
          
          )
        
      
    
    {\displaystyle h(\mathbf {x} )=\operatorname {sgn} \left(\sum _{j=1}^{M}\alpha _{j}h_{j}(\mathbf {x} )\right)}
  


=== Cascade architecture ===
On average only 0.01% of all sub-windows are positive (faces)
Equal computation time is spent on all sub-windows
Must spend most time only on potentially positive sub-windows.
A simple 2-feature classifier can achieve almost 100% detection rate with 50% FP rate.
That classifier can act as a 1st layer of a series to filter out most negative windows
2nd layer with 10 features can tackle “harder” negative-windows which survived the 1st layer, and so on…
A cascade of gradually more complex classifiers achieves even better detection rates. The evaluation of the strong classifiers generated by the learning process can be done quickly, but it isn’t fast enough to run in real-time. For this reason, the strong classifiers are arranged in a cascade in order of complexity, where each successive classifier is trained only on those selected samples which pass through the preceding classifiers. If at any stage in the cascade a classifier rejects the sub-window under inspection, no further processing is performed and continue on searching the next sub-window. The cascade therefore has the form of a degenerate tree. In the case of faces, the first classifier in the cascade – called the attentional operator – uses only two features to achieve a false negative rate of approximately 0% and a false positive rate of 40%. The effect of this single classifier is to reduce by roughly half the number of times the entire cascade is evaluated.
In cascading, each stage consists of a strong classifier. So all the features are grouped into several stages where each stage has certain number of features.
The job of each stage is to determine whether a given sub-window is definitely not a face or may be a face. A given sub-window is immediately discarded as not a face if it fails in any of the stages.
A simple framework for cascade training is given below:
f = the maximum acceptable false positive rate per layer.
d = the minimum acceptable detection rate per layer.
Ftarget = target overall false positive rate.
P = set of positive examples.
N = set of negative examples.

F(0) = 1.0; D(0) = 1.0; i = 0

while F(i) > Ftarget
    increase i
    n(i) = 0; F(i)= F(i-1)

    while F(i) > f × F(i-1)
      increase n(i)
      use P and N to train a classifier with n(I) features using AdaBoost
      Evaluate current cascaded classifier on validation set to determine F(i) and D(i)
      decrease threshold for the ith classifier 
        until the current cascaded classifier has a detection rate of at least d × D(i-1) (this also affects F(i))
      N = ∅
      if F(i) > Ftarget then 
        evaluate the current cascaded detector on the set of non-face images 
        and put any false detections into the set N.

The cascade architecture has interesting implications for the performance of the individual classifiers. Because the activation of each classifier depends entirely on the behavior of its predecessor, the false positive rate for an entire cascade is:

  
    
      
        F
        =
        
          ∏
          
            i
            =
            1
          
          
            K
          
        
        
          f
          
            i
          
        
        .
      
    
    {\displaystyle F=\prod _{i=1}^{K}f_{i}.}
  
Similarly, the detection rate is:

  
    
      
        D
        =
        
          ∏
          
            i
            =
            1
          
          
            K
          
        
        
          d
          
            i
          
        
        .
      
    
    {\displaystyle D=\prod _{i=1}^{K}d_{i}.}
  
Thus, to match the false positive rates typically achieved by other detectors, each classifier can get away with having surprisingly poor performance. For example, for a 32-stage cascade to achieve a false positive rate of 10−6, each classifier need only achieve a false positive rate of about 65%. At the same time, however, each classifier needs to be exceptionally capable if it is to achieve adequate detection rates. For example, to achieve a detection rate of about 90%, each classifier in the aforementioned cascade needs to achieve a detection rate of approximately 99.7%.


== Using Viola-Jones for object tracking ==
In videos of moving objects, one need not apply object detection to each frame. Instead, one can use tracking algorithms like the KLT algorithm to detect salient features within the detection bounding boxes and track their movement between frames. Not only does this improve tracking speed by removing the need to re-detect objects in each frame, but it improves the robustness as well, as the salient features are more resilient than the Viola-Jones detection framework to rotation and photometric changes.


== References ==


== External links ==
Slides Presenting the Framework
Information Regarding Haar Basis Functions
Extension of Viola–Jones framework using SURF feature
IMMI - Rapidminer Image Mining Extension - open-source tool for image mining
Robust Real-Time Face Detection
An improved algorithm on Viola-Jones object detector
Citations of the Viola–Jones algorithm in Google Scholar
Video lecture on Viola–Jones algorithm on YouTube - Adaboost Explanation from ppt by Qing Chen, Discovery Labs, University of Ottawa and a video lecture by Ramsri Goutham.


=== Implementations ===
Implementing the Viola–Jones Face Detection Algorithm by Ole Helvig Jensen
MATLAB: [1], [2]
OpenCV: implemented as cvHaarDetectObjects().
Haar Cascade Detection in OpenCV
Cascade Classifier Training in OpenCV"
82,String interpolation,30925309,14460,"In computer programming, string interpolation (or variable interpolation, variable substitution, or variable expansion) is the process of evaluating a string literal containing one or more placeholders, yielding a result in which the placeholders are replaced with their corresponding values. It is a form of simple template processing or, in formal terms, a form of quasi-quotation (or logic substitution interpretation). String interpolation allows easier and more intuitive string formatting and content-specification compared with string concatenation.
String interpolation is common in many programming languages which make heavy use of string representations of data, such as Apache Groovy, Kotlin, Perl, PHP, Python, Ruby, Scala, and Swift, and most Unix shells. Two modes of literal expression are usually offered: one with interpolation enabled, the other without (termed raw string). Placeholders are usually represented by a bare or a named sigil (typically $ or %), e.g. $placeholder or %123. Expansion of the string usually occurs at run time.


== Variations ==
Some languages do not offer string interpolation, instead offering a standard function where one parameter is the printf format string, and other(s) provide the values for each placeholder.
Ruby uses the # symbol for interpolation, and allows interpolating any expression, not only variables. Other languages may support more advanced interpolation with a special formatting function, such as printf, in which the first argument, the format, specifies the pattern in which the remaining arguments are substituted.


== Algorithms ==
There are two main types of expand variable algorithms for variable interpolation:
Replace and expand placeholders: creating a new string from the original one, by find-replace operations. Find variable-reference (placeholder), replace it by its variable-value. This algorithm offers no cache strategy.
Split and join string: splitting the string into an array, and merging it with the corresponding array of values; then join items by concatenation. The split string can be cached to reuse.


== Security issues ==
String interpolation, like string concatenation, may lead to security problems. If user input data is improperly escaped or filtered, the system will be exposed to SQL injection, script injection, XML External Entity Injection (XXE), and cross-site scripting (XSS) attacks.
An SQL injection example:

query = ""SELECT x, y, z FROM Table WHERE id='$id' ""

If $id is replaced with ""'; DELETE FROM Table; SELECT * FROM Table WHERE id='"", executing this query will wipe out all the data in Table.


== Examples ==
The following Perl code works identically in PHP:

produces the output: Alice said Hello World to the crowd of people.


=== Bash ===

The output will be:


=== Boo ===

The output will be:


=== C# ===

The output will be:


=== ColdFusion Markup Language ===

ColdFusion Markup Language (CFML) script syntax:

Tag syntax:

The output will be:

I have 4 apples


=== CoffeeScript ===

The output will be:


=== Dart ===

The output will be:


=== Groovy ===

In groovy, interpolated strings are known as GStrings:

The output will be:


=== Haxe ===

The output will be:


=== JavaScript ===

JavaScript, as of the ECMAScript 2015 (ES6) standard, supports string interpolation using backticks ``. This feature is called template literals. Here is an example:

The output will be:


=== Kotlin ===

The output will be:


=== Nemerle ===

It also supports advanced formatting features, such as:

The output will be:


=== Perl ===

The output will be:


=== PHP ===

The output will be:


=== Python ===

The output will be:


=== Ruby / Crystal ===

The output will be:


=== Rust ===

Rust provides string interpolation via the std::fmt module, which is interfaced with through various macros such as format!, write!, and print!. These macros are converted into Rust source code at compile-time, whereby each argument interacts with a formatter. The formatter supports positional parameters, named parameters, argument types, and defining various formatting traits.

The output of each of these will be:

There are 4 apples and 3 bananas.


=== Scala ===

Scala 2.10+ has implemented the following string interpolators: s, f and raw. It is also possible to write custom ones or override the standard ones.
The f interpolator is a compiler macro that rewrites a format string with embedded expressions as an invocation of String.format. It verifies that the format string is well-formed and well-typed.


==== The standard interpolators ====
Scala 2.10+'s string interpolation allows embedding variable references directly in processed string literals. Here is an example:

 The output will be:


=== Sciter (tiscript) ===

In Sciter any function with name starting from $ is considered as interpolating function and so interpolation is customizable and context sensitive:

Where

gets compiled to this:


=== Swift ===

In Swift, a new String value can be created from a mix of constants, variables, literals, and expressions by including their values inside a string literal. Each item inserted into the string literal is wrapped in a pair of parentheses, prefixed by a backslash.

The output will be:


=== TypeScript ===

As of version 1.4, TypeScript supports string interpolation using backticks ``. Here is an example:

The output will be:

The console.log function can be used as a printf function. The above example can be rewritten, thusly:

The output remains the same.


== See also ==
Concatenation
Improper input validation
printf format string
Quasi-quotation
String literal


== Notes =="
83,Computational geometry,176927,14036,"Computational geometry is a branch of computer science devoted to the study of algorithms which can be stated in terms of geometry. Some purely geometrical problems arise out of the study of computational geometric algorithms, and such problems are also considered to be part of computational geometry. While modern computational geometry is a recent development, it is one of the oldest fields of computing with history stretching back to antiquity.
Computational complexity is central to computational geometry, with great practical significance if algorithms are used on very large datasets containing tens or hundreds of millions of points. For such sets, the difference between O(n2) and O(n log n) may be the difference between days and seconds of computation.
The main impetus for the development of computational geometry as a discipline was progress in computer graphics and computer-aided design and manufacturing (CAD/CAM), but many problems in computational geometry are classical in nature, and may come from mathematical visualization.
Other important applications of computational geometry include robotics (motion planning and visibility problems), geographic information systems (GIS) (geometrical location and search, route planning), integrated circuit design (IC geometry design and verification), computer-aided engineering (CAE) (mesh generation), computer vision (3D reconstruction).
The main branches of computational geometry are:
Combinatorial computational geometry, also called algorithmic geometry, which deals with geometric objects as discrete entities. A groundlaying book in the subject by Preparata and Shamos dates the first use of the term ""computational geometry"" in this sense by 1975.
Numerical computational geometry, also called machine geometry, computer-aided geometric design (CAGD), or geometric modeling, which deals primarily with representing real-world objects in forms suitable for computer computations in CAD/CAM systems. This branch may be seen as a further development of descriptive geometry and is often considered a branch of computer graphics or CAD. The term ""computational geometry"" in this meaning has been in use since 1971.


== Combinatorial computational geometry ==
The primary goal of research in combinatorial computational geometry is to develop efficient algorithms and data structures for solving problems stated in terms of basic geometrical objects: points, line segments, polygons, polyhedra, etc.
Some of these problems seem so simple that they were not regarded as problems at all until the advent of computers. Consider, for example, the Closest pair problem:
Given n points in the plane, find the two with the smallest distance from each other.
One could compute the distances between all the pairs of points, of which there are n(n-1)/2, then pick the pair with the smallest distance. This brute-force algorithm takes O(n2) time; i.e. its execution time is proportional to the square of the number of points. A classic result in computational geometry was the formulation of an algorithm that takes O(n log n). Randomized algorithms that take O(n) expected time, as well as a deterministic algorithm that takes O(n log log n) time, have also been discovered.


=== Problem classes ===
The core problems in computational geometry may be classified in different ways, according to various criteria. The following general classes may be distinguished.


==== Static problems ====
In the problems of this category, some input is given and the corresponding output needs to be constructed or found. Some fundamental problems of this type are:
Convex hull: Given a set of points, find the smallest convex polyhedron/polygon containing all the points.
Line segment intersection: Find the intersections between a given set of line segments.
Delaunay triangulation
Voronoi diagram: Given a set of points, partition the space according to which points are closest to the given points.
Linear programming
Closest pair of points: Given a set of points, find the two with the smallest distance from each other.
Largest empty circle: Given a set of points, find a largest circle with its center inside of their convex hull and enclosing none of them.
Euclidean shortest path: Connect two points in a Euclidean space (with polyhedral obstacles) by a shortest path.
Polygon triangulation: Given a polygon, partition its interior into triangles
Mesh generation
Boolean operations on polygons
The computational complexity for this class of problems is estimated by the time and space (computer memory) required to solve a given problem instance.


==== Geometric query problems ====
In geometric query problems, commonly known as geometric search problems, the input consists of two parts: the search space part and the query part, which varies over the problem instances. The search space typically needs to be preprocessed, in a way that multiple queries can be answered efficiently.
Some fundamental geometric query problems are:
Range searching: Preprocess a set of points, in order to efficiently count the number of points inside a query region.
Point location: Given a partitioning of the space into cells, produce a data structure that efficiently tells in which cell a query point is located.
Nearest neighbor: Preprocess a set of points, in order to efficiently find which point is closest to a query point.
Ray tracing: Given a set of objects in space, produce a data structure that efficiently tells which object a query ray intersects first.
If the search space is fixed, the computational complexity for this class of problems is usually estimated by:
the time and space required to construct the data structure to be searched in
the time (and sometimes an extra space) to answer queries.
For the case when the search space is allowed to vary, see ""Dynamic problems"".


==== Dynamic problems ====
Yet another major class is the dynamic problems, in which the goal is to find an efficient algorithm for finding a solution repeatedly after each incremental modification of the input data (addition or deletion input geometric elements). Algorithms for problems of this type typically involve dynamic data structures. Any of the computational geometric problems may be converted into a dynamic one, at the cost of increased processing time. For example, the range searching problem may be converted into the dynamic range searching problem by providing for addition and/or deletion of the points. The dynamic convex hull problem is to keep track of the convex hull, e.g., for the dynamically changing set of points, i.e., while the input points are inserted or deleted.
The computational complexity for this class of problems is estimated by:
the time and space required to construct the data structure to be searched in
the time and space to modify the searched data structure after an incremental change in the search space
the time (and sometimes an extra space) to answer a query.


==== Variations ====
Some problems may be treated as belonging to either of the categories, depending on the context. For example, consider the following problem.
Point in polygon: Decide whether a point is inside or outside a given polygon.
In many applications this problem is treated as a single-shot one, i.e., belonging to the first class. For example, in many applications of computer graphics a common problem is to find which area on the screen is clicked by a pointer. However, in some applications, the polygon in question is invariant, while the point represents a query. For example, the input polygon may represent a border of a country and a point is a position of an aircraft, and the problem is to determine whether the aircraft violated the border. Finally, in the previously mentioned example of computer graphics, in CAD applications the changing input data are often stored in dynamic data structures, which may be exploited to speed-up the point-in-polygon queries.
In some contexts of query problems there are reasonable expectations on the sequence of the queries, which may be exploited either for efficient data structures or for tighter computational complexity estimates. For example, in some cases it is important to know the worst case for the total time for the whole sequence of N queries, rather than for a single query. See also ""amortized analysis"".


== Numerical computational geometry ==

This branch is also known as geometric modelling and computer-aided geometric design (CAGD).
Core problems are curve and surface modelling and representation.
The most important instruments here are parametric curves and parametric surfaces, such as Bézier curves, spline curves and surfaces. An important non-parametric approach is the level set method.
Application areas of computational geometry include shipbuilding, aircraft, and automotive industries.


== See also ==
List of combinatorial computational geometry topics
List of numerical computational geometry topics
CAD/CAM/CAE
List of geometric algorithms
Solid modeling
Computational topology
Digital geometry
Discrete geometry (combinatorial geometry)
Space partitioning
Tricomplex number
Wikiversity:Topic:Computational geometry
Wikiversity:Computer-aided geometric design


== References ==


== Further reading ==
List of books in computational geometry


=== Journals ===


==== Combinatorial/algorithmic computational geometry ====
Below is the list of the major journals that have been publishing research in geometric algorithms. Please notice with the appearance of journals specifically dedicated to computational geometry, the share of geometric publications in general-purpose computer science and computer graphics journals decreased.
ACM Computing Surveys
ACM Transactions on Graphics
Acta Informatica
Advances in Geometry
Algorithmica
Ars Combinatoria
Computational Geometry: Theory and Applications
Communications of the ACM
Computer Aided Geometric Design
Computer Graphics and Applications
Computer Graphics World
Discrete & Computational Geometry
Geombinatorics
Geometriae Dedicata
IEEE Transactions on Graphics
IEEE Transactions on Computers
IEEE Transactions on Pattern Analysis and Machine Intelligence
Information Processing Letters
International Journal of Computational Geometry and Applications
Journal of Combinatorial Theory, series B
Journal of Computational Geometry
Journal of Differential Geometry
Journal of the ACM
Journal of Algorithms
Journal of Computer and System Sciences
Management Science
Pattern Recognition
Pattern Recognition Letters
SIAM Journal on Computing
SIGACT News; featured the ""Computational Geometry Column"" by Joseph O'Rourke
Theoretical Computer Science
The Visual Computer


== External links ==
Computational Geometry
Computational Geometry Pages
Geometry In Action
""Strategic Directions in Computational Geometry—Working Group Report"" (1996)
Journal of Computational Geometry
(Annual) Winter School on Computational Geometry"
84,Dijkstra Prize,7350883,14017,"The Edsger W. Dijkstra Paper Prize in Distributed Computing is given for outstanding papers on the principles of distributed computing, whose significance and impact on the theory and/or practice of distributed computing has been evident for at least a decade. The paper prize has been presented annually since 2000.
Originally the paper prize was presented at the ACM Symposium on Principles of Distributed Computing (PODC), and it was known as the PODC Influential-Paper Award. It was renamed in honor of Edsger W. Dijkstra in 2003, after he received the award for his work in self-stabilization in 2002 and died shortly thereafter.
Since 2007, the paper prize is sponsored jointly by PODC and the EATCS International Symposium on Distributed Computing (DISC), and the presentation takes place alternately at PODC (even years) and DISC (odd years). The paper prize includes an award of $2000.


== Winners ==


== Funding ==
The award is financed by ACM PODC and EATCS DISC, each providing an equal share of $1,000 towards the $2,000 of the award.
The PODC share is financed by an endowment at ACM that is based on gifts from the ACM Special Interest Group on Algorithms and Computation Theory (SIGACT), the ACM Special Interest Group on Operating Systems (SIGOPS), the AT&T Corporation, the Hewlett-Packard Company, the International Business Machines (IBM) Corporation, the Intel Corporation, and Sun Microsystems, Inc.
The DISC share is financed by an endowment at EATCS that is based on contributions from several year's DISC budgets, and gifts from Microsoft Research, the Universidad Rey Juan Carlos and the Spanish Ministry of Science and Innovation.


== See also ==
List of important publications in concurrent, parallel, and distributed computing


== Notes ==


== References ==
EATCS web site: Awards: Dijkstra Prize.
PODC web site: Edsger W. Dijkstra Prize in Distributed Computing.
DISC web site: Edsger W. Dijkstra Prize in Distributed Computing."
85,Astroinformatics,28326718,13984,"Astroinformatics is an interdisciplinary field of study involving the combination of astronomy, data science, informatics, and information/communications technologies.


== Background ==
Astroinformatics is primarily focused on developing the tools, methods, and applications of computational science, data science, and statistics for research and education in data-oriented astronomy. Early efforts in this direction included data discovery, metadata standards development, data modeling, astronomical data dictionary development, data access, information retrieval, data integration, and data mining in the astronomical Virtual Observatory initiatives. Further development of the field, along with astronomy community endorsement, was presented to the National Research Council (United States) in 2009 in the Astroinformatics ""State of the Profession"" Position Paper for the 2010 Astronomy and Astrophysics Decadal Survey. That position paper provided the basis for the subsequent more detailed exposition of the field in the Informatics Journal paper Astroinformatics: Data-Oriented Astronomy Research and Education.
Astroinformatics as a distinct field of research was inspired by work in the fields of Bioinformatics and Geoinformatics, and through the eScience work of Jim Gray (computer scientist) at Microsoft Research, whose legacy was remembered and continued through the Jim Gray eScience Awards.
Though the primary focus of Astroinformatics is on the large worldwide distributed collection of digital astronomical databases, image archives, and research tools, the field recognizes the importance of legacy data sets as well—using modern technologies to preserve and analyze historical astronomical observations. Some Astroinformatics practitioners help to digitize historical and recent astronomical observations and images in a large database for efficient retrieval through web-based interfaces. Another aim is to help develop new methods and software for astronomers, as well as to help facilitate the process and analysis of the rapidly growing amount of data in the field of astronomy.
Astroinformatics is described as the Fourth Paradigm of astronomical research. There are many research areas involved with astroinformatics, such as data mining, machine learning, statistics, visualization, scientific data management, and semantic science. Data mining and machine learning play significant roles in Astroinformatics as a scientific research discipline due to their focus on ""knowledge discovery from data"" (KDD) and ""learning from data"".
The amount of data collected from astronomical sky surveys has grown from gigabytes to terabytes throughout the past decade and is predicted to grow in the next decade into hundreds of petabytes with the Large Synoptic Survey Telescope and into the exabytes with the Square Kilometre Array. This plethora of new data both enables and challenges effective astronomical research. Therefore, new approaches are required. In part due to this, data-driven science is becoming a recognized academic discipline. Consequently, astronomy (and other scientific disciplines) are developing information-intensive and data-intensive sub-disciplines to an extent that these sub-disciplines are now becoming (or have already become) standalone research disciplines and full-fledged academic programs. While many institutes of education do not boast an astroinformatics program, such programs most likely will be developed in the near future.
Informatics has been recently defined as ""the use of digital data, information, and related services for research and knowledge generation"". However the usual, or commonly used definition is ""informatics is the discipline of organizing, accessing, integrating, and mining data from multiple sources for discovery and decision support."" Therefore, the discipline of astroinformatics includes many naturally-related specialties including data modeling, data organization, etc. It may also include transformation and normalization methods for data integration and information visualization, as well as knowledge extraction, indexing techniques, information retrieval and data mining methods. Classification schemes (e.g., taxonomies, ontologies, folksonomies, and/or collaborative tagging) plus Astrostatistics will also be heavily involved. Citizen science projects (such as Galaxy Zoo) also contribute highly valued novelty discovery, feature meta-tagging, and object characterization within large astronomy data sets. All of these specialties enable scientific discovery across varied massive data collections, collaborative research, and data re-use, in both research and learning environments.
In 2012, two position papers were presented to the Council of the American Astronomical Society that led to the establishment of formal working groups in Astroinformatics and Astrostatistics for the profession of astronomy within the USA and elsewhere.
Astroinformatics provides a natural context for the integration of education and research. The experience of research can now be implemented within the classroom to establish and grow Data Literacy through the easy re-use of data. It also has many other uses, such as repurposing archival data for new projects, literature-data links, intelligent retrieval of information, and many others.


== Conferences ==


== See also ==
Astronomy and Computing
Astrophysics Data System
Astrophysics Source Code Library
Astrostatistics
Galaxy Zoo
International Astrostatistics Association
International Virtual Observatory Alliance (IVOA)
MilkyWay@home
Virtual Observatory
WorldWide Telescope
Zooniverse


== External links ==
Astronomical Data Analysis Software and Systems (ADASS)
Astrostatistics and Astroinformatics Portal
Cosmostatistics Initiative (COIN)
Astroinformatics and Astrostatistics Commission of the International Astronomical Union


== References =="
86,Computer Pioneer Award,24659135,13587,"The Computer Pioneer Award was established in 1981 by the Board of Governors of the IEEE Computer Society to recognize and honor the vision of those people whose efforts resulted in the creation and continued vitality of the computer industry. The award is presented to outstanding individuals whose main contribution to the concepts and development of the computer field was made at least fifteen years earlier. The recognition is engraved on a silver medal specially struck for the Society.
All members of the profession are invited to nominate a colleague who they consider most eligible to be considered for this award. The nomination deadline is the 15 October of each year.
The award has two type of recipients:
Computer Pioneer Charter Recipients - At the inauguration of this award the individuals who already meet the Computer Pioneer Award criteria, and also have received IEEE Computer Society awards prior to 1981.
Computer Pioneer Recipients - Awarded annually since 1981.


== Computer Pioneer Charter Recipients ==
Howard H. Aiken - Large-Scale Automatic Computation
Samuel N. Alexander - SEAC
Gene M. Amdahl - Large-Scale Computer Architecture
John W. Backus - FORTRAN
Robert S. Barton - Language-Directed Architecture
C. Gordon Bell - Computer Design
Frederick P. Brooks, Jr. - Compatible Computer Family System/IBM 360
Wesley A. Clark - First Personal Computer
Fernando J. Corbato - Timesharing
Seymour R. Cray - Scientific Computer Systems
Edsger W. Dijkstra - Multiprogramming Control
J. Presper Eckert - First All-Electronic Computer: ENIAC
Jay W. Forrester - First Large-Scale Coincident Current Memory
Herman H. Goldstine - Contributions to Early Computer Design
Richard W. Hamming - Error-correcting code
Jean A. Hoerni - Planar Semiconductor Manufacturing Process
Grace M. Hopper - Automatic Programming
Alston S. Householder - Numerical Methods
David A. Huffman - Sequential Circuit Design
Kenneth E. Iverson - APL
Tom Kilburn - Paging Computer Design
Donald E. Knuth - Science of Computer Algorithms
Herman Lukoff - Early Electronic Computer Circuits
John W. Mauchly - First All-Electronic Computer: ENIAC
Gordon E. Moore - Integrated Circuit Production Technology
Allen Newell - Contributions to Artificial Intelligence
Robert N. Noyce - Integrated Circuit Production Technology
Lawrence G. Roberts - Packet Switching
George R. Stibitz - First Remote Computation
Shmuel Winograd - Efficiency of Computational Algorithms
Maurice V. Wilkes - Microprogramming
Konrad Zuse - First Process Control Computer
See external list of Computer Pioneer Charter Recipients


== Computer Pioneer Recipients ==
Source: IEEE Computer Society


== Nomination process ==
Nomination process


== See also ==
List of prizes, medals and awards
Prizes named after people


== External links ==
IEEE Computer Society Awards
IEEE Computer Pioneer Award"
87,Hack Reactor,47132182,13476,"Hack Reactor is a 12-week software engineering Coding Bootcamp  education program founded in San Francisco by Anthony Phillips, Shawn Drost, Marcus Phillips, and Douglas Calhoun in 2012.
Cofounder Drost has described the program as, ""optimized for people who want to be software engineers as their main, day-to-day work. Their life's work."" The curriculum focuses on JavaScript and associated technologies including the MEAN stack, React and Backbone.
In 2015 Hack Reactor acquired Austin-based MakerSquare as ""their first deal in a plan to develop a network of coding bootcamps"" in an effort to ""make a large dent in transforming the old education system into one that focuses on student outcomes."" The following month, a pair of Hack Reactor alumni partnered with the company to open Telegraph Academy ""to teach software engineering to under-represented minorities"" and create a ""growing community of diverse software engineers."" In November 2016, Hack Reactor rebranded all of its schools to share the Hack Reactor name.


== Admissions process ==
Hack Reactor’s admissions process consists of a simple coding challenge, followed by a technical interview. The coding challenge focuses on basic JavaScript concepts, such as objects, arrays and functions. The technical interview is more involved and tests both technical skills and soft skills, such as the student’s willingness and ability to learn.
The admissions standard has been described as ""highly selective, only accepting ten to fifteen percent of applicants for each cohort."" Though many applicants who do not pass the first admission interview are encouraged to try again when they feel they are better prepared.
Hack Reactor has created financial partnerships with Pave, SkillsFund and Climb Credit and to assist students with paying tuition. As of 2016, WeFinance and Reactor Core (Hack Reactor's parent company) have launched a platform that allows anyone to lend to incoming students.


== Course Structure ==
Accepted students are assigned pre-course work, which takes ""at least 50-80 hours"" and is due prior to the start of their cohort.
Hack Reactor’s course is 12 weeks long. During the first half of the program, students work in pairs on two-day “sprints.” Pair and group work helps teach communication and collaboration skills. During this part of the course, the day typically starts with a “toy problem,” which is a programming challenge designed to illustrate core concepts. This is followed by a lecture in which the instructor frequently checks in with students to assess how well they understand the material. The JavaScript tools and technologies taught at Hack Reactor include Angular, Node, MongoDB, Express, React, Backbone and ES6. The goal of this part of the course is for students to become “autonomous learners and programmers.”
The second half of the course focuses on projects. Students complete “increasingly elaborate” coding projects of their own design, using whatever languages and frameworks they choose. Students often adopt technologies not taught in the course using “fundamentals and self-teaching methods” taught in the first half of the course.


== Student Outcomes ==
Hack Reactor tracks and records its student outcomes under the Standard Student Outcome Methodology (SSOM), a protocol released by Reactor Core. This methodology creates standardized systems for capturing data on student graduation rate, placement rate and average starting salary. The 2015 SSOM report stated that Hack Reactor has a 98% graduate placement rate and graduates have on average a $104,000 average starting salary. Their job placement rate after 3 months have fallen from 99% in 2015 to 50% placement rate in 2016.


== Hack Reactor Remote ==
In July 2014, Hack Reactor launched an online program, Hack Reactor Remote Beta. This program has the same curriculum, course structure and teaching method as Hack Reactor’s onsite program. Students attend and participate in the lectures at the same time as the other students, work on the same assignments, and benefit from the same job search and placement resources as the onsite program. Hack Reactor Remote has a 95% placement rate, and graduates have a $94,000 average starting salary.


== MakerSquare Acquisition ==
In January 2015, Hack Reactor acquired coding bootcamp MakerSquare, which had locations in Austin and San Francisco. MakerSquare has since expanded into Los Angeles and New York City.
MakerSquare has the same admissions process, hiring partner network, and the same curriculum with a few small modifications. In November 2016, they rebranded to share the Hack Reactor name.


== Social Responsibility ==


=== Code.7370 ===
In collaboration with The Last Mile, Hack Reactor launched Code.7370, a coding program in San Quentin State Prison. Inmates have to apply to be a part of the program. Once accepted, they learn HTML, CSS, Python and JavaScript for 8 hours a day, 4 days a week. Hack Reactor instructors volunteered as teachers. In addition to class time students are also given time to work on personal projects. Because inmates are not permitted access to the internet, Code.7370 operates by a proprietary programming platform that simulates a normal web environment. The goal of Code.7370 is to reduce recidivism and help felons reenter the workforce.


=== ReBootKAMP ===
Hack Reactor helped launch ReBootKAMP, a coding bootcamp in Jordan that focuses on Syrian refugees. ReBootKAMP uses Hack Reactor’s curriculum, and received volunteer assistance from Hack Reactor staff and alumni. ReBootKAMP executives also received training on coding bootcamp best practices from Hack Reactor and Reactor Core.


== See also ==
Web Development
MakerSquare


== References ==


== External links ==
Official website
Hack Reactor Interview Process"
88,Technology Education and Literacy in Schools,47162990,13423,"Technology Education and Literacy in Schools (TEALS) is a program that pairs high schools with software engineers who serve as part-time computer science teachers.
The program was started in 2009 by Microsoft software engineer Kevin Wang. Microsoft incubated the program after Wang's divisional president learned about the program. TEALS' goal is to create self-perpetuating computer science programs within two or three years. Volunteers undergo a three-month summer class that teach them about making lesson plans and leading classes. Afterwards, software engineers visit classrooms four or five mornings a week for the entire school year to teach computer science concepts to both students and teachers.
TEALS volunteers are not required to be Microsoft employees and can have formal degrees or be self-taught in computer science. TEALS offers support for three classes: Introduction to Computer Science, Web Design, and AP Computer Science A.


== History ==
Kevin Wang graduated from the University of California, Berkeley in 2002 with a degree in electrical engineering and computer science. To pursue his teaching passion, he declined several industry job offers. Wang taught in the Bay Area for several years, and attended the Harvard Graduate School of Education, where he received a Master of Education. He became a computer science teacher at Woodside Priory School in Portola Valley, California, teaching grades seven–twelve for three years. He convinced fellow Microsoft employees and other acquaintances to teach computer science at other schools. After joining Microsoft, Wang started volunteering to teach the morning computer science class at Issaquah High School, a nearby high school, in 2009.
In 2009, Wang founded Technology Education and Literacy in Schools (TEALS), a program that aims to bring software engineers to high school classrooms to teach computer science part-time. He thought that he would have to resign from Microsoft to oversee the program's significant expansion. Wang sold his Porsche 911 to bankroll the program. After the vice president of Wang's Microsoft division discovered TEALS, the vice president took him to the divisional president who recommended he work full-time at Microsoft on managing TEALS. According to CNN, Microsoft chose to ""incubate"" TEALS for three primary reasons. First, the program fit with Microsoft's philanthropic goals. Second, Microsoft founder Bill Gates had an enduring desire to advocate for learning. Third, the software industry had a shortage of engineers. In a 2012 interview with GeekWire, Wang said TEALS has two long-term goals. The first is to give every American high school student the opportunity to take an introductory computer science course and an AP Computer Science course. The second is to have the same proportion of students taking AP Computer Science as those taking AP Biology, AP Chemistry, and AP Physics.
TEALS is part of YouthSpark, a Microsoft initiative that plans to give more educational and employment to 300 million young people between 2012 and 2015. A 2015 article in the Altavista Journal quoted the TEALS website, noting that the United States has 80,000 unfilled jobs that need a computer science degree. The Altavista Journal further reported that this would cause the United States to lose $500 billion over the following 10 years and that only 10% of American high schools have computer science courses. TEALS is managed by Microsoft's Akhtar Badshah, the senior director of citizenship and public affairs.


== Program format ==
Wang designed a three-month summer class for Microsoft employees who wanted to volunteer with TEALS. The class taught the employees about devising lesson plans and leading classes. TEALS aims to create self-perpetuating computer science programs within two or three years. The software engineers commit to being physically present at the school for around four or five days weekly. The classes are scheduled for first period since many volunteers do not start work until later in the morning. For rural schools that lack the capital to run a computer science class, TEALS enables software engineers to instruct students distantly through videoconferencing.
The first two semesters, the software engineers to educate the teachers side by side with the students. The third semester, the software engineers and teachers coteach the students. By the fourth semester, the teachers lead the class, and the software engineers become ""teaching assistants"". The aim is to enable the teachers who have math and science backgrounds in the future to lead the classes by themselves.
TEALS provides support for three classes. Two of the classes are one-semester long: Introduction to Computer Science and Web Design. The third class, Advanced Placement Computer Science A, is two-semesters long. In a 2015 interview with the Altavista Journal, Microsoft spokesperson Kate Frischmann said, ""TEALS is open to everyone, inside and outside of Microsoft, who have a background or formal degree in the field of computer science.""


== School participation ==
In the 2010–2011 school year, the program's trial year, ten TEALS volunteers instructed 250 Puget Sound region high school students from four schools. In 2011–2012 school year, TEALS expanded to 30 volunteers and six assistants educating 800 high school students in 13 schools. In the 2012–2013 school year, 22 schools around Seattle participated in TEALS. Microsoft invited the students in Seattle to visit the company's campus, hoping to spark excitement in technology. That school year, TEALS expanded to 120 volunteers in seven states teaching 2,000 students at 37 high schools. The schools were in Washington, Kentucky, California, Virginia, Utah, Washington, D.C., Minnesota, and North Dakota. In the 2013–2014 school year, TEALS grew to 280 volunteers in 12 states educating 3,000 students at 70 schools. In the 2014–2015 school year, 490 TEALS volunteers worked in 131 schools educating 6,600 students.


== References ==


== External links ==
Official website
TEALS at Microsoft"
89,Computational lithography,20606961,13377,"Computational lithography (also known as computational scaling) is the set of mathematical and algorithmic approaches designed to improve the resolution attainable through photolithography. Computational lithography has come to the forefront of photolithography in 2008 as the semiconductor industry grappled with the challenges associated with the transition to 22 nanometer CMOS process technology and beyond.


== Context: industry forced to extend 193nm deep UV photolithography ==
The periodic enhancement in the resolution achieved through photolithography has been a driving force behind Moore's Law. Resolution improvements enable printing of smaller geometries on an integrated circuit. The minimum feature size that a projection system typically used in photolithography can print is given approximately by:

  
    
      
        C
        D
        =
        
          k
          
            1
          
        
        ⋅
        
          
            λ
            
              N
              A
            
          
        
      
    
    {\displaystyle CD=k_{1}\cdot {\frac {\lambda }{NA}}}
  
where

  
    
      
        
        C
        D
      
    
    {\displaystyle \,CD}
   is the minimum feature size (also called the critical dimension).

  
    
      
        
        λ
      
    
    {\displaystyle \,\lambda }
   is the wavelength of light used.

  
    
      
        
        N
        A
      
    
    {\displaystyle \,NA}
   is the numerical aperture of the lens as seen from the wafer.

  
    
      
        
        
          k
          
            1
          
        
      
    
    {\displaystyle \,k_{1}}
   (commonly called k1 factor) is a coefficient that encapsulates process-related factors.
Historically, resolution enhancements in photolithography have been achieved through the progression of stepper illumination sources to smaller and smaller wavelengths — from ""g-line"" (436 nm) and ""i-line"" (365 nm) sources based on mercury lamps, to the current systems based on deep ultraviolet excimer lasers sources at 193 nm. However the progression to yet finer wavelength sources has been stalled by the intractable problems associated with extreme ultraviolet lithography and x-ray lithography, forcing semiconductor manufacturers to extend the current 193 nm optical lithography systems until some form of next-generation lithography proves viable (although 157 nm steppers have also been marketed, they have proven cost-prohibitive at $50M each). Efforts to improve resolution by increasing the numerical aperture have led to the use of immersion lithography. As further improvements in resolution through wavelength reduction or increases in numerical aperture have become either technically challenging or economically unfeasible, much attention has been paid to reducing the k1-factor. The k1 factor can be reduced through process improvements, such as phase-shift photomasks. These techniques have enabled photolithography at the 32 nanometer CMOS process technology node using a wavelength of 193 nm (deep ultraviolet). However, with the ITRS roadmap calling for the 22 nanometer node to be in use by 2011, photolithography researchers have had to develop an additional suite of improvements to make 22 nm technology manufacturable. While the increase in mathematical modeling has been underway for some time, the degree and expense of those calculations has justified the use of a new term to cover the changing landscape: computational lithography.


== A short history of computational lithography ==
Computational Lithography means the use of computers to simulate printing of micro-lithography structures. Pioneering work was done by Chris Mack at NSA in developing PROLITH, Rick Dill at IBM and Andy Neureuther at University of California, Berkeley from the early 1980s. These tools were limited to lithography process optimization as the algorithms were limited to a few square micrometres of resist. Commercial full-chip optical proximity correction, using model forms, was first implemented by TMA (now a subsidiary of Synopsys) and Numerical Technologies (also part of Synopsys) around 1997. Since then the market and complexity has grown significantly. With the move to sub-wavelength lithography at the 180 nm and 130 nm nodes, RET techniques such as Assist features, Phase Shift Masks started to be used together with OPC. For the transition from 65 nm to 45 nm nodes customers were worrying that not only that design rules were insufficient to guarantee printing without yield limiting hotspots, but also that tape-out time may need thousands of CPUs or weeks of run time. This predicted exponential increase in computational complexity for mask synthesis on moving to the 45 nm process node spawned a significant venture capital investment in Design for Manufacturing start-up companies. A number of startup companies promoting their own disruptive solutions to this problem started to appear, techniques from custom hardware acceleration to radical new algorithms such as Inverse Lithography were touted to resolve the forthcoming bottlenecks. Despite all this activity, incumbent OPC suppliers were able to adapt and keep their major customers, with RET and OPC being used together as for previous nodes, but now on more layers and with larger data files, and turn around time concerns were met by new algorithms and improvements in multi-core commodity processors. The term computational lithography was first used by Brion Technology (now a subsidiary of ASML) in 2005 to promote their hardware accelerated full chip lithography simulation platform. Since then the term has been used by the industry to describe full chip mask synthesis solutions. As 45 nm goes into full production and EUV lithography introduction is delayed, 32 nm and 22 nm are expected to run on existing 193 nm scanners technology. Now, not only are throughput and capabilities concerns resurfacing, but also new computational lithography techniques such as Source Mask Optimization (SMO) is seen as a way to squeeze better resolution specific to a given design. Today, all the major Mask Synthesis vendors have settled on the term ""Computational Lithography"" to describe and promote the set of Mask Synthesis technologies required for 22 nm.


== Techniques comprising computational lithography ==
Computational lithography makes use of a number of numerical simulations to improve the performance (resolution and contrast) of cutting-edge photomasks. The combined techniques include Resolution Enhancement Technology (RET), Optical Proximity Correction (OPC), Source Mask Optimization (SMO), etc. The techniques vary in terms of their technical feasibility and engineering sensible-ness, resulting in the adoption of some and the continual R&D of others.


=== Resolution Enhancement Technology (RET) ===
Resolution Enhancement Technology, first used in the 90 nanometer generation, using the mathematics of diffraction optics to specify multi-layer phase-shift photomasks that use interference patterns in the photomask that enhance resolution on the printed wafer surface.


=== Optical Proximity Correction (OPC) ===
Optical proximity correction uses computational methods to counteract the effects of diffraction-related blurring and under-exposure by modifying on-mask geometries with means such as:
adjusting linewidths depending on the density of surrounding geometries (a trace surrounded by a large open area will be over-exposed compared with the same trace surrounded by a dense pattern)
adding ""dog-bone"" endcaps to the end of lines to prevent line shortening
correcting for electron beam proximity effects
OPC can be broadly divided into rule-based and model-based. Inverse lithography technology, which treats the OPC as an inverse imaging problem, is also a useful technique because it can provide unintuitive mask patterns.


=== Complex modeling of the lens system and photoresist ===
Beyond the models used for RET and OPC, computational lithographics attempts to improve chip manufacturability and yields such as by using the signature of the scanner to help improve accuracy of the OPC model:
polarization characteristics of the lens pupil
Jones matrix of the stepper lens
optical parameters of the photoresist stack
diffusion through the photoresist
stepper illumination control variables


== A CPU-century worth of calculations or more ==
The computational effort behind these methods is immense. According to one estimate, the calculations required to adjust OPC geometries to take into account variations to focus and exposure for a state-of-the-art integrated circuit will take approximately 100 CPU-years of computer time. This does not include modeling the 3D polarization of the light source or any of the several other systems that need to be modeled in production computational photolithographic mask making flows. Brion Technologies, a subsidiary of ASML, the largest manufacturer of photolithography systems, markets a rack-mounted hardware accelerator dedicated for use in making computational lithographic calculations — a mask-making shop can purchase a large number of their systems to run in parallel. Others have claimed significant acceleration using re-purposed off-the-shelf graphics cards for their high parallel throughput.


== References =="
90,Engineering informatics,20389237,13335,"Engineering Informatics is an engineering discipline combining information technology (IT) – or informatics – with engineering concepts; It is an interdisciplinary scientific area focusing on the application of advanced computing, information and communication technologies to engineering; The study of use of information and the design of information structures that facilitate the practice of engineering and of designed artifacts that embody and embed information technology and science to achieve social, economic and environmental goals. Given this perspective, the rest of the introduction identifies different strands of concepts that inform and support the evolution of engineering informatics as a distinct discipline that lives at the interface between engineering and informatics, in the same vein as bioinformatics, geoinformatics, medical informatics, and other applied disciplines.


== Engineering Informatics as a discipline of field study ==
Computer-aided design (CAD), intelligent CAD, engineering analysis, collaborative design support, computer-aided engineering, and product life-cycle management are some of the terms that have emerged over the past decades of computing in engineering. Codification and automation of engineering knowledge and methods have had major impact on engineering practice. The use of computers by engineers has consistently tracked advancements in computer and information sciences. Computing, algorithms, computational methods, and engineering have increasingly intertwined themselves as developments in theory and practice in both disciplines influence each other. Therefore, it is now time to begin using the term “engineering informatics” to cover the science of the information that flows through these processes.
Informatics, with origins in the German word ""Informatik"" referring to automated information processing, has evolved to its current broad definition. The rise of the term informatics can be attributed to the breadth of disciplines that are now accepted and envisioned as contributing to the field of computing and information sciences. A common definition of informatics adopted by many departments/schools of informatics comes from the University of Edinburgh: ""the study of the structure, behavior, and interactions of natural and artificial computational systems that store, process and communicate information.” Informatics includes the science of information, the practice of information processing, and the engineering of information systems.
The history of engineering and computers shows a trend of increasing sophistication in the type of engineering problems being solved. Early CAD was primarily geometry driven (using mathematics and computer science). Then came the engineering use of AI, driven by theories of cognitive science and computational models of cognition (logic and pattern based). More recently, models of collaboration and representation and acquisition of collective knowledge have been introduced, driven by fields of social sciences (ethnography, sociology of work) and philosophy.
Information technology and sciences to have both created the need for, and play a role in, facilitating the management of complex sociotechnical processes. Information is context specific and its engineering is an integral part of any exchange among people and machines. Thus, informatics is the process of:
creating and codifying the linguistic worlds (representational structures) represented by the object worlds in the relevant domain, and
managing the attendant meanings through their contexts of use and accumulation through synthesis and classification.
Engineering informatics is a reflective task beyond the software/hardware that supports engineering; it is a cross-disciplinary perspective on the nature of collective intellectual work. It thereby becomes critical that a consciousness of the use of languages and their implications in the storage and retrieval of information in a work community be addressed as part of any information engineering task.
The role informatics plays in engineering products and services has become significant in the past decades. Most of the development has happened in an ad hoc manner, as can be expected. Techniques appeared in computer science and in programming practice; these techniques get used in engineering as is. Early computing in engineering was limited due to the capacities of computers. Computational power and telecommunications systems have started to converge, resulting in the possibilities of untethered connections and exchange of information that was just a distant dream in the early computing days. These developments have made the problems of distance less onerous and allow for global design, manufacturing, and supply chains. However, the problem of managing a global supply chain still is a daunting task with numerous incompatibilities in information exchange and coordination.
The problem of integrating entire sets of industries in a flexible and ad hoc manner is still a dream especially for small-scale industries within the larger global environment. For this dream to become a reality, standards become critical. With technology evolving continuously, the task of creating information standards for varieties of exchanges from the syntactic to the semantic is a challenge yet to be resolved.
Computer scientists or engineers by themselves cannot solve engineering informatics problems or the processes required to manage information in the context of engineered systems—it has to be a collaborative effort. The lack of skills among computer scientists in engineering and engineers in computing has led to problems bridging the disciplines. What pedagogical stance can help prepare students to deal with the complexities that are inherent in the task of engineering informatics? The culture of learning has to encourage the appreciation of diversity at the same time looking for the core essence and canonical nature of the experiences. While the products of today are increasingly designed for variety, we still have not mastered this process conceptually, let alone are we preparing our students. The fundamental characteristic of engineering informatics is that it is applicable at local levels of decision making in a design process as well as at the holistic level of product management and organizational design.
Nowadays, people are entering an era of networks where different infrastructural networks can be connected through information networks. The information network can connect the manufacturing network to the design and supply chain network in almost real time using information systems that include sensors and ID tags. One’s imagination is the limit in this integrative power of information networks. It is this new complex world that we need to teach students, among other things, the ability to reflect on the information they use and how to handle this information, what it means to use (or not) computational tools, the need to create tools at different scales of inquiry and across disciplines, and how to view one’s own discipline from an engineering informatics point of view.


== Engineering technology areas ==
It encompasses engineering technology areas in:
Neural Network Engineering and Intelligent System Application
Decision Support System and Information Modelling System
Reverse Software Engineering and Reusable Software Engineering
The application of Cryptography in Computer Security System
Enterprise Architectural Framework and Application
Distributed Engineering and Business Services
Sensing, Monitoring, Control and Structural Dynamics
Human and Social Modelling for Design Simulations
Computational Engineering
Virtual Office and Optimization
Networking computing for Engineering
IT Applications in Engineering
Systems and Network Technologies
Interactive Media and Internet Development
Supply Chain and Logistics Management
etc.


== Universities and institutions offering Engineering Informatics ==
Engineering Informatics is a field of undergraduate study in some universities and polytechnics:


=== Czech Republic ===
Tomas Bata University in Zlín, Zlín, Czech Republic


=== Germany ===
Otto-von-Guericke University Magdeburg, Magdeburg, Germany
Hochschule für Technik und Wirtschaft Berlin, Berlin, Germany
Technische Universität Ilmenau, Ilmenau, Germany


=== Georgia ===
Georgian Technical University, Ibilisi, Georgia


=== Guatemala ===
Universidad Mesoamericana, Guatemala City, Guatemala


=== Greece ===
University of Western Macedonia, Kozani, Greece
International Hellenic University, Thessaloniki, Greece
Technological Educational Institute of Central Macedonia, Serres, Greece
Technological Educational Institute of West Macedonia, Kastoria, Greece


=== Hungary ===
Budapest University of Technology and Economics, Budapest, Hungary


=== Indonesia ===
Indonesia Computer University, Bandung, Indonesia
Institut Teknologi Bandung, Bandung, Indonesia
Telkom University, Bandung, Indonesia
Budi Luhur University, Jakarta, Indonesia
Pancasila University, Jakarta, Indonesia
University of Bunda Mulia, Jakarta, Indonesia
Universitas Bengkulu, Bengkulu, Indonesia
Pamulang University, South Tangerang, Indonesia
Indonesia Institute of Technology, South Tangerang, Indonesia
Multimedia Nusantara University, South Tangerang, Indonesia
Sanata Dharma University, Yogyakarta, Indonesia
Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia
Universitas Brawijaya, Malang, Indonesia


=== Japan ===
Waseda University, Shinjuku, Tokyo, Japan
University of Tokyo, Bunkyo, Tokyo, Japan


=== Lithuania ===
Vilnius Gediminas Technical University, Vilnius, Lithuania


=== Portugal ===
Madeira University, Funchal, Ilha da Madeira, Portugal
Polytechnic Institute of Viana do Castelo, Viana do Castelo, Portugal
and more


=== Singapore ===
Nanyang Polytechnic, Ang Mo Kio, Singapore


=== Taiwan ===
Chung Hua University, Hsinchu, Taiwan


=== United Kingdom ===
Newcastle University, Newcastle upon Tyne, North East England, United Kingdom
University of Cambridge, Cambridge, England, United Kingdom
Cardiff University, Cardiff, Wales, United Kingdom


=== United States ===
Columbia University, Manhattan, New York City, United States
Harvard University, Manhattan, Cambridge, Massachusetts, United States
Massachusetts Institute of Technology, Cambridge, Massachusetts, United States
Princeton University, New Jersey, United States
Stanford University, Stanford, California, United States
University of California, Berkeley, California, United States


=== Venezuela ===
Universidad Centroccidental Lisandro Alvarado, Barquisimeto, Venezuela
Andrés Bello Catholic University, Caracas, Venezuela
Alejandro de Humboldt University, Caracas, Venezuela
Universidad de Oriente, Anzoátegui, Venezuela
Universidad Nacional Experimental de Guayana, Ciudad Guayana
Universidad Politécnica Territorial de Mérida, Mérida, Venezuela


== Publications ==
Advanced Engineering Informatics is a journal publication in the field of engineering informatics.
The Need for a Science of Engineering Informatics. Artificial Intelligence for Engineering Design, Analysis and Manufacturing (AI-EDAM), 2007, 21:1(23–26).
JCISE Special Issue, March 2008 This special issue has a guest editorial and a few research papers in the Engineering Informatics domain.
Special Issue on “Engineering Informatics”, by Eswaran Subrahmanian and Sudarsan Rachuri, J. Comput. Inf. Sci. Eng. 8(1), 010301 (Feb 28, 2008).


== Research ==
Engineering Informatics Group, a research group at Stanford University, USA
Portal:Information technology


== References =="
91,Well-separated pair decomposition,42316777,13281,"In computational geometry, a well-separated pair decomposition (WSPD) of a set of points 
  
    
      
        S
        ⊂
        
          
            R
          
          
            d
          
        
      
    
    {\displaystyle S\subset \mathbb {R} ^{d}}
  , is a sequence of pairs of sets 
  
    
      
        (
        
          A
          
            i
          
        
        ,
        
          B
          
            i
          
        
        )
      
    
    {\displaystyle (A_{i},B_{i})}
  , such that each pair is well-separated, and for each two distinct points 
  
    
      
        p
        ,
        q
        ∈
        S
      
    
    {\displaystyle p,q\in S}
  , there exists precisely one pair which separates the two.
The graph induced by a well-separated pair decomposition can serve as a k-spanner of the complete Euclidean graph, and is useful in approximating solutions to several problems pertaining to this.


== Definition ==

Let 
  
    
      
        A
        ,
        B
      
    
    {\displaystyle A,B}
   be two disjoint sets of points in 
  
    
      
        
          
            R
          
          
            d
          
        
      
    
    {\displaystyle \mathbb {R} ^{d}}
  , 
  
    
      
        R
        (
        X
        )
      
    
    {\displaystyle R(X)}
   denote the axis-aligned minimum bounding box for the points in 
  
    
      
        X
      
    
    {\displaystyle X}
  , and 
  
    
      
        s
        >
        0
      
    
    {\displaystyle s>0}
   denote the separation factor.
We consider 
  
    
      
        A
      
    
    {\displaystyle A}
   and 
  
    
      
        B
      
    
    {\displaystyle B}
   to be well-separated, if for each of 
  
    
      
        R
        (
        A
        )
      
    
    {\displaystyle R(A)}
   and 
  
    
      
        R
        (
        B
        )
      
    
    {\displaystyle R(B)}
   there exists a d-ball of radius 
  
    
      
        ρ
      
    
    {\displaystyle \rho }
   containing it, such that the two spheres have a minimum distance of at least 
  
    
      
        s
        ρ
      
    
    {\displaystyle s\rho }
  .
We consider a sequence of well-separated pairs of subsets of 
  
    
      
        S
      
    
    {\displaystyle S}
  , 
  
    
      
        (
        
          A
          
            1
          
        
        ,
        
          B
          
            1
          
        
        )
        ,
        (
        
          A
          
            2
          
        
        ,
        
          B
          
            2
          
        
        )
        ,
        …
        ,
        (
        
          A
          
            m
          
        
        ,
        
          B
          
            m
          
        
        )
      
    
    {\displaystyle (A_{1},B_{1}),(A_{2},B_{2}),\ldots ,(A_{m},B_{m})}
   to be a well-separated pair decomposition (WSPD) of 
  
    
      
        S
      
    
    {\displaystyle S}
   if for any two distinct points 
  
    
      
        p
        ,
        q
        ∈
        S
      
    
    {\displaystyle p,q\in S}
  , there exists precisely one 
  
    
      
        i
      
    
    {\displaystyle i}
  , 
  
    
      
        1
        ≤
        i
        ≤
        m
      
    
    {\displaystyle 1\leq i\leq m}
  , such that either

  
    
      
        p
        ∈
        
          A
          
            i
          
        
      
    
    {\displaystyle p\in A_{i}}
   and 
  
    
      
        q
        ∈
        
          B
          
            i
          
        
      
    
    {\displaystyle q\in B_{i}}
  , or

  
    
      
        q
        ∈
        
          A
          
            i
          
        
      
    
    {\displaystyle q\in A_{i}}
   and 
  
    
      
        p
        ∈
        
          B
          
            i
          
        
      
    
    {\displaystyle p\in B_{i}}
  .


== Construction ==


=== Split tree ===
By way of constructing a fair split tree, it is possible to construct a WSPD of size 
  
    
      
        O
        (
        
          s
          
            d
          
        
        n
        )
      
    
    {\displaystyle O(s^{d}n)}
   in 
  
    
      
        O
        (
        n
        lg
        ⁡
        n
        )
      
    
    {\displaystyle O(n\lg n)}
   time.
The general principle of the split tree of a point set S is that each node u of the tree represents a set of points Su and that the bounding box R(Su) of Su is split along its longest side in two equal parts which form the two children of u and their point set. It is done recursively until there is only one point in the set.
Let Lmax(R(X)) denote the size of the longest interval of the bounding hyperrectangle of point set X and let Li(R(X)) denote the size of the i-th dimension of the bounding hyperrectangle of point set X. We give pseudocode for the Split tree computation below.

SplitTree(S)
  Let u be the node for S
  if |S| = 1
     R(u) := R(S) // R(S) is a hyperrectangle which each side has a length of zero.
     Store in u the only point in S.
  else
    Compute R(S)
    Let the i-th dimension be the one where Lmax(R(S)) = Li(R(S))
    Split R(S) along the i-th dimension in two same-size hyperrectangles and take the points contained in these hyperrectangles to form the two sets Sv and Sw.
    v := SplitTree(Sv)
    w := SplitTree(Sw)
    Store v and w as, respectively, the left and right children of u.
    R(u) := R(S)
  return u

This algorithm runs in 
  
    
      
        O
        (
        
          n
          
            2
          
        
        )
      
    
    {\displaystyle O(n^{2})}
   time.
We give a more efficient algorithm that runs in 
  
    
      
        O
        (
        n
        lg
        ⁡
        n
        )
      
    
    {\displaystyle O(n\lg n)}
   time below. The goal is to loop over the list in only 
  
    
      
        O
        (
        n
        )
      
    
    {\displaystyle O(n)}
   operations per step of the recursion but only call the recursion on at most half the points each time.
Let Sij be the j-th coordinate of the i-th point in S such that S is sorted for each dimension and p(Sij) be the point. Also, let h(R(S)) be the hyperplane that splits the longest side of R(S) in two. Here is the algorithm in pseudo-code:

SplitTree(S, u)
  if |S| = 1
    R(u) := R(S) // R(S) is a hyperrectangle which each side has a length of zero.
    Store in u the only point in S.
  else
    size := |S|
    repeat
      Compute R(S)
      R(u) := R(S)
      j : = 1
      k : = |S|
      Let the i-th dimension be the one where Lmax(R(S)) = Li(R(S))
      Sv : = ∅
      Sw : = ∅
      while Sij+1 < h(R(S)) and Sik-1 > h(R(S))
        size := size - 1
        Sv : = Sv ∪ {p(S_i^j)}
        Sw : = Sw ∪ {p(S_i^k)}
        j := j + 1
        k := k - 1
      
      Let v and w be respectively, the left and right children of u.
      if Sij+1 > h(R(S))
        Sw := S \ Sv
        u := w
        S := Sw
        SplitTree(Sv,v)
      else if Sik-1 < h(R(S))
        Sv := S \ Sw
        u := v
        S := Sv
        SplitTree(Sw,w)
    until size ≤ ​n⁄2
    SplitTree(S,u)

To be able to maintain the sorted lists for each node, linked lists are used. Cross-pointers are kept for each list to the others to be able to retrieve a point in constant time. In the algorithm above, in each iteration of the loop, a call to the recursion is done. In reality, to be able to reconstruct the list without the overhead of resorting the points, it is necessary to rebuild the sorted lists once all points have been assigned to their nodes. To do the rebuilding, walk along each list for each dimension, add each point to the corresponding list of its nodes, and add cross-pointers in the original list to be able to add the cross-pointers for the new lists. Finally, call the recursion on each node and his set.


=== WSPD computation ===

The WSPD can be extracted from such a split tree by calling the recursive FindPairs(v,w) function on the children of every node in the split tree. Let ul / ur denote the children of the node u. We give pseudocode for the FindWSPD(T, s) function below.

FindWSPD(T,s)
  for each node u that is not a leaf in the split tree T do
    FindPairs(ul, ur)

We give pseudocode for the FindPairs(v,w) function below.

FindPairs(v,w)
  if Sv and Sw are well-separated with respect to s 
    report pair(Sv,Sw)
  else
    if( Lmax(R(v)) ≤ Lmax(R(w)) )
      Recursively call FindPairs(v,wl) and FindPairs(v,wr)
    else
      Recursively call FindPairs(vl,w) and FindPairs(vr,w)

Combining the s-well-separated pairs from all the calls of FindPairs(v,w) gives the WSPD for separation s.

Each time the recursion tree split in two, there is one more pair added to the decomposition. So, the algorithm run-time is in the number of pairs in the final decomposition.
Callahan and Kosaraju proved that this algorithm finds a Well-separated pair decomposition (WSPD) of size 
  
    
      
        O
        (
        
          s
          
            d
          
        
        n
        )
      
    
    {\displaystyle O(s^{d}n)}
  .


== Properties ==
Lemma 1: Let 
  
    
      
        {
        A
        ,
        B
        }
      
    
    {\displaystyle \{A,B\}}
   be a well-separated pair with respect to 
  
    
      
        s
      
    
    {\displaystyle s}
  . Let 
  
    
      
        p
        ,
        
          p
          ′
        
        ∈
        A
      
    
    {\displaystyle p,p'\in A}
   and 
  
    
      
        q
        ∈
        B
      
    
    {\displaystyle q\in B}
  . Then, 
  
    
      
        
          |
        
        p
        
          p
          ′
        
        
          |
        
        ≤
        (
        2
        
          /
        
        s
        )
        
          |
        
        p
        q
        
          |
        
      
    
    {\displaystyle |pp'|\leq (2/s)|pq|}
  .
Proof: Because 
  
    
      
        p
      
    
    {\displaystyle p}
   and 
  
    
      
        
          p
          ′
        
      
    
    {\displaystyle p'}
   are in the same set, we have that 
  
    
      
        
          |
        
        p
        
          p
          ′
        
        
          |
        
        ≤
        2
        ρ
      
    
    {\displaystyle |pp'|\leq 2\rho }
   where 
  
    
      
        ρ
      
    
    {\displaystyle \rho }
   is the radius of the enclosing circle of 
  
    
      
        A
      
    
    {\displaystyle A}
   and 
  
    
      
        B
      
    
    {\displaystyle B}
  . Because 
  
    
      
        p
      
    
    {\displaystyle p}
   and 
  
    
      
        q
      
    
    {\displaystyle q}
   are in two well-separated sets, we have that 
  
    
      
        
          |
        
        p
        q
        
          |
        
        ≥
        s
        ρ
      
    
    {\displaystyle |pq|\geq s\rho }
  . We obtain that:

  
    
      
        
          
            
              
              
                
                  
                    
                      
                        |
                      
                      p
                      
                        p
                        ′
                      
                      
                        |
                      
                    
                    2
                  
                
                ≤
                ρ
                ≤
                
                  
                    
                      
                        |
                      
                      p
                      q
                      
                        |
                      
                    
                    s
                  
                
              
            
            
              
                ⇔
              
              
            
            
              
              
                
                  
                    
                      
                        |
                      
                      p
                      
                        p
                        ′
                      
                      
                        |
                      
                    
                    2
                  
                
                ≤
                
                  
                    
                      
                        |
                      
                      p
                      q
                      
                        |
                      
                    
                    s
                  
                
              
            
            
              
                ⇔
              
              
            
            
              
              
                
                
                  |
                
                p
                
                  p
                  ′
                
                
                  |
                
                ≤
                
                  
                    2
                    s
                  
                
                
                  |
                
                p
                q
                
                  |
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}&{\frac {|pp'|}{2}}\leq \rho \leq {\frac {|pq|}{s}}\\\Leftrightarrow &\\&{\frac {|pp'|}{2}}\leq {\frac {|pq|}{s}}\\\Leftrightarrow &\\&|pp'|\leq {\frac {2}{s}}|pq|\\\end{aligned}}}
  
Lemma 2: Let 
  
    
      
        {
        A
        ,
        B
        }
      
    
    {\displaystyle \{A,B\}}
   be a well-separated pair with respect to 
  
    
      
        s
      
    
    {\displaystyle s}
  . Let 
  
    
      
        p
        ,
        
          p
          ′
        
        ∈
        A
      
    
    {\displaystyle p,p'\in A}
   and 
  
    
      
        q
        ,
        
          q
          ′
        
        ∈
        B
      
    
    {\displaystyle q,q'\in B}
  . Then, 
  
    
      
        
          |
        
        
          p
          ′
        
        
          q
          ′
        
        
          |
        
        ≤
        (
        1
        +
        4
        
          /
        
        s
        )
        
          |
        
        p
        q
        
          |
        
      
    
    {\displaystyle |p'q'|\leq (1+4/s)|pq|}
  .
Proof: By the triangle inequality, we have:

  
    
      
        
          |
        
        
          p
          ′
        
        
          q
          ′
        
        
          |
        
        ≤
        
          |
        
        
          p
          ′
        
        p
        
          |
        
        +
        
          |
        
        p
        q
        
          |
        
        +
        
          |
        
        q
        
          q
          ′
        
        
          |
        
      
    
    {\displaystyle |p'q'|\leq |p'p|+|pq|+|qq'|}
  
From Lemma 1, we obtain:

  
    
      
        
          
            
              
                
                  |
                
                
                  p
                  ′
                
                
                  q
                  ′
                
                
                  |
                
              
              
                
                ≤
                (
                2
                
                  /
                
                s
                )
                
                  |
                
                p
                q
                
                  |
                
                +
                
                  |
                
                p
                q
                
                  |
                
                +
                (
                2
                
                  /
                
                s
                )
                
                  |
                
                p
                q
                
                  |
                
              
            
            
              
              
                
                =
                (
                1
                +
                4
                
                  /
                
                s
                )
                
                  |
                
                p
                q
                
                  |
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}|p'q'|&\leq (2/s)|pq|+|pq|+(2/s)|pq|\\&=(1+4/s)|pq|\end{aligned}}}
  


== Applications ==
The well-separated pair decomposition has application in solving a number of problems. WSPD can be used to:
Solve the closest pair problem in 
  
    
      
        O
        (
        n
        lg
        ⁡
        n
        )
      
    
    {\displaystyle O(n\lg n)}
   time.
Solve the k-closest pairs problem in 
  
    
      
        O
        (
        n
        lg
        ⁡
        n
        +
        k
        )
      
    
    {\displaystyle O(n\lg n+k)}
   time.
Solve the all-nearest neighbors problem in 
  
    
      
        O
        (
        n
        lg
        ⁡
        n
        )
      
    
    {\displaystyle O(n\lg n)}
   time.
Provide a 
  
    
      
        (
        1
        −
        ϵ
        )
      
    
    {\displaystyle (1-\epsilon )}
  -approximation of the diameter of a point set in 
  
    
      
        O
        (
        n
        lg
        ⁡
        n
        )
      
    
    {\displaystyle O(n\lg n)}
   time.
Directly induce a t-spanner of a point set.
Provide a t-approximation of the Euclidean minimum spanning tree in d dimensions in 
  
    
      
        O
        (
        n
        lg
        ⁡
        n
        )
      
    
    {\displaystyle O(n\lg n)}
   time.
Provide a 
  
    
      
        (
        1
        +
        ϵ
        )
      
    
    {\displaystyle (1+\epsilon )}
  -approximation of the Euclidean minimum spanning tree in d dimensions in 
  
    
      
        O
        (
        n
        lg
        ⁡
        n
        +
        (
        
          ϵ
          
            −
            2
          
        
        
          lg
          
            2
          
        
        ⁡
        
          
            1
            ϵ
          
        
        )
        n
        )
      
    
    {\displaystyle O(n\lg n+(\epsilon ^{-2}\lg ^{2}{\frac {1}{\epsilon }})n)}
   time.


== References =="
92,C++ string handling,6915658,13279,"The C++ programming language has support for string handling, mostly implemented in its standard library. The language standard specifies several string types, some inherited from C, some designed to make use of the language's features, such as classes and RAII. The most-used of these is std::string.
Since the initial versions of C++ had only the ""low-level"" C string handling functionality and conventions, multiple incompatible designs for string handling classes have been designed over the years and are still used instead of std::string, and C++ programmers may need to handle multiple conventions in a single application.


== History ==
The std::string type is the main string datatype in standard C++ since 1998, but it was not always part of C++. From C, C++ inherited the convention of using null-terminated strings that are handled by a pointer to their first element, and a library of functions that manipulate such strings. In modern standard C++, a string literal such as ""hello"" still denotes a NUL-terminated array of characters.
Using C++ classes to implement a string type offers several benefits of automated memory management and a reduced risk of out-of-bounds accesses, and more intuitive syntax for string comparison and concatenation. Therefore, it was strongly tempting to create such a class. Over the years, C++ application, library and framework developers produced their own, incompatible string representations, such as the one in AT&T's Standard Components library (the first such implementation, 1983) or the CString type in Microsoft's MFC. While std::string standardized strings, legacy applications still commonly contain such custom string types and libraries may expect C-style strings, making it ""virtually impossible"" to avoid using multiple string types in C++ programs and requiring programmers to decide on the desired string representation ahead of starting a project.
In a 1991 retrospective on the history of C++, its inventor Bjarne Stroustrup called the lack of a standard string type (and some other standard types) in C++ 1.0 the worst mistake he made in its development; ""the absence of those led to everybody re-inventing the wheel and to an unnecessary diversity in the most fundamental classes"".


=== Implementation issues ===
The various vendors' string types have different implementation strategies and performance characteristics. In particular, some string types use a copy-on-write strategy, where an operation such as

does not actually copy the content of a to b; instead, both strings share their contents and a reference count on the content is incremented. The actual copying is postponed until a mutating operation, such as appending a character to either string, makes the strings' contents differ. Copy-on-write can make major performance changes to code using strings (making some operations much faster and some much slower). Though std::string no longer uses it, many (perhaps most) alternative string libraries still implement copy-on-write strings.
Some string implementations store 16-bit or 32-bit code points instead of bytes, this was intended to facilitate processing of Unicode text. However, it means that conversion to these types from std::string or from arrays of bytes is a slow and often a lossy operation, dependent on the ""locale"", and can throw exceptions. Any processing advantages of 16-bit code units vanished when the variable-width UTF-16 encoding was introduced (though there are still advantages if you must communicate with a 16-bit API such as Windows). Qt's QString is an example.
Third-party string implementations also differed considerably in the syntax to extract or compare substrings, or to perform searches in the text.


== Standard string types ==
The std::string class is the standard representation for a text string since C++98. The class provides some typical string operations like comparison, concatenation, find and replace, and a function for obtaining substrings. An std::string can be constructed from a C-style string, and a C-style string can also be obtained from one.
The individual units making up the string are of type char, at least (and almost always) 8 bits each. In modern usage these are often not ""characters"", but parts of a multibyte character encoding such as UTF-8.
The copy-on-write strategy was deliberately allowed by the initial C++ Standard for std::string because it was deemed a useful optimization, and used by nearly all implementations. However, there were mistakes, in particular the operator[] returned a non-const reference in order to make it easy to port C in-place string manipulations (such code often assumed one byte per character and thus this may not have been a good idea!) This allowed the following code that shows that it must make a copy even though it is almost always used only to examine the string and not modify it:

This caused some implementations to abandon copy-on-write. It was also discovered that the overhead in multi-threaded applications due to the locking needed to examine or change the reference count was greater than the overhead of copying small strings on modern processors (especially for strings smaller than the size of a pointer). The optimization was finally disallowed in C++11, with the result that even passing a std::string as an argument to a function, viz.

must be expected to perform a full copy of the string into newly allocated memory. The common idiom to avoid such copying is to pass as a const reference:


=== Example usage ===


=== Related classes ===
std::string is a typedef for a particular instantiation of the std::basic_string template class. Its definition is found in the <string> header:

Thus string provides basic_string functionality for strings having elements of type char. There is a similar class std::wstring, which consists of wchar_t, and is most often used to store UTF-16 text on Windows and UTF-32 on most Unix-like platforms. The C++ standard, however, does not impose any interpretation as Unicode code points or code units on these types and does not even guarantee that a wchar_t holds more bits than a char. To resolve some of the incompatibilities resulting from wchar_t's properties, C++11 added two new classes: std::u16string and std::u32string (made up of the new types char16_t and char32_t), which are the given number of bits per code unit on all platforms. C++11 also added new string literals of 16-bit and 32-bit ""characters"" and syntax for putting Unicode code points into null-terminated (C-style) strings.
A basic_string is guaranteed to be specializable for any type with a char_traits struct to accompany it. As of C++11, only char, wchar_t, char16_t and char32_t specializations are required to be implemented in the standard library; any other types are implementation-defined. Each specialization is also a Standard Library container, and thus the Standard Library algorithms can be applied to the code units in strings.


=== Critiques ===
The design of std::string has held up as an example of monolithic design by Herb Sutter, who reckons that of the 103 member functions on the class in C++98, 71 could have been decoupled without loss of implementation efficiency.


== References =="
93,ICORES,51158586,13238,"The International Conference on Operations Research and Enterprise Systems (ICORES) is an annual conference in the field of operations research. Two tracks are held simultaneous, covering domain independent methodologies and technologies and also practical work developed in specific application areas. These tracks are present in the conference not only in technical sessions but also in poster sessions, keynote lectures and tutorials.
The works presented in the conference are published in the conference proceedings and are made available at the SCITEPRESS digital library. Usually, it's established a cooperation with Springer for a post-publication with some of the conference best papers. The first edition of ICORES was held in 2012 in conjunction with the International Conference on Agents and Artificial Intelligence (ICAART) and the International Conference on Pattern Recognition Applications and Methods (ICPRAM).


== Areas ==


=== Methodologies and Technologies ===
Stochastic optimization
Systems of systems
Simulation
Optimization
Game Theory
Management sciences
Information Systems
Industrial Engineering


=== Applications ===
OR in Health
Decision Support Systems
Supply Chain Management
Resource Allocation
Project Management
Logistics
Risk Management


== Current Chairs ==


=== Conference Chair ===
Marc Demange, School of Science, Mathematical and Geospatial Sciences, RMIT University, Australia


=== Program Co-Chairs ===
Greg H. Parlier, INFORMS, United States
Federico Liberatore, Universidad Carlos III de Madrid, Spain


== Editions ==


=== ICORES 2016 - Lisbon, Portugal ===
Proceedings - Proceedings of the International Conference on Operations Research and Enterprise Systems. ISBN 978-989-758-171-7 


==== Best Paper Award ====
Area: Applications - Yujie Chen, Fiona Polack, Peter Cowling, Philip Mourdjis and Stephen Remde. ""Risk Driven Analysis of Maintenance for a Large-scale Drainage System""
Area: Methodologies and Technologies - Vikas Vikram Singh, Oualid Jouini and Abdel Lisser. ""A Complementarity Problem Formulation for Chance-constraine Games""


==== Best Student Paper Award ====
Area: Applications - Jose L. Saez and Victor M. Albornoz. ""Delineation of Rectangular Management Zones Under Uncertainty Conditions""


==== Best PhD Project Award ====
Parisa Madhooshiarzanagh. ""Preference Dissagrigation Model of ELECTRE TRI-NC and Its Application on Identifying Preferred Climates for Tourism"".


=== ICORES 2015 - Lisbon, Portugal ===
Proceedings - Proceedings of the International Conference on Operations Research and Enterprise Systems. ISBN 978-989-758-075-8 


==== Best Paper Award ====
Area: Applications - L. Berghman, C. Briand, R. Leus and P. Lopez. ""The Truck Scheduling Problem at Crossdocking Terminals""
Area: Methodologies and Technologies - Kailiang Xu and Gang Zheng. ""Schedule Two-machine Flow-shop with Controllable Processing Times Using Tabu-search""


==== Best Student Paper Award ====
Area: Applications - Wasakorn Laesanklang, Dario Landa-Silva and J. Arturo Castillo Salazar. ""Mixed Integer Programming with Decomposition to Solve a Workforce Scheduling and Routing Problem""
Area: Methodologies and Technologies - Jan Bok and Milan Hladík. ""Selection-based Approach to Cooperative Interval Games""


==== Best PhD Project Award ====
Nigel M. Clay, John Hearne, Babak Abbasi and Andrew Eberhard. ""Ensuring Blood is Available When it is Needed Most""
Sandy Jorens, Annelies De Corte, Kenneth Sörensen and Gunther Steenackers. ""The Air Distribution Network Design Problem - A Complex Non-linear Combinatorial Optimization Problem ""


=== ICORES 2014 - ESEO, Angers, Loire Valley, France ===
Proceedings - Proceedings of the 3rd International Conference on Operations Research and Enterprise Systems. ISBN 978-989-758-017-8 


==== Best Paper Award ====
Area: Applications - Céline Gicquel and Michel Minoux. ""New Multi-product Valid Inequalities for a Discrete Lot-sizing Problem""
Area: Methodologies and Technologies - Nadia Chaabane Fakhfakh, Cyril Briand and Marie-José Huguet. ""A Multi-Agent Min-Cost Flow problem with Controllable Capacities""


==== Best Student Paper Award ====
Area: Applications - Nhat-Vinh Vo, Pauline Fouillet and Christophe Lenté. ""General Lower Bounds for the Total Completion Time in a Flowshop Scheduling Problem""
Area: Methodologies and Technologies - António Quintino, João Carlos Lourenço and Margarida Catalão-Lopes. ""Managing Price Risk for an Oil and Gas Company""


==== Best PhD Project Award ====
Laura Wagner and Mustafa Çagri Gürbüz. ""Sourcing Decisions for Goods with Potentially Imperfect Quality under the Presence of Supply Disruption ""


=== ICORES 2013 - Barcelona, Spain ===
Proceedings - Proceedings of the 2nd International Conference on Operations Research and Enterprise Systems. ISBN 978-989-8565-40-2 


==== Best Paper Award ====
Area: Applications - Daniel Reich, Sandra L. Winkler and Erica Klampfl. ""The Pareto Frontier for Vehicle Fleet Purchases""
Area: Methodologies and Technologies - N. Perel, J. L. Dorsman and M. Vlasiou. ""Cyclic-type Polling Models with Preparation Times""


==== Best Student Paper Award ====
Area: Applications - Ahmad Almuhtady, Seungchul Lee, Edwin Romeijn and Jun Ni. ""A Maintenance-optimal Swapping Policy""
Area: Methodologies and Technologies - Pablo Adasme, Abdel Lisser and Chen Wang. ""A Distributionally Robust Formulation for Stochastic Quadratic Bi-level Programming""


=== ICORES 2012 - Vilamoura, Algarve, Portugal ===
Proceedings - Proceedings of the 1st International Conference on Operations Research and Enterprise Systems. ISBN 978-989-8425-97-3 


==== Best Paper Award ====
Area: Applications - Rita Macedo, Saïd Hanafi, François Clautiaux, Cláudio Alves and J. M. Valério de Carvalho. ""GENERALIZED DISAGGREGATION ALGORITHM FOR THE VEHICLE ROUTING PROBLEM WITH TIME WINDOWS AND MULTIPLE ROUTES""
Area: Methodologies and Technologies - Herwig Bruneel, Willem Mélange, Bart Steyaert, Dieter Claeys and Joris Walraevens. ""IMPACT OF BLOCKING WHEN CUSTOMERS OF DIFFERENT CLASSES ARE ACCOMMODATED IN ONE COMMON QUEUE""


==== Best Student Paper Award ====
Area: Applications - Jianqiang Cheng, Stefanie Kosuch and Abdel Lisser. ""STOCHASTIC SHORTEST PATH PROBLEM WITH UNCERTAIN DELAYS""
Area: Methodologies and Technologies - A. Papayiannis, P. Johnson, D. Yumashev, S. Howell, N. Proudlove and P. Duck. ""CONTINUOUS-TIME REVENUE MANAGEMENT IN CARPARKS""


== References ==


== External links ==
Science and Technology Events
Conference website
Science and Technology Publications
Event management system
WikiCfp call for papers"
94,Discrete logarithm,181334,13117,"In the mathematics of the real numbers, the logarithm logb a is a number x such that bx = a, for given numbers a and b. Analogously, in any group G, powers bk can be defined for all integers k, and the discrete logarithm logb a is an integer k such that bk = a.
Discrete logarithms are quickly computable in a few special cases. However, no efficient method is known for computing them in general. Several important algorithms in public-key cryptography base their security on the assumption that the discrete logarithm problem over carefully chosen groups has no efficient solution.


== Definition ==
Let G be any group. Denote its group operation by multiplication and its identity element by 1. Let b be any element of G. For any positive integer k, the expression bk denotes the product of b with itself k times:

  
    
      
        
          b
          
            k
          
        
        =
        
          
            
              
                b
                ⋅
                b
                ⋅
                …
                ⋅
                b
              
              ⏟
            
          
          
            k
            
            
              t
              i
              m
              e
              s
            
          
        
        .
      
    
    {\displaystyle b^{k}=\underbrace {b\cdot b\cdot \ldots \cdot b} _{k\;\mathrm {times} }.}
  
Similarly, let b-k denote the product of b−1 with itself k times. For k = 0, the kth power is the identity: b0 = 1.
Let a also be an element of G. An integer k that solves the equation bk = a is termed a discrete logarithm (or simply logarithm, in this context) of a to the base b. One writes k = logb a.


== Examples ==


=== Powers of 10 ===
The powers of 10 form an infinite subset G = {…, 0.001, 0.01, 0.1, 1, 10, 100, 1000, …} of the rational numbers. This set G is a cyclic group under multiplication, and 10 is a generator. For any element a of the group, one can compute log10 a. For example, log10 10000 = 4, and log10 0.001 = −3. These are instances of the discrete logarithm problem.
Other base-10 logarithms in the real numbers are not instances of the discrete logarithm problem, because they involve non-integer exponents. For example, the equation log10 53 = 1.724276… means that 101.724276… = 53. While integer exponents can be defined in any group using products and inverses, arbitrary real exponents in the real numbers require other concepts such as the exponential function.


=== Powers of a fixed real number ===
A similar example holds for any non-zero real number b. The powers form a multiplicative subgroup G = {…, b−3, b−2, b−1, 1, b1, b2, b3, …} of the non-zero real numbers. For any element a of G, one can compute logb a.


=== Modular arithmetic ===
One of the simplest settings for discrete logarithms is the group (Zp)×. This is the group of multiplication modulo the prime p. Its elements are congruence classes modulo p, and the group product of two elements may be obtained by ordinary integer multiplication of the elements followed by reduction modulo p.
The kth power of one of the numbers in this group may be computed by finding its kth power as an integer and then finding the remainder after division by p. When the numbers involved are large, it is more efficient to reduce modulo p multiple times during the computation. Regardless of the specific algorithm used, this operation is called modular exponentiation. For example, consider (Z17)×. To compute 34 in this group, compute 34 = 81, and then divide 81 by 17, obtaining a remainder of 13. Thus 34 = 13 in the group (Z17)×.
The discrete logarithm is just the inverse operation. For example, consider the equation 3k ≡ 13 (mod 17) for k. From the example above, one solution is k = 4, but it is not the only solution. Since 316 ≡ 1 (mod 17)—as follows from Fermat's little theorem—it also follows that if n is an integer then 34+16n ≡ 34 × (316)n ≡ 13 × 1n ≡ 13 (mod 17). Hence the equation has infinitely many solutions of the form 4 + 16n. Moreover, because 16 is the smallest positive integer m satisfying 3m ≡ 1 (mod 17), these are the only solutions. Equivalently, the set of all possible solutions can be expressed by the constraint that k ≡ 4 (mod 16).


=== Powers of the identity ===
In the special case where b is the identity element 1 of the group G, the discrete logarithm logb a is undefined for a other than 1, and every integer k is a discrete logarithm for a = 1.


== Properties ==
Powers obey the usual algebraic identity bk + l = bk bl. In other words, the function

  
    
      
        f
        :
        
          Z
        
        →
        G
      
    
    {\displaystyle f:\mathbf {Z} \rightarrow G}
  
defined by f(k) = bk is a group homomorphism from the integers Z under addition onto the subgroup H of G generated by b. For all a in H, logb a exists. Conversely, logb a does not exist for a that are not in H.
If H is infinite, then logb a is also unique, and the discrete logarithm amounts to a group isomorphism

  
    
      
        
          log
          
            b
          
        
        :
        H
        →
        
          Z
        
        .
      
    
    {\displaystyle \log _{b}\colon H\rightarrow \mathbf {Z} .}
  
On the other hand, if H is finite of size n, then logb a is unique only up to congruence modulo n, and the discrete logarithm amounts to a group isomorphism

  
    
      
        
          log
          
            b
          
        
        :
        H
        →
        
          
            Z
          
          
            n
          
        
        ,
      
    
    {\displaystyle \log _{b}\colon H\rightarrow \mathbf {Z} _{n},}
  
where Zn denotes the additive group of integers modulo n.
The familiar base change formula for ordinary logarithms remains valid: If c is another generator of H, then

  
    
      
        
          log
          
            c
          
        
        ⁡
        a
        =
        
          log
          
            c
          
        
        ⁡
        b
        ⋅
        
          log
          
            b
          
        
        ⁡
        a
        .
      
    
    {\displaystyle \log _{c}a=\log _{c}b\cdot \log _{b}a.}
  


== Algorithms ==

The discrete logarithm problem is considered to be computationally intractable. That is, no efficient classical algorithm is known for computing discrete logarithms in general.
A general algorithm for computing logb a in finite groups G is to raise b to larger and larger powers k until the desired a is found. This algorithm is sometimes called trial multiplication. It requires running time linear in the size of the group G and thus exponential in the number of digits in the size of the group. Therefore, it is an exponential-time algorithm, practical only for small groups G.
More sophisticated algorithms exist, usually inspired by similar algorithms for integer factorization. These algorithms run faster than the naïve algorithm, some of them linear in the square root of the size of the group, and thus exponential in half the number of digits in the size of the group. However none of them run in polynomial time (in the number of digits in the size of the group).
Baby-step giant-step
Function field sieve
Index calculus algorithm
Number field sieve
Pohlig–Hellman algorithm
Pollard's rho algorithm for logarithms
Pollard's kangaroo algorithm (aka Pollard's lambda algorithm)
There is an efficient quantum algorithm due to Peter Shor.
Efficient classical algorithms also exist in certain special cases. For example, in the group of the integers modulo p under addition, the power bk becomes a product bk, and equality means congruence modulo p in the integers. The extended Euclidean algorithm finds k quickly.


== Comparison with integer factorization ==
While computing discrete logarithms and factoring integers are distinct problems, they share some properties:
both problems seem to be difficult (no efficient algorithms are known for non-quantum computers),
for both problems efficient algorithms on quantum computers are known,
algorithms from one problem are often adapted to the other, and
the difficulty of both problems has been used to construct various cryptographic systems.


== Cryptography ==
There exist groups for which computing discrete logarithms is apparently difficult. In some cases (e.g. large prime order subgroups of groups (Zp)×) there is not only no efficient algorithm known for the worst case, but the average-case complexity can be shown to be about as hard as the worst case using random self-reducibility.
At the same time, the inverse problem of discrete exponentiation is not difficult (it can be computed efficiently using exponentiation by squaring, for example). This asymmetry is analogous to the one between integer factorization and integer multiplication. Both asymmetries (and other possibly one-way functions) have been exploited in the construction of cryptographic systems.
Popular choices for the group G in discrete logarithm cryptography are the cyclic groups (Zp)× (e.g. ElGamal encryption, Diffie–Hellman key exchange, and the Digital Signature Algorithm) and cyclic subgroups of elliptic curves over finite fields (see elliptic curve cryptography).
While there is no publicly known algorithm for solving the discrete logarithm problem in general, the first three steps of the number field sieve algorithm only depend on the group G, not on the specific elements of G whose finite log is desired. By precomputing these three steps for a specific group, one need only carry out the last step, which is much less computationally expensive than the first three, to obtain a specific logarithm in that group.
It turns out that much Internet traffic uses one of a handful of groups that are of order 1024 bits or less, e.g. cyclic groups with order of the Oakley primes specified in RFC 2409. The Logjam attack used this vulnerability to compromise a variety of Internet services that allowed the use of groups whose order was a 512-bit prime number, so called export grade.
The authors of the Logjam attack estimate that the much more difficult precomputation needed to solve the discrete log problem for a 1024-bit prime would be within the budget of a large national intelligence agency such as the U.S. National Security Agency (NSA). The Logjam authors speculate that precomputation against widely reused 1024 DH primes is behind claims in leaked NSA documents that NSA is able to break much of current cryptography.


== References ==


== Further reading ==
Richard Crandall; Carl Pomerance. Chapter 5, Prime Numbers: A computational perspective, 2nd ed., Springer.
Stinson, Douglas Robert (2006), Cryptography: Theory and Practice (3rd ed.), London: CRC Press, ISBN 978-1-58488-508-5"
95,Humanyze,37735408,13073,"Humanyze, founded as Sociometric Solutions in 2010 in Boston, Massachusetts, is a people analytics software provider. Humanyze was founded by MIT doctoral students Ben Waber, Daniel Olguin, Taemie Kim, Tuomas Jaanu, and MIT Professor Alex Pentland. based on research from the MIT Media Lab, Humanyze's people analytics platform helps companies measure corporate communication data to uncover patterns on how work gets done.


== Company History ==
The founders Waber, Olguin, Kim, and Jaanu, met while completing their Ph.D.s at the MIT Media Lab in Professor “Sandy” Pentland’s Human Dynamics group. Sociometric Solutions Inc. was incorporated on October 26, 2010 as a research and consulting firm. In 2015, the company was rebranded as ""Humanyze"" and transitioned from a consulting firm to a software company. They raised $4M in Series A financing from Romulus Capital and launched their people analytics software, Humanyze Elements, at the end of 2016.


== Sociometric Badge ==
While at the MIT Media Lab, Humanyze’s founders developed a sociometric badge, a high-tech I.D. card. With the sociometric badge, they pioneered ways of collecting in-person collaboration data. This badge has sensors to measure the frequency and duration of face-to-face interactions. It does not record content, web activity, or personal activities. It does not have GPS.


== Products ==
Humanyze's people analytics platform, Humanyze Elements, measures corporate communication data to uncover patterns on how work gets done. Human resources, operations, and corporate real estate professionals use the platform to analyze workplace data in real-time to make better decisions around organizational health, workplace strategy, and business process optimization. The Humanyze Elements platform is powered by AI, ML, and behavioral science. It is a web-based dashboard. A variety of data sources can be plugged into the Elements Platform including Microsoft Office Exchange, Google Suite, Skype, Humanyze Badge, and HID Bluvision I.D.


== Data Privacy ==
Humanyze built their platform with data privacy in mind; no personal data is available on the platform, including employee profiles (names and email addresses). All data is encrypted, aggregated, and anonymized. No communication content is recorded. There is no personally identifiable information (PII) or private confidential information (PCI). Humanyze is GDPR and EU Privacy Shield compliant.


== News ==
The company has been covered in the media by various news and technology sources including New Scientist, The Boston Globe, CBS News, Businessweek, NPR, CNN, and the New York Times. Humanyze's clients have included Bank of America, the United States Army, NASA, and BCG. The company's research has supported the importance of face-to-face interactions and communication, and building larger networks among peers.


== Research ==
When Humanyze’s founders were researching their Ph.D.’s at the MIT Media Lab, they tested whether measuring all communication content from emails, texts, and in-person conversations would provide more insight. They measured over 100 MB of data per person per day.[source] With all of this data, they found that it was difficult to extract communication patterns. So, they tested measuring the frequency and duration of communication instead: timestamps of communication, length of conversations, and which teams talked to which teams. They found that the degree of accuracy was negligible; measuring content was extraneous and unnecessary.
Humanyze's founders have published 50+ research articles. Over the past 10 years, they have accumulated the largest depository of workplace data. One finding from the firm's research was that simple physical changes such as having larger tables in a cafeteria facilitated more interaction and collaboration among colleagues than smaller tables. Another finding emphasized the importance of group breaks and office spaces which facilitate such breaks as having an important effect on employee morale and efficacy.


== References ==


== External links ==
Official website"
96,ICPRAM,54428859,13055,"The International Conference on Pattern Recognition Applications and Methods (ICPRAM) is held annually since 2012. From the beginning it is held in conjunction with two other conferences: ICAART - International Conference on Agents and Artificial Intelligence and ICORES - International Conference on Operations Research and Enterprise Systems.
ICPRAM is composed by two main topics areas: theory and methods and applications. Each one of these areas is constituted by several sub-topics like Evolutionary Computation, Density Estimation, Spectral method, Combinatorial Optimization, Reinforcement learning, Meta learning, Convex optimization in the case of Theory and methods and Natural language processing, robotics, Signal processing, Information retrieval, perception in the applications area.
The conference papers are made available at the SCITEPRESS digital library and are published in the conference proceedings. It’s also made a selection of the best papers presented in the conference for publication in a Springer volume.
Besides the presentation of papers from the authors, the conference is composed by tutorials. For example, in the last edition, the conference had a tutorial on Secure our society - Computer Vision Techniques for Video Surveillance given by Huiyu Zhou from the Queen's University Belfast, UK.
Since the first edition, ICPRAM has counted on several keynote speakers like Alberto Sanfeliu, Tomaso Poggio, Josef Kittler, Hanan Samet, Nello Cristianini and John Shawe-Taylor.


== Editions ==
ICPRAM 2017 - Porto, PortugalICPRAM 2016 - Rome, ItalyICPRAM 2015 - Lisbon, PortugalICPRAM 2014 - ESEO, Angers, Loire Valley, FranceICPRAM 2013 - Barcelona, SpainICPRAM 2012 - Vilamoura, Algarve, Portugal


== Current Chairs ==


=== Conference Chair ===
Ana Fred, Instituto de Telecomunicações / IST, Portugal


=== Program Co-Chairs ===
Maria De Marsico, Sapienza Università di Roma, Italy
Gabriella Sanniti di Baja, ICAR-CNR, Italy


== Best Paper Awards ==


=== 2017 ===
Area: Theory and Methods
Best Paper Award: Seiya Satoh and Ryohei Nakano. ""How New Information Criteria WAIC and WBIC Worked for MLP Model Selection""
Best Student Award: Xiaoyi Chen and Régis Lengellé. ""Domain Adaptation Transfer Learning by SVM Subject to a Maximum-Mean-Discrepancy-like Constraint""
ApplicationsBest Paper Award:Sarah Ahmed and Tayyaba Azim. ""Compression Techniques for Deep Fisher Vectors""
Best Student Award:Niels Ole Salscheider, Eike Rehder and Martin Lauer. ""Analysis of Regionlets for Pedestrian Detection"" 


=== 2016 ===
Area: Theory and Methods
Best Paper Award: Anne C. van Rossum, Hai Xiang Lin, Johan Dubbeldam and H. Jaap van den Herik. ""Nonparametric Bayesian Line Detection - Towards Proper Priors for Robotic Computer Vision ""
Best Student Award: Roghayeh Soleymani, Eric Granger and Giorgio Fumera. ""Classifier Ensembles with Trajectory Under-Sampling for Face Re-Identification ""
ApplicationsBest Paper Award: Jeonghwan Park, Kang Li and Huiyu Zhou. ""k-fold Subsampling based Sequential Backward Feature Elimination ""
Best Student Award: Julia Richter, Christian Wiede, Enes Dayangac, Markus Heß and Gangolf Hirtz. ""Activity Recognition based on High-Level Reasoning - An Experimental Study Evaluating Proximity to Objects and Pose Information ""


=== 2015 ===
Area: Theory and MethodsBest Paper Award: Mohamed-Rafik Bouguelia, Yolande Belaïd and Abdel Belaïd. ""Stream-based Active Learning in the Presence of Label Noise""
Best Student Paper: João Costa and Jaime S. Cardoso. ""oAdaBoost""
Area: ApplicationsBest Paper Award: Wei Quan, Bogdan Matuszewski and Lik-Kwan Shark. ""3-D Shape Matching for Face Analysis and Recognition""
Best Student Paper: Julia Richter, Christian Wiede and Gangolf Hirtz. ""Mobility Assessment of Demented People Using Pose Estimation and Movement Detection""


=== 2014 ===
Area: Theory and MethodsBest Paper Award: Jameson Reed, Mohammad Naeem and Pascal Matsakis. ""A First Algorithm to Calculate Force Histograms in the Case of 3D Vector Objects""
Best Student Paper: Johannes Herwig, Timm Linder and Josef Pauli. ""Removing Motion Blur using Natural Image Statistics""
Area: ApplicationsBest Paper Award: Sebastian Kurtek, Chafik Samir and Lemlih Ouchchane. ""Statistical Shape Model for Simulation of Realistic Endometrial Tissue""
Best Student Paper: Florian Baumann, Jie Lao, Arne Ehlers and Bodo Rosenhahn. ""Motion Binary Patterns for Action Recognition""


=== 2013 ===
Area: Theory and MethodsBest Paper Award: Barbara Hammer, Andrej Gisbrecht and Alexander Schulz. ""Applications of Discriminative Dimensionality Reduction""
Best Student Paper: Cristina Garcia-Cardona, Arjuna Flenner and Allon G. Percus. ""Multiclass Diffuse Interface Models for Semi-supervised Learning on Graphs""
Area: ApplicationsBest Paper Award: Yoshito Otake, Carneal Catherine, Blake Lucas, Gaurav Thawait, John Carrino, Brian Corner, Marina Carboni, Barry DeCristofano, Michale Maffeo, Andrew Merkle and Mehran Armand. ""Prediction of Organ Geometry from Demographic and Anthropometric Data based on Supervised Learning Approach using Statistical Shape Atlas""
Best Student Paper: James Lotspeich and Mathias Kolsch. ""Tracking Subpixel Targets with Critically Sampled Optics""


=== 2012 ===
Area: Theory and MethodsBest Paper Award: Martin Emms and Hector-Hugo Franco-Penya. ""ON ORDER EQUIVALENCES BETWEEN DISTANCE AND SIMILARITY MEASURES ON SEQUENCES AND TREES""
Best Student Paper: Anna C. Carli, Mario A. T. Figueiredo, Manuele Bicego and Vittorio Murino. ""GENERATIVE EMBEDDINGS BASED ON RICIAN MIXTURES""
Area: ApplicationsBest Paper Award: Laura Antanas, Martijn van Otterlo, José Oramas, Tinne Tuytelaars and Luc De Raedt. ""A RELATIONAL DISTANCE-BASED FRAMEWORK FOR HIERARCHICAL IMAGE UNDERSTANDING""
Best Student Paper: Laura Brandolini and Marco Piastra. ""COMPUTING THE REEB GRAPH FOR TRIANGLE MESHES WITH ACTIVE CONTOURS""


== External Links ==
Science and Technology Events
Conference website
Science and Technology Publications
Event management system
WikiCfp call for papers


== References =="
97,Vitech Corporation,54589352,13051,"Vitech Corporation
Vitech Corporation is a systems engineering company responsible for the development and management of two model-based systems engineering tools, CORE and GENESYS. Vitech products have a range of applications and have been used for program management by the U.S. Department of Energy,. railway modernization and waste management in Europe, and space station and ground-based air defense system development in Australia. In an effort to promote the study of model-based systems engineering, Vitech partners with universities throughout the United States, providing them with its software for instructional and research purposes.


== History ==
Vitech Corporation was established in 1992 in Vienna, Virginia by David Long, then an undergrad student at Virginia Tech. Long, who at the time was majoring in engineering science and mechanics and studying under Benjamin Blanchard and Wolter Fabrycky, developed a software program to meet the requirements for a senior project. He began the project as a tool for academic use, then refined it to make CORE, a modeling environment for systems engineering problems, while earning his master’s degree in Systems Engineering at Virginia Tech. Long initially sought to license the program through an existing company, but eventually opted to manage and market the product himself, establishing Vitech Corporation in the process.
CORE has gone on to become a tool used in the teaching of model-based systems engineering, and is cited in engineering textbooks such as Systems Engineering: Design Principles and Models, by Dahai Liu, The Engineering Design of Systems Models and Methods (pp. 62-66), by Dennis M. Buede and William D. Miller, and System Engineering Management (p. 243), by Benjamin S. Blanchard and John E. Blyler. CORE is offered free to universities such as MIT and the Naval Postgraduate School as part of the Vitech University Program.
In 2011, David Long, president, and Zane Scott, vice president of Professional Services at Vitech, wrote A Primer for Model-Based Systems Engineering. The book outlines the systems thinking approach and reviews the basic concepts of model-based systems engineering.


== Products ==
CORE - Vitech's original product, CORE, is a systems engineering software tool whose principal feature is a single, integrated model that supports model-based systems engineering. CORE is used widely at corporations and governmental organizations and referenced in systems engineering textbooks such as A Practical Guide to SysML, Systems Engineering: Design Principles and Models, The Engineering Design of Systems Models and Methods, and System Engineering Management.
GENESYS - In 2011, Vitech launched GENESYS, a systems engineering tool built on the .NET Framework with MATLAB connectivity that delivers connected, enterprise-wide systems engineering. GENESYS is equipped to integrate the systems development process while offering full connectivity to the greater enterprise environment including analytical engineering, project management, and stakeholder engagement. BusinessWire reported on the announcement and release of GENESYS, highlighting its new features.
A Primer for Model-Based Systems Engineering - Published by Vitech in 2011 and authored by Vitech President David Long and Vice President for Professional Services Zane Scott, this primer presents the basic concepts of model-based systems engineering. The book is offered free from Vitech, and is frequently used in university courses to introduce concepts in model-based systems engineering. In 2012, BusinessWire noted the release of the second edition of the book. In the fall of 2017, Massachusetts Institute of Technology used the primer as part of its four-course online program, Architecture and Systems Engineering: Models and Methods to Manage Complex Systems.


== Services ==
Vitech Corporation offers a range of systems engineering consulting services to private sector firms as well as governmental organizations such as DOD, DOE, and NASA.


== Service of company officers to INCOSE ==
David Long, President of Vitech, and Zane Scott, Vice President of Professional Services at Vitech, have both served as officers for the International Council on Systems Engineering, or INCOSE, the international nonprofit trade organization for the discipline. Long served as president from 2014-2016 and as committee chair of the INCOSE Nominations and Elections Committee (2016 - present), and Scott as member of the INCOSE Working Groups (2013 - present), the INCOSE Board of Directors (2016 - present), and as co-chair of the INCOSE Corporate Advisory Board (2016 - ongoing). Scott is also a member of INCOSE's Institute for Technical Leadership.


== References ==


== External Links ==
Official website
A Primer for Model-Based Systems Engineering


== Further reading ==
Dennis M. Buede and William D. Miller, The Engineering Design of Systems Models and Methods, third edition (Hoboken: John Wiley & Sons, 2016), 62-66.
Benjamin S. Blanchard and John E. Blyler, System Engineering Management, fifth edition (Hoboken: John Wiley & Sons, 2016), 243.
Sanford Friedenthal, Alan Moore, and Rick Steiner, A Practical Guide to SysML (Waltham: Morgan Kaufmann Publishing, an imprint of Elsevier, 2012), 11.
Dahai Liu, Systems Engineering Design Principles and Models (CRC Press, 2016), 59, 104-108, 127-133, 138-139."
98,One-way function,363890,12805,"In computer science, a one-way function is a function that is easy to compute on every input, but hard to invert given the image of a random input. Here, ""easy"" and ""hard"" are to be understood in the sense of computational complexity theory, specifically the theory of polynomial time problems. Not being one-to-one is not considered sufficient of a function for it to be called one-way (see Theoretical Definition, below).
The existence of such one-way functions is still an open conjecture. In fact, their existence would prove that the complexity classes P and NP are not equal, thus resolving the foremost unsolved question of theoretical computer science. The converse is not known to be true, i.e. the existence of a proof that P and NP are not equal would not directly imply the existence of one-way functions.
In applied contexts, the terms ""easy"" and ""hard"" are usually interpreted relative to some specific computing entity; typically ""cheap enough for the legitimate users"" and ""prohibitively expensive for any malicious agents"". One-way functions, in this sense, are fundamental tools for cryptography, personal identification, authentication, and other data security applications. While the existence of one-way functions in this sense is also an open conjecture, there are several candidates that have withstood decades of intense scrutiny. Some of them are essential ingredients of most telecommunications, e-commerce, and e-banking systems around the world.


== Theoretical definition ==
A function f : {0,1}* → {0,1}* is one-way if and only if f can be computed by a polynomial time algorithm, but any polynomial time randomized algorithm 
  
    
      
        F
      
    
    {\displaystyle F}
   that attempts to compute a pseudo-inverse for f succeeds with negligible probability. That is, for all randomized algorithms 
  
    
      
        F
      
    
    {\displaystyle F}
   , all positive integers c and all sufficiently large n = length(x) ,

  
    
      
        Pr
        [
        
        f
        (
        F
        (
        f
        (
        x
        )
        )
        )
        =
        f
        (
        x
        )
        
        ]
        
        
        <
        
        
        
          n
          
            
            −
            
            c
          
        
        
        
        ,
      
    
    {\displaystyle \Pr[\;f(F(f(x)))=f(x)\;]\,\,<\,\,n^{\!-\,c}\;\;,}
  
where the probability is over the choice of x from the discrete uniform distribution on {0,1}n, and the randomness of 
  
    
      
        F
      
    
    {\displaystyle F}
  .
Note that, by this definition, the function must be ""hard to invert"" in the average-case, rather than worst-case sense. This is different from much of complexity theory (e.g., NP-hardness), where the term ""hard"" is meant in the worst-case. That is why even if some candidates for one-way functions (described below) are known to be NP-complete, it does not imply their one-wayness. The latter property is only based on the lack of known algorithm to solve the problem.
It is not sufficient to make a function ""lossy"" (not one-to-one) to have a one-way function. In particular, the function that outputs the string of n zeros on any input of length n is not a one-way function because it is easy to come up with an input that will result in the same output. More precisely: For such a function that simply outputs a string of zeroes, an algorithm F that just outputs any string of length n on input f(x) will ""find"" a proper preimage of the output, even if it is not the input which was originally used to find the output string.


== Related concepts ==
A one-way permutation is a one-way function that is also a permutation—that is, a one-way function that is both injective and surjective. One-way permutations are an important cryptographic primitive, and it is not known if their existence is implied by the existence of one-way functions.
A trapdoor one-way function or trapdoor permutation is a special kind of one-way function. Such a function is hard to invert unless some secret information, called the trapdoor, is known.
A collision-free hash function f is a one-way function that is also collision-resistant; that is, no randomized polynomial time algorithm can find a collision—distinct values x, y such that f(x) = f(y)—with non-negligible probability.


== Theoretical implications of one-way functions ==
If f is a one-way function, then the inversion of f would be a problem whose output is hard to compute (by definition) but easy to check (just by computing f on it). Thus, the existence of a one-way function implies that FP≠FNP, which in turn implies that P≠NP. However, it is not known whether P≠NP implies the existence of one-way functions.
The existence of a one-way function implies the existence of many other useful concepts, including:
Pseudorandom generators
Pseudorandom function families
Bit commitment schemes
Private-key encryption schemes secure against adaptive chosen-ciphertext attack
Message authentication codes
Digital signature schemes (secure against adaptive chosen-message attack)
The existence of one-way functions also implies that there is no natural proof for P≠NP.


== Candidates for one-way functions ==
The following are several candidates for one-way functions (as of April 2009). Clearly, it is not known whether these functions are indeed one-way; but extensive research has so far failed to produce an efficient inverting algorithm for any of them.


=== Multiplication and factoring ===
The function f takes as inputs two prime numbers p and q in binary notation and returns their product. This function can be ""easily"" computed in O(n2) time, where n is the total length (number of bits) of the inputs. Inverting this function requires finding the factors of a given integer N. The best factoring algorithms known run in 
  
    
      
        O
        
          (
          
            exp
            ⁡
            
              
                
                  
                    
                      64
                      9
                    
                  
                  b
                  (
                  log
                  ⁡
                  b
                  
                    )
                    
                      2
                    
                  
                
                
                  3
                
              
            
          
          )
        
      
    
    {\displaystyle O\left(\exp {\sqrt[{3}]{{\frac {64}{9}}b(\log b)^{2}}}\right)}
  time, where b is the number of bits needed to represent N.
This function can be generalized by allowing p and q to range over a suitable set of semiprimes. Note that f is not one-way for randomly selected integers p,q>1, since the product will have 2 as a factor with probability 3/4 (because the probability that an arbitrary p is odd is 1/2, and likewise for q, so if they're chosen independently, the probability that both are odd is therefore 1/4; hence the probability that p or q is even is 1 - 1/4 = 3/4).


=== The Rabin function (modular squaring) ===
The Rabin function, or squaring modulo 
  
    
      
        N
        =
        p
        q
      
    
    {\displaystyle N=pq}
  , where p and q are primes is believed to be a collection of one-way functions. We write

  
    
      
        
          
            R
            a
            b
            i
            n
          
          
            N
          
        
        (
        x
        )
        ≜
        
          x
          
            2
          
        
        
          mod
          
            N
          
        
      
    
    {\displaystyle \mathrm {Rabin} _{N}(x)\triangleq x^{2}{\bmod {N}}}
  
to denote squaring modulo N: a specific member of the Rabin collection. It can be shown that extracting square roots, i.e. inverting the Rabin function, is computationally equivalent to factoring N (in the sense of polynomial-time reduction). Hence it can be proven that the Rabin collection is one-way if and only if factoring is hard. This also holds for the special case in which p and q are of the same bit length. The Rabin cryptosystem is based on the assumption that this Rabin function is one-way.


=== Discrete exponential and logarithm ===
The function f takes a prime number p and an integer x between 0 and p−1; and returns the remainder of 2x divided by p. Modular exponentiation can be done in time O(n3) where n is the number of bits in p. Inverting this function requires computing the discrete logarithm modulo p; namely, given a prime p and an integer y between 0 and p−1, find x such that 2x = y. As of 2009, there is no published algorithm for this problem that runs in polynomial time. The ElGamal encryption scheme is based on this function.


=== Cryptographically secure hash functions ===
There are a number of cryptographic hash functions that are fast to compute, such as SHA 256. Some of the simpler versions have fallen to sophisticated analysis, but the strongest versions continue to offer fast, practical solutions for one-way computation. Most of the theoretical support for the functions are more techniques for thwarting some of the previously successful attacks.


=== Elliptic curves ===
An elliptic curve is a set of pairs of elements of a field satisfying y2 = x3 + ax + b. For cryptography, finite fields must be used. The elements of the curve form a group under an operation called ""point addition"" (which is not the same as the addition operation of the field). Multiplication kP of a point P by an integer k (i.e., a group action of the additive group of the integers) is defined as repeated addition of the point to itself. If k and P are known, it is easy to compute R = kP, but if only R and P are known, it is assumed to be hard to compute k.


=== Other candidates ===
Other candidates for one-way functions have been based on the hardness of the decoding of random linear codes, the subset sum problem (Naccache-Stern knapsack cryptosystem), or other problems.


== Universal one-way function ==
There is an explicit function f that has been proved to be one-way, if and only if one-way functions exist. In other words, if any function is one-way, then so is f. Since this function was the first combinatorial complete one-way function to be demonstrated, it is known as the ""universal one-way function"". The problem of finding a one way function is thus reduced to proving that one such function exists.


== See also ==
One-way compression function
Cryptographic hash function
Geometric cryptography
Trapdoor function


== References ==


== Further reading ==
Jonathan Katz and Yehuda Lindell (2007). Introduction to Modern Cryptography. CRC Press. ISBN 1-58488-551-3.
Michael Sipser (1997). Introduction to the Theory of Computation. PWS Publishing. ISBN 0-534-94728-X.  Section 10.6.3: One-way functions, pp. 374–376.
Christos Papadimitriou (1993). Computational Complexity (1st ed.). Addison Wesley. ISBN 0-201-53082-1.  Section 12.1: One-way functions, pp. 279–298."
99,"World Multiconference on Systemics, Cybernetics and Informatics",2379841,12798,"WMSCI, the World Multi-conference on Systemics, Cybernetics and Informatics, is a conference that has occurred annually since 1995, which emphasizes the systemic relationships that exist or might exist among different disciplines in the fields of Systemics, Cybernetics, and Informatics. Critics describe the conference as having ""loose standards"", since it has accepted papers of dubious academic merit. Organizers stress inter-disciplinary communication, describing the conference as both wide in scope as a general international scientific meeting, and specifically focused in the manner of a subject-area conference.


== History ==
WMSCI is organized by the International Institute of Informatics and Systemics: IIIS (www.iiis.org). Its General Chair has usually been retired Professor Nagib Callaos. The conference is often held in Orlando, Florida.
The annual WMSCI Conference started in Baden-Baden, Germany in 1995 as ISAS (Information Systems Analysis and Synthesis). About 50 papers were presented. In 1997, after earning the non-financial sponsorship of the World Organization of Systems and Cybernetics (WOSC), the conference name was changed to World Conference on Systemics, Cybernetics, and Informatics: SCI. In 2005 the acronym was changed to WMSCI because SCI coincided with the acronym of the Scientific Citation Index, and colocated conferences had been added since then. The multi-conference has grown to have about 900 registered participants, as of 2004; the majority of attendees present papers. Since its inception in 1995, more than 10,000 papers have been presented in WMSCI and its collocated conferences.
Until 2005, WMSCI allowed about 15% of non-reviewed submissions, based on the importance of the topic or the potential presenter's curriculum vitae. In a workshop founded by the National Science Foundation, the general chair of the conference explained that they accepted non-reviewed papers because the conference is multi-disciplinary and scholarly associations of several conferences accept almost all submissions on a non-reviewed basis. The most prestigious and largest conferences of OR/MS (IFORS and INFORMS), for example, explicitly state on their web sites that ""Contributed abstracts are not reviewed and virtually all abstracts are accepted."" Consequently, WMSCI 2005 general chair, Dr. Nagib Callaos, stated that since the conference is multi-disciplinary, he saw no problem with accepting for presentation (not necessarily publication) 15% of non-reviewed submissions, in order to follow the standards of other disciplines like those represented by the International Federation of Operations Research Societies (IFORS), Institute for Operations Research and the Management Sciences (INFORMS, the International Federation of Operations Research Societies (IFORS), the American Mathematical Society, etc.
Since 2006 just-reviewed papers are accepted, authors of accepted papers have access to the reviews of the reviewers who recommended the acceptance of their paper, and the reviewing process is based on double-blind and non-blind reviewing.


== Mission ==
WMSCI is a multi-disciplinary conference where participants focusing on one discipline may attend conferences from related areas. According to the organizers, ""this systemic approach stimulates cross-fertilization among different disciplines, inspiring scholars, originating new hypothesis, supporting production of innovations and generating analogies, which is ... a fundamental aim in cybernetics"". Objectives of the conference include identification of synergetic relationships among Systemics, Cybernetics and Informatics, and establishing communication channels among academic, professional, and business worlds. Their mission statement is as ""a forum for focusing into specific disciplinary research, as well as for multi, inter and trans-disciplinary studies and projects"". Some say that its mission is rather opaque. This opaqueness might be perceived from a strictly disciplinary, or sub-disciplinary perspective. WMSCI is intended to be a forum for both inter-disciplinary scholars, researchers, and professionals, as well as disciplinary researchers who are interested in presenting their disciplinary research and share information and knowledge with researchers from other disciplinary researchers aiming for potential cross-fertilization and analogical thinking, which provide input to logical thinking and empirical hypothesis formulation.


== SCIgen paper acceptance and exposure of citing non-refereed papers ==

In 2005, Jeremy Stribling, Daniel Aguayo, and Maxwell Krohn, three computer-science graduate students at MIT, submitted to WMSCI the paper Rooter: A Methodology for the Typical Unification of Access Points and Redundancy, generated by SCIgen, software they had developed to create nonsense papers in Computer Science. Stribling stated that the paper was submitted to WMSCI because of its repeated e-mails. In his words: ""You see lists of speakers, and there's no one you've ever heard of... They spam us."" Papers generated by this software were submitted to other similar conferences. The promotional policy of WMSCI followed strictly the USA Congress CAN-SPAM Act of 2003-2008. Following criticism of the per-conference-participant fees and their acceptance of talks without any review the organization responded as follows:

[We] think it is legitimate and academically respectful to accept non-reviewed papers, especially if we take into account that in the call for papers in our conferences has always been clearly stated that we accept NON-RESEARCH papers submission, as it is the case of position papers, invited papers, case studies, panels' presentations, reports, etc. which are usually accepted, or not, on a non-reviewing base.

Since 2006, the organization abandoned the policy of accepting for presentation (not necessarily publication)15% of non-refereed papers. Papers of non-refereed presentations were eligible for publication in the post-conference edition of the proceedings if they are selected as the best paper by the respective session audience. Motivating their previous acceptance policy, based on 15% of non-refereed presentation which respective papers might be published if they are selected as the best of the respective sessions, the organization says:

In a survey made by the National Cancer Institute where ""active, resilient, generally successful scientist researchers"" were interviewed, just 17.7 percent of them disagreed with the statement ""reviewers are reluctant to support unorthodox or high-risk research""... This is one of the reasons why, in WMSCI Conferences, we accepted in the past non-reviewed papers taking the intrinsic risks of this kind of paper acceptances. Deception was a risk that was not perceived at the moment of examining the risks of this kind of acceptance policy.


=== Post-2005 changes of refereeing policies ===
The average acceptance rate in 2009 at WMSCI and related conferences was 32.12%, with the average of 4.93 reviewers per paper/abstract. A combination of double blind peer reviews and non-anonymous peer reviews was used, with a total of 11902 reviews for the 2413 submissions. The organizers claim that for the refereed papers, the acceptance policy of WMSCI is the majority rule in both kind of reviewing, i.e. A majority of double blind reviewers and a majority of non-anonymous reviewers should recommend the acceptance of a submissions in order to be accepted for presentation and publication in the respective proceedings. More details regarding this two-tier reviewing methodology can be found at http://iiis.org/peer-reviewing.asp. when there is a tie between any kind of reviewers (double blind or non-anonymous)the tendency is to accept the respective submission. When there are a double tie among both kind of reviewers the tendency is to not accept the respective submission. This policy is (according to some people who contributed to this article) more liberal than that of other conferences using the alternative strategy, where papers are accepted when there is the positive agreement of the reviewers (This affirmation need to be referenced) According to the organizers, WMSCI's policy would probably increase the chances of accepting poor papers, but certainly decreases the chances of rejecting good papers. Motivating their acceptance policy, the organizers cite Ernst et al. who showed that the same paper was rated from ""unacceptable"" to ""excellent"" according to 6 out of 9 measures by 45 field experts. According to the organizers, their acceptance policy may be better suited for the purpose of bringing together multi-disciplinary engineering communities, and may reduce possibilities of plagiarism and fraud generated by the reviewing process (see also peer review, peer review failure).


== See also ==
Oxford Round Table


== References =="
100,IEEE Computer Society,12953246,12768,"IEEE Computer Society (sometimes abbreviated Computer Society or CS) is a professional society of IEEE. Its purpose and scope is ""to advance the theory, practice, and application of computer and information processing science and technology"" and the ""professional standing of its members."" The CS is the largest of 39 technical societies organized under the IEEE Technical Activities Board.
The Computer Society sponsors workshops and conferences, publishes a variety of peer-reviewed literature, operates technical committees, and develops IEEE computing standards. It supports more than 200 chapters worldwide and participates in educational activities at all levels of the profession, including distance learning, accreditation of higher education programs in computer science, and professional certification in software engineering.
The IEEE Computer Society is also a member organization of the Federation of Enterprise Architecture Professional Organizations (a worldwide association of professional organizations which have come together to provide a forum to standardize, professionalize, and otherwise advance the discipline of Enterprise Architecture).


== History ==

IEEE Computer Society traces its origins to the Subcommittee on Large-Scale Computing, established in 1946 by the American Institute of Electrical Engineers (AIEE), and to the Professional Group on Electronic Computers (PGEC), established in 1951 by the Institute of Radio Engineers (IRE). When the AIEE merged with the IRE in 1963 to form the Institute of Electrical and Electronics Engineers (IEEE), these two committees became the IEEE Computer Group. The group established its own constitution and bylaws in 1971 to become the IEEE Computer Society.
The CS maintains its headquarters in Washington, D.C. and additional offices in California and Japan.


== Main activities ==

The Computer Society maintains volunteer boards in six program areas: education, membership, professional activities, publications, standards, and technical and conference activities. In addition, 12 standing committees administer activities such as the CS elections and its awards programs to recognize professional excellence.


=== Education and professional development ===
The Computer Society participates in ongoing development of college computing curricula, jointly with the Association for Computing Machinery (ACM). Other educational activities include software development certification programs and online access to e-learning courseware and books.


=== Publications ===
The Computer Society is a leading publisher of technical material in computing. Its publications include 13 peer-reviewed technical magazines and 20 scholarly journals called Transactions, as well as conference proceedings, books, and a variety of digital products.
The Computer Society Digital Library (CSDL) provides subscriber access to all CS publications. In 2008, the Computer Society launched Computing Now, a Web portal featuring free access to a rotation of CSDL articles, along with technical news, CS blogs, and multimedia content.
As most publications were delivered digitally in 2014, the Computer Society launched the complementary monthly digest Computing Edge magazine, which consists of curated articles from its magazines.


=== Technical conferences and activities ===

The Computer Society sponsors more than 170 technical conferences each year and coordinates the operation of several technical committees, councils, and task forces.
The CS also maintains 12 standards committees to develop IEEE standards in various areas of computer and software engineering (e.g., the Design Automation Standards Committee and the IEEE 802 LAN/MAN Standards Committee).
In 2010 the CS introduced Special Technical Communities (STCs) as a new way for members to develop communities focusing on selected technical areas. Current topics include broadening participation, cloud computing, education, eGov, haptics, multicore, operating systems, smart grids, social networking, sustainable computing, systems engineering, and wearable and ubiquitous technologies.


=== Technical committees ===
IEEE-CS currently has 26 technical committees and two technical councils. A technical committee (TC) is an international network of professionals with common interests in computer hardware, software, its applications, and interdisciplinary fields within the umbrella of IEEE-CS. A technical council is essentially a very large technical committee, and a task force is an emerging technical committee. A TC serves as the focal point of the various technical activities within a technical discipline which influences the standards development, conferences, publications, and educational activities of IEEE-CS. Following are the current technical committees and councils:
Technical Committee on Business Informatics and Systems (TCBIS)
Technical Committee on Computer Architecture (TCCA)
Technical Committee on Computational LIfe Sciences (TCCLS)
Technical Committee on Computer Communications (TCCC)
Technical Committee on Data Engineering (TCDE)
Technical Committee on Dependable Computing and Fault Tolerance (TCFT)
Technical Committee on Digital Libraries (TCDL)
Technical Committee on Distributed Processing (TCDP)
Technical Committee on Intelligent Informatics (TCII)
Technical Committee on Internet (TCI)
Technical Committee on Learning Technology (TCLT)
Technical Committee on Mathematical Foundations of Computing (TCMF)
Technical Committee on Microprocessors and Microcomputers (TCMM)
Technical Committee on Microprogramming and Microarchitecture (TCuARCH)
Technical Committee on Multimedia Computing (TCMC)
Technical Committee on Multiple-Valued Logic (TCMVL)
Technical Committee on Pattern Analysis and Machine Intelligence (TCPAMI)
Technical Committee on Parallel Processing (TCPP)
Technical Committee on Real-Time Systems (TCRTS)
Technical Committee on Scalable Computing (TCSC)
Technical Committee on Security and Privacy (TCSP)
Technical Committee on Semantic Computing (TCSEM)
Technical Committee on Services Computing (TCSVC)
Technical Committee on Simulation (TCSIM)
Technical Committee on Visualization and Graphics (VGTC)
Technical Committee on VLSI
Technical Council on Software Engineering (TCSE)
Technical Council on Test Technology (TTTC)


=== Awards ===
The IEEE Computer Society recognizes outstanding work by computer professionals who advance the field in three areas of achievement: Technical Awards (e.g., the Seymour Cray Computer Engineering Award or the IEEE Computer Pioneer Award), Education Awards (e.g., Taylor L. Booth Education Award), and Service Awards (e.g., Richard E. Merwin Distinguished Service Award).


== See also ==
Association for Computing Machinery
Association of Information Technology Professionals
Australian Computer Society
British Computer Society
Canadian Information Processing Society
Computer Society of India
IEEE Technical Activities Board
Institute of Electrical and Electronics Engineers
Institution of Analysts and Programmers
ISCA Influential Paper Award
New Zealand Computer Society


== References ==


== External links ==
Official website"
101,Symbolic execution,607674,12741,"In computer science, symbolic execution (also symbolic evaluation) is a means of analyzing a program to determine what inputs cause each part of a program to execute. An interpreter follows the program, assuming symbolic values for inputs rather than obtaining actual inputs as normal execution of the program would, a case of abstract interpretation. It thus arrives at expressions in terms of those symbols for expressions and variables in the program, and constraints in terms of those symbols for the possible outcomes of each conditional branch.
The field of symbolic simulation applies the same concept to hardware. Symbolic computation applies the concept to the analysis of mathematical expressions.


== Example ==
Consider the program below, which reads in a value and fails if the input is 6.

During a normal execution (""concrete"" execution), the program would read a concrete input value (e.g., 5) and assign it to y. Execution would then proceed with the multiplication and the conditional branch, which would evaluate to false and print OK.
During symbolic execution, the program reads a symbolic value (e.g., λ) and assigns it to y. The program would then proceed with the multiplication and assign λ * 2 to z. When reaching the if statement, it would evaluate λ * 2 == 12. At this point of the program, λ could take any value, and symbolic execution can therefore proceed along both branches, by ""forking"" two paths. Each path gets assigned a copy of the program state at the branch instruction as well as a path constraint. In this example, the path constraint is λ * 2 == 12 for the then branch and λ * 2 != 12 for the else branch. Both paths can be symbolically executed independently. When paths terminate (e.g., as a result of executing fail() or simply exiting), symbolic execution computes a concrete value for λ by solving the accumulated path constraints on each path. These concrete values can be thought of as concrete test cases that can, e.g., help developers reproduce bugs. In this example, the constraint solver would determine that in order to reach the fail() statement, λ would need to equal 6.


== Limitations ==


=== Path explosion ===
Symbolically executing all feasible program paths does not scale to large programs. The number of feasible paths in a program grows exponentially with an increase in program size and can even be infinite in the case of programs with unbounded loop iterations. Solutions to the path explosion problem generally use either heuristics for path-finding to increase code coverage, reduce execution time by parallelizing independent paths, or by merging similar paths.


=== Program-dependent efficacy ===
Symbolic execution is used to reason about a program path-by-path which is an advantage over reasoning about a program input-by-input as other testing paradigms use (e.g. Dynamic program analysis). However, if few inputs take the same path through the program, there is little savings over testing each of the inputs separately.


=== Environment interactions ===
Programs interact with their environment by performing system calls, receiving signals, etc. Consistency problems may arise when execution reaches components that are not under control of the symbolic execution tool (e.g., kernel or libraries). Consider the following example:

This program opens a file and, based on some condition, writes different kind of data to the file. It then later reads back the written data. In theory, symbolic execution would fork two paths at line 5 and each path from there on would have its own copy of the file. The statement at line 11 would therefore return data that is consistent with the value of ""condition"" at line 5. In practice, file operations are implemented as system calls in the kernel, and are outside the control of the symbolic execution tool. The main approaches to address this challenge are:
Executing calls to the environment directly. The advantage of this approach is that it is simple to implement. The disadvantage is that the side effects of such calls will clobber all states managed by the symbolic execution engine. In the example above, the instruction at line 11 would return ""some datasome other data"" or ""some other datasomedata"" depending on the sequential ordering of the states.
Modeling the environment. In this case, the engine instruments the system calls with a model that simulates their effects and that keeps all the side effects in per-state storage. The advantage is that one would get correct results when symbolically executing programs that interact with the environment. The disadvantage is that one needs to implement and maintain many potentially complex models of system calls. Tools such as KLEE, Cloud9, and Otter take this approach by implementing models for file system operations, sockets, IPC, etc.
Forking the entire system state. Symbolic execution tools based on virtual machines solve the environment problem by forking the entire VM state. For example, in S2E each state is an independent VM snapshot that can be executed separately. This approach alleviates the need for writing and maintaining complex models and allows virtually any program binary to be executed symbolically. However, it has higher memory usage overheads (VM snapshots may be large).


== Tools ==
See also awesome-symbolic-execution, for an ordering of tools per target language.


== History ==
The concept of symbolic execution was introduced academically with descriptions of: the Select system, the EFFIGY system, the DISSECT system, and Clarke's system. See a bibliography of more technical papers published on symbolic execution.


== See also ==

Abstract interpretation
Symbolic simulation
Symbolic computation
Concolic testing
Control flow graph
Dynamic recompilation


== References ==


== External links ==
Symbolic Execution for finding bugs
Symbolic Execution and Software Testing presentation at NASA Ames
Symbolic Execution for Software Testing in Practice – Preliminary Assessment"
102,Computational cognition,1988689,12633,"Computational cognition (sometimes referred to as computational cognition science) is the study of the computational basis of learning and inference by mathematical modeling, computer simulation, and behavioral experiments. In psychology, it is an approach which develops computational models based on experimental results. It seeks to understand the basis behind the human method of processing of information. Early on computational cognitive scientists sought to bring back and create a scientific form of Brentano’s psychology


== Artificial intelligence ==

There are two main purposes for the productions of artificial intelligence: to produce intelligent behaviors regardless of the quality of the results, and to model after intelligent behaviors found in nature. In the beginning of its existence, there was no need for artificial intelligence to emulate the same behavior as human cognition. Until 1960s, economist Herbert Simon and Allen Newell attempted to formalize human problem-solving skills by using the results of psychological studies to develop programs that implement the same problem-solving techniques as people would. Their works laid the foundation for symbolic AI and computational cognition, and even some advancements for cognitive science and cognitive psychology.
The field of symbolic AI is based on the physical symbol systems hypothesis by Simon and Newell, which states that expressing aspects of cognitive intelligence can be achieved through the manipulation of symbols. However, John McCarthy focused more on the initial purpose of artificial intelligence, which is to breakdown the essence of logical and abstract reasoning regardless of whether or not human employs the same mechanism.
Over the next decades, the progress made in artificial intelligence started to be focused more on developing logic-based and knowledge-based programs, veering away from the original purpose of symbolic AI. Researchers started to believe that artificial intelligence may never be able to imitate some intricate processes of human cognition like perception or learning. A chief failing of AI is not being able to achieve a complete likeness to human cognition due to the lack of emotion and the impossibility of implementing it into an AI. They began to take a “sub-symbolic” approach to create intelligence without specifically representing that knowledge. This movement led to the emerging discipline of computational modeling, connectionism, and computational intelligence.


== Computational modeling ==

As it contributes more to the understanding of human cognition than artificial intelligence, computational cognitive modeling emerged from the need to define various cognition functionalities (like motivation, emotion, or perception) by representing them in computational models of mechanisms and processes. Computational models study complex systems through the use of specific algorithms and extensive computational resources, or variables, to produce computer simulation. Simulation is achieved by adjusting the variables, changing one alone or even combining them together, to observe the effect on the outcomes. The results help experimenters make predictions about what would happen in the real system if those similar changes were to occur.
When computational models attempt to mimic human cognitive functioning, all the details of the function must be known for them to transfer and display properly through the models, allowing researchers to thoroughly understand and test an existing theory because no variables are vague and all variables are modifiable. Consider a model of memory built by Atkinson and Shiffrin in 1968, it showed how rehearsal leads to long-term memory, where the information being rehearsed would be stored. Despite the advancement it made in revealing the function of memory, this model fails to provide answers to crucial questions like: how much information can be rehearsed at a time? How long does it take for information to transfer from rehearsal to long-term memory? Similarly, other computational models raise more questions about cognition than they answer, making their contributions much less significant for the understanding of human cognition than other cognitive approaches. An additional shortcoming of computational modeling is its reported lack of objectivity.
Nevertheless, computational cognitive models can still contribute to the study of cognition mostly when it is combined with other research approaches, as implements by John Anderson with his ACT-R model. Anderson, a cognitive architecture, uses the functions of computational models and the findings of cognitive neuroscience to develop ACT-R, Adaptive Control of Thought-Rational. The model is based on the theory that the brain consists of several modules which perform specialized functions separate of each other. Since it only focuses on the properties appropriate for understanding the specific cognitive function of memory, the ACT-R model is classified as a symbolic approach to cognitive science.


== Connectionist network ==

Another approach which deals more with the semantic content of cognitive science is connectionism or neural network modeling. Connectionism relies on the idea that the brain consists of simple units or nodes and the behavioral response comes primarily from the layers of connections between the nodes and not from the environmental stimulus itself.
Connectionist network differs from computational modeling specifically because of two functions: neural back-propagation and parallel-processing. Neural back-propagation is a method utilized by connectionist network to show evidence of learning. After a connectionist network produce a response, the stimulated results are compared to real-life situational results. The feedback provided by the backward propagation of errors would be used to improve accuracy for the network’s subsequent responses. The second function, parallel-processing, stemmed from the belief that knowledge and perception are not limited to specific modules but rather are distributed throughout the cognitive networks. The present of parallel distributed processing has been shown in psychological demonstrations like the Stroop effect, where the brain seems to be analyzing the perception of color and meaning of language at the same time. However, this theoretical approach has been continually disproved because the two cognitive functions for color-perception and word-forming are operating separately and simultaneously, not parallel of each other.
The field of cognition may have benefitted from the use of connectionist network but because of the completed system, setting up the neural network models can be quite a tedious task and the results may be less interpretable than the system they are trying to model. Therefore, the results can be used as evidence for broad theory of cognition without explaining the particular process happening within the cognitive function. Other disadvantages of connectionism lie in the research methods it employs or hypothesis it tests, which has been proven inaccurate or ineffective often, taking connectionist models further from an accurate representation of how the brain functions. These issues cause neural network models to be ineffective on studying higher forms of information-processing, and hinder connectionism from advancing the general understanding of human cognition.


== References ==


== External links and bibliography ==
Berkeley Computational Cognitive Science Lab
Flinders Artificial Intelligence and Cognitive Science Group
MIT Computational Cognitive Science Group
NYU Computation and Cognition Lab
Stanford Computation and Cognition Lab
UCI Memory and Decision Lab
Jacob Feldman"
103,Ahmed K. Elmagarmid,33236835,12396,"Dr Ahmed K. Elmagarmid (born 1954) is a computer scientist, academic and executive. He is the founding Executive Director of Qatar Computing Research Institute, a national research institute under Hamad bin Khalifa University, a member of the Qatar Foundation for Education, Science and Community Development. Since his appointment in 2010, Elmagarmid has focused on large-scale computing challenges that address national priorities for growth and development.
Before joining the Qatar Foundation, Elmagarmid held a number of posts in academia and industry. He was an associate then a full Professor of Computer Science at Purdue University, where he was involved in teaching and research for 22 years, and a director of the Purdue Cyber Center.
He served as a Chief Scientist for Hewlett-Packard's office of Strategy and Technology. He also has worked with or consulted for Telcordia Technologies (formerly known as Bell Communications Research), Bellcore, IBM, CSC, Harris Corporation, D. H. Brown and Associates, MCC, Bell Northern Research, Molecular Design Labs, SOGEI (Italy) and UniSql.
Elmagarmid serves as the Vice Chair of the IT Executive Committee of Sidra Medical and Research Center and is on the board of directors at MEEZA. He is a member of the Executive Committee of Kasra (formerly MENAPOST) and Qatar National Library. He is a founding member of the Qatar Genome Project Committee as well as the Sidra External Scientific Advisory Council.
Elmagarmid is a recipient of the Presidential Young Investigator Award from US President Ronald Reagan. He is an IEEE Fellow, an ACM Fellow and an AAAS Fellow. The University of Dayton and Ohio State University have both named him among their distinguished alumni. He has chaired and served on several program committees and editorial boards.
He also a well-published scientist, authoring six books and more than 180 papers, and has run several well-funded research programs. 


== Early life and education ==
Ahmed K. Elmagarmid was born in Libya in 1954. He received his Bachelor of Science in computer science from the University of Dayton in 1977 and his M.S. (1981) and Ph.D. (1985) in computer science from Ohio State University.


== Work ==
He taught at Pennsylvania State University before joining Purdue University in 1988. While at Purdue University, Elmagarmid founded two successful organizations with funding from the Lilly Endowment and the State of Indiana: the Indiana Center for Database Systems (ICDS) and the Cyber Center (CC). He was responsible for the development of ICDS, securing its initial funding and growing it into the largest academic database group in the United States. His second organizational initiative, The Cyber Center at Discovery Park, promotes information technology and cyber infrastructure across the whole of Purdue University. The Cyber Center is the best funded unit in Purdue’s history, receiving a grant for $105M grant from National Science Foundation. He also founded and led for 3 years a third research center, the Indiana Telemedicine Incubator.
Elmagarmid was appointed corporate Chief Scientist for Hewlett-Packard during HP’s acquisition of Compaq, reporting to the technology council of HP and to the rest of the executive leadership team on competitive threats, deviations and changes in strategies, divestiture, and possible new acquisitions. He also was in charge of product road maps and web services strategy for the company.
He served as Chief of Data Quality at Telcordia Technologies, working on several key applications, including the 1-800 telephone billing system. At Harris Corporation, Elmagarmid was brought in to ensure the timely completion of a large Harris Commercial System’s Southern Company contract for the new XA/21 power control station system.
Between 2006 and 2010, Elmagarmid had served as an advisor to Qatar Foundation to develop a research initiative led by Arab Expatriate Scientists, creating institutes and large-scale projects in various areas of science and technology. He serves as an advisor to the Sidra Medical and Research Center, which is one of the world’s most advanced women and children’s hospitals.
In 1994, he worked with SOGEI in Italy to establish standards for data quality for the Italian Treasury. He worked as an advisor to the Italian Authority for Public Administration (AIPA), an arm of the Italian Government that audits data quality processes for several government systems. He also worked for Techno Padova, an arm of the Chamber of Commerce in the Veneto region in Italy, and lectured at the University of Padova.


== Professional recognition ==
Elmagarmid is an IEEE fellow, an ACM fellow and as AAAS fellow. He received the National Science Foundation’s Presidential Young Investigator Award in 1988. Ohio State University and the University of Dayton have both named him among their distinguished alumni.
Elmagarmid has chaired and served on several program committees and served on several editorial boards. He was the general chair of the 2010 ACM SIGMOD [1]


== Publications ==
Books
McIver, William J. and Elmagarmid, A.K. Advances in Digital Government, Kluwer Academic Press, 2002. 978-1402070679.
Bougettaya, A, Benatallah, B., and A.K., Elmagarmid. Interconnecting Heterogeneous Information Systems, Kluwer Academic Press, 1998. 978-0792382164.
Elmagarmid, A.K., Rusinkiewics, M., Sheth, A. Management of Heterogeneous and Autonomous Database Systems. Published by Morgan Kaufmann, Oct. 1998, 432 pages. ISBN 978-1-55860-216-8.
Elmagarmid, A.K., Jiang, H., Helal, A., Joshi, A., and M. Ahmed. Video Data Bases: Issues, Products and Applications. Kluwer Academic Press, March 1997. 978-0792398721.
Bukhres, O.A. and Elmagarmid, A.K., eds. (1995). Object-Oriented Multidatabase Systems. Prentice Hall Publishing. 978-0131038134.
Elmagarmid, A.K. (Editor). Database Transaction Models for Advanced Applications. Published by Morgan Kaufmann Press, Mar. 1992, 611 pages. ISBN 978-1-55860-214-4.
Journals
Axel Heitmuller, Sarah Henderson, Will Warburton, Ahmed Elmagarmid, Alex Pentland and Ara Darzi. Developing public policy to advance the use of Big Data in health care. Health Affairs, September 2014. 33:1523-1530.


== References ==


== External links ==
Home page at Qatar Computing Research Institute
Qatar Computing Research Institute
http://www.mldas.org/
http://www.qna.org.qa/en-us/News/16031616240071/QCRI-MIT-CSAIL-Scientists-Assemble-for-Annual-Research-Projects-Meeting"
104,Girl Develop It,53073378,12265,"Girl Develop It (GDI) is a nonprofit organization devoted to getting women the materials they need to pursue careers in software development. It provides affordable programs for adult women interested in learning web and software development in a judgment-free environment. Their mission is to give women of any income level, nationality, education level, and upbringing an environment in which to learn the skills to build websites and learn code to build programs with hands-on classes in two countries in 56 cities.


== History ==
Girl Develop It was started in 2010 by Vanessa Hurst and Sara Chipps with their flagship location in New York City. GDI started with just one class that sold out in one day. Today more than 55,000 members have been helped. Between that, they have built up their organization to 53 cities in 33 states and districts in the United States and one in Ottawa, Ontario Canada. Recently, Girl Develop It has empowered more than 1,000 students per month with coding skills. In late 2017, GDI will start addressing international inquiries. The organization and local chapters have hosted or participated in hackathons. During the Buffalo chapter's second event in 2016, developers competed to create websites for nonprofit woman- and minority-owned organizations. The organization has also hosted hackathons in Camden, in Wilmington, and in Seattle.


== Chapters ==


== Curriculum ==
Girl Develop It (GDI) offers materials on their website that are licensed under a Creative Commons (CC BY-NC-SA 4.0) license and that provide visitors with tools and resources to develop online. The curriculum is hosted and constructed by the GDI community on the web-based version control repository GitHub and presented in a slide format, divided by topic. On the GitHub curriculum page, materials are broken up in a color coded format that shows whether they have been reviewed by other members of the community or if the topics meet the requirements or recommendations of the curriculum. These materials are accessible by anyone on the Internet but Girl Development offers in-person courses and social communities at established chapters.
The topics of the curriculum include:
Web Concepts
Beginner and Intermediate level HTML and CSS
Beginner JavaScript
Beginner Angular.js
Web Accessibility
Beginner PHP/MySQL
Beginner GitHub Usage


== Founders ==


=== Vanessa Hurst ===
Hurst is a computer programmer, social entrepreneur, teacher, and lifetime girl scout, and a co-founder of Girl Develop It. In 2013 she launched the CodeMontage platform. She is also responsible for founding and running Developers For Good and also NYC-based Network of Technologists. She is a strong advocator for computing, software, and coding for everyone. She is currently based in New York City.


=== Sara Chipps ===
Chipps is a JavaScript developer and co-founder of Girl Develop It. Once the CTO of Flatiron School, she is currently the CEO of Jewelbots, a friendship bracelet that helps children learn to code. She is currently based in New York City.


== Current Leaders ==


=== Corinne Warnshuis, Executive Director ===
Corinne has previously held positions with the Technical.ly news network, producing large-scale technology conferences like Philly Tech Week and Baltimore Innovation Week. 


=== Vanessa Hurst, Co-Founder, Advisor ===
Vanessa is a lifetime Girl Scout, a teacher, and a passionate supporter of open source software. She also founded organizes Developers for Good. 


=== Bindu Jallabah, Operations Director ===
Prior to joining GDI, Bindu won awards for her work developing and executing the operational strategy for the Elwyn Baring Street Center. Bindu is also founder and Board Chair of Karanso Africa.


== Supporters ==
Girl Develop It (GDI) lists numerous companies and organizations on their website that have backed, partnered with, or supported them and their cause.
Notable Supporters include:
American Online (AOL)
Black Girls CODE
Bold is Beautiful Project
CodeMontage
Craigslist
DuckDuckGo
Knight Foundation
Salesforce
Sticker Mule
Qualcomm


== References ==


== External links ==
Official website"
105,List of important publications in cryptography,24562032,12222,"This is a list of important publications in cryptography, organized by field.
Some reasons why a particular publication might be regarded as important:
Topic creator – A publication that created a new topic
Breakthrough – A publication that changed scientific knowledge significantly
Influence – A publication which has significantly influenced the world or has had a massive impact on the teaching of cryptography.


== Cryptanalysis ==


=== The index of coincidence and its applications in cryptology ===
Friedman, William F. (1922). ""The index of coincidence and its applications in cryptology"". Department of Ciphers. Publ 22. Geneva, Illinois, USA: Riverbank Laboratories. 
Description: Presented the index of coincidence method for codebreaking.


=== Treatise on the Enigma ===
Turing, Alan (1939–1942). Treatise on the Enigma. CS1 maint: Date format (link)
Description: The breaking of the Enigma.


=== The Codebreakers: The Story of Secret Writing ===
Kahn, David (1967). The Codebreakers: The Story of Secret Writing. New York: The Macmillan Company. ISBN 0-684-83130-9. 
Description: Almost nothing had been published in cryptography in several decades and very few non-government researchers were thinking about it. The Codebreakers, a popular and non academic book, made many more people aware and contains a lot of technical information, although it requires careful reading to extract it. Its 1967 appearance was followed by the appearance of many papers over the next few years.


=== Differential Cryptanalysis of DES-like Cryptosystems ===
Biham, Eli; Shamir, Adi (1991). ""Differential Cryptanalysis of DES-like Cryptosystems"". Journal of Cryptology. 4 (1): 3–72. doi:10.1007/bf00630563. 
Description: The method of differential cryptanalysis.


=== A new method for known plaintext attack of FEAL cipher ===
Matsui, Mitsuru; Yamagishi, Atsuhiro (1993). ""A New Method for Known Plaintext Attack of FEAL Cipher"". Advances in Cryptology — EUROCRYPT '92. Lecture Notes in Computer Science. 658. pp. 81–91. doi:10.1007/3-540-47555-9_7. 
Description: The method of linear cryptanalysis.


== Theory ==


=== Communication Theory of Secrecy Systems ===
Shannon, C.E. (1949). ""Communication Theory of Secrecy Systems"" (PDF). Bell System Technical Journal. 28 (28-4): 656–715. doi:10.1002/j.1538-7305.1949.tb00928.x. 
Description: Information theory based analysis of cryptography. The original form of this paper was a confidential Bell Labs report from 1945, not the one published.


=== Probabilistic Encryption ===
Goldwasser, Shafi; Micali, Silvio (April 1984). ""Probabilistic Encryption"" (PDF). Journal of Computer and Systems Sciences. 28 (2): 270–299. doi:10.1016/0022-0000(84)90070-9. 
Description: The paper provides a rigorous basis to encryption (e.g., partial information) and shows that it possible to equate the slightest cryptanalysis to solve a pure math problem. Second, it introduces the notion of computational indistinguishability.


=== Proofs that Yield Nothing But their Validity or All Languages in NP have Zero-Knowledge Proofs ===
Goldreich, O.; Micali, S.; Wigderson, A. (July 1991). ""Proofs that yield nothing but their validity or all languages in NP have zero-knowledge proof systems"" (PDF). Journal of the ACM. 38 (3): 690–728. doi:10.1145/116825.116852. 
Description: This paper explains how to construct a zero-knowledge proof system for any language in NP.


== Private key cryptography ==


=== Cryptographic Coding for Data-Bank Privacy ===
Feistel, Horst (18 March 1970). ""Cryptographic Coding for Data-Bank Privacy"". IBM Research Report 2827. 
Description: Feistel ciphers are a form of cipher of which DES is the most important. It would be hard to overestimate the importance of either Feistel or DES. Feistel pushed a transition from stream ciphers to block ciphers. Although most ciphers operate on streams, most of the important ciphers today are block ciphers at their core.


=== Data Encryption Standard ===
NBS Federal Standard FIPS PUB 46, 15 Jan 1977.
Description: DES is not only one of the most widely deployed ciphers in the world but has had a profound impact on the development of cryptography. Roughly a generation of cryptographers devoted much of their time to attacking and improving DES.


== Public Key Cryptography ==


=== New directions in cryptography ===
Diffie, W.; Hellman, M. (1976). ""New directions in cryptography"" (PDF). IEEE Transactions on Information Theory. 22 (6): 644–654. doi:10.1109/TIT.1976.1055638. 
Description: This paper suggested public key cryptography and presented Diffie–Hellman key exchange. For more information about this work see: W.Diffie, M.E.Hellman, ""Privacy and Authentication: An Introduction to Cryptography"", in Proc. IEEE, Vol 67(3) Mar 1979, pp 397–427.


=== On the Signature Reblocking Problem in Public Key Cryptography ===
Kohnfelder, Loren M. (1978). ""On the Signature Reblocking Problem in Public Key Cryptography"". Communications of the ACM. 21 (2): 179. 
Description: In this paper (along with Loren M. Kohnfelder,""Using Certificates for Key Distribution in a Public-Key Cryptosystem"", MIT Technical report 19 May 1978), Kohnfelder introduced certificates (signed messages containing public keys) which are the heart of all modern key management systems.


=== Secure Communications Over Insecure Channels ===
Merkle, R. C. (April 1978). ""Secure Communications Over Insecure Channels"". Communications of the ACM. 21 (4): 294–299. doi:10.1145/359460.359473. 
Description: This paper introduced a branch of public key cryptography, known as public key distribution systems. Merkle's work predated ""New directions in cryptography"" though it was published after it. The Diffie–Hellman key exchange is an implementation of such a Merkle system. Hellman himself has argued that a more correct name would be Diffie–Hellman–Merkle key exchange.


=== A Method for Obtaining Digital Signatures and Public Key Cryptosystems ===
Rivest, R. L.; Shamir, A.; Adleman, L. (1978). ""A method for obtaining digital signatures and public-key cryptosystems"". Communications of the ACM. 21 (2): 120–126. doi:10.1145/359340.359342. Archived from the original on 2003-12-04. 
Description: The RSA encryption method. The first public-key encryption method.


=== How to Share a Secret ===
Shamir, A. (November 1979). ""How to share a secret"". Communications of the ACM. 22 (11): 612–613. doi:10.1145/359168.359176. Archived from the original on 2015-05-04. 
Description: A safe method for sharing a secret.


=== On the security of public key protocols ===
Dolev, D.; Yao, A. (1983). ""On the security of public key protocols"". IEEE Transactions on Information Theory. 29 (2): 198–208. doi:10.1109/TIT.1983.1056650. ISSN 0018-9448. 
Description: Introduced the adversarial model against which almost all cryptographic protocols are judged.


== Protocols ==


=== Using encryption for authentication in large networks of computers ===
Needham, R. M.; Schroeder, M. D. (1978). ""Using encryption for authentication in large networks of computers"" (PDF). Communications of the ACM. 21 (12): 993–999. doi:10.1145/359657.359659. 
Description: This paper introduced the basic ideas of cryptographic protocols and showed how both secret-key and public-key encryption could be used to achieve authentication.


=== Kerberos ===
Neuman, B.C.; Ts'o, T. (1994). ""Kerberos: an authentication service for computer networks"". IEEE Communications Magazine. 32 (9): 33–38. doi:10.1109/35.312841. ISSN 0163-6804. 
Steiner, J. G.; Neuman, B. C.; Schiller, J. I. (February 1988). ""Kerberos: an authentication service for open network systems"". Usenix Conference Proceedings. Dallas, Texas. 
Description: The Kerberos authentication protocol, which allows individuals communicating over an insecure network to prove their identity to one another in a secure and practical manner.


=== A Protocol for Packet Network Interconnection ===
Cerf, Vint; Kahn, Bob (1974). ""A Protocol for Packet Network Interconnection"" (PDF). IEEE Transactions on Communication Technology. 22: 637–648. doi:10.1109/tcom.1974.1092259. 


=== A Dynamic Network Architecture ===
O'Malley, Sean W.; Peterson, Larry L. (May 1992). ""A Dynamic Network Architecture"". ACM Transactions on Computer Systems. 10 (2): 110–143. doi:10.1145/128899.128901. 
Description: Network software in distributed systems.


== References ==

The Codebreakers
https://users.cs.jmu.edu/abzugcx/public/Cryptology/Journal-Articles-on-Crypto-POSTED.pdf"
106,International Electron Devices Meeting,39219444,12075,"The IEEE International Electron Devices Meeting (IEDM) is an annual micro- and nanoelectronics conference held each December that serves as a forum for reporting technological breakthroughs in the areas of semiconductor and related device technologies, design, manufacturing, physics, modeling and circuit-device interaction.
The IEDM is where ""Moore’s Law"" got its name -- Gordon Moore first published his predictions in an article in Electronics magazine in 1965. Ten years later he refined them in a talk at the IEDM, and from that point on people began referring to them as Moore's Law. Moore’s Law states that the complexity of integrated circuits would double approximately every two years.
IEDM brings together managers, engineers, and scientists from industry, academia, and government around the world to discuss nanometer-scale CMOS transistor technology, advanced memory, displays, sensors, MEMS devices, novel quantum and nanoscale devices using emerging phenomena, optoelectronics, power, energy harvesting, and ultra-high-speed devices, as well as process technology and device modeling and simulation. The conference also encompasses discussions and presentations on devices in silicon, compound and organic semiconductors, and emerging material systems. In addition to technical paper presentations, IEDM includes multiple plenary presentations, panel sessions, tutorials, short courses, and invited talks and an entrepreneurship panel session conducted by experts in the field from around the globe.
The 64th annual IEDM will be held at the Hilton San Francisco Union Square hotel December 1–5, 2018.


== Sponsor ==
The International Electron Devices Meeting is sponsored by the Electron Devices Society of the Institute of Electrical and Electronics Engineers (IEEE).


== History ==
The First Annual Technical Meeting on Electron Devices (renamed the International Electron Devices Meeting in the mid-1960s) took place on October 24–25, 1955 at the Shoreham Hotel in Washington D.C. with approximately 700 scientists and engineers in attendance. At that time, the seven-year-old transistor and the electron tube reigned as the predominant electron-device technology. Fifty-four papers were presented on the then state-of-the-art in electron device technology, the majority of them from four U.S. companies -- Bell Telephone Laboratories, RCA Corporation, Hughes Aircraft Co. and Sylvania Electric Products. The need for an electron devices meeting was driven by two factors: commercial opportunities in the fast-growing new ""solid-state"" branch of electronics, and the U.S. government's desire for solid-state components and better microwave tubes for aerospace and defense.


== IEDM 2017 ==
The 2017 IEEE International Devices Meeting took place at the Hilton San Francisco Union Square from December 2–6, 2017. Highlights: Nobel Prize winner Hiroshi Amano spoke on ‘Transformative Electronics’; AMD President & CEO Lisa Su spoke on multi-chip technologies for high-performance computing; Intel and Globalfoundries detailed their competing new FinFET technology platforms; and IBM’s Dan Edelstein gave a retrospective on copper interconnect. Copper interconnect (i.e., the wiring on computer chips) revolutionized the industry 20 years ago.


== IEDM 2016 ==
The 2016 IEEE International Devices Meeting took place at the Hilton San Francisco Union Square from December 3–7, 2016. The 2016 edition of the IEDM emphasized
advanced transistors
new memory technologies
brain-inspired computing
bioelectronics
power electronics


== IEDM 2015 ==
The 2015 International Electron Devices Meeting took place at the Washington Hilton Hotel from December 5–9, 2015. The major topics  included:
ultra-small transistors 
advanced memories 
low-power devices for mobile & Internet of Things (IoT) applications 
alternatives to silicon transistors 
3D integrated circuit (IC) technology 
a broad range of papers addressing some of the fastest-growing specialized areas in micro/nanoelectronics, including silicon photonics, physically flexible circuits  and brain-inspired computing 


== IEDM 2014 ==
The 2014 International Electron Devices Meeting took place at the Hilton San Francisco Union Square from December 15–17, 2014. The 2014 edition of the IEDM emphasized:
14 nm FinFET transistor processes 
power electronics 
bio-sensors and MEMS/NEMS technologies for medical applications 
new memory devices 
display and sensor technologies 
3D device architectures 


== References ==


== Additional Information ==
IEDM
IEDM on Facebook
IEDM on Twitter: @ieee_iedm
Electron Device Society of the IEEE
IEEE
IEEE Xplore Digital Library


== Related Conferences ==
Symposia on VLSI Technology and Circuits
International Solid-State Circuits Conference
Device Research Conference
Hot Chips: A Symposium of High Performance Chips"
107,Computational humor,14760591,11979,"Computational humor is a branch of computational linguistics and artificial intelligence which uses computers in humor research. It is a relatively new area, with the first dedicated conference organized in 1996.
The first ""computer model of a sense of humor"" was suggested by Suslov as early as 1992. Investigation of the general scheme of information processing shows the possibility of a specific malfunction, conditioned by the necessity of a quick deletion from consciousness of a false version. This specific malfunction can be identified with a humorous effect on psychological grounds: it exactly corresponds to incongruity-resolution theory. However, an essentially new ingredient, the role of timing, is added to the well-known role of ambiguity. In biological systems, a sense of humor inevitably develops in the course of evolution, because its biological function consists of quickening the transmission of the processed information into consciousness and in a more effective use of brain resources. A realization of this algorithm in neural networks justifies naturally Spencer's hypothesis on the mechanism of laughter: deletion of a false version corresponds to zeroing of some part of the neural network and excessive energy of neurons is thrown out to the motor cortex, arousing muscular contractions.
A practical realization of this algorithm needs extensive databases, whose creation in the automatic regime was suggested recently. As a result, this magistral direction was not developed properly and subsequent investigations accepted somewhat specialized colouring.


== Joke generators ==


=== Pun generation ===
An approach to analysis of humor is classification of jokes. A further step is an attempt to generate jokes basing on the rules that underlie classification.
Simple prototypes for computer pun generation were reported in the early 1990s, based on a natural language generator program, VINCI. Graeme Ritchie and Kim Binsted in their 1994 research paper described a computer program, JAPE, designed to generate question-answer-type puns from a general, i.e., non-humorous, lexicon. (The program name is an acronym for ""Joke Analysis and Production Engine"".) Some examples produced by JAPE are:
Q: What is the difference between leaves and a car?
A: One you brush and rake, the other you rush and brake.
Q: What do you call a strange market?
A: A bizarre bazaar.
Since then the approach has been improved, and the latest report, dated 2007, describes the STANDUP joke generator, implemented in the Java programming language. The STANDUP generator was tested on children within the framework of analyzing its usability for language skills development for children with communication disabilities, e.g., because of cerebral palsy. (The project name is an acronym for ""System To Augment Non-speakers' Dialog Using Puns"" and an allusion to standup comedy.) Children responded to this ""language playground"" with enthusiasm, and showed marked improvement on certain types of language tests.

The two young people, who used the system over a ten-week period, regaled their peers, staff, family and neighbors with jokes such as: ""What do you call a spicy missile? A hot shot!"" Their joy and enthusiasm at entertaining others was inspirational.


=== Other ===
Stock and Strapparava described a program to generate funny acronyms.
""AskTheBrain"" (2002) [1] used clustering and bayesian analysis to associate concepts in a comical way.


== Joke recognition ==
A statistical machine learning algorithm to detect whether a sentence contained a ""That's what she said"" double entendre was developed by Kiddon and Brun (2011). There is an open-source Python implementation of Kiddon & Brun's TWSS system.
A program to recognize knock-knock jokes was reported by Taylor and Mazlack. This kind of research is important in analysis of human-computer interaction.
An application of machine learning techniques for the distinguishing of joke texts from non-jokes was described by Mihalcea and Strapparava (2006).
Takizawa et al. (1996) reported on a heuristic program for detecting puns in the Japanese language.


== Applications ==
A possible application for the assistance in language acquisition is described in the section ""Pun generation"". Another envisioned use of joke generators is in cases of steady supply of jokes where quantity is more important than quality. Another obvious, yet remote, direction is automated joke appreciation.
It is known that humans interact with computers in ways similar to interacting with other humans that may be described in terms of personality, politeness, flattery, and in-group favoritism. Therefore, the role of humor in human-computer interaction is being investigated. In particular, humor generation in user interface to ease communications with computers was suggested.
Craig McDonough implemented the Mnemonic Sentence Generator, which converts passwords into humorous sentences. Basing on the incongruity theory of humor, it is suggested that the resulting meaningless but funny sentences are easier to remember. For example, the password AjQA3Jtv is converted into ""Arafat joined Quayle's Ant, while TARAR Jeopardized thurmond's vase"".


== Related research ==
John Allen Paulos is known for his interest in mathematical foundations of humor. His book Mathematics and Humor: A Study of the Logic of Humor demonstrates structures common to humor and formal sciences (mathematics, linguistics) and develops a mathematical model of jokes based on catastrophe theory.


== See also ==
Snowclone
Phrasal template
Theory of humor
World's funniest joke#Other findings


== Further reading ==
""Computational humor"", by Binsted, K.; Nijholt, A.; Stock, O.; Strapparava, C.; Ritchie, G.; Manurung, R.; Pain, H.; Waller, A.; Oapos;Mara, D., IEEE Intelligent Systems Volume 21, Issue 2, 2006, pp. 59 – 69 doi:10.1109/MIS.2006.22
O. Stock, C. Strapparava & A. Nijholt (eds.) ""The April Fools' Day Workshop on Computational Humour."" Proc. Twente Workshop on Language Technology 20 (TWLT20), ISSN 0929-0672, ITC-IRST, Trento, Italy, April 2002, 146 pp


== References =="
108,ACM Multimedia,12643594,11960,"ACM Multimedia (ACM-MM) is the Association for Computing Machinery (ACM)'s annual conference on multimedia, sponsored by the SIGMM special interest group on multimedia in the ACM. SIGMM specializes in the field of multimedia computing, from underlying technologies to applications, theory to practice, and servers to networks to devices.
In 2003, the conference was given an ""Estimated impact factor"" of 1.22 by CiteSeer, placing it in the top 15% of computer science publication venues. In 2006 the Computing Research and Education Association of Australasia awarded it an 'A+' ranking for conferences attended by Australian academics and in 2012 it received an 'A1' rating from the Brazilian ministry of education.


== Past Conferences ==


== ACM Multimedia workshops ==
The first international workshop on Continuous Archival and Retrieval of Personal Experience (CARPE 2004) covered ""capture, retrieval, organization, search, privacy, and legal issues"" surrounding ""continuous archival and retrieval of all media relating to personal experiences""; speakers included Steve Mann and Gordon Bell.


== Open Source Competition ==
Starting in 2004, ACM Multimedia hosts an Open Source competition, providing an award for the best Open Source computer program(s).
2015:
Winner: Chris Sweeney, Tobias Hollerer, Matthew Turk, ""Theia: A Fast and Scalable Structure-from-Motion Library""

2014:
Winner: Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, Trevor Darrell, ""Caffe: Convolutional Architecture for Fast Feature Embedding""

2013:
Winner: Dmitry Bogdanov, Nicolas Wack, Emilia Gómez, Sankalp Gulati, Perfecto Herrera, Oscar Mayor, Gerard Roma, Justin Salamon, Jose Zapata Xavier Serra (UPF), “ESSENTIA: an Audio Analysis Library for Music Information Retrieval”

2012:
Winner: Petr Holub, Jiri Matela, Martin Pulec, Martin Srom, “UltraGrid: Low-latency high-quality video transmissions on commodity hardware”

2011:
Winner: J. Hare, S. Samangooei, D. Dupplaw, “OpenIMAJ and ImageTerrier: Java Libraries and Tools for Scalable Multimedia Analysis and Indexing of Images”
Honorable Mention:“ClassX – An Open Source Interactive Lecture Streaming System” “Opencast Matterhorn 1.1: Reaching New Heights” Presented by Profs. Pablo Cesar and Wei Tsang Ooi

2010:
Andrea Vedaldi, Brian Fulkerson, VLFeat – An open and portable library of computer vision algorithms – VLFeat
Rob Hess, An Open-Source SIFT Library – Open-Source SIFT
Florian Eyben, Martin Woellmer, Bjoern Schuller, openSMILE – The Munich Versatile and Fast Open-Source Audio Feature Extractor – openSMILE

2009: Caliph & Emir, MPEG-7 photo annotation and retrieval
2008: Network-Integrated Multimedia Middleware (NMM).
2007: Programming Web Multimedia Applications with Hop.
2006: CLAM (C++ Library for Audio and Music) (CLAM), an open source framework for audio and music research and application development.
2005: OpenVIDIA, a GPU accelerated Computer Vision Library.
2004: Two winnersChucK, an audio programming language for real-time synthesis, composition, performance, and analysis.
Flavor, A Formal Language for Audio-Visual Object Representation


== Other conferences on the same topic ==
ACM International Conference on Multimedia Retrieval (ICMR)
ACM International Conference on Multimedia Modeling (MMM)
ACM Multimedia Systems Conference (MMSYS)
IEEE International Conference Multimedia Expo (ICME)
IEEE International Symposium on Multimedia (ISM)
IEEE International Packet Video Workshop (PV)


== Other conferences on related topics ==
ACM SIGGRAPH
NIME
International Computer Music Conference (ICMC)


== References ==


== External links ==
SIGMM
ACM Multimedia bibliographic information on Digital Bibliography & Library Project"
109,ProgramByDesign,3350103,11931,"The ProgramByDesign project, formerly known as TeachScheme! project, is an outreach effort of the PLT research group. The goal is to train college faculty, high school teachers and possibly even middle school teachers in programming and computing.


== History ==
Matthias Felleisen and PLT started the effort in 1995 (January, one day after the POPL symposium) in response to observations of his Rice freshmen students and the algebra curriculum of local public schools. His objective was to use functional programming to bring mathematics alive and to help inject design knowledge into the introductory computer science curriculum.
The group raised funds from several private foundations, the US Department of Education, and the National Science Foundation to create
software appropriate for novices in functional programming
courseware (curricula, lecture notes, exercises, mini-projects)
teacher training camps.
Over ten years, it ran several dozen one-week workshops for some 550 teachers. In 2005, the TeachScheme! project ran an Anniversary workshop where two dozen teachers presented their work with students.
In 2010, PLT renamed its major programming language Racket. At the same time, the group renamed DrScheme to DrRacket and a little later TeachScheme! to ProgramByDesign.


== Functional Programming, Computing and Algebra ==
The starting point of ProgramByDesign is the observation that students act as computers in grade school courses on arithmetic and middle/high school courses on pre/algebra. Teachers program them with rules and run specific problems via exercises. The key is that students execute purely functional programs.
If we can turn students into teachers that create functional programs and run them on computers, we can reinforce this content and show students how writing down mathematics and how writing down functional programs creates lively animated scenes and even computer games.
Here is an example:

This short program simulates an apple falling from the top to the bottom of a small white canvas. It consists of three parts:
a function definition for create-image, which is a one-line function in mathematics, assuming an algebra of images with place-image, circle, and empty-scene have been introduced;
two abbreviations, where names are equated with some value, just as in ""let x be 5"" in an algebra text; and
one line for running the program.
A teacher can explain create-image as easily as any ordinary function in an algebra course. For example, one can first draw a table with two rows and n columns where each column contains t at the top and an appropriate image at the bottom. That is, if the numbers increase from left to right, then on each image the red dot is a little bit lower.
Finally the animate line applies the given function, create-image, at the rate of 28 ticks per second to 0, 1, 2, 3, and so on. The resulting images are displayed on the computer monitor at the same pace. That's how movies are made.
The background needed for such an example is little more than knowledge about making movies, about the algebra of pictures in DrRacket (which is like the one for numbers), and minimal pre-algebra. The ProgramByDesign project claims, however, that children would have more fun with such ""live"" functions than with algebraic expressions that count the number of garden tiles [see Prentice Hall books for grades 8-9].
The ProgramByDesign project proposes that both traditional mathematics as well as science courses could benefit from an integration of this form of programming. In contrast to the traditional Basic or Visual Basic blocks in such books, a Racket program consists of as many lines as the mathematics. Moving between the mathematics and the program is thus straightforward. Better still, the meaning of the two are the same. DrRacket's algebraic stepper can illustrate how Racket evaluates the program as if it were a sixth or seventh grade student, step by step, using plain algebra.


== Functional Programming, Computing and Design in Programming 101 ==
For the introductory curriculum on programming, the ProgramByDesign project emphasizes that courses should focus on the role of systematic design. Even if students never program again, they should see how helpful a systematic approach to problem solving is. This should help them whether they become programmers or doctors or journalists or photographers. Thus, an introductory course in programming would not be perceived as a place where students learn about the syntax of the currently fashionable (and soon-to-be-obsolete) programming languages, but a place where they can learn something widely applicable.
The key design element of the ProgramByDesign curriculum is the design recipe. It has two dimensions: the process dimension and the data dimension.
Along the process dimension students learn that there are six steps to designing a (simple) program, before they can run it and others can use it:
problem analysis with the goal of describing the classes of data that go into the program and come out;
the reformulation of the problem statement as a concise purpose statement;
the creation of examples that illustrate the purpose statement and that serve as criteria for success;
the organization of givens, also called a template or inventory;
coding;
and the creation of a test suite from examples to ensure the program works properly on small inputs.
Note that, as in test-driven development, test cases are written before coding, as part of requirements analysis, rather than afterward as part of testing.
Almost any human endeavour can benefit from clearly understanding the problem, defining criteria for success, analyzing the available resources/givens, developing a proposed solution, and checking it against the criteria, in that order. A journalist, for example, benefits from a similar process: figuring out the major concepts in a story; coining a headline; lining up examples and specific data; organizing the article about the story of the givens and how the story unfolded; writing; and fact checking.
The data dimension can be summarized by the maxim the shape of the data determines the shape of the code and tests. For example, if the input or output data type has three variants, a test suite should have at least one test case from each variant, and program code will probably contain a three-way conditional (whether explicit or hidden in a polymorphic dispatch). If the input or output data type has three fields, a test suite will have to specify values for those three fields, and program code will have to refer to those three fields. If the input or output data type has a simple base case and one or more self-referential variants, the test suite should include a base case and one or more non-base cases, and the program code will probably have a base case and one or more self-referential cases, isomorphic to the data type. The technique of recursion, rather than being scary and mysterious, is simply the application of already-learned techniques to a self-referential data type.
Organizing the givens is the task of translating the descriptions of data into a program skeleton. Each form of description determines a specific form of program organization. The transformation is nearly mechanical and helps the students focus on the creative part of the task.
How to Design Programs is the text book authored by the core of the ProgramByDesign group.


== ProgramByDesign and choice of programming language ==
The name TeachScheme! appears to imply that this design recipe requires Scheme (now Racket) and is only teachable with Scheme. Neither conclusion is true, however. Members of PLT and their trainees have successfully applied the design recipe in Assembly, C, Java, ML, Python, and other programming languages, not to speak of poetry, geometry, and biology courses. The fundamental idea of ProgramByDesign is to stress programming as a design activity. This misconception is one of the reasons for the renaming actions taken in 2010.
To get started the ProgramByDesign project has produced three essential elements:
a series of successively more powerful and permissive teaching languages, which are dialects of Racket, matched to the design recipe but with error reporting matched to the student's level (for example, many things that are legal in standard Racket, but which a beginning student doesn't need, are flagged as errors in the Beginning Student level);
a beginner-friendly, freely-downloadable, pedagogic programming environment, DrRacket, that enforces these language levels;
a curriculum, encoded mostly in the book HTDP and its (draft) successor HtDP 2nd Edition
Their choice of Racket reflects their belief that Racket is a good language for a small team with little funding (in comparison to Java) to validate their conjectures. The PLT group has always tried to ensure, however, that the ideas remain portable to other contexts.


== ProgramByDesign for Java ==
Over the past few years, the team has also created a second part of the curriculum. It demonstrates how the same design recipe ideas apply to a complex object-oriented programming language, such as Java. This phase of the curriculum applies the same design recipe to Java, initially in a functional paradigm, then introducing object-oriented concepts such as polymorphism and inheritance, and then introducing the imperative techniques that are idiomatic in mainstream Java.
A part of the team has a grant from the National Science Foundation for conducting field tests in colleges and high schools. Professional-development workshops took place in the summer of 2007, 2008, 2009, and 2010. This part of the project is dubbed ReachJava; the accompanying book is tentatively titled ""How to Design Classes.""


== ProgramByDesign and Bootstrap ==
In 2006 PLT at Northeastern University and Citizen Schools from Boston made joint efforts to reach out to inner city students with after-school programs. Citizen Schools is a nationwide organization that matches volunteers with after-school program sites and gets them started with scripted curricula. The goal of the effort is to translate the material into a sixth-grade curriculum. The first few tests were a great success in Boston. The effect on the mathematics courses of this program has encouraged Microsoft and Google to fund a national scale-up effort, developing materials for training teachers and creating sites in Texas, California, and other volunteer cities.


== References ==


== External links ==
ProgramByDesign
Racket
How to Design Programs
Bootstrap"
110,List of computer science conference acronyms,23810743,11892,"This is a list of academic conferences in computer science, ordered by their acronyms or abbreviations.


== A ==
AAAI – AAAI Conference on Artificial Intelligence
AAMAS – International Conference on Autonomous Agents and Multiagent Systems
ABZ – International Conference on Abstract State Machines, Alloy, B and Z
ACL – Annual Meeting of the Association for Computational Linguistics
ALGO – ALGO Conference
AMCIS – Americas Conference on Information Systems
ANTS – Algorithmic Number Theory Symposium
ARES – International Conference on Availability, Reliability and Security
ASIACRYPT – International Conference on the Theory and Application of Cryptology and Information Security
ASP-DAC – Asia and South Pacific Design Automation Conference
ASE – IEEE/ACM International Conference on Automated Software Engineering
ASWEC – Australian Software Engineering Conference
ATMOS – Workshop on Algorithmic Approaches for Transportation Modeling, Optimization, and Systems


== C ==
CADE – Conference on Automated Deduction
CAV – Computer Aided Verification
CC – International Conference on Compiler Construction
CCSC – Consortium for Computing Sciences in Colleges
CHES – Workshop on Cryptographic Hardware and Embedded Systems
CHI – ACM Conference on Human Factors in Computing Systems
CIAA – International Conference on Implementation and Application of Automata
CIBB – International Meeting on Computational Intelligence Methods for Bioinformatics and Biostatistics
CICLing – International Conference on Intelligent Text Processing and Computational Linguistics
CIDR – Conference on Innovative Data Systems Research
CIKM – Conference on Information and Knowledge Management
CRYPTO – International Cryptology Conference
CUSEC – Canadian University Software Engineering Conference
CVPR – Conference on Computer Vision and Pattern Recognition


== D ==
DAC – Design Automation Conference
DATE – Design, Automation, and Test in Europe
DCFS – International Workshop on Descriptional Complexity of Formal Systems
DISC – International Symposium on Distributed Computing
DLT – International Conference on Developments in Language Theory
DSN – International Conference on Dependable Systems and Networks


== E ==
ECAI – European Conference on Artificial Intelligence
ECCO – Conference of the European Chapter on Combinatorial Optimization
ECIS – European Conference on Information Systems
ECML PKDD – European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases
ECOOP – European Conference on Object-Oriented Programming
ECSS – European Computer Science Summit
ER - International Conference on Conceptual Modeling
ESA – European Symposium on Algorithms
ESOP – European Symposium on Programming
ESWC – Extended (formerly European) Semantic Web Conference
ETAPS – European Joint Conferences on Theory and Practice of Software
EUROCRYPT – International Conference on the Theory and Applications of Cryptographic Techniques
Eurographics – Annual Conference of the European Association for Computer Graphics
EWSN – European Conference on Wireless Sensor Networks


== F ==
FASE – International Conference on Fundamental Approaches to Software Engineering
FAST – USENIX Conference on File and Storage Technologies
FCRC – Federated Computing Research Conference
FLoC – Federated Logic Conference
FOCS – IEEE Symposium on Foundations of Computer Science
FORTE – IFIP International Conference on Formal Techniques for Networked and Distributed Systems
FoSSaCS – International Conference on Foundations of Software Science and Computation Structures
FSE – Fast Software Encryption Workshop
FTP – International Workshop on First-Order Theorem Proving


== G ==
GD – International Symposium on Graph Drawing
GlobeCom – IEEE Global Communications Conference
GraphiCon – International Conference on Computer Graphics and Vision


== H ==
HICSS – Hawaii International Conference on System Sciences
HiPC – International Conference on High Performance Computing
HOPL – History of Programming Languages Conference
Hot Interconnects – IEEE Symposium on High Performance Interconnects


== I ==
ICALP – International Colloquium on Automata, Languages and Programming
ICASSP – International Conference on Acoustics, Speech, and Signal Processing
ICCAD – International Conference on Computer-Aided Design
ICC – IEEE International Conference on Communications
ICCIT – International Conference on Computer and Information Technology
ICCV – International Conference on Computer Vision
ICDCS – International Conference on Distributed Computing Systems
ICFP – International Conference on Functional Programming
ICIS – International Conference on Information Systems
ICL – International Conference on Interactive Computer Aided Learning
ICLP – International Conference on Logic Programming
ICML – International Conference on Machine Learning
ICPADS – International Conference on Parallel and Distributed Systems
ICSE – International Conference on Software Engineering
ICSOC – International Conference on Service Oriented Computing
ICSR – International Conference on Software Reuse
ICTer – International Conference on Advances in ICT for Emerging Regions
ICWS – International Conference on Web Services
IJCAI – International Joint Conference on Artificial Intelligence
IJCAR – International Joint Conference on Automated Reasoning
IJCBS – International Joint Conferences on Bioinformatics, Systems Biology and Intelligent Computing
IndoCrypt – International Conference on Cryptology in India
IPDPS – IEEE International Parallel and Distributed Processing Symposium
IPSN – ACM/IEEE International Conference on Information Processing in Sensor Networks
ISAAC – International Symposium on Algorithms and Computation
ISCA – International Symposium on Computer Architecture
ISCAS – IEEE International Symposium on Circuits and Systems
ISMAR – IEEE International Symposium on Mixed and Augmented Reality
ISWC – International Semantic Web Conference
ISPD – International Symposium on Physical Design
ISSCC – International Solid-State Circuits Conference
ISWC – International Symposium on Wearable Computers


== K ==
KDD – ACM SIGKDD Conference on Knowledge Discovery and Data Mining


== L ==
LICS – IEEE Symposium on Logic in Computer Science
LREC – International Conference on Language Resources and Evaluation


== M ==
MM – ACM International Conference on Multimedia
MobiCom – ACM International Conference on Mobile Computing and Networking
MobiHoc – ACM International Symposium on Mobile Ad Hoc Networking and Computing
MobileHCI – Conference on Human-Computer Interaction with Mobile Devices and Services


== N ==
NAACL – Annual Conference of the North American Chapter of the Association for Computational Linguistics
NIPS – Conference on Neural Information Processing Systems
NIME – New Interfaces for Musical Expression


== O ==
OOPSLA – Conference on Object-Oriented Programming, Systems, Languages, and Applications


== P ==
PACIS – Pacific Asia Conference on Information Systems
PIMRC – International Symposium on Personal, Indoor and Mobile Radio Communications
PKC – International Workshop on Practice and Theory in Public Key Cryptography
PKDD – European Conference on Principles and Practice of Knowledge Discovery in Databases
PLDI – ACM SIGPLAN Conference on Programming Language Design and Implementation
PLoP – Pattern Languages of Programs
PODC – ACM Symposium on Principles of Distributed Computing
PODS – ACM Symposium on Principles of Database Systems
POPL – Symposium on Principles of Programming Languages
POST – Conference on Principles of Security and Trust
PPoPP – ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming
PSB – Pacific Symposium on Biocomputing


== R ==
RECOMB – Research in Computational Molecular Biology
REV – International Conference on Remote Engineering and Virtual Instrumentation
RSA – RSA Conference
RTA – International Conference on Rewriting Techniques and Applications


== S ==
SAC – ACM SIGAPP Symposium on Applied Computing
SAC – Selected Areas in Cryptography
SAME – Semantic Ambient Media Experiences
SEAMS – Software Engineering for Adaptive and Self-Managing Systems
SEFM – International Conference on Software Engineering and Formal Methods
SenSys – ACM Conference on Embedded Networked Sensor Systems
SIGCOMM – ACM SIGCOMM Conference
SIGCSE – ACM Technical Symposium on Computer Science Education
SIGDOC – ACM International Conference on Design of Communication
SIGGRAPH – International Conference on Computer Graphics and Interactive Techniques
SIGIR – Annual International ACM SIGIR Conference
SIGMOD – ACM SIGMOD Conference
SPAA – ACM Symposium on Parallelism in Algorithms and Architectures
SRDS – IEEE International Symposium on Reliable Distributed Systems
STACS – Symposium on Theoretical Aspects of Computer Science
STOC – ACM Symposium on Theory of Computing
SWAT – Scandinavian Symposium and Workshops on Algorithm Theory


== T ==
TABLEAUX – International Conference on Automated Reasoning with Analytic Tableaux and Related Methods
TACAS – International Conference on Tools and Algorithms for the Construction and Analysis of Systems
TAMC – International Conference on Theory and Applications of Models of Computation
TCC – Theory of Cryptography Conference
TPHOLs – Theorem Proving in Higher-Order Logics
TSD – Text, Speech and Dialogue


== U ==
USENIX – USENIX Annual Technical Conference


== V ==
VIS – IEEE Visualization
VLDB – International Conference on Very Large Data Bases


== W ==
WABI – Workshop on Algorithms in Bioinformatics
WADS – Algorithms and Data Structures Symposium
WAE – Workshop on Algorithms Engineering
WAOA – Workshop on Approximation and Online Algorithms
WDAG – Workshop on Distributed Algorithms on Graphs
WikiSym – International Symposium on Wikis and Open Collaboration
WINE – Conference on Web and Internet Economics
WLAN – IEEE Workshops on Wireless LAN
WMSCI – World Multiconference on Systemics, Cybernetics and Informatics
WWW – World Wide Web Conference


== Z ==
ZUM – Z User Meeting


== See also ==
List of computer science conferences for more conferences organised by field.
Conference acronym index for conferences and workshops published in LNCS, LNAI and LNBI proceedings series by Springer.


== References =="
111,Australian Computer Society,5164145,11636,"The Australian Computer Society (ACS) is an association for information and communications technology professionals with over 26,000 members Australia-wide. According to its Constitution, its objects are ""to advance professional excellence in information technology"" and ""to promote the development of Australian information and communications technology resources"".
The ACS was formed on 1 January 1966 from five state based societies. It was formally incorporated in the Australian Capital Territory on 3 October 1967. Since 1983 there have been chapters in every state and territory.
The ACS is a member of the Australian Council of Professions (""Professions Australia""), the peak body for professional associations in Australia. Internationally, ACS is a member of the International Professional Practice Partnership (IP3), South East Asia Regional Computer Confederation, International Federation for Information Processing and The Seoul Accord.
The ACS is also a member organization of the Federation of Enterprise Architecture Professional Organizations (FEAPO), a worldwide association of professional organizations which have come together to provide a forum to standardize, professionalize, and otherwise advance the discipline of Enterprise Architecture.


== Activities ==
The ACS operates various chapters, annual conferences, special interest groups, and a professional development program. Members are required to comply with a Code of Ethics and a Code of Professional Conduct.


== Extent of representation ==
The ACS describes itself as ""the professional association for Australia’s Information and Communication Technology (ICT) sector"" and ""Australia’s primary representative body for the ICT workforce"", but industry analysts have questioned this based on the small percentage of IT professionals who are ACS members. The issue has been discussed in the press since at least 2004, and in 2013 the Sydney Morning Herald wrote that ""the ACS aggressively seeks to control the important software engineering profession in Australia, but ... less than 5 per cent of the professional IT workforce belongs to the ACS."" The ACS Foundation came up with a slightly higher figure: ""Depending on the data used to calculate the number of ICT professionals in Australia, however, [ACS] membership represents approximately 6.5 per cent of the total.""


== Presidents ==
The Australian Computer Society elects its National President every two years, who serves as the leader of the Society. Some of the most recent Presidents include:
Yohan Ramasundarah, 2018 - current
Anthony Wong, 2016 - 2017
Brenda Aynsley, 2014 - 2016
Nick Tate, 2012 - 2014
Kumar Parakala, 2008 - 2010
Philip Argy, 2006 - 2008
Edward Mandla, 2004 - 2006


== Young IT ==
The Young IT Professionals Board of the Australian Computer Society provides a voice for young IT professionals and students, as well as a range of services and benefits for members. Currently Young IT organises and runs a bi-annual YIT International Conference and other events such as local career days, soft skills and technical seminars, networking opportunities and social events (e.g. Young IT in the Pub) in each of the Australian States.
The most recent Young IT Conference was held in Melbourne in 2014.


== Publications ==
""Information Age"" is the official publication of the ACS. In February 2015 Information Age became an online only publication. Peer reviewed research publications of the ACS include:
Journal of Research and Practice in Information Technology
Conferences in Research and Practice in Information Technology
Australasian Journal of Information Systems
The Digital Library contains free journal articles and conference papers.


== Related organisations ==


=== Other Australian computer associations ===
AUUG - Now deregistered
Linux Australia
LUGs in Australia
SAGE-AU
Institute of Analytics Professionals of Australia (IAPA), incorporating business data analytics, business intelligence, data mining and related industries
Australian Software Innovation Forum, encourages collaboration and co-operation in Java EE and associated technologies


== Special Interest Groups ==
Special Interest Groups (SIGs) of the ACS are connected to each state branch with some SIGs of the same or similar name occurring in a number of states, depending on local interest, and include: Architects, Software Quality Assurance, Women in Technology, Business Requirements Analysis, Enterprise Capacity Management, Enterprise Solution Development, Free Open Source Software, Information Security, IT Management, Project Management, Service Oriented Computing, Web Services, Consultants and Contractors, IT Security, PC Recycling, Curry SIG, Information Technology in Education, Robotics, E-Commerce, IT Governance, Software Engineering and Cloud Computing. A recent addition is the Green ICT Group on computers and telecommunications for environmental sustainability. In 2007 the Telecommunications Society of Australia was absorbed into the Australian Computer Society as the Telecommunications Special Interest Group


== Education and Certification ==
The ACS runs the online Computer Professional Education Program (CPEP) for postgraduate education in subjects including: Green ICT Strategies; New Technology Alignment; Business, Strategy & IT; Adaptive Business Intelligence; Project Management; Managing Technology and Operations. CPEP uses the Australian developed Moodle course management system and is delivered via the web.
The Diploma of Information Technology (DIT) is equivalent to one academic year of a Bachelor of Information Technology at several universities. It has eight compulsory subjects: systems analysis, programming, computer organisation, data management, OO systems development, computer communications, professional practice and systems principles.
The ACS also certifies IT professionals at two levels, the Certified Professional and the Certified Technologist. Each certification level has a minimum level of experience and also required ongoing CPD (Certified Professional Development) hours of learning each year. In 2017 the ACS launched a cybersecurity specialisation within the certification framework.


== See also ==
Skills Framework for the Information Age


== References ==


== External links ==
Official website of the ACS
ACS Foundation
Information Age"
112,VIBOT,15537424,11505,"VIBOT is an abbreviation of VIsion and RoBOTics. It is a 2-year European Masters in Computer Vision and Robotics course, conducted by Heriot-Watt University in Edinburgh, Scotland, Universitat de Girona in Girona, Spain and Université de Bourgogne in Le Creusot, France. It started in 2006 as part of the European Commission's Erasmus Mundus programme.


== Introduction ==
VIBOT is a two-year Master Program in 3D Vision and Robotics accredited in 2006 by the European Commission in the framework of the Erasmus Mundus program, a co-operation and mobility program of the European Commission in the field of higher education in order to promote the European Union as a centre of excellence in learning around the world. It is the only Erasmus Mundus Master Program in 3D Vision and Robotics among the 103 Erasmus Mundus Master Programs accredited since 2004 in all disciplines.
VIBOT Master students have courses in the three collaborating universities: Université de Bourgogne in France, University of Girona in Spain and Heriot-Watt University in Scotland. They spend one semester in each of these three universities and the fourth semester in training. The admission of the brightest students from all over the world, some of whom were already working in companies, allows to create an international and mobile educated workforce of high level for the European community.
VIBOT Master courses are given by faculties and visiting professors who belong to well-known research laboratories that have long-standing reputation for high quality research. The courses start from a comprehensive coverage of the prerequisites in the field of digital imaging (hardware and software) and basic image processing algorithms, and end up with research level teaching of their applications in the fields of robotics, medical imaging and 3D vision systems. The close location of research laboratories on campus allows the faculties to involve students at all stages of research and offer them many opportunities to participate in state-of-the-art research work.


== Course Structure ==
The Masters Course is divided in four semesters, each having a value of 30 ECTS. All the courses are taught in English in the three partner universities. However, the order in which the students attend the universities has changed from the first generation to the second generation of the program. Irrespective of the generation, the last semester consists of a training period, which is an introduction to research carried out in one of the laboratories of the (wider) network.


=== First Generation ===
The so-called VIBOT first generation comprises the first five promotions of the program (promotions starting from 2006 to 2010). For this primer generation, the mobility order was the following: Scotland, Spain, France.
The first semester was carried out on the campus of Heriot Watt University in Scotland (UK). Its constituting modules covered an introduction to signal and image processing, as well as the key related area of artificial intelligence. These subjects were supplemented by local culture studies, and training for drafting documents in English.
The second semester took place in the Universitat de Girona, in Catalunya, Spain. Topics were currently highly relevant to scientific and industrial communities like the segmentation strategies, object recognition and description, the acquisition of 3D information, and the application of hardware for specific real time applications. This semester covered: fundamentals on robotics, autonomous robots, scene segmentation and interpretation, visual perception, real time image processing and study of the local culture.
The third semester was hosted by the university Center Condorcet in Le Creusot, which is a delocalization of the Université de Bourgogne. The subjects covered were: pattern recognition, image analysis (including the problems of multiresolution analysis and deconvolution). These modules were supplemented by the study of various types of image modalities such as: infra-red, ultrasonic, radio-isotopique and X-ray imagery, with an emphasis on their application to medical imagery. A module of local culture was also provided.


=== Second Generation ===
The VIBOT second generation starts in 2011 with the sixth promotion and is expected to last at least for five promotions. The main changes consist in the mobility order. Unlike the first generation, the second generation spends the semesters in the following order: France, Spain, Scotland. The overall objective is to start from science fundamentals and low-level image processing, then to introduce mid-level image processing, computer vision and applications in medical imaging, and to end with robotics and high-level integration so that the main methods and algorithms in the state-of-art of computer vision and robotics (through both a theoretical and applied point of view) are covered.
The organization of the semesters is as follows. An induction week is organized starting the first semester, in September, in Le Creusot during which the enrolled students are informed about the program, the assessment and the rules. The induction week also permits new students to meet second-year students or former VIBOT students. The first semester is held at Universite de Bougogne (UB) where the modules of Fundamentals on image processing, sensors and acquisition, as well as mathematics and computer science are taught. The second semester is hosted at Universitat de Girona (UdG) with subjects as improvements on image processing and pattern recognition, complements on computer vision and medical imaging and an introduction to robotics. The third semester is developed at Heriot-Watt University (HWU), and more advanced courses related to methods and high-level processing for real-time implementation and integration of vision and robotics are taught.


=== Training Period ===
The training period in research corresponds to the fourth semester of the course. The laboratories belonging to the consortium of universities are the laboratories where the students are initially expected to do the training period. These laboratories are: Vision, Image and Signal Processing (VISP) and Ocean Systems Laboratory (OSL) in Heriot-Watt University, the laboratory for Computer Vision and Robotics (VICOROB) at the Universitat de Girona, and the Laboratory of Electronics, Informatics and Image (Le2i) in Le Creusot. However, the training period can also take place in any validated host laboratory or research center throughout the world. The laboratories concerned belong to the worldwide research networks which the participating institutes have jointly developed through the international-class research activities of their laboratories. Some of the laboratories where some previous students have done their research fourth semester are: Honda Research Institute Europe GmbH (Offenbach am Main, Germany), Erasmus University Rotterdam (Rotterdam, Netherlands), Laboratory for Analysis and Architecture of Systems LAAS-CNRS (Toulouse, France), the French National Institute for Research in Computer Science and Control INRIA (Grenoble, France), Toshiba Medical Visualization Systems Europe (Edinburgh, UK), Information and Communication Technologies (Queensland, Australia), SeeByte, Germany's National Research Center for Aeronautics and Space DLR (Wessling, Germany), the Computational Vision and Robotics Laboratory CVRL (Crete,Greece), Fraunhofer Institutes (Germany) among others.


== VIBOT Days ==
VIBOT Day 2008  was held in Le Creusot, France, on September 9, 2008. It was a one-day event during which participants learned more about the VIBOT Master program and the collaborative opportunities that were offered to companies wishing to sponsor the program. Participants also attended several oral and poster presentations given by newly graduated students that raised their awareness of the academic excellence of the students as well as the high level of the courses taught in the program. Moreover, the one-day event also gave an opportunity for participants to meet and discuss with past, current and future international students.
VIBOT Day 2009  was held at Heriot-Watt University Post-Graduate School, Edinburgh, UK, on June 19, 2009. It was a one-day event during which there were oral and poster presentations. The collaborative opportunities with different companies were also exposed.
VIBOT Day 2010 was held at the Science and Technology Park of the University of Girona in Spain during June 2010.
Vibot Day 2011 will be held also at the University of Girona on June 16, 2011.
Vibot Day 2012 was held again at the Science and Technology Park of the University of Girona during June 2012.


== External links ==
Official VIBOT Website
Official Facebook Page
VIBOT graduates page
Official Erasmus Mundus Website
VIBOT Season 1 Video
86 étudiants pour le ViBot Day
Université de Bourgogne
Universitat de Girona
Heriot-Watt University


== References =="
113,Georgia Tech Online Master of Science in Computer Science,55339410,11447,"Georgia Tech Online Master of Science in Computer Science is a master of science degree offered by the College of Computing at Georgia Tech. The program is offered in partnership with Udacity and AT&T and delivered through the massive open online course format. The course has received attention for offering a full master's degree program for under $7,000 that gives students from all over the world the opportunity to enroll in a top 10-ranked computer science program. The program has been recognized by the University Professional and Continuing Education Association and Fast Company for excellence and innovation.


== Background ==
The College of Computing at the Georgia Institute of Technology launched its online Master of Science in Computer Science degree in January 2014. The program was conceived by John P. Imlay Jr., Dean of Computing Zvi Galil, and Udacity founder Sebastian Thrun. It was offered in partnership with Udacity and AT&T and delivered through the massive open online course format, and was designed to provide a low-cost, high-quality alternative to traditional master's degree programs by delivering instructional content and academic support via the massive open online course format.
As of Spring 2018, the program has 6,365 enrolled students located in 110 countries. It admits all applicants deemed to possess a reasonable chance of success—almost two-thirds of applicants to date—which is significantly higher than the university’s on-campus graduate admissions rate. From its creation in 2014 until the spring semester of 2018, the program has graduated nearly 900 students and received nearly 20,000 applications. The program has received significant media attention since its announcement in May 2013, including a front-page story in The New York Times and a segment on the PBS NewsHour series ""Rethinking Education.""


== Curriculum and culture ==
The online master's program currently offers 30 courses and four specializations—Computational Perception and Robotics, Computing Systems, Interactive Intelligence, and Machine Learning.
A study entitled “Can Online Delivery Increase Access to Education,” by John F. Kennedy School of Government at Harvard University Associate Professor Joshua Goodman, Ivan Allen College of Liberal Arts at the Georgia Institute of Technology Associate Professor Julia Melkers, and Harvard Faculty of Arts and Sciences Associate Professor Amanda Pallais, explored the structure and industry impact of the online master's program and concluded that it supplies the need of “a vast untapped market for highly affordable degrees from prestigious colleges.”
Due to the online format of the program, social media has played a significant role in the development of robust student communities on social media, including a student-led community on Google+ with about 7,500 members. The program also utilizes an artificial intelligence teaching assistant called “Jill” Watson, built using IBM’s Watson platform. Jill is able to answer questions posed in natural language and assists students enrolled in the program's Knowledge-Based Artificial Intelligence course, led by Professor Ashok Goel.


== Recognition and impact ==
Former President Barack Obama publicly praised Georgia Tech's online master's program on two occasions, as providing a model to both address the STEM worker shortage and control the costs of higher education. The program was the recipient of University Professional and Continuing Education Association’s Outstanding Program Award in the credit category. and was cited as the reason Georgia Tech was named to Fast Company’s 2017 list of Most Innovative Companies in the World, third in the education sector.
Other universities have modeled similar programs after Georgia Tech's, including The University of Illinois at Urbana-Champaign, which has launched three such master’s programs in business administration, data science, and accounting. In early 2018, the University of Colorado Boulder announced a comparable program in electrical engineering. The success of the online model also led Georgia Tech to create a second program with this structure, the Online Master of Science in Analytics (OMS Analytics), which launched in 2017. In March 2018, Coursera announced six new, MOOC-based degree programs in collaboration with the University of Michigan, Arizona State University, the University of London, and Imperial College London.


== References =="
114,Computational epistemology,1853711,11186,"Computational epistemology is a subdiscipline of formal epistemology that studies the intrinsic complexity of inductive problems for ideal and computationally bounded agents. In short, computational epistemology is to induction what recursion theory is to deduction.


== Themes ==
Some of the themes of computational epistemology include:
the essential likeness of induction and deduction (as illustrated by systematic analogies between their respective complexity classes)
the treatment of discovery, prediction and assessment methods as effective procedures (algorithms) as originates in algorithmic learning theory.
the characterization of inductive inference problems as consisting of:
a set of relevant possibilities (possible worlds), each of which specifies some potentially infinite sequence of inputs to the scientist’s method,
a question whose potential answers partition the relevant possibilities (in the set theoretic sense),
a convergent success criterion and
a set of admissible methods
the notion of logical reliability for inductive problems


== Quotations ==
Computational epistemology definition:
""Computational epistemology is an interdisciplinary field that concerns itself with the relationships and constraints between reality, measure, data, information, knowledge, and wisdom"" (Rugai, 2013)
On making inductive problems easier to solve:
""Eliminating relevant possibilities, weakening the convergence criterion, coarsening the question, or augmenting the collection of potential strategies all tend to make a problem easier to solve"" (Kelly, 2000a)
On the divergence of computational epistemology from Bayesian confirmation theory and the like:
""Whenever you are inclined to explain a feature of science in terms of probability and confirmation, take a moment to see how the issue would look in terms of complexity and success""(Kelly, 2000a)
Computational epistemology in a nutshell:

Formal learning theory is very simple in outline. An inductive problem specifies a range of epistemically possible worlds over which to succeed and determines what sort of output would be correct, where correctness may embody both content and truth (or some analogous virtue like empirical adequacy). Each possible world produces an input stream which the inductive method processes sequentially, generating its own output stream, which may terminate (ending with a mark indicating this fact) or go on forever. A notion of success specifies how the method should converge to a correct output in each possible world. A method solves the problem (in a given sense) just in case the method succeeds (in the appropriate sense) in each of the possible worlds specified by the problem. We say that such a method is reliable since it succeeds over all the epistemically possible worlds. Of two non-solutions, one is as reliable as the other just in case it succeeds in all the worlds the other one succeeds in. That's all there is to it! (Kelly et al. 1997)

On the proper role of methodology:
""It is for empirical science to investigate the details of the mechanisms whereby we track, and for methodologists to devise and refine even better (inferential) mechanisms and methods"" (Nozick, 1981)


== References ==
Blum, M. and Blum, L. (1975). ""Toward a Mathematical Theory of Inductive Inference"", Information and Control, 28.
Feldman, Richard, Naturalized Epistemology, The Stanford Encyclopedia of Philosophy (Fall 2001 Edition), Edward N. Zalta (ed.).
Glymour, C. and Kelly, K. (1992). ‘Thoroughly Modern Meno’, in: Inference, Explanation and Other Frustrations, ed. John Earman, University of California Press.
Gold, E. M. (1965) ""Limiting Recursion"", Journal of Symbolic Logic 30: 27-48.
Gold, E. Mark (1967), Language Identification in the Limit (PDF), 10, Information and Control, pp. 447–474  [1]
Hájek, Alan, Interpretations of Probability, The Stanford Encyclopedia of Philosophy (Summer 2003 Edition), Edward N. Zalta (ed.).
Harrell, M. (2000). Chaos and Reliable Knowledge, Ph.D. Thesis, University of California at San Diego.
Harrell, M. and Glymour, C. (2002). ""Confirmation And Chaos,"" Philosophy of Science, volume 69 (2002), pages 256–265
Hawthorne, James, Inductive Logic, The Stanford Encyclopedia of Philosophy (Winter 2005 Edition), Edward N. Zalta (ed.).
Hendricks, Vincent F. (2001). The Convergence of Scientific Knowledge, Dordrecht: Springer.
Hendricks, Vincent F. (2006). Mainstream and Formal Epistemology, New York: Cambridge University Press.
Hendricks, Vincent F., John Symons Epistemic Logic, The Stanford Encyclopedia of Philosophy (Spring 2006 Edition), Edward N. Zalta (ed.).
Hodges, Wilfrid, Logic and Games, The Stanford Encyclopedia of Philosophy (Winter 2004 Edition), Edward N. Zalta (ed.).
Kelly, Kevin (1996). The Logic of Reliable Inquiry, Oxford: Oxford University Press.
Kelly, Kevin (2000a). ‘The Logic of Success’, British Journal for the Philosophy of Science 51:4, 639-660.
Kelly, Kevin (2000b). ""Naturalism Logicized"", in After Popper, Kuhn and Feyerabend: Current Issues in Scientific Method, R. Nola and H. Sankey, eds, 34 Dordrecht: Kluwer, 2000, pp. 177–210.
Kelly, Kevin (2002). ""Efficient Convergence Implies Ockham's Razor"", Proceedings of the 2002 International Workshop on Computational Models of Scientific Reasoning and Applications, Las Vegas, USA, June 24–27, 2002.
Kelly, Kevin (2004a). ""Uncomputability: The Problem of Induction Internalized, Theoretical Computer Science, pp. 317: 2004, 227-249.
Kelly, Kevin (2004b). ""Learning Theory and Epistemology, in Handbook of Epistemology, I. Niiniluoto, M. Sintonen, and J. Smolenski, eds. Dordrecht: Kluwer, 2004
Kelly, Kevin (2004c). ""Justification as Truth-finding Efficiency: How Ockham's Razor Works"", Minds and Machines 14: 2004, pp. 485–505.
Kelly, Kevin (2005a). ""Simplicity, Truth, and the Unending Game of Science"" manuscript
Kelly, Kevin (2005b).""Learning, Simplicity, Truth, and Misinformation"" manuscript
Kelly, K., and Glymour, C. (2004). ""Why Probability Does Not Capture the Logic of Scientific Justification"", in Christopher Hitchcock, ed., Contemporary Debates in the Philosophy of Science, London: Blackwell, 2004.Kelly, K., and Schulte, O. (1995) ‘The Computable Testability of Theories Making Uncomputable Predictions’, Erkenntnis 43, pp. 29–66.
Kelly, K., Schulte, O. and Juhl, C. (1997). ‘Learning Theory and the Philosophy of Science’, Philosophy of Science 64, 245-67.Kelly, K., Schulte, O. and Hendricks, V. (1995) ‘Reliable Belief Revision’. Proceedings of the XII Joint International Congress for Logic, Methodology and the Philosophy of Science.
Nozick, R. (1981) Philosophical Explanations, Cambridge: Harvard University Press.
Osherson, D., Stob, M. and Weinstein, S. (1985). Systems that Learn, 1st Ed., Cambridge: MIT Press.
Putnam, H. (1963). ""'Degree of Confirmation' and 'Inductive Logic'"", in The Philosophy of Rudolf Carnap, ed. P.a. Schilpp, La Salle, Ill: Open Court.
Putnam, H. (1965). ""Trial and error predicates and the solution to a problem of Mostowski"", Journal of Symbolic Logic, 30(1):49-57, 1965.
Quine, W. V. (1992) Pursuit of Truth, Cambridge: Harvard University Press.
Reichenbach, Hans (1949). ""The pragmatic justification of induction,"" in Readings in Philosophical Analysis, ed. H. Feigl and W. Sellars (New York: Appleton-Century-Crofts, 1949), pp. 305–327.
Rugai, N. (2013) 'Computational Epistemology: From Reality to Wisdom', Second Edition, Book, Lulu Press, ISBN 978-1-300-47723-5.
Salmon, W. (1967) The Logic of Scientific Inference, Pittsburgh: University of Pittsburgh Press.
Salmon, W. (1991). ‘Hans Reichenbach's Vindication of Induction,’ Erkenntnis 35:99-122.
Schulte, O. (1999a). “Means-Ends Epistemology,” British Journal for the Philosophy of Science, 50, 1-31.
Schulte, O. (1999b). ‘The Logic of Reliable and Efficient Inquiry’, Journal of Philosophical Logic 28, 399-438.
Schulte, O. (2000). ‘Inferring Conservation Principles in Particle Physics: A Case Study in the Problem of Induction’, The British Journal for the Philosophy of Science, 51: 771-806.
Schulte, O. (2003). Formal Learning Theory, The Stanford Encyclopedia of Philosophy (Fall 2003 Edition), Edward N. Zalta (ed.).
Schulte, O., and Juhl, C. (1996). ‘Topology as Epistemology’, The Monist 79, 1:141-147.
Sieg, Wilfried (2002a). ""Calculations by Man & Machine: Mathematical presentation"" in: Proceedings of the Cracow International Congress of Logic, Methodology and Philosophy of Science, Synthese Series, Kluwer Academic Publishers, 2002, 245-260.
Sieg, Wilfried (2002b). ""Calculations by Man & Machine: Conceptual analysis"" in: Reflections on the Foundations of Mathematics, (Sieg, Sommer, and Talcott, eds.), 2002, 396-415
Steup, Matthias, Epistemology, The Stanford Encyclopedia of Philosophy (Winter 2005 Edition), Edward N. Zalta (ed.).
Talbott, William, Bayesian Epistemology, The Stanford Encyclopedia of Philosophy (Fall 2001 Edition), Edward N. Zalta (ed.).


== External links ==
Research Areas: Computational Epistemology, Kevin Kelly
LearningEpistemology.com ""teaches computational epistemology intuitively using narrated animation and interactive explanations."" part of Seth Casana's Masters thesis for the Department of Philosophy at Carnegie Mellon University


== See also ==
Algorithmic learning theory
Bayesian confirmation theory
Belief revision
Computational learning theory
Epistemology
Formal epistemology
Inductive reasoning
Language identification in the limit
Machine learning
Methodology
Philosophy of science
Problem of induction
Scientific method"
115,BIOSTEC,51801762,11107,"The International Joint Conference on Biomedical Engineering Systems and Technologies - BIOSTEC - is an international joint conference composed of five co-located conferences each specialized in a different knowledge area:
Biomedical Electronics and Devices – BIODEVICES
Bioimaging – BIOIMAGING
Bioinformatics Models, Methods and Algorithms – BIOINFORMATICS
Bio-inspired Systems and Signal Processing – BIOSIGNALS
Health Informatics – HEALTHINF
This joint conference is held annually and it seems to be interested in the dissemination of the novelties in the topics covered by its sub-conferences.
BIOSTEC had its first edition in 2008 counting with the participation of some keynote speakers like Kevin Warwick. Since then, several names have been invited to deliver keynotes to the BIOSTEC attendees. Among them: David Rose (MIT Media Lab, United States), Bradley Nelson, (ETH Zurich, Switzerland), Edward H. Shortliffe, (Arizona State University, United States), José C. Príncipe (University of Florida, United States), Alberto Cliquet Jr (University of São Paulo & University of Campinas, Brazil), Tanja Schultz (University of Bremen, Germany) e Vimla L. Patel, (Arizona State University, United States).
Besides the presentation of invited talks, the BIOSTEC conferences are composed by different kind of sessions like poster sessions, technical sessions, tutorials, special sessions, workshops, doctoral consortiums, panels and industrial tracks. The papers presented in the conference are made available at the SCITEPRESS digital library, published in the conference proceedings and some of the best papers are invited to a post-publication with Springer.
The 2017 edition of the conference will be held in cooperation with ACM SIGKDD, EUROMICRO, ISfTeH, AAAI, iSCB, BMES.


== Editions ==


== References ==


== External links ==
Science and Technology Events
Conference website
Event management system
WikiCfp call for papers"
116,Bachelor of Science in Information Technology,24378325,11086,"A Bachelor of Science in Information Technology, (abbreviated BSIT or B.Sc IT), is a Bachelor's degree awarded for an undergraduate course or program in the Information technology field. The degree is normally required in order to work in the Information technology industry.
A Bachelor of Science in Information Technology degree program typically takes three to four years depending on the country. This degree is primarily focused on subjects such as software, databases, and networking. In general, computer science degrees tend to focus on the mathematical and theoretical foundations of computing rather than emphasizing specific technologies. The degree is a Bachelor of Science degree with institutions conferring degrees in the fields of information technology and related fields. This degree is awarded for completing a program of study in the field of software development, software testing, software engineering, web design, databases, programming, computer networking and computer systems.
Graduates with an information technology background are able to perform technology tasks relating to the processing, storing, and communication of information between computers, mobile phones, and other electronic devices. Information technology as a field emphasizes the secure management of large amounts of variable information and its accessibility via a wide variety of systems both local and worldwide.


== Skills taught ==
Generally, software and information technology companies look for people who have strong programming skills, system analysis, and software testing skills.
Many or colleges teach practical skills that are become a software developer. As logical reasoning and critical thinking are important in becoming a software professional, this degree encompasses the complete process of software development from software design and development to final testing.
Students who complete their undergraduate education in software engineering at a satisfactory level often pursue graduate studies such as a Master of Science in Information Technology (M.Sc IT) and sometimes continuing onto a doctoral program and earning a doctorate such as a Doctor of Information Technology (DIT).


== International variations ==


=== Australia ===
In Australia, Bachelor of Information Technology/Science (BInfTech) programs are three to four years in duration. Honors awarded to graduates who successfully complete a four-year program.


=== Belgium ===
In Belgium, the Bachelor of Science in Information Technology is a 3-year degree after the compulsory education, but outside the universities; with specializing in certain fields, usually databases, network, realtime operating systems and/or web design. The graduated students can join a postgraduate cycle in order to get a Master of Science in Information Technology degree, this time in the university system.


=== Bangladesh ===
In Bangladesh, the Bachelor of Engineering in Information Technology is awarded following a four-year course of study under the Dhaka University, Jahangirnagar University, Bangladesh University of Professionals, university of information technology and sciences  and Stamford University Bangladesh.


=== Canada ===
In Canada, the Bachelor of Science (B.S.) program in Information Technology (IT) with a minor in Business Administration offers an interdisciplinary curriculum focusing on both information technology and business administration. In addition, the program is unique in that it merges traditional academic topics with leading edge and current IT practices and technology. This program is offered under the written consent of the British Columbia Ministry of Advanced Education.


=== Germany ===
In Germany, Bachelor of Science in Information Technology integrates a professional degree in information technology with a major in another country or culture and its language, enhancing professional training and career options. The course is of three to five years duration. Students spend two semesters of study at a university or other higher education institution in the country of their major.
The information technology component provides a sound education in all aspects of computing and information technology for a career in the profession.


=== India ===
In India, a Bachelor of Science in Information Technology(BSc IT) is a 3-year undergraduate program. The Bachelor of Information Science degree is a distinctly different from a general B. Sc. The Bachelor of Engineering in Information Technology, however is a 4-year program and the degree awarded is referred to as B.Tech or B.E.
One can apply for BSc IT after completing HSC or after completing Engineering Diploma.


=== Malaysia ===
In Malaysia Information Technology course is being studied for 6 or 7 semesters (3 or 3 1/2 years) with specializing in certain field and by undergoing one semester (6 months) of industrial training.


=== Netherlands ===
In the Netherlands, the Bachelor of Science in Information Technology degree is awarded after four years of study with specializing in a certain field.


=== Namibia ===
In Namibia, Bachelor of Science in Information Technology degrees are awarded after three and four years with specialization in areas such as Business Computing, System Administration & Networks as well as Software Development. The Polytechnic of Namibia, University of Namibia and other educational institutions in Namibia are key producers of graduates in this field.


=== Nepal ===
In Nepal, Bachelor of Science in Computer Science and Information Technology (B.Sc.CSIT ) is a four-year course of study. The Bachelor of Computer Science and Information Technology is provided by Tribhuvan University and the degree awarded is referred to as BScCSIT.


=== Pakistan ===
In Pakistan, a Bachelor of Science in Information Technology (BS-IT) is 4-year program. Some universities also named as Bachelor of Engineering in Information Technology (BE-IT).BEIT was started in Dr.A.Q Khan institute of Computer Sciences and Information Technology(KICSIT) from 2001 to 2013.Now it has been converted into BSIT.www.kicsit.edu.pk site for Visit.PUCIT is the only university who's ranked 'W'(highest)by HEC in BS(IT) among Pakistan universities.


=== Philippines ===
In the Philippines, BSIT program normally takes 4 years to complete. Schools with trimester system has less time to complete this course. A total number of 486 hours was set by the CHED during internships of the program.
On May 4, 2000, the ILOVEYOU or Love Bug Virus has spread all over the world affecting millions of computers with an estimated damage of $10 billion. The virus was originally a thesis proposal of Onel de Guzman which was unanimously rejected by the College of Computer Studies' academic board of AMA Computer College Makati, the pioneering IT school in the Philippines.


=== Portugal ===
In Portugal, the Bachelor of Science in Information Technology degree is awarded following a ten-year course of study without specialization.


=== Sri Lanka ===
In Sri Lanka, the Bachelor of Science in Information Technology (B.Sc. IT) is either a four-year degree with a specialization, called a major, with honors or a three-year degree without any specialization, called a general degree. University of Colombo has offered Bachelor of Science in Information and Communication Technology and later it was changed as Bachelor of Science in Information Systems. University of Moratuwa and Sri Lanka Institute of Information Technology also offers IT degrees. University of Moratuwa gives B.Sc.(Hons) Information Technology 4-year degree.Sri Lanka's first IT faculty was in University of Moratuwa. Bachelor of Science in Information and Communication Technology (Special) degree program was newly started at Faculty of Humanities and Social Sciences (FHSS) at University of Sri Jayewardenepura.


=== United States ===
In the United States, a B.S. in Information Technology is awarded after a four-year course of study. Some degree programs are accredited by the Computing Accreditation Commission of the Accreditation Board for Engineering and Technology (ABET).


== See also ==

Bachelor of Computing
Bachelor of Information Technology
Bachelor of Computer Science
Bachelor of Software Engineering
Bachelor of Computer Information Systems


== References =="
117,Multiven,12866752,10929,"Multiven Group BV provides multi-vendor Internet Protocol network infrastructure, technical support, maintenance and consulting services to large enterprises, Internet service providers, small, medium businesses, Telecommunications companies, Fortune 500, Academia and government agencies.


== History ==


=== Origins ===
Multiven was founded in 2005 in Palo Alto California by Peter Alfred-Adekeye. His aim was to build a company able to maintain the integrity of the world's Internet infrastructure without a hardware, software or political agenda. Multiven's core mission is to provide owners and operators of Internet networks with a single, independent end-to-end network maintenance, management and cyber-defence services.
Headquartered in Rotterdam, Netherlands, Multiven is today the world's first and only independent provider of software management, maintenance and cyber-defence services for Internet networks. Multiven has regional sales offices in Paris, France, London, UK, Dubai, UAE.


=== History of Market ===
The Internet, a short form of the word Internetwork, is an international ubiquitous network of networks. Given the amount of data traversing it, its criticality to personal, corporate and national security cannot be over-emphasized.
The Internet infrastructure comprises software-driven networked switches, routers, firewalls, servers and storage hardware that switch, route, protect and store all voice, video and text data intelligently across the world.
Internet equipment manufacturers like Cisco Systems, Hewlett-Packard, etc.. have a quasi monopoly on the multi-Billion dollar software maintenance services market for networking equipment through their own maintenance contracts and authorized third party maintenance. Multiven is the only provider offering a one-stop alternative for businesses of all sizes.


=== The Multiven Open Marketplace ===
In March 2018, Multiven launched the development of the Multiven Open Marketplace, a blockchain project aiming to transition its existing business into a decentralized organisation; Multiven is developing a blockchain-based open Marketplace to allow customers to buy and sell IT hardware, software and services, without intermediaries, solely powered by small contracts. Multiven's maintenance and cybersecurity services will be accessible on the platform and can be bound to any hardware device.


== Services ==
Multiven proposes different ranges of services depending on the need of their customers.
Multiven handles software asset management through a free cloud-based application that simplifies the way organizations inventory and manage all their IT and network hardware and software assets. The company provides software maintenance by using features that include 24x7 direct-to-expert support, proactive solutions, AI-enabled knowledge-engine and lifetime software support. It can be deployed on a monthly, quarterly, bi-annual or annual basis for a customers' entire network install-base.
One of the most important service is network-based cyberdefense, a service that guarantees to stop cyber-attacks within a network and restore service within 24 hours of an attack. The service is neutral, impartial and independent with no allegiance to any nation state. It focuses on eliminating Advanced Persistent Threats, which are the most intrusive and pervasive form of cyberattacks. It is powered by an elite team of 1000+ network and security experts in 55 countries, called Pingsta.
Software recycling is a service conducted through an online platform that provides owners of pre-owned Cisco software licenses with the unprecedented ability to recycle their unwanted Cisco software and resell it. Currently, only Cisco software licenses within the European Economic Area (i.e. the EU 28 nation states, Norway, Iceland, Switzerland and Liechtenstein) can be resold in the Multiven Marketplace. Buyers can come from anywhere in the world.
These services will gradually be transitioned to Multiven's blockchain initiative.


== Controversies ==


=== Antitrust lawsuit against Cisco Systems ===
On December 1, 2008, Multiven filed an antitrust lawsuit[3][4][5][6][7][8][9] against Cisco Systems, Inc. in an effort to open up the network maintenance services marketplace for Cisco equipment, promote competition and ensure consumer choice and value. Multiven’s complaint alleges that Cisco harmed Multiven and consumers by bundling and tying bug fixes/patches and updates for its operating system software to its maintenance services (“SMARTnet”) and through a series of other illegal exclusionary and anticompetitive acts designed to maintain Cisco’s alleged monopoly in the network maintenance services market for Cisco networking equipment.[10]


=== CEO criminal charges ===
Cisco in turn accused Multiven CEO Peter Alfred-Adekeye of illegal access to Cisco material using a Cisco ID. Alfred-Adekeye is a naturalised British citizen, a resident of Zurich, and a former Cisco employee. After several months in which US authorities prevented his entry into the country for participation in the litigation, a special hearing in the case took place at a Canadian hotel on 20 May 2010, involving a US special master and four Cisco lawyers. He was arrested from the court session by Canadian police based on a misleading US arrest warrant.
Alfred-Adekeye was released after 28 days in custody but was forced to remain in Canada for another year before he was allowed to return to Zurich. In June 2011, a Canadian judge stayed the extradition, ruling that the strict standard of ""extraordinary misconduct"" was met by the circumstances and speaking of the ""audacity of it all"", of ""Cisco's duplicity"", and the ""shocking"" act of preventing someone's participation in a judicial proceeding by arresting them to supposedly force them to participate. False material in the US attorney's letter had misled the judge who signed the Canadian arrest warrant. ""Grotesquely inflated"" charges and the unjustified portrayal of Alfred-Adekeye as a Nigerian-born scam artist and flight risk had misled the Canadian judicial system further. The underlying civil case by Multiven against Cisco had been withheld from them.


=== After the Lawsuit ===
Since the Cisco case, Peter Adekeye says, Multiven has recruited almost 1200 engineers to offer businesses various levels of assistance.[13]
The company operates a free web application which provides key information about a range of network equipment. The site highlights devices and support issues that are ‘trending’ – i.e. seeing a spike in support tickets – allowing users to address issues proactively.
Adekeye says the company has made a “humble"" but ""growing” foothold of customers in the US, and is starting to expand in Europe, Africa and Middle East.


== References ==


== External links ==
Official website
Blockchain project"
118,Webist,50809337,10901,"The International Conference on Web Information Systems and Technologies is a major point of contact between scientific researchers, on the areas of web-based information systems. The conference has been held every year since 2005.
Webist has technical sessions, tutorial talks, poster sessions and keynote lectures. Papers accepted are included in the Proceedings book.
WEBIST achieved a fast popularity in 2007 and its maturity along of the subsequent years, reaching a stable conference-size. Usually, some papers are selected to be included in a post publication published by Springer Science+Business Media in series ""Lecture Notes in Business Information Processing"".


== Areas ==
Internet Technology
Service based Information Systems
Web InterfacesWeb intelligence
Mobile Information Systems


== Current Chairs ==


=== Conference Co-Chairs ===
Valérie Monfort, LAMIH Valenciennes UMR CNRS 8201, France
Karl-Heinz Krempels, RWTH Aachen University, Germany


=== Program Co-Chairs ===
Tim A. Majchrzak, University of Agder, Kristiansand, Norway
Paolo Traverso, Center for Information Technology - IRST (FBK-ICT), Italy


== Editions ==


=== WEBIST 2015 - Lisbon, Portugal ===
Proceedings - Proceedings of the 11th International Conference on Web Information Systems and Technologies. ISBN 978-989-758-106-9 
Best Paper Award - Giseli Rabello Lopes, Bernardo Pereira Nunes, Luiz André P. Paes Leme, Terhi Nurmikko-Fuller and Marco A. Casanova. ""Knowing the past to Plan for the Future""
Best Student Paper - Martin Leginus, Leon Derczynski and Peter Dolog. ""Enhanced Information Access to Social Streams Through Word Clouds with Entity Grouping""


=== WEBIST 2014 - Barcelona, Spain ===
Proceedings - Proceedings of the 10th International Conference on Web Information Systems and Technologies. ISBN 978-989-758-023-9 
Best Paper Award - Nan Tian, Yue Xu, Yuefeng Li, Ahmad Abdel-Hafez and Audun Josang. ""Product Feature Taxonomy Learning based on User Reviews""
Best Student Paper - Karsten Seipp and Kate Devlin. ""The One Hand Wonder""


=== WEBIST 2013 - Aachen, Germany ===
Proceedings - Proceedings of the 9th International Conference on Web Information Systems and Technologies. ISBN 978-989-8565-54-9 
Best Paper Award - Christos Makris, Yannis Plegas, Giannis Tzimas and Emmanouil Viennas. ""SerfSIN: Search Engines Results' Refinement using a Sense-driven Inference Network""
Tyler Corbin, Tomasz Müldner and Jan Krzysztof Miziolek. ""Pre-order Compression Schemes for XML in the Real Time Environment""
Best Student Paper - Ricardo Lage, Peter Dolog and Martin Leginus. ""Classifying Short Messages on Social Networks using Vector Space Models""
Maria Chroni, Angelos Fylakis and Stavros D. Nikolopoulos. ""Watermarking Images in the Frequency Domain by Exploiting Self-inverting Permutations""


=== WEBIST 2012 - Porto, Portugal ===
Proceedings - Proceedings of the 8th International Conference on Web Information Systems and Technologies. ISBN 978-989-8565-08-2 
Best Paper Award - Alexander Zibula and Tim A. Majchrzak. ""DEVELOPING A CROSS-PLATFORM MOBILE SMART METER APPLICATION USING HTML5, JQUERY MOBILE AND PHONEGAP""
Best Student Paper - Mikhail Zolotukhin, Timo Hämäläinen and Antti Juvonen. ""GROWING HIERARCHICAL SELF-ORGANISING MAPS FOR ONLINE ANOMALY DETECTION BY USING NETWORK LOGS""


=== WEBIST 2011 - Noordwijkerhout, Netherlands ===
Proceedings - Proceedings of the 7th International Conference on Web Information Systems and Technologies. ISBN 978-989-8425-51-5 
Best Paper Award - Olfa Bouchaala, Mohamed Sellami, Walid Gaaloul, Samir Tata and Mohamed Jmaiel. ""GRAPH-BASED MANAGEMENT OF COMMUNITIES OF WEB SERVICE REGISTRIES""
Best Student Paper - Daniel Hienert, Benjamin Zapilko, Philipp Schaer and Brigitte Mathiak. ""VIZGR""


=== WEBIST 2010 - Valencia, Spain ===
Proceedings - Proceedings of the 6th International Conference on Web Information Systems and Technology. ISBN 978-989-674-025-2 


=== WEBIST 2009 - Lisbon, Portugal ===
Proceedings - Proceedings of the Fifth International Conference on Web Information Systems and Technologies. ISBN 978-989-8111-81-4 


=== WEBIST 2008 - Funchal, Madeira, Portugal ===
Proceedings - Proceedings of the Fourth International Conference on Web Information Systems and Technologies. ISBN 978-989-8111-26-5 


=== WEBIST 2007 - Barcelona, Spain ===
Proceedings - Proceedings of the Third International Conference on Web Information Systems and Technologies. ISBN 978-972-8865-77-1 


=== WEBIST 2006 - Setúbal, Portugal ===
Proceedings - Proceedings of WEBIST 2006 - Second International Conference on Web Information Systems and Technologies. ISBN 978-972-8865-46-7 


=== WEBIST 2005 - Miami, United States ===
Proceedings - Proceedings of the First International Conference on Web Information Systems and Technologies. 972-8865-20-1


== External links ==
Science and Technology Events
Conference website
WikiCfp call for papers


== References =="
119,International Conference on Computer and Information Technology,12066155,10901,"International Conference on Computer and Information Technology or ICCIT is a series of computer science and information technology based conferences that is hosted in Bangladesh since 1997 by a different university each year. ICCIT provides a forum for researchers, scientists, and professionals from both academia and industry to exchange up-to-date knowledge and experience in different fields of Computer Science/Engineering and Information and Communication Technology (ICT). This is a regularly held ICT based major annual conference (held typically in December) in Bangladesh now in its 15th year. ICCIT series has succeeded in engaging the most number of universities in Bangladesh from both public and private sectors. Each new university in Bangladesh have been investing in computer science, computer engineering, information systems, and related fields.
Starting 2008, the ICCIT is co-sponsored by IEEE. For ICCIT 2011, for example, 353 manuscripts were submitted from all over the world; 126 of the submitted manuscripts (approximately 35%) were accepted for presentation and inclusion in the IEEE Xplore Digital Library, one of the largest scholarly research database containing over two million records that indexes, abstracts, and provides full-text for articles and papers on computer science, electrical engineering, electronics, information technology, and physical sciences.
ICCIT 2017 will be held on 22–24 December 2017 at the University of Asia Pacific, at Dhaka, Bangladesh.


== History ==
ICCIT trace its history to 1997 when University of Dhaka organised a conference, National Conference on Computer and Information Systems (NCCIS) based on IT and Computer Science. Probably it was the first initiative to organise an IT based conference in Bangladesh with participation from multiple universities. Very next year in 1998, this conference was renamed to its current name and gained international status by opening its door to the participants from outside of Bangladesh. Since then each year a university approved by the ICCIT committee hosts this event during late December.


== Areas ==
ICCIT is mainly focused on computer science and information technology but also covers related electronic engineering topics. Major areas of ICCIT include, but not limited to:
Algorithms
Artificial Intelligence
Bengali Language Processing
Bio-informatics
Computer Vision
Computer Graphics and Multimedia
Computer Network and Data Communications
Computer Based Education
Database Systems
Digital Signal Processing and Image Processing
Digital System and Logic Design
Distributed and Parallel Processing
E-commerce and E-governance
Human Computer Interaction
Information Systems
Internet and Web Applications
Knowledge Data Engineering
Neural Networks
Pattern Recognition
Robotics
Software Engineering
System Security
Ubiquitous Computing
VLSI
Wireless Communications and Mobile Computing
""Technical Challenges and Design Issues in Bengali Language Processing"" , edited by Mohammad Ataul Karim, Mohammad Kaykobad and Manzur Murshed, is an outcome of ICCIT series that seeks to provide a state-of-the-art platform for Bengali-based computing work, of significance to nearly 300 million Bengali-speaking people who live in Bangladesh and India.


== Past Conferences ==
Starting 1997, ICCIT has had 15 successful events at 14 different universities.
1997 University of Dhaka, Dhaka (as NCCIS '97)
1998 Bangladesh University of Engineering and Technology (BUET), Dhaka
1999 Shahjalal University of Science and Technology, Sylhet
2000 North South University, Dhaka
2001 University of Dhaka, Dhaka
2002 East West University, Dhaka
2003 Jahangirnagar University, Savar
2004 BRAC University, Dhaka
2005 Islamic University of Technology (IUT), Gazipur
2006 Independent University Bangladesh (IUB), Dhaka
2007 United International University (UIU), Dhaka
2008 Khulna University of Engineering and Technology (KUET), Khulna
2009 Independent University Bangladesh (IUB) and Military Institute of Science and Technology, Dhaka
2010 Ahsanullah University of Science and Technology, Dhaka
2011 American International University-Bangladesh, Dhaka
2012 Chittagong University, Chittagong
2013 Khulna University, Khulna
2014 Daffodil International University, Dhaka


== Recent Events ==
Daffodil International University (DIU), Dhaka, ICCIT 2014 (17th ICCIT).


== International Program Committee ==
The key to the success of ICCIT is its International Program Committee (IPC), chaired by Professor Mohammad Ataul Karim, Provost & Executive Vice-Chancellor of University of Massachusetts Dartmouth. The IPC for ICCIT 2012, for example, is a body of eighty five (85) field experts all of who are affiliated with either a university or a research organisation from outside of Bangladesh. The national make-up of the latest IPC is as follows: USA (43), Australia (12), Canada(6), UK (5), Malaysia (4), Japan (3), Germany (2), India (2), Korea (2), New Zealand (2), Belgium (1), China (1), Ireland (1), Norway (1), and Switzerland (1).


== Journal Special Issues ==
Starting with ICCIT 2008, a selected number of manuscripts after further enhancement and extensive review process are being included in one of several journal special issues. ICCIT doesn’t end with just conference proceedings but with those that are indexed worldwide and takes many of its better papers to its next logical level to the journals. To date, 14 journal special issues have been produced by ICCIT IPC featuring works of Bangladesh-based researchers in the fields of communications, computing, multimedia, networks, and software. This is a serious feat for Bangladesh its many researchers; the outcome from this single conference is allowing for about 30–35 team of researchers each year to be able to showcase their research through archival and indexed journals that really matter. It is a major scholarly milestone which makes ICCIT series different from all other technical conferences held in Bangladesh. In its latest iteration, 32 selected enhanced ICCIT 2011 manuscripts after having gone through extensive reviews have been accepted now for inclusion in the following international journals.
Journal of Communications
Guest Editors: M.N. Islam, SUNY Farmingdale, US; K.M. Iftekharuddin, Old Dominion University, US; M.A. Karim, Old Dominion University, US; M.A. Salam, Southern University & A&M College, Louisiana, US
Journal of Computers
Guest Editors: S.M. Aziz, University of South Australia, Australia; M.S. Alam, University of South Alabama, US; K.V. Asari, University of Dayton, US; M. Alamgir Hossain, University of Northumbria, UK; M.A. Karim, Old Dominion University, US; M. Milanova, University of Arkansas at Little Rock, US
Journal of Multimedia
Guest Editors: M. Murshed, Monash University, Australia; M.A. Karim, Old Dominion University, US; M. Paul, Monash University, Australia; S. Zhang, College of Staten Island, US
Journal of Networks
Guest Editors: S. Jabir, France Telecom, Japan; J. Abawajy, Deakin University, Australia; F. Ahmed, Johns Hopkins University Applied Physics Laboratory, US; M.A. Karim, Old Dominion University, US; J. Kamruzzaman, Monash University, Australia; Nurul I. Sarkar, Auckland University of Technology, New Zealand


== References ==


== External links ==
11th ICCIT Home page
15th ICCIT Home page"
120,Haiyan Zhang,56686233,10774,"Haiyan Zhang is a Designer and Engineer. She is Director of Innovation at Microsoft Research. She appeared on the BBC show ""Big Life Fix"".


== Education ==
Zhang was born in China, and migrated to Australia with her parents aged eight. She attended Monash University, where she earned first-class honours for a bachelor's degree in computer science in 1998. Zhang worked for Space-Time Research until 2000, when she left Australia to further her studies in design. Zhang moved to Canada to complete a Post Graduate Diploma in Interactive Multimedia at Sheridan College, which she completed in 2003. Whilst there she helped develop a tool for visualising electroencephalography for intra-operative monitoring. She moved to Interaction Design Institute Ivrea, where she completed a Masters in Interaction Design.


== Career ==
Zhang worked as an Interaction Designer for the British Design Council and Stanford University. Zhang joined IDEO as Principal Interaction Designer in 2006. Here she ""created new technology experiences for community building, entertainment, financial services"". with Mattel, Electronic Arts, HBO, France Telecom, Alcatel, Cisco, and AT&T. She was a founder of the innovation platform OpenIDEO.com, which brings together makers around the world to solve challenges for social good. The site has over 150,000 users worldwide.
She joined Microsoft in 2013, working in the Lift London studio on games and wearables. In 2015 she became Innovation Director at Microsoft Research. She is interested in technology for ""Connected Play"" and wellness. Zhang developed a geiger counter that can be built simply by non-experts, which are distributed around Fukushima.


=== Big Life Fix ===
The BBC Two show ""Big Life Fix"" paired technology experts with members of the public who were facing real life challenges. Zhang was one of seven makers. Zhang developed two prodcuts, the Emma Watch and Fizzyo. Fizzyo is a toy to encourage children with cystic fibrosis to do their breathing exercises to clear their lungs. It comprises a wireless electronic sensor in the mouthpiece, which sends an electronic signal to control a computer game on a tablet. The Emma Watch is a device for sufferers Parkinson's disease, which looks to reduce limb tremors by disrupting the ""feedback loop between the bran and hand"". Microsoft unveiled the device at their annual developer's conference. It was named after Emma Lawton, a graphic designer who Zhang met on the show. The invention received significant media coverage.
Zhang regularly delivers public talks and appears on podcasts, where she discusses design and engineering. She is an advocate for increasing the representation of women in technology. She is a Fellow of the Royal Society of the Arts (FRSA) and British Academy of Film and Television Arts (BAFTA).


== References =="
121,Animal-computer interaction,55866337,10725,"Animal-computer interaction (ACI) is a field of research which studies the design and use of technology with, for and by animals. ACI emerged from, and is heavily influenced by, the discipline of human-computer interaction (HCI).
In an ACI Manifesto (2011), Mancini defines one aim of ACI as understanding ""the interaction between animals and computing technology within the contexts in which animals habitually live, are active, and socialize with members of the same or other species, including humans"". Mancini's ACI Manifesto additionally proposes three core design goals for the field: enhancing animals' quality of life and wellbeing; supporting animals in the functions assigned to them by humans; and supporting human-animal relationships. Accordingly, ACI research gives considerable attention to questions of animal ethics, welfare, consent and power.
A central theme of ACI research lies in establishing how user-centred design approaches and HCI methods can be adapted to designing with and for animals. Accordingly, many ACI studies seek to adopt 'animal-centred' approaches to design and research.
Much ACI endeavour focuses on technologies to support communication and relationships between animals and humans. A number of researchers have investigated digital technologies for dogs, including systems for remote communication with dogs left at home, wearable interactive devices for dogs, and interfaces for working dogs. ACI researchers have also explored technology to foster interactions with other domestic animals, including cats.


== References =="
122,Made with Code,46367882,10685,"Made with Code is an initiative launched by Google on 19 July 2014. Google aimed to empower young women in middle and high schools with computer programming skills. Made with Code was created after Google’s own research found out that encouragement and exposure are the critical factors that would influence young females to pursue Computer Science. It was reported that Google is funding $50 million to Made with Code, on top of the initial $40 million invested since 2010 in organizations like Code.org, Black Girls Code, and Girls Who Code. The Made with Code initiative involves both online activities as well as real life events, collaborating with notable firms like Shapeways and App Inventor.


== Projects ==
Made with Code revolves primarily around providing online activities for young girls to learn coding on its website. Many of Made with Code’s projects use Blockly programming, a visual editor that writes programs by assembling individual blocks. Step by step instructions are provided to guide users. Along the way, works may either be discarded or saved and downloaded.


=== Dance Visualiser ===
Dance Visualiser mixes dance with code by modelling a visualiser that mirrors a dancer’s motions. Through the application of Blockly programming, users track the different parts such as the head, chest, hip and four limbs of the dancer’s body. After inputting the necessary details, a customized visualization is generated accordingly.


=== Music Mixer ===
In Music Mixer, users manipulate the number of notes and set the speed of each instrument to produce a colorful rotating visual music mixer. The range of instruments that are available include Acapella, Country, Electronic, HipHop, Pop and Rock.


=== Beats ===
Beats connects Blockly programming language and virtual instruments together to produce a string of beats. Users set the speed from a minimum of 30 to a maximum of 300 beats. The range of virtual instruments available include hi-hat, clave, cowbell, cymbal, tom, kick, snare and clap.


=== Avatar ===
Project Avata r allows users to customize their own avatar. Through Blockly programming language, users input different shapes on a virtual 2D work space, then arrange the shapes into a 3D avatar.


=== Accessorizer ===
Accessorizer allow to accessorize (put accessories on) a selfie with Blockly programming language. The first step is selecting an image, either by snapping a picture or selecting the available characters including Dorothy, Rose, Smoosh, Raul and Blanche. The next step is to position the accessories on top of the character or image. Accessories include the eyes, mouths, shirts, hats and wigs.


=== GIF ===
GIF lets users make a custom animation with a background and a series of frame. With the Blockly programming language, four images can be constructed which will then cycle so as to form an animation. The first step is to select the background, which includes characters such as Licky Ricky, Mayday Mary, Puss in Moon Boots, Purple Mess, Flappy the Uni-Horn, Tonsil Tammy, Bucky, Long Lidia, Permy and Mr. Hula Hips. The next step is to select frame(s), which includes various shapes and colors.


=== Kaleidoscope ===
Kaleidoscope lets users manipulate the size, speed, and images of a kaleidoscope animation. After selecting an animation, either star, cross or flower, users pick an image and select a rotation speed. Next, select the image size by entering a percentage value.


=== Yeti ===
Yeti project allows users to create an animated Yeti with Blockly programming language. The first step is to drag and insert the YETI block onto the work space, followed by the character design block. Select the fur and skin color of your Yeti, as well as the hand and feet sizes. Next, select the animation command from the various range of actions provided.


=== Past notable projects ===


==== Code a Bracelet ====
Made with Code collaborated with Shapeways to allow girls to create their own customised bracelet. After designing the bracelet with Blockly, Shapeways prints the bracelets using nylon plastic on their 3D EOS printers.


== Mentors and makers ==
Made with Code website features videos of mentors, who are females in different industries who have used computer coding in their career, and makers, who are young females who have made a difference in society using their coding skills. Some of the mentors and makers were also invited to hold talks such as during Made with Code's kick off event in New York City. Over 100 teenage girls from local organizations and public schools worked on coding projects and witnessed first-hand how women use code in their dream jobs.


=== List of mentors ===
Ayah Bdeir
Danielle Feinberg
Erica Kochi
Limor Fried
Miral Kotb
Robin Hunicke


=== List of makers ===
Brittany Wenger
Kenzie Wilson
Maddy Maxey
EPA Chica Squad
Tesca Fitzgerald
Ebony ""WondaGurl"" Oshunrinde


== Events ==
Made with Code website has a resource directory for parents and girls to enter their ZIP code and find more information about new local events.


=== 92nd annual White House Christmas Tree Lighting Ceremony ===
Made with Code partnered with National Park Foundation to organised a campaign in 2014 to light up 56 official White House Christmas trees in President’s Park. More than 300,000 people, mostly young girls, participated and programmed the designs of the lights on the trees through selecting different shapes, sizes, and colors of the lights, and animate different patterns using introductory programming language.


== Others ==


=== Partnership with Code School ===
Made with Code has partnered with Code School in June 2014 and provided three months free accounts in Code School for women and minorities already in the technology industry to expand their skills. This initiative widened Made with Code's target group to also include those who have already started a career in the tech industry, in addition to the initial target group of young girls without prior coding experiences.
Google raise gender diversity in Code School by giving $50 million over three years to provide for related programs. They concurrently work with DonorsChoose.org and Codecademy or Khan Academy for the trial of a project. They also collaborated with the Science and Entertainment Exchange.


== References ==


== External links ==
Official website"
123,Cheminformatics,575697,10627,"Cheminformatics (also known as chemoinformatics, chemioinformatics and chemical informatics) is the use of computer and informational techniques applied to a range of problems in the field of chemistry. These in silico techniques are used, for example, in pharmaceutical companies in the process of drug discovery. These methods can also be used in chemical and allied industries in various other forms.


== History ==
The term chemoinformatics was defined by F.K. Brown in 1998:

Chemoinformatics is the mixing of those information resources to transform data into information and information into knowledge for the intended purpose of making better decisions faster in the area of drug lead identification and optimization.

Since then, both spellings have been used, and some have evolved to be established as Cheminformatics, while European Academia settled in 2006 for Chemoinformatics. The recent establishment of the Journal of Cheminformatics is a strong push towards the shorter variant.


== Basics ==
Cheminformatics combines the scientific working fields of chemistry, computer science and information science for example in the areas of topology, chemical graph theory, information retrieval and data mining in the chemical space. Cheminformatics can also be applied to data analysis for various industries like paper and pulp, dyes and such allied industries.


== Applications ==


=== Storage and retrieval ===

The primary application of cheminformatics is in the storage, indexing and search of information relating to compounds. The efficient search of such stored information includes topics that are dealt with in computer science as data mining, information retrieval, information extraction and machine learning. Related research topics include:
Unstructured data
Information retrieval
Information extraction

Structured data mining and mining of structured data
Database mining
Graph mining
Molecule mining
Sequence mining
Tree mining

Digital libraries


==== File formats ====

The in silico representation of chemical structures uses specialized formats such as the XML-based Chemical Markup Language or SMILES. These representations are often used for storage in large chemical databases. While some formats are suited for visual representations in 2 or 3 dimensions, others are more suited for studying physical interactions, modeling and docking studies.


=== Virtual libraries ===
Chemical data can pertain to real or virtual molecules. Virtual libraries of compounds may be generated in various ways to explore chemical space and hypothesize novel compounds with desired properties.
Virtual libraries of classes of compounds (drugs, natural products, diversity-oriented synthetic products) were recently generated using the FOG (fragment optimized growth) algorithm. This was done by using cheminformatic tools to train transition probabilities of a Markov chain on authentic classes of compounds, and then using the Markov chain to generate novel compounds that were similar to the training database.


=== Virtual screening ===

In contrast to high-throughput screening, virtual screening involves computationally screening in silico libraries of compounds, by means of various methods such as docking, to identify members likely to possess desired properties such as biological activity against a given target. In some cases, combinatorial chemistry is used in the development of the library to increase the efficiency in mining the chemical space. More commonly, a diverse library of small molecules or natural products is screened.


=== Quantitative structure-activity relationship (QSAR) ===

This is the calculation of quantitative structure-activity relationship and quantitative structure property relationship values, used to predict the activity of compounds from their structures. In this context there is also a strong relationship to chemometrics. Chemical expert systems are also relevant, since they represent parts of chemical knowledge as an in silico representation. There is a relatively new concept of matched molecular pair analysis or prediction-driven MMPA which is coupled with QSAR model in order to identify activity cliff.


== See also ==


== References ==


== External links ==
Indiana Cheminformatics Education Portal
Journal of Cheminformatics
OEChem Cheminformatics Programming Toolkit
The Blue Obelisk Movement
The eCheminfo Network and Community of Practice
Cheminformatics courses at Indiana University
ChemXSeer search engine and tool at Penn State
Cheminformatics at Rensselaer Polytechnic Institute
Collaborative Drug Discovery CDD Vault
The Chemical Structure Association Trust (see also CSA Trust).
Comprehensive cheminformatics link list and data set repository
A cheminformatics glossary
Chemoinformatics initiatives at NCL Pune, India
International Conference on Chemoinformatics at NCL,Pune
Crowd Computing for Chemoinformatics at Vinod Scaria Lab , India
Cheminformatics Crowd Computing for Tuberculosis Drug Discovery (3C4TB) Project Page
Famous Cheminformatics quotations
The Cheminformatics and QSAR Society
UK-QSAR and ChemoInformatics Group
Education and Research at the University of Hamburg
Cheminformatics research at the Unilever Centre for Molecular Informatics, Cambridge, UK
YACHS Yet Another CHemistry Summarizer, Laboratoire Informatique d'Avignon LIA, France
Cheminformatics research at NovaMechanics Cyprus
Weblink-Cheminformatics SW and DB
Cheminformatics studies from Unilever Centre for Molecular Informatics to OpenEye
International Journal of Chemoinformatics and Chemical Engineering"
124,Outline of computer science,169633,10597,"The following outline is provided as an overview of and topical guide to computer science:
Computer science (also called computing science) is the study of the theoretical foundations of information and computation and their implementation and application in computer systems. One well known subject classification system for computer science is the ACM Computing Classification System devised by the Association for Computing Machinery.


== What type of thing is computer science? ==
Computer science can be described as all of the following:
Academic discipline
Science
Applied science


== Subfields ==


=== Mathematical foundations ===
Coding theory – Useful in networking and other areas where computers communicate with each other.
Game theory – Useful in artificial intelligence and cybernetics.
Graph theory – Foundations for data structures and searching algorithms.
Mathematical logic – Boolean logic and other ways of modeling logical queries; the uses and limitations of formal proof methods
Number theory – Theory of the integers. Used in cryptography as well as a test domain in artificial intelligence.


=== Algorithms and data structures ===
Algorithms – Sequential and parallel computational procedures for solving a wide range of problems.
Data structures – The organization and manipulation of data.


=== Artificial intelligence ===
Outline of artificial intelligence
Artificial intelligence – The implementation and study of systems that exhibit an autonomous intelligence or behavior of their own.
Automated reasoning – Solving engines, such as used in Prolog, which produce steps to a result given a query on a fact and rule database, and automated theorem provers that aim to prove mathematical theorems with some assistance from a programmer.
Computer vision – Algorithms for identifying three-dimensional objects from a two-dimensional picture.
Soft computing, the use of inexact solutions for otherwise extremely difficult problems:
Machine learning - Automated creation of a set of rules and axioms based on input.
Evolutionary computing - Biologically inspired algorithms.

Natural language processing - Building systems and algorithms that analyze, understand, and generate natural (human) languages.
Robotics – Algorithms for controlling the behaviour of robots.


=== Communication and security ===
Networking – Algorithms and protocols for reliably communicating data across different shared or dedicated media, often including error correction.
Computer security – Practical aspects of securing computer systems and computer networks.
Cryptography – Applies results from complexity, probability, algebra and number theory to invent and break codes, and analyze the security of cryptographic protocols.


=== Computer architecture ===
Computer architecture – The design, organization, optimization and verification of a computer system, mostly about CPUs and Memory subsystem (and the bus connecting them).
Operating systems – Systems for managing computer programs and providing the basis of a usable system.


=== Computer graphics ===
Computer graphics – Algorithms both for generating visual images synthetically, and for integrating or altering visual and spatial information sampled from the real world.
Image processing – Determining information from an image through computation.
Information visualization – Methods for representing and displaying abstract data to facilitate human interaction for exploration and understanding.


=== Concurrent, parallel, and distributed systems ===
Parallel computing - The theory and practice of simultaneous computation; data safety in any multitasking or multithreaded environment.
Concurrency (computer science) – Computing using multiple concurrent threads of execution, devising algorithms for solving problems on multiple processors to achieve maximal speed-up compared to sequential execution.
Distributed computing – Computing using multiple computing devices over a network to accomplish a common objective or task and thereby reducing the latency involved in single processor contributions for any task.


=== Databases ===
Relational databases – the set theoretic and algorithmic foundation of databases.
Structured Storage - non-relational databases such as NoSQL databases.
Data mining – Study of algorithms for searching and processing information in documents and databases; closely related to information retrieval.


=== Programming languages and compilers ===
Compiler theory – Theory of compiler design, based on Automata theory.
Programming language pragmatics – Taxonomy of programming languages, their strength and weaknesses. Various programming paradigms, such as object-oriented programming.
Programming language theory
Formal semantics – rigorous mathematical study of the meaning of programs.
Type theory – Formal analysis of the types of data, and the use of these types to understand properties of programs — especially program safety.


=== Scientific computing ===
Computational science – constructing mathematical models and quantitative analysis techniques and using computers to analyze and solve scientific problems.
Numerical analysis – Approximate numerical solution of mathematical problems such as root-finding, integration, the solution of ordinary differential equations; the approximation of special functions.
Symbolic computation – Manipulation and solution of expressions in symbolic form, also known as Computer algebra.
Computational physics – Numerical simulations of large non-analytic systems
Computational chemistry – Computational modelling of theoretical chemistry in order to determine chemical structures and properties
Bioinformatics and Computational biology – The use of computer science to maintain, analyse, store biological data and to assist in solving biological problems such as Protein folding, function prediction and Phylogeny.
Computational neuroscience – Computational modelling of neurophysiology.


=== Software engineering ===
Formal methods – Mathematical approaches for describing and reasoning about software designs.
Software engineering – The principles and practice of designing, developing, and testing programs, as well as proper engineering practices.
Algorithm design – Using ideas from algorithm theory to creatively design solutions to real tasks.
Computer programming – The practice of using a programming language to implement algorithms.
Human–computer interaction – The study and design of computer interfaces that people use.
Reverse engineering – The application of the scientific method to the understanding of arbitrary existing software.


=== Theory of computation ===

Automata theory – Different logical structures for solving problems.
Computability theory – What is calculable with the current models of computers. Proofs developed by Alan Turing and others provide insight into the possibilities of what may be computed and what may not.
List of unsolved problems in computer science

Computational complexity theory – Fundamental bounds (especially time and storage space) on classes of computations.
Quantum computing theory – Explores computational models involving quantum superposition of bits.


== History ==
History of computer science
List of pioneers in computer science


== Professions ==
Programmer
Teacher/Professor
Software engineer
Software architect
Software developer
Software tester
Interaction designer
Network administrator


== Data and data structures ==
Data structure
Data type
Associative array and Hash table
Array
List
Tree
String
Matrix (computer science)
Database


== Programming paradigms ==
Imperative programming/Procedural programming
Functional programming
Logic programming
Object oriented programming
Class
Inheritance
Object


== See also ==

Abstraction
Big O notation
Closure
Compiler
Cognitive science


== External links ==

Outline of computer science at Curlie (based on DMOZ)
ACM report on a recommended computer science curriculum (2008)
Directory of free university lectures in Computer Science
Collection of Computer Science Bibliographies
Photographs of computer scientists (Bertrand Meyer's gallery)"
125,Outline of software,24775031,10526,"The following outline is provided as an overview of and topical guide to software:
Software – collection of computer programs and related data that provides the instructions for telling a computer what to do and how to do it. Software refers to one or more computer programs and data held in the storage of the computer for some purposes. In other words, software is a set of programs, procedures, algorithms and its documentation concerned with the operation of a data processing system. The term was coined to contrast to the old term hardware (meaning physical devices). In contrast to hardware, software ""cannot be touched"". Software is also sometimes used in a more narrow sense, meaning application software only. Sometimes the term includes data that has not traditionally been associated with computers, such as film, tapes, and records.


== What type of thing is software? ==
Software can be described as all of the following:
Technology
Computer technology
Tools


== Types of software ==
Application software – end-user applications of computers such as word processors or video games, and ERP software for groups of users.
Business software
Computer-aided design
Databases
Decision-making software
Educational software
Image editing
Industrial automation
Mathematical software
Medical software
Molecular modeling software
Quantum chemistry and solid state physics software
Simulation software
Spreadsheets
Telecommunications (i.e., the Internet and everything that flows on it)
Video editing software
Video games
Word processors

Middleware controls and co-ordinates distributed systems.
Programming languages – define the syntax and semantics of computer programs. For example, many mature banking applications were written in the language COBOL, invented in 1959. Newer applications are often written in more modern languages.
System software – provides the basic functions for computer usage and helps run the computer hardware and system. It includes a combination of the following:
Device driver
Operating system
Package management system
Server
Utility
Window system

Teachware – any special breed of software or other means of product dedicated to education purposes in software engineering and beyond in general education.
Testware – any software for testing hardware or software.
Firmware – low-level software often stored on electrically programmable memory devices. Firmware is given its name because it is treated like hardware and run (""executed"") by other software programs. Firmware often is not accessible for change by other entities but the developers' enterprises.
Shrinkware is the older name given to consumer-purchased software, because it was often sold in retail stores in a shrink wrapped box.
Device drivers – control parts of computers such as disk drives, printers, CD drives, or computer monitors.
Programming tools – assist a programmer in writing computer programs, and software using various programming languages in a more convenient way. The tools include:
Compilers
Debuggers
Interpreters
Linkers
Text editors
Integrated development environment (IDE) – single application for managing all of these functions.


== Software products ==


=== By publisher ===
List of Adobe software
List of Microsoft software


=== By platform ===
List of Macintosh software
List of old Macintosh software


=== By genre ===
List of software categories
List of 2D animation software
List of 3D animation software
List of 3D computer graphics software
List of 3D modeling software
List of antivirus software
List of compilers
List of computer algebra systems
List of computer-assisted organic synthesis software
List of computer simulation software
List of concept- and mind-mapping software
List of content management systems
List of graphing software
List of information graphics software
List of Linux distributions
List of operating systems
List of protein structure prediction software
List of molecular graphics systems
List of numerical analysis software
List of optimization software
List of quantum chemistry and solid state physics software
List of spreadsheet software
List of statistical packages
List of Unified Modeling Language tools
List of video editing software
List of web browsers


=== Comparisons ===
Comparison of 3D computer graphics software
Comparison of accounting software
Comparison of audio player software
Comparison of computer-aided design editors
Comparison of data modeling tools
Comparison of database tools
Comparison of desktop publishing software
Comparison of digital audio editors
Comparison of DOS operating systems
Comparison of email clients
Comparison of force field implementations
Comparison of instant messaging clients
Comparison of issue tracking systems
Comparison of Linux distributions
Comparison of mail servers
Comparison of network monitoring systems
Comparison of nucleic acid simulation software
Comparison of operating systems
Comparison of raster graphics editors
Comparison of software for molecular mechanics modeling
Comparison of system dynamics software
Comparison of text editors
Comparison of vector graphics editors
Comparison of web frameworks
Comparison of web server software
Comparison of word processors
comparison of social software


== History of software ==
History of software engineering
History of free and open-source software
History of software configuration management
History of programming languages
Timeline of programming languages

History of operating systems
History of Mac OS X
History of Microsoft Windows
Timeline of Microsoft Windows

History of the web browser
Web browser history


== Software development ==
Software development  (outline) – development of a software product, which entails computer programming (process of writing and maintaining the source code), but also encompasses a planned and structured process from the conception of the desired software to its final manifestation. Therefore, software development may include research, new development, prototyping, modification, reuse, re-engineering, maintenance, or any other activities that result in software products.


=== Computer programming ===
Computer programming  (outline) –


=== Software engineering ===
Software engineering  (outline) –


== Software distribution ==
Software distribution –
Software licenses
Beerware
Free
Free and open source software
Freely redistributable software
Open-source software
Proprietary software
Public domain software

Revenue models
Adware
Donationware
Freemium
Freeware
Commercial software
Nagware
Postcardware
Shareware

Delivery methods
Digital distribution
List of mobile software distribution platforms

On-premises software
Pre-installed software
Product bundling
Software as a service
Software plus services

Scams
Scareware
Malware

End of software life cycle
Abandonware


== Software industry ==
Software industry


== Software publications ==
Free Software Magazine
InfoWorld
PC Magazine
Software Magazine
Wired (magazine)


== Persons influential in software ==
Bill Gates
Steve Jobs
Jonathan Sachs
Wayne Ratliff


== See also ==

Outline of information technology
Outline of computers
Outline of computing

List of computer hardware terms
Bachelor of Science in Information Technology
Custom software
Functional specification
Marketing strategies for product software
Service-Oriented Modeling Framework
Bus factor
Capability Maturity Model
Software publisher
User experience


== References ==


== External links =="
126,Software visualization,3070397,10488,"Software visualization or software visualisation refers to the visualization of information of and related to software systems—either the architecture of its source code or metrics of their runtime behavior- and their development process by means of static, interactive or animated 2-D or 3-D visual representations of their structure, execution, behavior, and evolution.


== Software system information ==
Software visualization uses a variety of information available about software systems. Key information categories include:
implementation artifacts such as source codes,
software metric data from measurements or from reverse engineering,
traces that record execution behavior,
software testing data (e.g., test coverage)
software repository data that tracks changes.


== Objectives ==
The objectives of software visualization are to support the understanding of software systems (i.e., its structure) and algorithms (e.g., by animating the behavior of sorting algorithms) as well as the analysis and exploration of software systems and their anomalies (e.g., by showing classes with high coupling) and their development and evolution. One of the strengths of software visualization is to combine and relate information of software systems that are not inherently linked, for example by projecting code changes onto software execution traces.
Software visualization can be used as tool and technique to explore and analyze software system information, e.g., to discover anomalies similar to the process of visual data mining. For example, software visualization is used to monitoring activities such as for code quality or team activity. Visualization is inherently not a method for software quality assurance.


== Types ==
Tools for software visualization might be used to visualize source code and quality defects during software development and maintenance activities. There are different approaches to map source code to a visual representation such as by software maps Their objective includes, for example, the automatic discovery and visualization of quality defects in object-oriented software systems and services. Commonly, they visualize the direct relationship of a class and its methods with other classes in the software system and mark potential quality defects. A further benefit is the support for visual navigation through the software system.
More or less specialized graph drawing software is used for software visualization. A small-scale 2003 survey of researchers active in the reverse engineering and software maintenance fields found that a wide variety of visualization tools were used, including general purpose graph drawing packages like GraphViz and GraphEd, UML tools like Rational Rose and Borland Together, and more specialized tools like Visualization of Compiler Graphs (VCG) and Rigi. The range of UML tools that can act as a visualizer by reverse engineering source is by no means short; a 2007 book noted that besides the two aforementioned tools, ESS-Model, BlueJ, and Fujaba also have this capability, and that Fujaba can also identify design patterns.


== See also ==
Programs
Imagix 4D
NDepend
SonarJ
Sotoarc
Related concepts
Software maintenance
Software maps
Software diagnosis
Cognitive dimensions of notations
Software archaeology


== References ==


== Further reading ==
Roels, R., Mestereaga, P., and Signer, B. (2016). ""An Interactive Source Code Visualisation Plug-in for the MindXpres Presentation Platform"". Communications in Computer and Information Science (CCIS), 583, 2016
Burch, M., Diehl, S., and Weißgerber, P. (2005). Visual data mining in software archives. Proceedings of the 2005 ACM symposium on Software visualization (SoftVis '05). ACM, New York, NY, USA, 37-46. doi:10.1145/1056018.1056024
Diehl, S. (2002). Software Visualization. International Seminar. Revised Papers (LNCS Vol. 2269), Dagstuhl Castle, Germany, 20–25 May 2001 (Dagstuhl Seminar Proceedings).
Diehl, S. (2007). Software Visualization — Visualizing the Structure, Behaviour, and Evolution of Software. Springer, 2007, ISBN 978-3-540-46504-1
Eades, P. and Zhang, K. (1996). ""Software Visualisation"", Series on Software Engineering and Knowledge Engineering, Vol.7, World Scientific Co., Singapore, 1996, ISBN 981-02-2826-0, 268 pages.
Gîrba, T., Kuhn, A., Seeberger, M., and Ducasse, S., ""How Developers Drive Software Evolution,"" Proceedings of International Workshop on Principles of Software Evolution (IWPSE 2005), IEEE Computer Society Press, 2005, pp. 113–122. PDF
Keim, D. A. (2002). Information visualization and visual data mining. IEEE Transactions on Visualization and Computer Graphics, USA * vol 8 (Jan. March 2002), no 1, p 1 8, 67 refs.
Knight, C. (2002). System and Software Visualization. In Handbook of software engineering & knowledge engineering. Vol. 2, Emerging technologies (Vol. 2): World Scientific Publishing Company.
Kuhn, A., and Greevy, O., ""Exploiting the Analogy Between Traces and Signal Processing,"" Proceedings IEEE International Conference on Software Maintenance (ICSM 2006), IEEE Computer Society Press, Los Alamitos CA, September 2006. PDF
Lanza, M. (2004). CodeCrawler — polymetric views in action. Proceedings. 19th International Conference on Automated Software Engineering, Linz, Austria, 20 24 Sept. 2004 * Los Alamitos, CA, USA: IEEE Comput. Soc, 2004, p 394 5.
Lopez, F. L., Robles, G., & Gonzalez, B. J. M. (2004). Applying social network analysis to the information in CVS repositories. ""International Workshop on Mining Software Repositories (MSR 2004)"" W17S Workshop 26th International Conference on Software Engineering, Edinburgh, Scotland, UK, 25 May 2004 * Stevenage, UK: IEE, 2004, p 101 5.
Marcus, A., Feng, L., & Maletic, J. I. (2003). 3D representations for software visualization. Paper presented at the Proceedings of the 2003 ACM symposium on Software visualization, San Diego, California.
Soukup, T. (2002). Visual data mining : techniques and tools for data visualization and mining. New York: Chichester.
Staples, M. L., & Bieman, J. M. (1999). 3-D Visualization of Software Structure. In Advances in Computers (Vol. 49, pp. 96–143): Academic Press, London.
Stasko, J. T., Brown, M. H., & Price, B. A. (1997). Software Visualization: MIT Press.
Van Rysselberghe, F. (2004). Studying Software Evolution Information By Visualizing the Change History. Proceedings. 20th International Conference On Software Maintenance. pp 328–337, IEEE Computer Society Press, 2004
Wettel, R., and Lanza, M., Visualizing Software Systems as Cities. In Proceedings of VISSOFT 2007 (4th IEEE International Workshop on Visualizing Software For Understanding and Analysis), pp. 92 – 99, IEEE Computer Society Press, 2007.
Zhang, K. (2003). ""Software Visualization - From Theory to Practice"". Kluwer Academic Publishers, Boston, April 2003, ISBN 1-4020-7448-4, 468 pages.


== External links ==
SoftVis the ACM Symposium on Software Visualization
VISSOFT 2nd IEEE Working Conference on Software Visualization
EPDV Eclipse Project Dependencies Viewer


=== Research groups ===
SoftVis at Hasso Plattner Institute for IT Systems Engineering
SoftVis at University of Groningen
SoftVis at Georgia Tec (GVU)
SoftVis at Helsinki University of Technology"
127,C. K. Raut,31171962,10399,"C. K. Raut (full: Chandra Kant Raut, Nepali: चन्द्र कान्त राउत), PhD (Cambridge) is a former US-based computer scientist, author and political leader, who has been actively involved in demanding a separate Madhesh country for the Madhesi people. He is often placed under house-arrest by the Government of Nepal .


== Early life ==
Dr. Raut was born in Mahadeva Village, in the Saptari district, Sagarmatha Zone of Nepal. He attended primary school in his home village, and secondary school in nearby Katti . He went on to study at Tribhuvan University (Nepal), Tokyo University (Japan) and Cambridge University (UK)..He is a recipient of Young Nepalese Engineer Award, Mahendra Bidhya Bhusan, Kulratna Gold Medal, and Trofimenkoff Academic Achievement Award among others.


== Professional career ==
He worked as a computer scientist for the US based company Raytheon BBN technologies.


== Political movement ==
Dr. Raut is the founding president of the Non-Resident Madhesi Association, a global diaspora organisation of Madhesi people living abroad. He is also the founder of the Injot Movement, a social transformation initiative targeted at Nepal, Ethiopia,and Bangladesh. In recent years, he has been advocating for an end to racism, colonisation and discrimination imposed on the Madheshi people, by the Nepali ""ruling class"" by his definition.


== Arrest ==
On 13 September 2014, the Government of Nepal, arrested him for giving a speech to indigenous people, called Santhals, in their annual festival. Dr. Raut started fast unto death in custody from 20 September arguing that his right to freedom and expression was violated by the government. He was hospitalised on 25 September Thursday after complains of severe stomach pain. Nepali Congress Vice-President Ram Chandra Poudel, Minister for Information and Communications Minendra Rijal and Agriculture Minister Hari Parajuli went to the hospital to urge Raut to end his fast. He ended his fast on 1 October after 11 days, upon government's request and commitment to respect freedom of expression. The Attorney Office filed a sedition case at the Special Court on 8 October.


== International Support ==
New York City based Human Right Watch issued this statement on the arrest and subsequent filing of seadation charges against Dr Raut.
""The Nepal authorities should immediately withdraw sedition charges and unconditionally release rights campaigner Chandra Kant Raut. His arrest threatens the chances of a robust debate on federalism, and undermines the promise of inclusion. Raut's arrest shows that minority voices can and will be easily sidelined.""
Asian Human Rights Commission has stated
""Acknowledging that everyone has the rights to freedom of speech, movement, peaceful assembly and association are the fundamental rights of all human beings everywhere, as also mandated by the United Nations Universal Declaration of Human Rights, of which Nepal is a signatory, we expect acknowledgement and assurance of our rights from the state and its agencies. The AHRC urges the government of Nepal to immediately release Dr Chandra Kant Raut. The AHRC fears possible torture, harassment and other ill treatment during their arbitrary arrest by the Morang Police.""
Amnesty International wrote a letter to Minister for Home Affairs demanding Raut's release.


== Speech in Biratnagar ==
On 3 January 2015 Dr. CK Raut was again arrested while giving a speech at Jutwa College. Many supporters were injured following clash with the police. A dozen police personnels were injured in a .Dr. Raut's supporter claimed police has used brutal force and didn't allow DR. Raut to be hospitalised locally for injuries incurred during the clash.


== Alliance for Independent Madhesh (AIM) ==
Dr.Raut is the president and the founding member of Alliance for Independent Madhesh (AIM), which in its manifesto has described itself as a coalition of Terai people discriminated by Pahari civilisation. It has people of various sub-ethnicities, activists, parties and various organisations that are working towards establishing an independent Madhesh. Although it was established in 2007, it only announced their manifesto through a press conference in Kathmandu on 21 May 2012. The manifesto states the main objective of AIM is to achieve independence of Madhesh through peaceful and non-violent means. It has also demanded an end an end to racism, slavery and discrimination imposed on Nepali people of Madheshi origin by the Pahari people. It claims to have three pillars (a) independent Madhesh of Nepal (b) non-violence and peaceful means (c) democratic system.


== Books and films ==
मधेश स्वराज / Madhesh Swaraj
मधेश का इतिहास / A History of Madhesh
वीर मधेशी / Bir Madheshi
वैरागदेखि बचावसम्म (आत्मकथा) [Denial to Defense]
Black Buddhas: The Madheshis of Nepal (documentary)


== References =="
128,Subrata Dasgupta,42809091,10363,"Subrata Dasgupta is a bi-cultural multidisciplinary scholar, scientist, and writer. Born in Calcutta (in 1944), he was educated in England, India, and Canada.


== Education ==
He received his schooling at Bemrose School, Derby (UK) and La Martiniere Calcutta. He holds a first class bachelor of engineering (B.E.) degree in metallurgy from the University of Calcutta having studied at Bengal Engineering College (later, Bengal Engineering and Science University, now the Indian Institutes of Engineering Science and Technology), India’s second oldest engineering school. His graduate studies were at the University of Alberta, Edmonton, Canada, where he obtained the M.Sc and Ph.D degrees in computer science in 1974 and 1976 respectively.


== Career ==


=== Academic ===
He has taught at Ohio State University, University of Alberta, University of Louisiana at Lafayette, and the University of Manchester Institute of Science and Technology (where he was the first Dowty Professor of Computer Systems Engineering). Since 1993, he has held the Computer Science Trust Fund Eminent Scholar Chair in the University of Louisiana at Lafayette, where he is also a professor, Department of History and, from 1999 to 2013 was the Director of the Institute of Cognitive Science. He has held visiting appointments in Wolfson College, Oxford, the Computer Laboratory, Cambridge, Simon Fraser University (Vancouver), the Indian Institute of Science and the Center for the Development of Advanced Computing, C-DAC (Bangalore, India), Aachen Technical University, and the University of Oldenberg (both in Germany). He has served as a consultant for the United Nations Development Programme (UNDP). Dasgupta has featured in interviews and articles in multiple BBC Radio programs, British national newspapers and magazines, Indian national newspapers, and regional Spanish and Canadian newspapers. His biographical entries appear in Who’s Who in America, Who’s Who in the World, Writers Directory, and Contemporary Authors.
Subrata Dasgupta’s research from 1974 to 1991 spanned such branches of computer science as computer architecture, microprogramming, hardware description languages, program verification, and design theory. Since 1992, his work has focused on the cognitive and historical nature of creativity in the natural and artificial sciences, engineering and technology, art, literary scholarship, and cross-cultural milieux. In addition to numerous papers, essays and articles, he is the author of fourteen books.


=== Computer Science and the Sciences of the Artificial ===
Influenced by Herbert Simon’s seminal book The Sciences of the Artificial, Dasgupta has written extensively on the historical, epistemological and cognitive nature of the artificial sciences and, in particular, on computer science as a science of the artificial. His books include: The Design and Description of Computer Architectures (John Wiley, 1984), Computer Architecture: A Modern Synthesis. Volume 1: Foundations and Volume 2: Advanced Topics (John Wiley, 1989), Design Theory and Computer Science (Cambridge University Press, 1991), and It Began with Babbage : The Genesis of Computer Science (Oxford University Press, 2014)  which was selected as an 'Outstanding Academic Title for 2014' by the journal Choice. Most recently, his Computer Science: A Very Short Introduction (2016) was published as a volume in Oxford University Press's best-selling and prestigious ""Very Short Introductions"" series.'


=== Cognitive-Historical Studies of Creativity ===
He has published numerous papers and essays on the cognitive-historical-biographical aspects of creativity in art, science, technology and scholarship in such print journals as Perspectives on Science, Transactions of the Newcomen Society, Notes & Records of the Royal Society, Creativity Research Journal, Physics News, Physics of Life Reviews and the online journal PsyArt. His books include Creativity in Invention and Design (Cambridge University Press, 1994), Technology and Creativity (Oxford University Press, 1996), Jagadis Chandra Bose and the Indian Response to Western Science (Oxford University Press, 1999), 
Twilight of the Bengal Renaissance: R.K. Dasgupta and His Quest for a World Mind (Dey’s Publishing, 2005), The Bengal Renaissance: Identity and Creativity from Rammohun Roy to Rabindranath Tagore (Permanent Black, 2007), and Awakening: The Story of the Bengal Renaissance (Random House India, 2010).
One of his special interests is the nature of the creative mind spanning multiple cultures, especially the ‘Indo-Western mind’. His work on the 19th century creative and intellectual movement in Bengal called the Bengal Renaissance addresses this mentality.


=== Creative Writing ===
Dasgupta’s interests in cross-cultural creativity has extended to ‘creative literary writing’ of both fiction and nonfiction. He has published two novels, Three Times a Minority (Writers Workshop Calcutta, 2003), and The Golden Jubilee (Amaryllis, 2013), and a memoir of his English childhood, Salaam Stanley Matthews (Granta, 2006). He has recently completed another novel that explores cross-cultural psychology, titled Voice of the Rain Season.


== Books ==
Computer Science: A Very Short Introduction, 2016,
It Began with Babbage: The Genesis of Computer Science, 2014,
The Golden Jubilee (Novel), 2013,
The Awakening: The Story of the Bengal Renaissance, 2010,
Bengal Renaissance: Identity and Creativity from Rammohun Roy to Rabindranath Tagore, 2007,
Salaam Stanley Matthews, 2006,
Twilight of the Bengal Renaissance: R.K. Dasgupta and his Quest for a World Mind, 2003,
Three Times a Minority (Novel), 2003,
Jagadis Chandra Bose and the Indian Response to Western Science, 2000,
Technology and Creativity, 1996,
Creativity in Invention and Design, 1994,
Design Theory and Computer Science (Cambridge Tracts in Theoretical Computer Science), 1991,
Computer Architecture: A Modern Synthesis, Volume 1 Foundations, 1988,
Computer Architecture, Advanced Topics (Wie Computer Architecture Advanced Topics) (Volume 2), 1989,
Design and Description of Computer Architectures, 1984


== Personal Information ==
He is married to Sarmistha Dasgupta, daughter of the comparative literature scholar and intellectual historian Rabindra Kumar Das Gupta, the first director of the Indian National Library. He has two sons, Jaideep and Shome.


== References =="
129,Simplified Instructional Computer,568646,10289,"The Simplified Instructional Computer (also abbreviated SIC) is a hypothetical computer system introduced in System Software: An Introduction to Systems Programming, by Leland Beck. Due to the fact that most modern microprocessors include subtle, complex functions for the purposes of efficiency, it can be difficult to learn systems programming using a real-world system. The Simplified Instructional Computer solves this by abstracting away these complex behaviors in favor of an architecture that is clear and accessible for those wanting to learn systems programming.


== SIC Architecture ==
The SIC machine has basic addressing, storing most memory addresses hexadecimal integer format. Similar to most modern computing systems, the SIC architecture stores all data in binary and uses the two's complement to represent negative values at the machine level. Memory storage in SIC consists of 8-bit bytes, and all memory addresses in SIC are byte addresses. Any three consecutive bytes form a 24-bit 'word' value, addressed by the location of the lowest numbered byte in the word value. Numeric values are stored as word values, and character values use the 8-bit ASCII system. The SIC machine does not support floating-point hardware and have at most 32,768 bytes of memory. There is also a more complicated machine built on top of SIC called the Simplified Instruction Computer with Extra Equipment (SIC/XE). The XE expansion of SIC adds a 48-bit floating point data type, an additional memory addressing mode, and extra memory (1 megabyte instead of 32,768 bytes) to the original machine. All SIC assembly code is upwards compatible with SIC/XE.
SIC machines have several registers, each 24 bits long and having both a numeric and character representation:

A (0): Used for basic arithmetic operations; known as the accumulator register.
X (1): Stores and calculates addresses; known as the index register.
L (2): Used for jumping to specific memory addresses and storing return addresses; known as the linkage register.
PC (8): Contains the address of the next instruction to execute; known as the program counter register.
SW (9): Contains a variety of information, such as carry or overflow flags; known as the status word register.

In addition to the standard SIC registers, there are also four additional general-purpose registers specific to the SIC/XE machine:

B (3): Used for addressing; known as the base register.
S (4): No special use, general purpose register.
T (5): No special use, general purpose register.
F (6): Floating point accumulator register (This register is 48-bits instead of 24).

These five/nine registers allow the SIC or SIC/XE machine to perform most simple tasks in a customized assembly language. In the System Software book, this is used with a theoretical series of operation codes to aid in the understanding of assemblers and linker-loaders required for the execution of assembly language code.


== Addressing Modes for SIC and SIC/XE ==
The Simplified Instruction Computer has three instruction formats, and the Extra Equipment add-on includes a fourth. The instruction formats provide a model for memory and data management. Each format has a different representation in memory:

Format 1: Consists of 8 bits of allocated memory to store instructions.
Format 2: Consists of 16 bits of allocated memory to store 8 bits of instructions and two 4-bits segments to store operands.
Format 3: Consists of 6 bits to store an instruction, 6 bits of flag values, and 12 bits of displacement.
Format 4: Only valid on SIC/XE machines, consists of the same elements as format 3, but instead of a 12-bit displacement, stores a 20-bit address.

Both format 3 and format 4 have six-bit flag values in them, consisting of the following flag bits:

n: Indirect addressing flag
i: Immediate addressing flag
x: Indexed addressing flag
b: Base address-relative flag
p: Program counter-relative flag
e: Format 4 instruction flag


== Addressing Modes for SIC/XE ==
Rule 1:
e = 0 : format 3
e = 1 : format 4
format 3:
b = 1, p = 0 (base relative)
b = 0, p = 1 (pc relative)
b = 0, p = 0 (direct addressing)

format 4:
b = 0, p = 0 (direct addressing)
x = 1 (index)
i = 1, n = 0 (immediate)
i = 0, n = 1 (indirect)
i = 0, n = 0 (SIC)
i = 1, n = 1 (SIC/XE for SIC compatible)

Rule 2:
i = 0, n =0 (SIC)
b, p, e is part of the address.


== SIC Assembly Syntax ==
SIC uses a special assembly language with its own operation codes that hold the hex values needed to assemble and execute programs. A sample program is provided below to get an idea of what a SIC program might look like. In the code below, there are three columns. The first column represents a forwarded symbol that will store its location in memory. The second column denotes either a SIC instruction (opcode) or a constant value (BYTE or WORD). The third column takes the symbol value obtained by going through the first column and uses it to run the operation specified in the second column. This process creates an object code, and all the object codes are put into an object file to be run by the SIC machine.

      COPY   START  1000      FIRST  STL    RETADR      CLOOP  JSUB   RDREC             LDA    LENGTH             COMP   ZERO             JEQ    ENDFIL             JSUB   WRREC             J      CLOOP      ENDFIL LDA    EOF             STA    BUFFER             LDA    THREE             STA    LENGTH             JSUB   WRREC             LDL    RETADR             RSUB      EOF    BYTE   C'EOF'      THREE  WORD   3      ZERO   WORD   0      RETADR RESW   1      LENGTH RESW   1      BUFFER RESB   4096      .      .      SUBROUTINE TO READ RECORD INTO BUFFER      .      RDREC  LDX    ZERO             LDA    ZERO      RLOOP  TD     INPUT             JEQ    RLOOP             RD     INPUT             COMP   ZERO             JEQ    EXIT             STCH   BUFFER,X             TIX    MAXLEN             JLT    RLOOP      EXIT   STX    LENGTH             RSUB      INPUT  BYTE   X'F1'      MAXLEN WORD   4096      .      .      SUBROUTINE TO WRITE RECORD FROM BUFFER      .      WRREC  LDX    ZERO      WLOOP  TD     OUTPUT             JEQ    WLOOP             LDCH   BUFFER,X             WD     OUTPUT             TIX    LENGTH             JLT    WLOOP             RSUB      OUTPUT BYTE   X'06'             END    FIRST

If you were to assemble this program, you would get the object code depicted below. The beginning of each line consists of a record type and hex values for memory locations. For example, the top line is an 'H' record, the first 6 hex digits signify its relative starting location, and the last 6 hex digits represent the program's size. The lines throughout are similar, with each 'T' record consisting of 6 hex digits to signify that line's starting location, 2 hex digits to indicate the size (in bytes) of the line, and the object codes that were created during the assembly process.

      HCOPY 00100000107A
      T0010001E1410334820390010362810303010154820613C100300102A0C103900102D
      T00101E150C10364820610810334C0000454F46000003000000
      T0020391E041030001030E0205D30203FD8205D2810303020575490392C205E38203F
      T0020571C1010364C0000F1001000041030E02079302064509039DC20792C1036
      T002073073820644C000006
      E001000


== Sample program ==
Given below is a program illustrating data movement in SIC.
LDA FIVE
STA ALPHA
LDCH CHARZ
STCH C1
ALPHA RESW 1
FIVE WORD 5
CHARZ BYTE C'Z'
C1 RESB 1


== Emulating the SIC System ==
Since the SIC and SIC/XE machines are not real machines, the task of actually constructing a SIC emulator is often part of coursework in a systems programming class. The purpose of SIC is to teach introductory-level systems programmers or collegiate students how to write and assemble code below higher-level languages like C and C++. With that being said, there are some sources of SIC-emulating programs across the web, however infrequent they may be.

An assembler and a simulator written by the author, Leland in Pascal is available on his educational home page at ftp://rohan.sdsu.edu/faculty/beck
SIC/XE Simulator And Assembler downloadable at https://sites.google.com/site/sarimohsultan/Projects/sic-xe-simulator-and-assembler
SIC Emulator, Assembler and some example programs written for SIC downloadable at http://sicvm.sourceforge.net/home.php
SicTools - virtual machine, simulator, assembler and linker for the SIC/XE computer available at https://jurem.github.io/SicTools/


== See also ==
Computer
MIX
System software
Assembly language
Processor register
Virtual machine


== References ==

Beck, Leland (1996), System Software: An Introduction to Systems Programming (3 ed.), Addison-Wesley, ISBN 0-201-42300-6 
Information of SIC and SIC/XE systems: http://www-rohan.sdsu.edu/~stremler/2003_CS530/SicArchitecture.html
List of SIC and SIC/XE instructions: http://solomon.ipv6.club.tw/~solomon/Course/SP.941/sic-instruction.html
Brief memory addressing information: http://www.unf.edu/~cwinton/html/cop3601/s10/class.notes/basic4-SICfmts.pdf
SIC/XE Mode Addressing: http://uhost.rmutp.ac.th/wanapun.w/--j--/ch2-2.pdf


== External links ==
SICvm A Virtual Machine based on a Simplified Instructional Computer (SIC)"
130,Nigel A. L. Clarke,56825414,10118,"Nigel Andrew Lincoln Clarke, (born 20 October 1971) is a Jamaican Member of Parliament, company director, business executive and statesman. He has served as Chairman or director of over 20 Jamaican public and private sector economic enterprises.  His public sector directorships have included the Bank of Jamaica (Jamaica’s central bank and financial services regulator); Chairman of the Port Authority of Jamaica (the regulator of Jamaica’s ports and the developer and owner of Jamaica’s cargo and cruise ports); Chairman of the National Housing Trust (Jamaica’s state-owned mortgage lender and housing developer) and Chairman of the HEART Trust NTA (Jamaica’s largest tertiary level vocational training and certification institution. In 2016, Dr. Clarke was appointed by the Most Hon. Andrew Holness, Prime Minister of Jamaica, to serve as Jamaica’s Ambassador-at-Large for Economic Affairs within the Office of the Prime Minister . Ambassador Clarke previously served as a Senator in the Upper House of the Jamaican parliament between 2013 and 2015.
Prior to his government service, Clarke served as Vice Chairman of the Musson Group, having served previously as Chief Operating Officer and Chief Financial Officer of the Group and as Chief Executive Officer of its major subsidiaries. Clarke played an integral executive leadership role in the expansion of the Musson Group from a substantially Jamaican base to having operations and subsidiaries in over 30 countries with over US$1 billion in revenues and market leading businesses in telecommunications, information technology, consumer goods and food manufacturing.  The Musson Group is a leading Jamaica-based multinational with four associated companies that are listed on the Jamaica Stock Exchange and over 50 other privately held subsidiaries and associated companies. Clarke’s executive business experience spanned leadership of transnational mergers and acquisitions, corporate leadership, business development and emerging market business leadership. 
Clarke is married and has two children. Clarke attended St. Richards Primary School in Kingston and graduated from Munro College in St. Elizabeth Jamaica where he was awarded the Jamaica Independent Scholarship, for study at the University of the West Indies. He graduated from the University of the West Indies with a degree in Mathematics and Computer Science where he received First Class Honours and won the prize for the best degree with the highest marks among students in the Faculty of Natural Sciences across all campuses of UWI in Jamaica, Trinidad and Barbados.  He was awarded the Commonwealth Scholarship to attend Oxford University where he earned an M.Sc. in Applied Statistics. In his thesis he examined volatility on the Jamaica Stock Exchange. He subsequently won the Rhodes Scholarship and earned a Doctor of Philosophy degree in Mathematics (Numerical Analysis) from Oxford University.


== Career ==


=== Political Leadership ===
He currently serves as Member of Parliament for the Jamaican constituency of St. Andrew North West and has done so since March 8, 2018.  He has previously served as Senator in the Upper House of the Jamaican Parliament from 2013 to 2015. 


=== Leadership in Business and Finance ===
Nigel Clarke played a leadership role in the growth and expansion of the Musson Group from a base in Jamaica to having operations and subsidiaries in over 30 countries in the Caribbean, Central America, Europe and the Pacific.
Clarke is recognised for having led, managed or executed dozens of acquisitions and corporate transactions on behalf of the Musson Group across many sectors. Notable transactions include the acquisition of Nestle’s manufacturing business in Jamaica, the acquisition of Kraft Foods manufacturing business in Jamaica, the acquisition of the Serge dairy businesses, the acquisition of several  technology-related  and telecom businesses in the Caribbean, Central America and Europe among several other such transactions.
Clarke started his career as a derivatives trader in London at Goldman Sachs, the prestigious international investment bank.  Since returning to Jamaica in 1999, he has structured and negotiated over US$1 billion of inbound investment for private equity, business development, acquisition finance, trade finance and infrastructure development. Notable private sector transactions include: the securing of over US$300m of direct private equity and debt investment for Jamaican businesses from major international banks and multilateral financial institutions including Citibank NA, the International Finance Corporation, the Inter American Development Bank, the European Investment Bank and the Caribbean Development Bank; the first ever regionally syndicated, locally arranged loan (US$182 million) spanning several jurisdictions in the Caribbean and Central America and across multiple currencies to fund business growth and development across the Caribbean and Central America; the foundation and development of the first ever fund established in the region to finance regional private equity and venture capital (the US$32m Caribbean Investment Fund); and the foundation of Jamaica’s first specialist investment fund dedicated to mezzanine financing for business growth (the US$15m Caribbean Mezzanine Fund). Notable public sector initiatives include the completion of the US$400 million transaction to divest and expand the Kingston Container Terminal; the facilitation of over 5200 new housing starts in 2017 by way of direct construction, direct financing and financing partnerships with financial institutions (representing the fastest pace of low cost housing construction in Jamaica over two decades); and the role of principal interlocutor with the International Monetary Fund, on behalf of the Government of Jamaica, in negotiating the early successful termination of the Extended Fund Facility agreement and the entry into the US$1.6 billion Precautionary Stand-By Arrangement with the Fund. He has delivered policy related addresses to the annual general meetings or other high level fora of the International Monetary Fund, the Organisation of American States, the Clinton Global Initiative and the Caribbean Development Bank.
Clarke’s private sector directorships have cut across several industries and include: in the trading and manufacturing sector, Vice Chairman of the Musson Group (a privately held distribution enterprise covering food, technology and telecommunications and employing 5,000 persons across 30 countries), in the financial services sector, the NCB Financial Group (Jamaica’s largest financial services holding company), where he also served as a member of the bank’s credit committee; in the manufacturing, sector, Red Stripe (the brewing and bottling company); in the agri-business and food processing sectors, Jamaican Broilers Group and Seprod Limited (which collectively include the largest protein, dairy, vegetable oil, and grain producers and distributors operating in Jamaica) In addition, Clarke has served as Chairman of Eppley Limited, a company listed on the Jamaica Stock Exchange that specialises in sourcing, originating and investing in credit-related opporunities. He has also served as Vice Chairman of the PBS Group, the largest regional business solutions and technology distribution company in the Caribbean region with operations and subsidiaries throughout 14 countries of Central America and the Caribbean, which is listed on the Jamaica Stock Exchange.
In addition to his leadership of major economic institutions in the Jamaican public and private sector, Clarke is nationally recognised for his leadership in education and youth empowerment.  He served as Chairman of the Heart Trust NTA (Jamaica’s largest tertiary level vocational training and certification institution); and founder and Chairman of the National Youth Orchestra of Jamaica which delivers music for social change programmes in Jamaica. His international educational affiliations include service as a Director of the Youth Orchestra of the Americas and the role of Co-organiser and Host of TEDx Jamaica.


== Education ==
Nigel Clarke holds a Bachelor's Degree in Mathematics and Computer Science from the University of the West Indies, Mona on the Jamaica Independence Scholarship where he graduated in 1992 . He also graduate from the University of Oxford with a Master's Degree in Applied Statistics in 1994 and a Doctor of Philosophy (Ph.D.) in Numerical Analysis in 1997. This was done through the Commonwealth Scholarship and the Rhodes Scholarship respectively.


== Honours ==
Dr Clarke was handed the Kiwanis Community Service Award in 2012 from the Kiwanis Club of Kingston.
Dr. Clarke was also handed the PSOJ ""50 under 50"" Business Leadership Award in 2012 by the Private Sector Organisation of Jamaica.


== References =="
131,Technical Committee on Visualization and Graphics,53699804,10065,"The Technical Committee on Visualization and Graphics (VGTC) is a constituency of IEEE Computer Society (IEEE-CS) that oversees various technical activities related to visualization, computer graphics, virtual and augmented reality, and interaction.. VGTC is one of the 26 technical committees/councils of IEEE-CS that covers various specializations of computer science and computer engineering.


== Membership ==
Membership in VGTC is open and free of charge to researchers, practitioners and students. Membership in IEEE or the IEEE Computer Society is not required for membership, and members are eligible to serve in most positions including chairing annual conferences or serving on the executive committee. Members are also able to nominate others for technical awards.


=== Visualization Pioneers Group ===
The Visualization Pioneers Group is open to members after 20 years of contributions to some aspect of visualization. Members commit to serving the visualization community, be it through conference or journal paper reviews, financial support for the mentoring program, or serving as a mentor. Members also pay USD $50 in dues per year.


== Conferences ==
The VGTC has two flagship annual conferences. The annual executive committee meeting is held during the same week as IEEE Visualization.


=== IEEE Visualization ===
Also known, as VIS, the IEEE Visualization conference is considered one of the top-tier venues for presenting visualization research. Past conferences:
 2016: Baltimore, Maryland
 2015: Chicago, Illinois,
 2014: Paris, France
 2013: Atlanta, Georgia
 2012: Seattle, Washington
 2011: Providence, Rhode Island
 2010: Salt Lake City, Utah
 2009: Atlantic City, New Jersey
 2008: Columbus, Ohio
 2007: Sacramento, California
 2006: Baltimore, Maryland
 2005: Minneapolis, Minnesota
 2004: Austin, Texas
 2003: Seattle, Washington
 2002: Boston, Massachusetts
 2001: San Diego, California
 2000: Salt Lake City, Utah
 1999: San Francisco, California
 1998: Research Triangle Park, North Carolina
 1997: Phoenix, Arizona
 1996: San Francisco, California
Future conferences:
 2017: Phoenix, Arizona
 2018: Berlin, Germany


=== IEEE Virtual Reality ===
Past conferences:
 2016: Greenville, South Carolina
 2015: Arles, Camargue-Provence, France
 2014: Minneapolis, Minnesota
 2013: Orlando, Florida, United States
 2012: Orange County, California
 2011: Singapore
Future conferences:
 2017: Los Angeles, California
 2018: Berlin, Germany


=== Sponsored symposia ===
The VGTC also sponsors other related IEEE symposia:
IEEE Symposium on 3D User Interfaces (3DUI)
International Conference on Artificial Reality and Telexistence (ICAT)
EG/IEEE Symposium on Visualization (EuroVis - formerly VisSym)
IEEE Information Visualization Conference (InfoVis - formerly Symposium on Information Visualization)
International Symposium on Mixed and Augmented Reality (ISMAR)
IEEE Pacific Visualization Symposium (PacificVis - formerly APVIS)
IEEE Visual Analytics Science and Technology Conference (VAST - formerly Symposium on Visual Analytics Science and Technology)
IEEE Scientific Visualization Conference (SciVis - formerly Vis)


== Publications ==
The IEEE VGTC has run special issues of Computer and IEEE Computer Graphics and Applications, and is the sponsor of the IEEE Transactions on Visualization and Computer Graphics (T-VCG).


== Awards ==
The IEEE VGTC grants awards for individuals who have made significant contributions to the community through their research and volunteer efforts. The Technical Achievement Awards are presented during the opening sessions of the annual Visualization and Virtual Reality conferences. Members who have given outstanding service are routinely awarded with the IEEE Certificate of Appreciation and IEEE Meritorious Service Awards. Graduate student members are eligible for the IEEE VGTC VPG Doctoral Dissertation Award presented through the Visualization Pioneers Group annually at the IEEE VIS conference.


=== Visualization Technical Achievement Awards ===
2016
Career: John C. Dill
Technical Achievement: David S. Ebert

2015
Career: Markus Gross
Technical Achievement: Tamara Munzner

2014
Career: Kenneth Joy
Technical Achievement: Claudio T. Silva

2013
Career: Gregory M. Nielson
Technical Achievement: Kwan-Liu Ma

2012
Career: Ben Shneiderman
Technical Achievement: John Stasko

2011
Career: Frits Post
Technical Achievement: Daniel A. Keim

2010
Career: Christopher R. Johnson
Technical Achievement: Hanspeter Pfister

2009
Career: Hans Hagen
Technical Achievement: Jock D. Mackinlay

2008
Career: Lawrence J. Rosenblum
Technical Achievement: David Laidlaw

2007
Career: Stuart Card
Technical Achievement: Jarke van Wijk

2006
Career: Pat Hanrahan
Technical Achievement: Thomas Ertl

2005
Career: Arie Kaufman
Technical Achievement: Charles D. Hansen

2004
Career: Bill Lorensen
Technical Achievement: Amitabh Varshney


=== Virtual and Augmented Reality Technical Achievement Awards ===
2016
Career: Thomas A. Furness III
Technical Achievement: Anthony Steed

2015
Career: Michitaka Hirose
Technical Achievement: Brendan Iribe, Michael Antonov, and Palmer Luckey

2014
Career: Steven K. Feiner
Technical Achievement: Doug Bowman

2013
Career: Henry Fuchs
Technical Achievement: Mark Billinghurst

2012
Career: Lawrence J. Rosenblum
Technical Achievement: Dieter Schmalstieg

2010
Career: Frederick P. Brooks, Jr.
Technical Achievement: Ming C. Lin

2009
Career: Jaron Lanier
Technical Achievement: Hirokazu Kato

2008
Career: Bowen Loftin
Technical Achievement: Bernd Fröhlich

2007
Career: Susumu Tachi
Technical Achievement: Carolina Cruz-Neira

2006
Career: Larry F. Hodges
Technical Achievement: Kay Stanney

2005
Career: Mel Slater
Technical Achievement: Mark Bolas


== Leadership ==


=== Executive Committee ===
Chair - Claudio Silva, New York University
Vice Chair - Rachael Brady, Cisco Systems
Vice Chair for Conferences - Mark Livingston, Naval Research Laboratory


=== Directors ===
Chair for Technical Awards Committee for Virtual Reality - Arie Kaufman, Stony Brook University
Chair for Technical Awards Committee for Visualization; IEEE Service Awards Chair for ISMAR, Visualization, and Virtual Reality - Larry Rosenblum, University of Maryland
William Ribarsky, University of North Carolina at Charlotte
Robert Moorhead, Mississippi State University
Hanspeter Pfister, Harvard University
Amitabh Varshney, University of Maryland, College Park
Klaus Mueller, Stony Brook University


== References =="
132,International Conference on Service Oriented Computing,45485797,9923,"The International Conference on Service Oriented Computing, short ICSOC, is an annual conference providing an outstanding forum for academics, industry researchers, developers, and practitioners to report and share groundbreaking work in service-oriented computing. ICSOC has an 'A' rating from the Excellence in Research in Australia (ERA) and an 'A2' rating from the Brazilian ministry of education. Calls for Papers are regularly published on WikiCFP and on the conference website. The conference is also listed in Elsevier's Global Events List.
ICSOC fosters cross-community scientific excellence by gathering experts from various disciplines, such as business process management, distributed systems, computer networks, wireless and mobile computing, cloud computing, networking, scientific workflows, services science, management science, and software engineering. Since 2007 ICSOC operates under the auspices of the Scientific Academy for Service Technology e.V. (ServTech), a non-profit association located in Hagen, Germany. Traditionally the ICSOC venue changes from year to year between Europe, the Americas and Asia/Pacific Rim. Except for 2004, all conference proceedings were published by Springer as Lecture Notes in Computer Science.


== Locations and organizers ==


== Steering committee members ==
Boualem Benatallah (since 2008)
Fabio Casati (since 2003)
Paco Curbera (2003-2013)
Asit Dan (2007-2013)
Bernd Krämer (since 2007)
Winfried Lamersdorf (since 2012)
Heiko Ludwig (since 2013)
Mike Papazoglou (since 2003, chair)
Paolo Traverso (2003-2013)
Jian Yang (since 2013)
Liang Zhang (since 2013)


== Proceedings ==
https://link.springer.com/book/10.1007/978-3-319-69035-3 Service-Oriented Computing - 14th International Conference, ICSOC 2017, Maximilien, M., Vallecillo, A., Wang, J., Oriol, M. (Eds.), Malaga, Spain, November 13–16, 2017. Lecture Notes in Computer Science 10601, Springer Berlin Heidelberg, ISBN 978-3-319-69034-6, DOI 10.1007/978-3-319-69035-3
https://www.springer.com/de/book/9783319462943 Service-Oriented Computing - 14th International Conference, ICSOC 2016, Sheng, Q.Z., Stroulia, E., Tata, S., Bhiri, S. (Eds.), Banff, Canada, October 16–19, 2016. Lecture Notes in Computer Science 9936, Springer Berlin Heidelberg, ISBN 978-3-319-46294-3, DOI 10.1007/978-3-319-46295-0
https://www.springer.com/us/book/9783662486153 Service-Oriented Computing - 13th International Conference, ICSOC 2015, Barros, A., Grigori, D., Narendra, N.C., Dam, H.K. (Eds.), Goa, India, November 16–19, 2015. Lecture Notes in Computer Science 9435, Springer Berlin Heidelberg, ISBN 978-3-662-48615-3, DOI 10.1007/978-3-662-48616-0
https://www.springer.com/computer/swe/book/978-3-662-45390-2 Service-Oriented Computing - 12th International Conference], ICSOC 2014, Franch, X., Ghose, A.K., Lewis, G.A., Bhiri, S. (Eds.), Paris, France, November 3–6, 2014. Lecture Notes in Computer Science, Springer Berlin / Heidelberg, ISSN 0302-9743, Volume 8831, DOI 10.1007/978-3-662-45391-9, ISBN 978-3-662-45391-9.
https://www.springer.com/gp/book/9783642450044 Service-Oriented Computing - 11th International Conference, ICSOC 2013, Basu, S., Pautasso, C., Zhang, L., Fu, X. (Eds.), Lecture Notes in Computer Science, Springer Berlin / Heidelberg, ISBN 978-3-642-45005-1, Vol. 8274, DOI 10.1007/978-3-642-45005-1
https://www.springer.com/gp/book/9783642343209 Service-Oriented Computing - 10th International Conference, ICSOC 2012, Liu, C., Ludwig, H., Toumani, F., Yu, Q. (Eds.), Lecture Notes in Computer Science, Springer Berlin / Heidelberg, ISBN 978-3-642-34321-6, Vol. 7636, DOI 10.1007/978-3-642-34321-6
https://www.springer.com/de/book/9783642255342 Service-Oriented Computing - 9th International Conference, ICSOC 2011, Kappel, G., Maamar, Z., Motahari-Nezhad, H.R. (Eds.), Lecture Notes in Computer Science, Springer Berlin / Heidelberg, ISBN 978-3-642-25535-9, Vol. 7084, DOI 10.1007/978-3-642-25535-9
https://www.springer.com/gp/book/9783642173578 Service-Oriented Computing - 8th International Conference, ICSOC 2010, Maglio, P.P., Weske, M., Yang, J., Fantinato, M. (Eds.), Lecture Notes in Computer Science, Springer Berlin / Heidelberg, ISBN 978-3-642-17358-5, Vol. 6470, DOI 10.1007/978-3-642-17358-5
https://www.springer.com/de/book/9783642161315 Service-Oriented Computing - 7th International Joint Conference, ICSOC-ServiceWave 2009, Baresi, L., Chi, C.-H., Suzuki, J. (Eds.), Lecture Notes in Computer Science, Springer Berlin / Heidelberg, ISBN 978-3-642-10383-4, Vol. 5900, DOI 10.10.1007/978-3-642-10383-4
https://www.springer.com/gp/book/9783540896470 Service-Oriented Computing - 6th International Conference, ICSOC 2008, Bouguettaya, A., Krüger, I., Margaria, T. (Eds.), Lecture Notes in Computer Science, Springer Berlin / Heidelberg, ISBN 978-3-540-89652-4, Vol. 6470, DOI 10.10.1007/978-3-540-89652-4
ISBN 978-3-540-74974-5 Service-Oriented Computing - Fifth International Conference, ICSOC 2007, Krämer, B.J., Lin, K.-J., Narasimhan, P. (Eds.), Lecture Notes in Computer Science, Springer Berlin / Heidelberg, ISBN 978-3-540-74974-5, Vol. 4749, DOI 10.1007/978-3-540-74974-5
https://www.springer.com/gp/book/9783540681472 Service-Oriented Computing - 4th International Conference, ICSOC 2006, Dan, A., Lamersdorf, W. (Eds.), Lecture Notes in Computer Science, Springer Berlin / Heidelberg, ISBN 978-3-540-68148-9, Vol. 4294, DOI 10.1007/11948148
https://link.springer.com/book/10.1007%2F11596141 Service-Oriented Computing - Third International Conference, ICSOC 2005, Benatallah, B., Casati, F., Traverso, P. (Eds.), Lecture Notes in Computer Science, Springer Berlin / Heidelberg, ISBN 978-3-540-32294-8, Vol. 3826, DOI 10.1007/11596141
http://dl.acm.org/citation.cfm?id=1035167&CFID=651049220&CFTOKEN=47826368 2nd International Conference on Service Oriented Computing, ICSOC '04, Aiello, M., Aoyama, M., Curbera, F., Papazoglou, M.P. (Eds.), ACM New York, NY, USA, ISBN 1-58113-871-7, Order Number 104045
https://www.springer.com/gp/book/9783540206811 Service-Oriented Computing - First International Conference, ICSOC 2003, Orlowska, M.E., Weerawarana, S., Papazoglou, M.P., Yang, J. (Eds.), Lecture Notes in Computer Science, Springer Berlin / Heidelberg, ISBN 978-3-540-24593-3, Vol. 2910, DOI 10.1007/b94513


== References ==


== External links ==
Pointers to ICSOC conference websites


== See also ==
List of computer science conferences
List of computer science conference acronyms"
133,Computing Community Consortium,21320004,9875,"The Computing Community Consortium (CCC) is an organization whose goal is to catalyze and empower the U.S. computing research community to pursue audacious, high-impact research.
Established in 2006 through a cooperative agreement between the Computing Research Association (CRA) — representing over 220 North American academic departments, industrial research labs, and professional societies with computing research interests — and the U.S. National Science Foundation, the CCC provides:

a voice for the national computing research community. The CCC facilitates the development of a bold, multi-themed vision for computing research, and communicates that vision to a wide range of major stakeholders.

The CCC is governed by an 18-member Council. Susan Graham (U.C. Berkeley) serves as Chair. Ann Drobnis serves as staff Director.
The CCC is housed at CRA's headquarters in Washington, D.C., United States.


== Activities ==
The CCC is part of the national computing research community — and it works with the community to envision and enable the pursuit of high-impact research directions. Among its activities:
Numerous talks/articles, a blog, and a Computing Research Highlight of the Week — to inspire and engage the community.
Community visioning activities (more than a dozen thus far). These bring together members of the community to coalesce around research visions, articulate these visions in compelling ways, and ideally translate these visions into funded programs under the guidance of the CCC. These are initiated by the community, the CCC Council, or funding agencies. Recently, officials at the White House Office of Science and Technology Policy (OSTP) and the NSF credited a roadmap that resulted from a CCC visioning activity on robotics for the creation of the National Robotics Initiative.
CCC-sponsored Research Frontiers sessions at major conferences that explore out-of-the-box research ideas in the field.
""URO"" (Undergraduate Research Opportunities) Zone — a website designed to inspire undergraduates to pursue research.
White papers describing strategic areas of investment in computing research. A senior official noted these ""have had a clear influence on Administration budget... and already sparked collaborations between government, industry, and academia."" 
A daylong symposium at the Library of Congress, Computing Research that Changed the World, describing the accomplishments and potential of computing research. Valuable collateral materials (slides, short illustrated papers, videos) were created and disseminated. There have been more than 85,000 YouTube views of the videos.
The Computing Innovation Fellows (CIFellows) Project (2009 & 2010) — a postdoctoral program with many unique, beneficial characteristics. Over 1,200 registered as prospective mentors during the first year, and over 700 applied. In mid-year surveys, every one of the 107 CIFellows reported “highly successful” or “moderately successful” experiences. Over 90% of the CIFellows participated in a December 2010 CIFellows Research Meeting & Career Mentoring Workshop.
A compendium of Landmark Contributions by Students in Computer Science, emphasizing the role of education in creating high-impact research breakthroughs. Regina Dugan, the new DARPA Director, highlighted a number of these in early talks.
A workshop series that yielded a Network Science and Engineering (NetSE) Research Agenda — contributing to re-orienting the NSF-funded GENI Project. CCC gave voice to the community, arguing that the field did not need the GENI instrument as initially envisioned.
Major national multi-agency workshops on Discovery and Innovation in Health IT and the Role of Information Sciences and Engineering on Sustainability. These differ from community visioning activities in that CCC takes end-to-end responsibility.
A community-wide discussion of the role of postdoctoral programs in our field, currently underway — a discussion initiated by the CCC but carried out under the CRA banner in order to emphasize that the discussion is broader than the CIFellows Project.
The recent assessment by the President's Council of Advisors on Science and Technology (PCAST) of the 14-agency, $4.3 billion Federal Networking and Information Technology Research and Development (NITRD) program, Designing a Digital Future: Federally Funded Research and Development in Networking and Information Technology — in effect a blueprint for the direction of the computing research field. Five CCC Council members were appointed to the 14-person Working Group, and the final report drew heavily upon their understanding of the computing research landscape developed through their CCC involvement, as well as upon various CCC outputs.
A Leadership in Science Policy Institute (LiSPI) that educates computing researchers on how science policy is formulated.


== History ==
In March 2006, NSF issued a solicitation indicating its desire to establish a Computing Community Consortium. In October of that year, CRA responded to the solicitation, submitting a proposal that was backed by explicit letters of support from 132 Ph.D.-granting academic programs, 16 leading corporations, 7 major national laboratories and research centers, and five professional societies in the field. Pursuant to positive external peer review, the CCC was established in late 2006 through a cooperative agreement between NSF and CRA.
An interim CCC Council was appointed by the proposal team in December 2006. Following an open recruitment process, Ed Lazowska (University of Washington) was selected as Chair of the CCC Council in March 2007. The membership of the inaugural CCC Council was selected through a transparent process and announced in June 2007. The first public activity of the CCC was a set of five plenary talks at the Federated Computing Research Conference (FCRC 2007) that month.
Early on, CCC Council member Susan Graham (U.C. Berkeley) assumed the role of Vice Chair. Andrew Bernat, CRA’s Executive Director, served the CCC in the role of staff Director until Erwin Gianchandani was recruited as full-time staff Director in April 2010.
Today, the CCC Council has 18 members on 3-year staggered terms, representing the diverse nature of the computing research field, plus two officers (Lazowska, Graham) and two ex-officio members (Bernat, Gianchandani).


== Current Structure ==
The CCC operates as a standing committee of CRA under CRA's bylaws: its membership only slightly overlaps the CRA's Board of Directors; it has significant autonomy; and it has a great deal of synergistic mutual benefit with CRA.
The CCC Council meets three times every calendar year, including at least one meeting in Washington, D.C., and has biweekly conference calls between these meetings. The CCC leadership has biweekly conference calls with the leadership of NSF's Directorate for Computer and Information Science and Engineering (CISE).
The CCC is broadly inclusive, and any computing researcher who wishes to become involved is encouraged to do so. For example, each fall, the CCC issues a call for nominations for Council members effective the following January.


== References =="
134,Radhika Nagpal,43516734,9826,"Radhika Nagpal is an American computer scientist and researcher in the fields of self-organising computer systems, biologically-inspired robotics, and biological multi-agent systems. She is the Fred Kavli Professor of Computer Science at Harvard University and the Harvard School of Engineering and Applied Sciences. She is also a Core Faculty Member of the Harvard Wyss Institute for Biologically Inspired Engineering. In 2017, Nagpal co-founded a robotics company under the name of Root Robotics. This educational company works to create many different opportunities for those unable to code to learn how.


== Education and Academic Career ==
Nagpal received an S.B. and S.M. in Electrical Engineering and Computer Science from the Massachusetts Institute of Technology in 1994, and a Ph.D. in Electrical Engineering and Computer Science from MIT in 2001. Her dissertation, ""Programmable Self-Assembly using Biologically-Inspired Local Interactions and Origami Mathematics"", was supervised by Gerald Sussman and Harold Abelson. In it, she presented a language for instructing a sheet of identically-programmed agents to self-assemble into a desired shape making use only of local interactions, and in a manner robust to irregularities, communication failure, and agent malfunction.
From 2001 to 2003, she served as a Postdoctoral Lecturer at the MIT Computer Science and Artificial Intelligence Laboratory, as a member of the Amorphous Computing Group. From 2004 to 2009, she served as an Assistant Professor of Computer Science at the Harvard School of Engineering and Applied Sciences; from 2009 to 2012, she served as the Thomas D. Cabot Associate Professor of Computer Science at Harvard SEAS. Since 2012, she has served as the Fred Kavli Professor of Computer Science at Harvard SEAS, where she heads the Self-Organizing Systems Research Group.


== Academic Research ==
Her research group focuses on biologically-inspired multi-agent systems: collective algorithms, programming paradigms, modular and swarm robotics, and on biological multi-agent systems: models of multicellular morphogenesis, collective insect behavior. This work lies at the intersection of computer science (AI/robotics) and biology. It studies bio-inspired algorithms, programming paradigms, and hardware designs for swarm/modular robotic systems and smart materials, drawing inspiration mainly from social insects and multicellular biology. It also investigates models of self-organization in biology, specifically how cells cooperate during the development of multicellular organisms.


=== Programming paradigms for robust collective behavior ===
Her primary research interest is developing programming paradigms for robust collective behavior, inspired by biology. Ultimately, the goal is to create a framework for the design and analysis of self-organising multi-agent systems. Her group's approach is to formalize these strategies as algorithms, analysis, theoretical models, and programming languages. They are especially interested in global-to-local compilation, the ability to specify user goals at the high level and automatically derive provable strategies at the agent level.


=== Understanding robust collective behavior in biological systems ===
Another of her research interests is in understanding robust collective behavior in biological systems. Building artificial systems can give us insights into how complex global properties can arise from identically-programmed parts --- for example, how cells can form scale-independent patterns, how large morphological variations can arise from small genetic changes, and how complex cascades of decisions can tolerate variations in timing. She is interested in mathematical and computational models of multi-cellular behavior, that capture hypotheses of cell behavior and cell-cell interactions as multi-agent systems, and can be used to provide insights into systems level behavior that should emerge. Her group works in close collaboration with biologists, and currently studies growth and pattern formation in the fruit fly wing.


== Academic Positions ==
Nagpal has an impressive record of positions as a researcher and an academic, including:
Bell Laboratories, Murray Hill, NJ from 1994-1995 as a technical staff member
MIT Computer Science and Artificial Intelligence Laboratory, Amorphous Computing Group from 2001-2003 as a postdoctoral lecturer
Harvard Medical School from 2003-2004 as a research fellow
Harvard School of Engineering and Applied Sciences from 2004-2009 as an assistant professor of Computer Science
Harvard Medical School, Department of Systems Biology since 2004 as an affiliated faculty member
Harvard Wyss Institute for Biological-inspired Engineering since 2008 as a Core Faculty Member
Harvard School of Engineering and Applied Sciences from 2009 to 2012 as an associate professor of computer science
Harvard School of Engineering and Applied Sciences since 2012 as the Fred Kavli Professor of Computer Science


== Awards and honors ==
National Talent Search Scholarship Award, India (1987)
AT&T Bell Labs GRPW Fellowship (1995-2001)
Microsoft New Faculty Fellowship (2005)
NSF Career Award (2007)
Anita Borg Early Career Award (2010)
Radcliffe Fellowship (2012)
named one of Nature's 10 ""people who mattered"" of 2014.
McDonald Mentoring Award (2015)
During her time as Radcliffe Fellow, she worked with experimental biologists to develop a better understanding of collective intelligence in social insects through the application of computer science.


== References =="
135,Raft (computer science),40226710,9758,"Raft is a consensus algorithm designed as an alternative to Paxos. It was meant to be more understandable than Paxos by means of separation of logic, but it is also formally proven safe and offers some additional features. Raft offers a generic way to distribute a state machine across a cluster of computing systems, ensuring that each node in the cluster agrees upon the same series of state transitions. It has a number of open-source reference implementations, with full-spec implementations in Go, C++, Java, and Scala.


== Basics ==
Raft achieves consensus via an elected leader. A server in a raft cluster is either a leader or a follower, and can be a candidate in the precise case of an election (leader unavailable). The leader is responsible for log replication to the followers. It regularly informs the followers of its existence by sending a heartbeat message. Each follower has a timeout (typically between 150 and 300 ms) in which it expects the heartbeat from the leader. The timeout is reset on receiving the heartbeat. If no heartbeat is received the follower changes its status to candidate and starts a leader election.


=== Approach of the consensus problem in Raft ===
Raft implements consensus by a leader approach. The cluster has one and only one elected leader which is fully responsible for managing log replication on the other servers of the cluster. It means that the leader can decide on new entries placement and establishment of data flow between it and the other servers without consulting other servers. A leader leads until it fails or disconnects, in which case a new leader is elected.
The consensus problem is decomposed in Raft into two relatively independent subproblems listed down below.


==== Leader Election ====
When the existing leader fails or when you start your algorithm, a new leader needs to be elected.
In this case, a new term starts in the cluster. A term is an arbitrary period of time on the server during which a new leader needs to be elected. Each term starts with a leader election. If the election is completed successfully (i.e. a single leader is elected) the term keeps going with normal operations orchestrated by the new leader. If the election is a failure, a new term starts, with a new election.
A leader election is started by a candidate server. A server becomes a candidate if it receives no communication by the leader over a period called the election timeout, so it assumes there is no acting leader anymore. It starts the election by increasing the term counter, voting for itself as new leader, and sending a message to all other servers requesting their vote. A server will vote only once per term, on a first-come-first-served basis. If a candidate receives a message from another server with a term number at least as large as the candidate's current term, then the candidate's election is defeated and the candidate changes into a follower and recognizes the leader as legitimate. If a candidate receives a majority of votes, then it becomes the new leader. If neither happens, e.g., because of a split vote, then a new term starts, and a new election begins.
Raft uses randomized election timeout to ensure that split votes problem are resolved quickly. This should reduce the chance of a split vote because servers won't become candidates at the same time: a single server will timeout, win the election, then become leader and sends heartbeat messages to other servers before any of the followers can become candidates.


==== Log Replication ====
The leader is responsible for the log replication. It accepts client requests. Each client request consists of a command to be executed by the replicated state machines in the cluster. After being appended to the leader's log as a new entry, each of the requests is forwarded to the followers as AppendEntries messages. In case of unavailability of the followers, the leader retries AppendEntries messages indefinitely, until the log entry is eventually stored by all of the followers.
Once the leader receives confirmation from the majority of its followers that the entry has been replicated, the leader applies the entry to its local state machine, and the request is considered committed. This event also commits all previous entries in the leader's log. Once a follower learns that a log entry is committed, it applies the entry to its local state machine. It provides consistency for the logs between all the servers through the cluster, ensuring that the safety rule of Log Matching is respected.
In the case of leader crash, the logs can be left inconsistent, with some logs from the old leader not being fully replicated through the cluster. The new leader will then handle inconsistency by forcing the followers to duplicate its own log. To do so, for each of its followers, the leader will compare its log with the log from the follower, find the last entry where they agree, then delete all the entries coming after this critical entry in the follower log and replace it with its own log entries. This mechanism will restore log consistency in a cluster subject to failures.


=== Safety ===


==== Safety rules in Raft ====
Raft guarantees each of these safety properties :
Election safety: at most one leader can be elected in a given term.
Leader Append-Only: a leader can only append new entries to its logs (it can neither overwrite nor delete entries).
Log Matching: if two logs contain an entry with the same index and term, then the logs are identical in all entries up through the given index.
Leader Completeness: if a log entry is committed in a given term then it will be present in the logs of the leaders since this term
State Machine Safety: if a server has applied a particular log entry to its state machine, then no other server may apply a different command for the same log.
The four first rules are guaranteed by the details of the algorithm described in the previous section. The State Machine Safety is guaranteed by a restriction on the election process.


==== State Machine Safety ====
This rule is ensured by a simple restriction: a candidate can't win an election unless its log contains all committed entries. In order to be elected, a candidate has to contact a majority of the cluster, and given the rules for logs to be committed, it means that every committed entry is going to be present on at least one of the servers the candidates contact.
Raft determines which of two logs (carried by two distinct servers) is more up-to-date by comparing the index term of the last entries in the logs. If the logs have a last entry with different terms, then the log with the later term is more up-to-date. If the logs end with the same term, then whichever log is longer is more up-to-date.
In Raft, the request from a candidate to a voter includes information about the candidate's log. If its own log is more up-to-date than the candidate's log, the voter denies its vote to the candidate. This implementation ensures the State Machine Safety rule.


==== Follower crashes ====
If a follower crashes, AppendEntries and vote requests sent by other servers will fail. Such failures are handled by the servers trying indefinitely to reach the downed follower. If the follower restarts, the pending requests will complete. If the request has already been taken into account before the failure, the restarted follower will just ignore it.


==== Timing and availability ====
Timing is critical in Raft to elect and maintain a steady leader over time, in order to have a perfect availability of your cluster. Stability is ensured by respecting the timing requirement of the algorithm :

broadcastTime << electionTimeout << MTBF

broadcastTime is the average time it takes a server to send request to every server in the cluster and receive responses. It is relative to the infrastructure you are using.
MTBF is the average time between failures for a server. It is also relative to your infrastructure.
electionTimeout is the same as described in the Leader Election section. It is something you must choose.
Typical number for these values can be 0.5ms to 20ms for broadcastTime, which implies that you set the electionTimeout somewhere between 10ms and 500ms. It can take several weeks or months between single server failures, which means the values are all right for a stable cluster to work.


== References ==


== External links ==
Official website
List of implementations"
136,Computational astrophysics,34240222,9747,"Computational astrophysics refers to the methods and computing tools developed and used in astrophysics research. Like computational chemistry or computational physics, it is both a specific branch of theoretical astrophysics and an interdisciplinary field relying on computer science, mathematics, and wider physics. Computational astrophysics is most often studied through an applied mathematics or astrophysics programme at PhD level.
Well-established areas of astrophysics employing computational methods include magnetohydrodynamics, astrophysical radiative transfer, stellar and galactic dynamics, and astrophysical fluid dynamics. A recently developed field with interesting results is numerical relativity.


== Research ==
Many astrophysicists use computers in their work, and a growing number of astrophysics departments now have research groups specially devoted to computational astrophysics. Important research initiatives include the US Department of Energy (DoE) SciDAC collaboration for astrophysics and the now defunct European AstroSim collaboration. A notable active project is the international Virgo Consortium, which focuses on cosmology.
In August 2015 during the general assembly of the International Astronomical Union a new commission C.B1 on Computational Astrophysics was inaugurated, therewith recognizing the importance of astronomical discovery by computing.
Important techniques of computational astrophysics include particle-in-cell (PIC) and the closely related particle-mesh (PM), N-body simulations, Monte Carlo methods, as well as grid-free (with smoothed particle hydrodynamics (SPH) being an important example) and grid-based methods for fluids. In addition, methods from numerical analysis for solving ODEs and PDEs are also used.
Simulation of astrophysical flows is of particular importance as many objects and processes of astronomical interest such as stars and nebulae involve gases. Fluid computer models are often coupled with radiative transfer, (Newtonian) gravity, nuclear physics and (general) relativity to study highly energetic phenomena such as supernovae, relativistic jets, active galaxies and gamma-ray bursts and are also used to model stellar structure, planetary formation, evolution of stars and of galaxies, and exotic objects such as neutron stars, pulsars, magnetars and black holes. Computer simulations are often the only means to study stellar collisions, galaxy mergers, as well as galactic and black hole interactions.
In recent years the field has made increasing use of parallel and high performance computers.


== Tools ==
Computational astrophysics as a field makes extensive use of software and hardware technologies. These systems are often highly specialized and made by dedicated professionals, and so generally find limited popularity in the wider (computational) physics community.


=== Hardware ===
Like other similar fields, computational astrophysics makes extensive use of supercomputers and computer clusters . Even on the scale of a normal desktop it is possible to accelerate the hardware. Perhaps the most notable such computer architecture built specially for astrophysics is the GRAPE (gravity pipe) in Japan.
As of 2010, the biggest N-body simulations, such as DEGIMA, do general-purpose computing on graphics processing units.


=== Software ===
Many codes and software packages, exist along with various researchers and consortia maintaining them. Most codes tend to be n-body packages or fluid solvers of some sort. Examples of n-body codes include ChaNGa, MODEST, nbodylab.org and Starlab .
For hydrodynamics there is usually a coupling between codes, as the motion of the fluids usually has some other effect (such as gravity, or radiation) in astrophysical situations. For example, for SPH/N-body there is GADGET and SWIFT; for grid-based/N-body RAMSES, ENZO, FLASH, and ART.
AMUSE [3], takes a different approach (called Noah's Arc ) than the other packages by providing an interface structure to a large number of publicly available astronomical codes for addressing stellar dynamics, stellar evolution, hydrodynamics and radiative transport.


== See also ==
Millennium Simulation, Eris, and Bolshoi Cosmological Simulation are astrophysical supercomputer simulations
Plasma modeling
Computational physics
Theoretical astronomy and theoretical astrophysics
Center for Computational Relativity and Gravitation
University of California High-Performance AstroComputing Center


== References ==


== Further reading ==
Beginner/intermediate level:
Astrophysics with a PC: An Introduction to Computational Astrophysics, Paul Hellings. Willmann-Bell; 1st English ed edition.
Practical Astronomy with your Calculator, Peter Duffett-Smith. Cambridge University Press; 3rd edition 1988.
Advanced/graduate level:
Numerical Methods in Astrophysics: An Introduction (Series in Astronomy and Astrophysics): Peter Bodenheimer, Gregory P. Laughlin, Michal Rozyczka, Harold. W Yorke. Taylor & Francis, 2006.
Open cluster membership probability based on K-means clustering algorithm, Mohamed Abd El Aziz & I. M. Selim & A. Essam, Exp Astron., 2016
Automatic Detection of Galaxy Type From Datasets of Galaxies Image Based on Image Retrieval Approach, Mohamed Abd El Aziz, I. M. Selim & Shengwu Xiong Scientific Reports 7, 4463, 2017
Journals (Open Access):
Living Reviews in Computational Astrophysics
Computational Astrophysics and Cosmology"
137,Software studies,17017917,9715,"Software studies is an emerging interdisciplinary research field, which studies software systems and their social and cultural effects.


== Overview ==
The implementation and use of software has been studied in recent fields such as cyberculture, Internet studies, new media studies, and digital culture, yet prior to software studies, software was rarely ever addressed as a distinct object of study.
Software studies is an interdisciplinary field. To study software as an artifact, it draws upon methods and theory from the digital humanities and from computational perspectives on software. Methodologically, software studies usually differs from the approaches of computer science and software engineering, which concern themselves primarily with software in information theory and in practical application; however, these fields all share an emphasis on computer literacy, particularly in the areas of programming and source code. This emphasis on analyzing software sources and processes (rather than interfaces) often distinguishes software studies from new media studies, which is usually restricted to discussions of interfaces and observable effects.


== History ==
The conceptual origins of software studies include Marshall McLuhan's focus on the role of media in themselves, rather than the content of media platforms, in shaping culture. Early references to the study of software as a cultural practice appear in Friedrich Kittler's essay, ""Es gibt keine Software,"" Lev Manovich's Language of New Media, and Matthew Fuller's Behind the Blip: Essays on the culture of software. Much of the impetus for the development of software studies has come from videogame studies, particularly platform studies, the study of videogames and other software artifacts in their hardware and software contexts. New media art, software art, motion graphics, and computer-aided design are also significant software-based cultural practices, as is the creation of new protocols and platforms.
The first conference events in the emerging field were Software Studies Workshop 2006 and SoftWhere 2008.
In 2008, MIT Press launched a Software Studies book series with an edited volume of essays (Matthew Fuller's ""Software Studies: a Lexicon""), and the first academic program was launched, (Lev Manovich, Benjamin H. Bratton and Noah Wardrip-Fruin's ""Software Studies Initiative"" at U. California San Diego).
In 2011, a number of mainly British researchers established Computational Culture, an open-access peer-reviewed journal. The journal provides a platform for ""inter-disciplinary enquiry into the nature of the culture of computational objects, practices, processes and structures.""


== Related fields ==
Software studies is closely related to a number of other emerging fields in the digital humanities that explore functional components of technology from a social and cultural perspective. Software studies' focus is at the level of the entire program, specifically the relationship between interface and code. Notably related are critical code studies, which is more closely attuned to the code rather than the program, and platform studies, which investigates the relationships between hardware and software.


== See also ==
Critical code studies
Digital Humanities
Cultural studies
New media
Computer science
Software engineering
Digital sociology


== References ==


== Bibliography ==
Bassett, C. (2007) The Arc and the Machine: Narrative and New Media. Manchester:Manchester University Press.
Black, M. J, (2002) The Art of Code. PhD dissertation, University ofPennsylvania.
Berry, D. M. (2011) The Philosophy of Software: Code and Mediation in the Digital Age, Basingstoke: Palgrave Macmillan.
Berry, D. M. (2008) Copy, Rip, Burn: The Politics of Copyleft and Open Source, London: Pluto Press.
Chopra, S. and Dexter, S. (2008) Decoding Liberation: The Promise of Free and Open Source Software. Oxford: Routledge.
Chun, W. H. K. (2008) ‘On “Sourcery,” or Code as Fetish’, Configurations, 16:299–324.
Fuller, M. (2003) Behind the Blip: Essays on the Culture of Software. London: Autonomedia.
Fuller, M. (2006) Software Studies Workshop, retrieved 13/04/2010
Fuller, M. (2008) Software Studies: A Lexicon. London: MIT Press.
Hayles, N. K. (2004) ‘Print Is Flat, Code Is Deep: The Importance of Media-Specific Analysis’, Poetics Today, 25(1): 67–90.
Heim, M. (1987) Electric Language: A Philosophical Discussion of Word Processing. London: Yale University Press.
Kirschenbaum, M. (2004) ‘Extreme Inscription: Towards a Grammatology of the Hard Drive’, TEXT Technology, No. 2, pp. 91–125.
Kittler, F. (1997). Literature, Media, Information Systems, Johnston, J. (ed.). Amsterdam: OPA.
Kittler, F. (1999) Gramophone, Film, Typewriter. Stanford: Stanford University Press.
Mackenzie, A. (2003) The problem of computer code: Leviathan or common power, retrieved 13/03/2010 from http://www.lancs.ac.uk/staff/mackenza/papers/code-leviathan.pdf
Mackenzie, A. (2006) Cutting Code: Software and Sociality, Oxford: Peter Lang.
Manovich, L. (2001) The Language of New Media. London: MIT Press.
Manovich, L. (2008) Software takes Command, first draft released under CC license, retrieved 03/05/2010
Manovich, L. (2013) Software takes Command, London and New York: Bloomsbury Academic.
Manovich, L. and Douglas, J. (2009) Visualizing Temporal Patterns In Visual Media: Computer Graphics as a Research Method, retrieved 10/10/09
Marino, M. C. (2006) Critical Code Studies, Electronic Book Review, accessed 16 Sept 2011
Montfort, N. and Bogost, I. (2009) Racing the Beam: The Atari Video Computer System, London: MIT Press.
Wardrip-Fruin, N. (2011) Expressive Processing. London: MIT Press.


== External links ==
Software studies bibliography at Monoskop.org"
138,Computational genomics,2571276,9700,"Computational genomics (often referred to as Computational Genetics) refers to the use of computational and statistical analysis to decipher biology from genome sequences and related data, including both DNA and RNA sequence as well as other ""post-genomic"" data (i.e., experimental data obtained with technologies that require the genome sequence, such as genomic DNA microarrays). These, in combination with computational and statistical approaches to understanding the function of the genes and statistical association analysis, this field is also often referred to as Computational and Statistical Genetics/genomics. As such, computational genomics may be regarded as a subset of bioinformatics and computational biology, but with a focus on using whole genomes (rather than individual genes) to understand the principles of how the DNA of a species controls its biology at the molecular level and beyond. With the current abundance of massive biological datasets, computational studies have become one of the most important means to biological discovery.


== History ==
The roots of computational genomics are shared with those of bioinformatics. During the 1960s, Margaret Dayhoff and others at the National Biomedical Research Foundation assembled databases of homologous protein sequences for evolutionary study. Their research developed a phylogenetic tree that determined the evolutionary changes that were required for a particular protein to change into another protein based on the underlying amino acid sequences. This led them to create a scoring matrix that assessed the likelihood of one protein being related to another.
Beginning in the 1980s, databases of genome sequences began to be recorded, but this presented new challenges in the form of searching and comparing the databases of gene information. Unlike text-searching algorithms that are used on websites such as Google or Wikipedia, searching for sections of genetic similarity requires one to find strings that are not simply identical, but similar. This led to the development of the Needleman-Wunsch algorithm, which is a dynamic programming algorithm for comparing sets of amino acid sequences with each other by using scoring matrices derived from the earlier research by Dayhoff. Later, the BLAST algorithm was developed for performing fast, optimized searches of gene sequence databases. BLAST and its derivatives are probably the most widely used algorithms for this purpose.
The emergence of the phrase ""computational genomics"" coincides with the availability of complete sequenced genomes in the mid-to-late 1990s. The first meeting of the Annual Conference on Computational Genomics was organized by scientists from The Institute for Genomic Research (TIGR) in 1998, providing a forum for this speciality and effectively distinguishing this area of science from the more general fields of Genomics or Computational Biology. The first use of this term in scientific literature, according to MEDLINE abstracts, was just one year earlier in Nucleic Acids Research. The final Computational Genomics conference was held in 2006, featuring a keynote talk by Nobel Laureate Barry Marshall, co-discoverer of the link between Helicobacter pylori and stomach ulcers. As of 2014, the leading conferences in the field include Intelligent Systems for Molecular Biology (ISMB) and Research in Computational Molecular Biology (RECOMB).
The development of computer-assisted mathematics (using products such as Mathematica or Matlab) has helped engineers, mathematicians and computer scientists to start operating in this domain, and a public collection of case studies and demonstrations is growing, ranging from whole genome comparisons to gene expression analysis. This has increased the introduction of different ideas, including concepts from systems and control, information theory, strings analysis and data mining. It is anticipated that computational approaches will become and remain a standard topic for research and teaching, while students fluent in both topics start being formed in the multiple courses created in the past few years.


== Contributions of computational genomics research to biology ==
Contributions of computational genomics research to biology include:
discovering subtle patterns in genomic sequences 
proposing cellular signalling networks
proposing mechanisms of genome evolution
predict precise locations of all human genes using comparative genomics techniques with several mammalian and vertebrate species
predict conserved genomic regions that are related to early embryonic development
discover potential links between repeated sequence motifs and tissue-specific gene expression
measure regions of genomes that have undergone unusually rapid evolution


== Latest Development (from 2012) ==


=== First Computer Model of an Organism ===
Researchers at Stanford University created the first software simulation of an entire organism. They mapped the 525 genes of the bacteria Mycoplasma genitalium, the smallest free-living organism. With data from more than 900 scientific papers reported on the bacterium, researchers developed the software model using the object-oriented programming approach. A series of modules mimic the various functions of the cell, and then integrated it into a whole simulated organism. The simulation runs on a single CPU, recreating the complete life span of the cell at the molecular level, reproducing the interactions of molecules in cell processes including metabolism and cell division.
The ‘silicon cell’ will act as computerized laboratories that could perform experiments which are difficult to do on an actual organism, or could carry out procedures much faster. The applications will include faster screening of new compounds, understanding of basic cellular principles and behavior.


== See also ==
Bioinformatics
Biowiki
Computational biology
Genomics
Microarray
BLAST
Computational epigenetics


== References ==


== External links ==
Harvard Extension School Biophysics 101, Genomics and Computational Biology, http://www.courses.fas.harvard.edu/~bphys101/info/syllabus.html
University of Bristol course in Computational Genomics, http://www.computational-genomics.net/"
139,Polygon partition,48191322,9677,"A partition of a polygon is a set of primitive units (e.g. squares), which do not overlap and whose union equals the polygon. A polygon partition problem is a problem of finding a partition which is minimal in some sense, for example a partition with a smallest number of units or with units of smallest total side-length.
Polygon partitioning is an important class of problems in computational geometry. There are many different polygon partition problems, depending on the type of polygon being partitioned and on the types of units allowed in the partition.
The term polygon decomposition is often used as a general term that includes both covering and partitioning.


== Applications ==
Polygon decomposition is applied in several areas:
Pattern recognition techniques extract information from an object in order to describe, identify or classify it. An established strategy for recognising a general polygonal object is to decompose it into simpler components, then identify the components and their interrelationships and use this information to determine the shape of the object.
In VLSI artwork data processing, layouts are represented as polygons, and one approach to preparation for electron-beam lithography is to decompose these polygon regions into fundamental figures. Polygon decomposition is also used in the process of dividing the routing region into channels.
In computational geometry, algorithms for problems on general polygons are often more complex than those for restricted types of polygons such as convex or star-shaped. The point inclusion problem is one example. A strategy for solving some of these types of problems on general polygons is to decompose the polygon into simple component parts, solve the problem on each component using a specialized algorithm, and then combine the partial solutions.
Other applications include data compression, database systems, image processing and computer graphics.


== Partitioning a polygon to triangles ==

The most well-studied polygon partition problem is partitioning to a smallest number of triangles, also called triangulation. For a hole-free polygon with 
  
    
      
        n
      
    
    {\displaystyle n}
   vertices, a triangulation can be calculated in time 
  
    
      
        Θ
        (
        n
        )
      
    
    {\displaystyle \Theta (n)}
  . For a polygon with holes, there is a lower bound of 
  
    
      
        Ω
        (
        n
        log
        ⁡
        n
        )
      
    
    {\displaystyle \Omega (n\log n)}
  .
A related problem is partitioning to triangles with a minimal total edge length, also called minimum-weight triangulation.


== Partitioning a polygon to pseudo-triangles ==

The same two variants of the problem were studied for the case in which the pieces should be pseudotriangles – polygons that like triangles have exactly three convex vertices. The variants are: partitioning to a smallest number of pseodutriangles, and partitioning to pseudotriangles with a minimal total edge length.


== Partitioning a rectilinear polygon to rectangles ==
A special sub-family of polygon partition problems arises when the large polygon is a rectilinear polygon (also called: orthogonal polygon). In this case, the most important component shape to consider is the rectangle.
Rectangular partitions have many applications. In VLSI design, it is necessary to decompose masks into the simpler shapes available in lithographic pattern generators, and similar mask decomposition problems also arise in DNA microarray design. Rectangular partitions can simplify convolution operations in image processing and can be used to compress bitmap images. Closely related matrix decomposition problems have been applied to radiation therapy planning, and rectangular partitions have also been used to design robot self-assembly sequences.
Several polynomial-time algorithms for this problem are known; see  and  for a review.
The problem of partitioning a rectilinear polygon to a smallest number of squares (in contrast to arbitrary rectangles) is NP-hard.


== Partition a polygon to trapezoids ==
in VLSI artwork processing systems, it is often required to partition a polygonal region into the minimum number of trapezoids, with two horizontal sides. A triangle with a horizontal side is considered to be a trapezoid with two horizontal sides one of which is degenerate. For a hole-free polygon with 
  
    
      
        n
      
    
    {\displaystyle n}
   sides, a smallest such partition can be found in time 
  
    
      
        O
        (
        
          n
          
            3
          
        
        )
      
    
    {\displaystyle O(n^{3})}
  .
If the number of trapezoids need not be minimal a trapezoidation can be found in time 
  
    
      
        O
        (
        n
        )
      
    
    {\displaystyle O(n)}
  , as a by-product of a polygon triangulation algorithm.
If the polygon does contain holes, the problem is NP-complete, but a 3-approximation can be found in time 
  
    
      
        O
        (
        n
        log
        ⁡
        n
        )
      
    
    {\displaystyle O(n\log n)}
  .


== Partition a polygon to convex quadrilaterals ==
A quadrilateralization or a quadrangulation is a partition into quadrilaterals.
A recurring characteristic of quadrangulation problems is whether they Steiner point are allowed, i.e., whether the algorithm is allowed to add points which are not vertices of the polygon. Allowing Steiner points may enable smaller divisios, but then it is much more difficult to guarantee that the divisions found by an algorithms have minimum size.
There are linear-time algorithms for quadrangulations of hole-free polygons with Steiner points, but they are not guaranteed to find a smallest partition.


== Partition a polygon to m-gons ==
A generalization of previous problems is the problem of partitioning to polygons that have exactly m sides, or at most m sides. Here the goal is to minimize the total edge length. This problem can be solved in time polynomial in n and m.


== More general component shapes ==
More general shapes of pieces have been studied, including: arbitrary convex polygons, spiral shapes, star polygons and monotone polygons. See  for a survey.


== See also ==
Polygon covering - a related problem in which the pieces are allowed to overlap.
Packing problem - a related problem in which the pieces have to fit within the entire large object but do not have to cover it entirely.
Euclidean tilings by convex regular polygons - a problem of partitioning the entire plane to simple polygons such as rectangles.
Squaring the square - a problem of partitioning an integral square using only other integral squares.
Tiling puzzle - a puzzle of packing several given pieces into a given larger polygon.


== References =="
140,Computational topology,1248833,9623,"Algorithmic topology, or computational topology, is a subfield of topology with an overlap with areas of computer science, in particular, computational geometry and computational complexity theory.
A primary concern of algorithmic topology, as its name suggests, is to develop efficient algorithms for solving problems that arise naturally in fields such as computational geometry, graphics, robotics, structural biology and chemistry, using methods from computable topology.


== Major algorithms by subject area ==


=== Algorithmic 3-manifold theory ===
A large family of algorithms concerning 3-manifolds revolve around normal surface theory, which is a phrase that encompasses several techniques to turn problems in 3-manifold theory into integer linear programming problems.
Rubinstein and Thompson's 3-sphere recognition algorithm. This is an algorithm that takes as input a triangulated 3-manifold and determines whether or not the manifold is homeomorphic to the 3-sphere. It has exponential run-time in the number of tetrahedral simplexes in the initial 3-manifold, and also an exponential memory profile. Moreover, it is implemented in the software package Regina. Saul Schleimer went on to show the problem lies in the complexity class NP. Furthermore, Raphael Zentner showed that the problem lies in the complexity class coNP, provided that the generalized Riemann hypothesis holds. He uses instanton gauge theory, the geometrization theorem of 3-manifolds, and subsequent work of Greg Kuperberg  on the complexity of knottedness detection.
The connect-sum decomposition of 3-manifolds is also implemented in Regina, has exponential run-time and is based on a similar algorithm to the 3-sphere recognition algorithm.
Determining that the Seifert-Weber 3-manifold contains no incompressible surface has been algorithmically implemented by Burton, Rubinstein and Tillmann  and based on normal surface theory.
The Manning algorithm is an algorithm to find hyperbolic structures on 3-manifolds whose fundamental group have a solution to the word problem.
At present the JSJ decomposition has not been implemented algorithmically in computer software. Neither has the compression-body decomposition. There are some very popular and successful heuristics, such as SnapPea which has much success computing approximate hyperbolic structures on triangulated 3-manifolds. It is known that the full classification of 3-manifolds can be done algorithmically.


==== Conversion algorithms ====
SnapPea implements an algorithm to convert a planar knot or link diagram into a cusped triangulation. This algorithm has a roughly linear run-time in the number of crossings in the diagram, and low memory profile. The algorithm is similar to the Wirthinger algorithm for constructing presentations of the fundamental group of link complements given by planar diagrams. Similarly, SnapPea can convert surgery presentations of 3-manifolds into triangulations of the presented 3-manifold.
D.Thurston and F.Costantino have a procedure to construct a triangulated 4-manifold from a triangulated 3-manifold. Similarly, it can be used to construct surgery presentations of triangulated 3-manifolds, although the procedure is not explicitly written as an algorithm in principle it should have polynomial run-time in the number of tetrahedra of the given 3-manifold triangulation.
S. Schleimer has an algorithm which produces a triangulated 3-manifold, given input a word (in Dehn twist generators) for the mapping class group of a surface. The 3-manifold is the one that uses the word as the attaching map for a Heegaard splitting of the 3-manifold. The algorithm is based on the concept of a layered triangulation.


=== Algorithmic knot theory ===
Determining whether or not a knot is trivial is known to be in the complexity class NP 
The problem of determining the genus of a knot is known to have complexity class PSPACE.
There are polynomial-time algorithms for the computation of the Alexander polynomial of a knot.


=== Computational homotopy ===
Computational methods for homotopy groups of spheres.
Computational methods for solving systems of polynomial equations.
Brown has an algorithm to compute the homotopy groups of spaces that are finite Postnikov complexes, although it is not widely considered implementable.


=== Computational homology ===
Computation of homology groups of cell complexes reduces to bringing the boundary matrices into Smith normal form. Although this is a completely solved problem algorithmically, there are various technical obstacles to efficient computation for large complexes. There are two central obstacles. Firstly, the basic Smith form algorithm has cubic complexity in the size of the matrix involved since it uses row and column operations which makes it unsuitable for large cell complexes. Secondly, the intermediate matrices which result from the application of the Smith form algorithm get filled-in even if one starts and ends with sparse matrices.
Efficient and probabilistic Smith normal form algorithms, as found in the LinBox library.
Simple homotopic reductions for pre-processing homology computations, as in the Perseus software package.
Algorithms to compute persistent homology of filtered complexes.


== See also ==
Computable topology (the study of the topological nature of computation)
Computational geometry
Digital topology
Topological data analysis
Spatial-temporal reasoning
Experimental mathematics
Geometric modeling


== References ==


== External links ==
CompuTop software archive
Workshop on Application of Topology in Science and Engineering
Computational Topology at Stanford University
Computational Homology Software (CHomP) at Rutgers University.
Computational Homology Software (RedHom) at Jagellonian University.
The Perseus software project for (persistent) homology.
The javaPlex Persistent Homology software at Stanford.
PHAT: persistent homology algorithms toolbox.


== Books ==
Tomasz Kaczynski; Konstantin Mischaikow; Marian Mrozek (2004). Computational Homology. Springer. ISBN 0-387-40853-3. 
Afra J. Zomorodian (2005). Topology for Computing. Cambridge. ISBN 0-521-83666-2. 
Computational Topology: An Introduction, Herbert Edelsbrunner, John L. Harer, AMS Bookstore, 2010, ISBN 978-0-8218-4925-5"
141,Computer science in sport,25852537,9600,"Computer science in sport is an interdisciplinary discipline that has its goal in combining the theoretical as well as practical aspects and methods of the areas of informatics and sport science. The main emphasis of the interdisciplinarity is placed on the application and use of computer-based but also mathematical techniques in sport science, aiming in this way at the support and advancement of theory and practice in sports. The reason why computer science has become an important partner for sport science is mainly connected with ""the fact that the use of data and media, the design of models, the analysis of systems etc. increasingly requires the support of suitable tools and concepts which are developed and available in computer science"".


== Historical background ==
Going back in history, computers in sports were used for the first time in the 1960s, when the main purpose was to accumulate sports information. Databases were created and expanded in order to launch documentation and dissemination of publications like articles or books that contain any kind of knowledge related to sports science. Until the mid-1970s also the first organization in this area called IASI (International Association for Sports Information) was formally established. Congresses and meetings were organized more often with the aim of standardization and rationalization of sports documentation. Since at that time this area was obviously less computer-oriented, specialists talk about sports information rather than sports informatics when mentioning the beginning of this field of science.
Based on the progress of computer science and the invention of more powerful computer hardware in the 1970s, also the real history of computer science in sport began. This was as well the first time when this term was officially used and the initiation of a very important evolution in sports science.
In the early stages of this area statistics on biomechanical data, like different kinds of forces or rates, played a major role. Scientists started to analyze sports games by collecting and looking at such values and features in order to interpret them. Later on, with the continuous improvement of computer hardware - in particular microprocessor speed – many new scientific and computing paradigms were introduced, which were also integrated in computer science in sport. Specific examples are modeling as well as simulation, but also pattern recognition, design, and (sports) data mining.
As another result of this development, the term 'computer science in sport' has been added in the encyclopedia of sports science in 2004.


== Areas of research ==
The importance and strong influence of computer science as an interdisciplinary partner for sport and sport science is mainly proven by the research activities in computer science in sport. The following IT concepts are thereby of particular interest:
Data acquisition and data processing
Databases and expert systems
Modelling (mathematical, IT based, biomechanical, physiological)
Simulation (interactive, animation etc.)
Presentation
Based on the fields from above, the main areas of research in computer science in sport include amongst others:
Training and coaching
Biomechanics
Sports equipment and technology
Computer-aided applications (software, hardware) in sports
Ubiquitous computing in sports
Multimedia and Internet
Documentation
Education


== Research communities ==
A clear demonstration for the evolution and propagation towards computer science in sport is also the fact that nowadays people do research in this area all over the world. Since the 1990s many new national and international organizations regarding the topic of computer science in sport were established. These associations are regularly organizing congresses and workshops with the aim of dissemination as well as exchange of scientific knowledge and information on all sort of topics regarding the interdisciplinary discipline.


=== Historical survey ===
As a first example, in Australia and New Zealand scientists have built up the MathSport group of ANZIAM (Australia and New Zealand Industrial and Applied Mathematics), which since 1992 organizes biennial meetings, initially under the name ""Mathematics and Computers in Sport Conferences"", and now ""MathSport"". Main topics are mathematical models and computer applications in sports, as well as coaching and teaching methods based on informatics.
The European community was also among the leading motors of the emergence of the field. Some workshops on this topic were successfully organized in Germany since the late 1980s. In 1997 the first international meeting on computer science in sport was held in Cologne. The main aim was to spread out and share applications, ideas and concepts of the use of computers in sports, which should also make a contribution to the creation of internationalization and thus to boost research work in this area.
Since then, such international symposia took place every two years all over Europe. As the first conferences were a raving success, it was decided to go even further and the foundation of an organization was the logical consequence. This step was accomplished in 2003, when the International Association of Computer Science in Sport (IACSS) was established during the 4th international symposium in Barcelona, when Prof. Jürgen Perl was also chosen as the first president. A few years earlier, the first international e-journal on this topic (International Journal of Computer Science in Sport) was released already. The internationalization is confirmed moreover by the fact that three conferences already took place outside of Europe - in Calgary (Canada) in 2007, Canberra (Australia) in 2009 and Shanghai (China) in 2011. During the symposium in Calgary additionally the president position changed - it has been assigned to Prof. Arnold Baca, who has been re-elected in 2009 and 2011. The following Symposia on Computer Science in Sport took place in Europe again, in Istanbul (Turkey) in 2013 and in Loughborough (UK) in 2015. In 2017 the 11th Symposium of Computer Science in Sport took place in Constance (Germany). During the conference in Istanbul Prof. Martin Lames was elected as president of the IACSS. He was re-elected in 2015 and 2017.


=== National organizations ===
In addition to the international associations from above, currently the following national associations on computer science in sport exist (if available, the web addresses are also given):
Austrian Association of Computer Science in Sport - http://www.sportinformatik.at
British Association of Computer Science in Sport and Exercise
Chinese Association of Computer Science in Sport
Croatian Association of Computer Science in Sport
Section Computer Science in Sport of the German Association of Sport Science - http://www.dvs-sportinformatik.de (in German)
Indian Federation of Computer Science in Sport - http://www.ifcss.in
Portuguese Association of Computer Science in Sport
Turkish Association of Computer Science in Sport


== References ==


== Further reading ==
Dabnichki P. & Baca, A. (2008). Computers in Sport, WIT Press. ISBN 978-1-84564-064-4
Baca, A. (2015). Computer Science in Sport - Research and practice, Routledge. ISBN 978-1-315-88178-2


== External links ==
MathSport - ANZIAM (Australia and New Zealand Industrial and Applied Mathematics)
ECSS (European College of Sport Science)
ISEA (International Sports Engineering Association)
IACSS (International Association of Computer Science in Sport)"
142,Central Computer and Telecommunications Agency,12986427,9589,"The Central Computer and Telecommunications Agency (CCTA) was a UK government agency providing computer and telecoms support to government departments.


== History ==


=== Formation ===
In 1957, the UK government formed the Technical Support Unit (TSU) of HM Treasury (HMT) to evaluate and advise on computers, initially based around engineers from the telecommunications service.
As this unit evolved, it morphed into the Central Computer and Telecommunications Agency (CCTA), which also had responsibilities as a central procurement body for government technological equipment.
CCTA's work during the 1970s, 1980s and 1990s was primarily to (a) develop central government IT professionalism, (b) create a body of knowledge and experience in the successful development and implementation of IS/IT within UK central government (c) to brief Government Ministers on the opportunities for use of IS/IT to support policy initiatives (e.g. ""Citizen's Charter"" / ""e-government"") and (d) to encourage and assist UK private sector companies to develop and offer products and services aligned to government needs.
Over the 3 decades, CCTA's focus shifted from hardware to a business oriented systems approach with strong emphasis on business led IS/IT Strategies which crossed Departmental (Ministry) boundaries encompassing several ""Departments"" (e.g. CCCJS – Computerisation of the Central Criminal Justice System). This inter-departmental approach, (first mooted in the mid to late 1980s) was revolutionary and met considerable political and departmental opposition.
In October 1994, MI5 took over its work on computer security from hacking into the government's (usually the Treasury) network. In November 1994, CCTA launched its website. In February 1998 it built and ran the government's secure intranet. The MoD was connected to a separate network. In December 1998, the DfEE moved its server from CCTA at Norwich to NISS (National Information Services and Systems) in Bath when it relaunched its website.
Between 1989 and 1992, CCTA's ""Strategic Programmes"" Division undertook research on exploiting Information Systems as a medium for improving the relationship between citizens, businesses and government. This paralled the launch of the ""Citizen's Charter"" by the then Prime Minister, John Major, and the creation within the Cabinet Office of the ""Citizen's Charter Unit"" (CCTA had at this point been moved from HM Treasury to the Cabinet Office). The research and work focused on identifying ways of simplifying the interaction between citizens and government through the use of IS/IT. Two major TV documentaries were produced by CCTA – ""Information and the Citizen"" and ""Hymns Ancient and Modern"" which explored the business and political issues associated with what was to become ""e-government"". These were aimed at widening the understanding of senior civil servants (the Whitehall Mandarins) of the significant impact of the ""Information Age"" and identifying wider social and economic issues likely to arise from e-government.


=== Merger ===
During the late 1990s, its strategic role was eroded by the Cabinet Office's Central IT Unit (CITU – created by Michael Heseltine in November 1995), and in 2000 CCTA was fully subsumed into the Office of Government Commerce (OGC).


=== Successors ===
Since then, the non-procurement IT / Telecommunications co-ordination role has remained in the Cabinet Office, under a number of successive guises:
The Office of the E-Envoy (OeE)
The eGovernment Unit (eGU)
The Transformational Government (TG) Group
The Government Digital Service


== Activities ==
CCTA was the sponsor of a number of methodologies, including:
Structured Systems Analysis and Design Method (SSADM)
PRojects IN Controlled Environments (PRINCE, PRINCE2), which is an evolution of PROMPT, a project management method created by Simpact Systems Ltd in 1975 that was adopted by CCTA in 1979 for Government information system projects
Information Technology Infrastructure Library (ITIL), which has largely evolved through BS15000 into the ISO20000 series
The CCTA Risk Analysis and Management Method (CRAMM), developed at the request of the Cabinet Office in 1985
The CCTA Security Group created the first UK Government National Information Security Policy, and developed the early approaches to structured information security for commercial organisations which saw wider use in the DTI Security Code of Practice, BS_7799 and eventually ISO/IEC_27000
CCTA also promoted the use of emerging IT standards in UK government and in the EU, such as OSI and BS5750 (Quality Management) which led to the publishing of the Quality Management Library and the inception of the TickIT assessment scheme with DTI, MOD and participation of software development companies.
In addition to the development of methodologies, CCTA produced a comprehensive set of managerial guidance covering the development of Information Systems under 5 major headings: A. – Management and Planning of IS; B. – Systems Development; C. – Service Management; D – Office Users; E. – IS Services Industry. The guidance consisted of 27 individual guides and were published commercially as ""The Information Systems Guides"" (ISBN 0-471-92556-X) by John Wiley and Sons. The publication is no longer available. This guidance was developed from the practical experience and lessons learned from many UK Government Departments in planning, designing, implementing and monitoring Information Systems and was highly regarded as ""best practice"". Some parts were translated into other European languages and adopted as national standards.
It also was involved in technical developments, for instance as the sponsor of Project SPACE in the mid 1980s. Under Project SPACE, the ICL Defence Technology Centre (DTC), working closely with technical staff from CCTA and key security-intensive projects in the Ministry of Defence (such as OPCON CCIS) and in other sensitive departments, developed an enhanced security variant of VME.
It managed (ran the servers) of UK national government websites, including those such as the Royal Family's and www.open.gov.uk.


== Structure ==
CCTA's Headquarters were in London at Riverwalk House, Vauxhall Bridge Road, SW1, since used by the [Government Office for London]. This housed the main Divisions with a satellite office in Norwich which focused on IS/IT Procurement – a function which had been taken over from HMSO (the Stationery Office) when CCTA was formed.
The office in Norwich was in the east of the city, off the former A47 (now A1042), just west of the present A47 interchange near the former St Andrew's Hospital. The site is now used by the OGC.
The HQ in London had four divisions:
Project support – major IT programmes – software engineering
Specialist support – evaluation of individual items of hardware and software
Strategic Planning and Promotion – project management and office technology (hardware and office automation)
Advance Technology – telecommunications and advanced technology (latest generation of computers)


== References =="
143,Sidney Fernbach Award,25602338,9469,"The Sidney Fernbach Award established in 1992 by the IEEE Computer Society, in memory of Sidney Fernbach, one of the pioneers in the development and application of high performance computers for the solution of large computational problems as the Division Chief for the Computation Division at Lawrence Livermore Laboratory from the late 1950s through the 1970s. A certificate and $2,000 are awarded for outstanding contributions in the application of high performance computers using innovative approaches. Nomination deadline: 1 July of each year.   


== Sidney Fernbach Award Past Recipients ==
2016 Vipin Kumar. ""For foundational work on understanding scalability, and highly scalable algorithms for graph partitioning, sparse linear systems and data mining.""
2015 Alex Szalay. ""For his outstanding contributions to the development of data-intensive computing systems and on the application of such systems in many scientific areas including astrophysics, turbulence, and genomics.""
2014 Satoshi Matsuoka. ""For his work on software systems for high-performance computing on advanced infrastructural platforms, large-scale supercomputers, and heterogeneous GPU/CPU supercomputers."" 
2013 Christopher R. Johnson. ""For outstanding contributions and pioneering work introducing computing, simulation, and visualization into many areas of biomedicine.""
2012 Laxmikant V. Kale and Klaus Schulten. ""For outstanding contributions to the development of widely used parallel software for large biomolecular systems simulation.""
2011 Cleve Moler. ""For fundamental contributions to linear algebra, mathematical software, and enabling tools for computational science.""
2010 James Demmel. ""For computational science leadership in creating adaptive, innovative, high performance linear algebra software.""
2009 Roberto Car and Michele Parrinello. ""For leadership in creating the modern theoretical and practical foundations for modeling the chemistry and physics of materials. The software resulting from this work is one of the enabling tools for materials science modeling.""
2008 William D. Gropp. ""For outstanding contributions to the development of domain decomposition algorithms, scalable tools for the parallel numerical solution of PDEs, and the dominant HPC communications interface.""
2007 David E Keyes. ""For outstanding contributions to the development of scalable numerical algorithms for the solution of nonlinear partial differential equations and exceptional leadership in high-performance computation.""
2006 Edward Seidel. ""For outstanding contributions to the development of software for HPC and Grid computing to enable the collaborative numerical investigation of complex problems in physics; in particular, modeling black hole collisions.""
2005 John B. Bell. ""For outstanding contributions to the development of numerical algorithms, mathematical, and computational tools and on the application of those methods to conduct leading-edge scientific investigations in combustion, fluid dynamics, and condensed matter.""
2004 Marsha Berger. ""For her many contributions, and enormous, influence to computational fluid dynamics including adaptive mesh refinement methods, Cartesian grid methods, and practical mathematical algorithms for solving significantly heretofore intractable problems.""
2003 Jack Dongarra. ""For outstanding and sustained contributions to the area of mathematical software, most particularly in the areas of communication and numerical libraries and performance benchmarks for high performance computing.""
2002 Robert J. Harrison. ""For developing a computational chemistry software package for applications development, by integrating fundamental algorithm research, novel ideas in computer science, and scalability, while delivering unprecedented modeling capabilities for chemistry applications.""
2000 Stephen W. Attaway. ""For pioneering advances in methods for modeling transient dynamics phenomena, enabling simulations of unprecedented scale and fidelity.""
1999 Michael L. Norman. ""For his leading edge research in applying parallel computing to challenge grand problems in astrophysics and cosmology.""
1998 Phillip Colella. ""For fundamental contributions to the development of software methodologies used to solve numerical partial differential equations, and their application to substantially expand our understanding of shock physics and other fluid dynamics problem.""
1997 Charbel Farhat. ""For outstanding contributions to the development of parallel numerical algorithms and parallel software packages that have helped the mechanical engineering world to embrace parallel processing technology.""
1996 Gary A. Glatzmaier. ""For innovative computational numerical methods to perform the first realistic computer simulations of the Earth's geodynamo and its resultant time-dependent magnetic field.""
1995 Paul R. Woodward. ""For your work in developing new algorithmic techniques in fluid dynamics, & your relentless & innovative pursuit of the hardware & software capabilities to carry out & visualize in real time the largest turbulence simulations.""
1994 Charles S. Peskin. ""For innovative application of mathematical modeling methods to important practical research questions in blood flow and the heart that has for more than 15 years pushed forward the leading edge of computational capability and helped to develop supercomputing technology as a valuable tool for improving the quality of human life.""
1993 David H. Bailey. ""For contributions to numerical computational science including innovative algorithms for FFT's, matrix multiply and multiple precision arithmetic on vector computer architecture.""


== Nomination Process ==
IEEE Computer Society Nomination Process


== References ==


== External links ==
IEEE Computer Society Awards - Fernbach Awards
Sidney Fernbach Biography, IEEE Computer Society"
144,International Conference on Computer Vision,23376109,9392,"ICCV, the International Conference on Computer Vision, is a research conference sponsored by the Institute of Electrical and Electronics Engineers (IEEE) held every other year. It is considered, together with CVPR, the top level conference in computer vision.
The conference is usually spread over four to five days. Typically, leading experts in the focus areas give tutorial talks on the first day, then the technical sessions (and poster sessions in parallel) follow.


== Awards ==


=== Azriel Rosenfeld Lifetime Achievement Award ===
The Azriel Rosenfeld Award, or Azriel Rosenfeld Lifetime Achievement Award, recognizes researchers who have made significant contributions to the field of computer vision over their careers. It is named in memory of computer scientist and mathematician Azriel Rosenfeld.  Recipients:
Olivier Faugeras (2015)
Jan Koenderink (2013)
Thomas Huang (2011)
Berthold K.P. Horn (2009)
Takeo Kanade (2007)


=== Helmholtz Prize ===
The ICCV Helmholtz Prize, known as the Test of Time Award before 2013, is awarded every other year at the ICCV, recognizing ICCV papers from ten or more years earlier that had a significant impact on computer vision research. Winners are selected by the IEEE Computer Society's Technical Committee on Pattern Analysis and Machine Intelligence (TCPAMI). The award is named after the 19th century physician and physicist Hermann von Helmholtz, and the ICCV's award is not related to the various Helmholtz Prizes in physics, or the Hermann von Helmholtz Prize in neuroscience.


==== Helmholtz Prize recipients ====
Awarded at ICCV 2015:
Belongie, Serge, Jitendra Malik, and Jan Puzicha. ""Matching shapes."" In Computer Vision, 2001. ICCV 2001. Proceedings. Eighth IEEE International Conference on, vol. 1, pp. 454–461. IEEE, 2001.
Martin, David, Charless Fowlkes, Doron Tal, and Jitendra Malik. ""A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics."" In Computer Vision, 2001. ICCV 2001. Proceedings. Eighth IEEE International Conference on, vol. 2, pp. 416–423. IEEE, 2001.

Awarded at ICCV 2013:
Michael Kass, Andrew Witkin and Demetri Terzopoulos, ""Snakes: Active Contour Models"", ICCV 1987.
Michael J. Swain and Dana H. Ballard, ""Indexing via color histograms"", ICCV 1990.
Bill Freeman and Ted Adelson, ""Steerable filters for early vision, image analysis, and wavelet decomposition"", ICCV 1990.
Michael Black and P. Anandan, ""A framework for the robust estimation of optical flow"", ICCV 1993.
Paul Viola and William M. Wells III, ""Alignment by Maximization of Mutual Information"", ICCV 1995.
Richard Hartley, ""In Defence of the 8-Point Algorithm"", ICCV 1995.
Carlo Tomasi and Roberto Manduchi, ""Bilateral Filtering for Gray and Color Images"", ICCV 1998.
Yossi Rubner, Carlo Tomasi and Leonidas J. Guibas, ""A Metric for Distributions with Applications to Image Databases"", ICCV 1998.
Song-Chun Zhu, Tai Sing Lee and Alan Yuille, ""Region Competition: Unifying Snakes, Region Growing, Energy/Bayes/MDL for Multi-band Image Segmentation"", ICCV 1995.
Zhengyou Zhang, ""Flexible Camera Calibration by Viewing a Plane from Unknown Orientations"", ICCV 1999.
Alyosha Efros and Thomas K. Leung, ""Texture Synthesis by Non-parametric Sampling"", ICCV 1999.

Awarded at ICCV 2011:
David G. Lowe, ""Object Recognition from Local Scale-Invariant Features"", ICCV 1999.
Yuri Boykov, Olga Veksler, and Ramin Zabih, ""Fast Approximate Energy Minimization via Graph Cuts"", ICCV 1999.
Vincent Caselles, Ron Kimmel, and Guillermo Sapiro, ""Geodesic Active Contours"", ICCV 1995.

Awarded at ICCV 2009:
Yehezkel Lamdan and Haim J. Wolfson, ""Geometric Hashing: A General and Efficient Model-Based Recognition Scheme"", ICCV 1988.


=== Marr Prize ===
The best paper of the conference is awarded the Marr Prize, which is one of the top honors for a researcher in computer vision.


=== Mark Everingham Prize ===
The Mark Everingham Prize is an award given yearly by the Technical Committee on Pattern Analysis and Machine Intelligence (TCPAMI) of the IEEE Computer Society at the IEEE International Conference on Computer Vision (ICCV) or the European Conference on Computer Vision (ECCV) to commemorate the late Mark Everingham, “one of the rising stars of computer vision”, and to encourage others to follow in his footsteps by acting to further progress in the computer vision community as a whole. The prize is given to a researcher, or a team of researchers, who have made a selfless contribution of significant benefit to other members of the computer vision community. The Mark Everingham Prize for Rigorous Evaluation was an award given in 2012 at the British Machine Vision Conference (BMVC).


==== Mark Everingham Prize recipients ====
Awarded at ICCV 2013:
P. Jonathon Phillips for his work on the FERET database
the OpenCV team, represented by Gary Bradski

Awarded at ECCV 2014:
Ginger Boult and Terry Boult, for their numerous contributions towards organizing and running vision conferences

Awarded at ICCV 2015:
Daniel Scharstein and Rick Szeliski, for the Middlebury benchmarks
Andrea Vedaldi, for the VLFeat software package

Awarded at ECCV 2016:
ImageNet (Alex Berg, Jia Deng, Fei-Fei Li, Wei, Liu, Olga Russakovsky and team)
Ramin Zabih, for extensive, generous, service to the community


=== PAMI Distinguished Researcher Award ===
This award (until 2013 called Significant Researcher Award) is awarded to candidates whose research contributions have significantly contributed to the progress of Computer Vision. Awards are made based on major research contributions, as well as the role of those contributions in influencing and inspiring other research. Candidates are nominated by the community.


==== PAMI Distinguished Researcher Award recipients ====
Awarded at ICCV 2015:
Yann LeCun and David G. Lowe

Awarded at ICCV 2013:
Jitendra Malik and Andrew Zisserman

Awarded at ICCV 2011:
Richard Hartley and Katsushi Ikeuchi

Awarded at ICCV 2009:
Andrew Blake

Awarded at ICCV 2007:
Demetri Terzopoulos


== References =="
145,Roman Yampolskiy,37120084,9154,"Roman V. Yampolskiy (Russian: Роман Владимирович Ямпольский) (born August 13, 1979) is a Latvian born computer scientist at the University of Louisville, known for his work on behavioral biometrics, security of cyberworlds, and artificial intelligence safety. He holds a PhD from the University at Buffalo (2008). He is currently the director of Cyber Security Laboratory in the department of Computer Engineering and Computer Science at the Speed School of Engineering.
Yampolskiy is an author of some 100 publications, including numerous books. His work is frequently profiled in popular media, such as: the BBC, MSNBC, Yahoo, New Scientist, Alex Jones Radio show and many others.


== AI safety ==
Yampolskiy has warned of the possibility of existential risk from advanced artificial intelligence, and has advocated research into ""boxing"" artificial intelligence.


== Intellectology ==
In 2015, Yampolskiy launched intellectology, a new field of study founded to analyze the forms and limits of intelligence. Yampolskiy considers AI to be a sub-field of this. An example of Yampolskiy's intellectology work is an attempt to determine the relation between various types of minds and the accessible fun space, i.e. the space of non-boring activities.


== Books ==
Artificial Superintelligence: a Futuristic Approach. Chapman and Hall/CRC Press (Taylor & Francis Group), 2015, ISBN 978-1482234435.
Game Strategy: a Novel Behavioral Biometric. Independent University Press, 2009, ISBN 0-578-03685-1
Computer Security: from Passwords to Behavioral Biometrics. New Academic Publishing, 2008, ISBN 0-6152-1818-0
Feature Extraction Approaches for Optical Character Recognition. Briviba Scientific Press, 2007, ISBN 0-6151-5511-1


== See also ==
AI box
AI-complete
Machine Intelligence Research Institute
Singularity University


== References ==


== External links ==
Roman Yampolskiy’s Homepage.
Cyber Security Lab at UofL.
Interview of Dr. Yampolskiy on EEweb."
146,Numerical algebraic geometry,52802478,9061,"Numerical algebraic geometry is a field of computational mathematics, particularly computational algebraic geometry, which uses methods from numerical analysis to study and manipulate algebraic varieties on a computer.


== Homotopy continuation ==

The primary computational method used in numerical algebraic geometry is homotopy continuation, in which a homotopy is formed between two polynomial systems, and the isolated solutions (points) of one are continued to the other. This is a specification of the more general method of numerical continuation.
Let 
  
    
      
        z
      
    
    {\displaystyle z}
   represent the variables of the system. By abuse of notation, and to facilitate the spectrum of ambient spaces over which one can solve system, we do not use vector notation for 
  
    
      
        z
      
    
    {\displaystyle z}
  . Similarly for the polynomial systems 
  
    
      
        f
      
    
    {\displaystyle f}
   and 
  
    
      
        g
      
    
    {\displaystyle g}
  .
Current canonical notation calls the start system 
  
    
      
        g
      
    
    {\displaystyle g}
  , and the target system – the system to be solved – 
  
    
      
        f
      
    
    {\displaystyle f}
  . A very common homotopy, the straight-line homotopy, between 
  
    
      
        f
      
    
    {\displaystyle f}
   and 
  
    
      
        g
      
    
    {\displaystyle g}
   is 
In the above homotopy, one starts the path variable at 
  
    
      
        
          t
          
            start
          
        
        =
        1
      
    
    {\displaystyle t_{\text{start}}=1}
   and continues toward 
  
    
      
        
          t
          
            end
          
        
        =
        0
      
    
    {\displaystyle t_{\text{end}}=0}
  . Another common choice is to run from 
  
    
      
        0
      
    
    {\displaystyle 0}
   to 
  
    
      
        1
      
    
    {\displaystyle 1}
  . In principle, the choice is completely arbitrary. In practice, regarding endgame methods for computing singular solutions using homotopy continuation, the target time being 
  
    
      
        0
      
    
    {\displaystyle 0}
   can significantly ease analysis, so this perspective is here taken.
Regardless of the choice of start and target times, the 
  
    
      
        H
      
    
    {\displaystyle H}
   ought to be formulated such that 
  
    
      
        H
        (
        z
        ,
        
          t
          
            start
          
        
        )
        =
        g
        (
        z
        )
      
    
    {\displaystyle H(z,t_{\text{start}})=g(z)}
  , and 
  
    
      
        H
        (
        z
        ,
        
          t
          
            end
          
        
        )
        =
        f
        (
        z
        )
      
    
    {\displaystyle H(z,t_{\text{end}})=f(z)}
  .
One has a choice in 
  
    
      
        g
        (
        z
        )
      
    
    {\displaystyle g(z)}
  , including
Roots of unity
Total degree
Polyhedral
Multi-homogeneous
and beyond these, specific start systems which closely mirror the structure of 
  
    
      
        f
      
    
    {\displaystyle f}
   may be formed for particular systems. The choice of start system impacts the computational time it takes to solve 
  
    
      
        f
      
    
    {\displaystyle f}
  , in that those which are easy to formulate (such as total degree) tend to have higher numbers of paths to track, and those which take significant effort (such as the polyhedral method) are much sharper. There is currently no great way to predict which will lead to the quickest time to solve.
Actual continuation is typically done using predictor–corrector methods, with additional features as implemented. Predicting is done using a standard ODE predictor method, such as Runge–Kutta, and correction often uses Newton–Raphson iteration.
Because 
  
    
      
        f
      
    
    {\displaystyle f}
   and 
  
    
      
        g
      
    
    {\displaystyle g}
   are polynomial, homotopy continuation in this context is theoretically guaranteed to compute all solutions of 
  
    
      
        f
      
    
    {\displaystyle f}
  , due to Bertini's theorem. However, this guarantee is not always achieved in practice, because of issues arising from limitations of the modern computer, most namely finite precision. That is, despite the strength of the probability-1 argument underlying this theory, without using a priori certified tracking methods, some paths may fail to track perfectly for various reasons.


== Witness set ==
A witness set 
  
    
      
        W
      
    
    {\displaystyle W}
   is a data structure used to describe algebraic varieties. The witness set for an affine variety that is equidimensional consists of three pieces of information. The first piece of information is a system of equations 
  
    
      
        F
      
    
    {\displaystyle F}
  . These equations define the algebraic variety 
  
    
      
        
          
            V
          
        
        (
        F
        )
      
    
    {\displaystyle {\mathbf {V} }(F)}
   that is being studied. The second piece of information is a linear space 
  
    
      
        
          
            L
          
        
      
    
    {\displaystyle {\mathcal {L}}}
  . The dimension of 
  
    
      
        
          
            L
          
        
      
    
    {\displaystyle {\mathcal {L}}}
   is the codimension of 
  
    
      
        
          
            V
          
        
        (
        F
        )
      
    
    {\displaystyle {\mathbf {V} }(F)}
  , and chosen to intersect 
  
    
      
        
          
            V
          
        
        (
        F
        )
      
    
    {\displaystyle {\mathbf {V} }(F)}
   transversely. The third piece of information is the list of points in the intersection 
  
    
      
        
          
            L
          
        
        ∩
        
          
            V
          
        
        (
        F
        )
      
    
    {\displaystyle {\mathcal {L}}\cap {\mathbf {V} }(F)}
  . This intersection has finitely many points and the number of points is the degree of the algebraic variety 
  
    
      
        
          
            V
          
        
        (
        F
        )
      
    
    {\displaystyle {\mathbf {V} }(F)}
  . Thus, witness sets encode the answer to the first two questions one asks about an algebraic variety: What is the dimension, and what is the degree? Witness sets also allow one to perform a numerical irreducible decomposition, component membership tests, and component sampling. This makes witness sets a good description of an algebraic variety.


== Certification ==
Solutions to polynomial systems computed using numerical algebraic geometric methods can be certified, meaning that the approximate solution is ""correct"". This can be achieved in several ways, either a priori using a certified tracker, or a posteriori by showing that the point is, say, in the basin of convergence for Newton's method.


== Software ==
There are several software packages which implement portions of the theoretical body of numerical algebraic geometry. Some notable packages include, in alphabetic order:
alphaCertified
Bertini 
Hom4PS 
Macaulay2 (core implementation of homotopy tracking and NumericalAlgebraicGeometry package)
PHCPack


== Theorem Star ==
Often researchers use results from both probabilistic symbolic computations and numerical computations to prove theorems. The notation Theorem Star is often used to indicate the reliance on such computations.

    Our methods include probabilistic symbolic computations and numerical computations. Though they have been carefully tested and produce completely reproducible results, they are technically only true with high probability, or up to the numerical precision of the computers we use. To indicate reliance on such computations, we designate those theorems, corollaries, and propositions with a star.


== References =="
147,Scottish Informatics and Computer Science Alliance,33615890,9061,"The Scottish Informatics and Computer Science Alliance (SICSA) is a ""research pool"" funded by the Scottish Funding Council. A research pool is a collaboration of Scottish university departments whose broad objective is to create a coherent research community that will improve the quality of research carried out in Scotland in the pool-related discipline.
SICSA's goals are to improve the quality of research in informatics and computer science across universities in Scotland, to promote the transfer of research results to benefit companies and the public sector in Scotland and to create a university community that represents all aspects of Scottish Informatics and Computer Science.
SICSA was launched in December 2008 and is funded with an award of £14.5 million from the Scottish Funding Council with SICSA member universities providing matching funding. It is managed by the SICSA Executive which is composed of: Director of Research (who is also the SICSA Director), the Director of the SICSA Graduate Academy, the Director of Knowledge Exchange, the Director of Education, the SICSA Executive Officer and the SICSA Executive Assistant.


== Membership ==
SICSA has adopted an inclusive membership policy and all universities in Scotland that have computer science or informatics departments/schools are eligible to be members of SICSA. In April 2013, the following departments/schools were SICSA members.
University of Aberdeen. Department of Computing Science
University of Abertay. School of Computing and Engineering Systems & Institute of Arts, Media and Computer Games
University of Dundee. School of Computing
University of Edinburgh. School of Informatics
Edinburgh Napier University. Institute for Informatics & Digital Innovation
University of Glasgow. School of Computing Science
Glasgow Caledonian University. School of Engineering and Computing
Heriot-Watt University. School of Mathematical and Computer Sciences
The Robert Gordon University. School of Computing
University of St Andrews. School of Computer Science
University of Stirling. Department of Computer Science and Mathematics
University of Strathclyde. Department of Computer and Information Sciences
University of the West of Scotland. School of Computing
University of the Highlands and Islands. School of Computing


== Research themes ==
SICSA’s research activities are organized around four broad research themes
Next-generation Internet
Complex Systems Engineering
Modelling and Abstraction
Multi-modal Interaction
Each research theme is managed by a theme leader and organizes workshops and events for the computer science research community across Scotland.


== SICSA staff appointments ==
To help achieve its goal of improving research quality, SICSA has funded the appointment of 30 academic staff across Scottish universities. Appointments have been made in Edinburgh, Glasgow, St Andrews, Stirling, Strathclyde, Abertay and Aberdeen Universities.


== The SICSA Graduate Academy ==
The SICSA Graduate Academy (SGA) is an international graduate school in informatics and computer science. It provides funding for PhD students, supports graduate training and summer schools and runs a Distinguished Visitor scheme which supports visits from distinguished computer scientists to Scotland. The SGA runs an annual conference for all PhD students working in computer science and informatics in Scotland.


=== SICSA Prize studentships ===
SICSA has made available 80 prize studentships for PhD study in Scotland to students from around the world, with the aim of attracting the research leaders of the future to work in Scotland.


=== SICSA summer schools ===
SICSA provides support for PhD students in Scotland to attend international summer school and sponsors up to 3 international summer schools per year for PhD students in Scotland. Nine summer schools have been sponsored in Scottish universities between June 2009 and August 2012.


=== SICSA Distinguished Visitor Scheme ===
The aim of the SICSA Distinguished Visitor scheme is to provide support for excellent researchers from around the world to interact with the Scottish Computer Science and Informatics research community.


== Interaction with industry ==
SICSA interacts extensively with local industry in Scotland with a view to transferring technology and informing the industrial community of informatics and computer science research in Scotland. An annual ""Demofest"" is organized that showcases Scottish University research. Specific projects with industry are concerned with smart tourism and migrating high-value software services to the cloud.
In 2013, SICSA developed a portfolio of funding mechanisms which have the specific aim of increasing exchanges between academics and industry. These mechanisms include: SICSA Postgraduate Industry Internship Program; SICSA Early Career Industry Fellowship; SICSA Distinguished Industrial Visitors Fellowship; SICSA Elevate - Incubator Program; SICSA Proof of Concept Program; SICSA Team Based Industrial Placements Program.


== Computer science education ==
Uniquely amongst the SFC research pools, SICSA has extended its remit to include education as well as research. The aim of this move is to allow SICSA to become the single representative body for all aspects of university informatics/computer science research and education in Scotland.
SICSA maintains information about undergraduate and postgraduate taught courses in Scotland for potential students and provides information for industry on opportunities for graduate recruitment.


== References ==


== External links ==
Official SICSA Web site"
148,Speculative multithreading,8307095,8948,"Thread Level Speculation (TLS), also known as Speculative Multithreading (SpMT) is a runtime parallelization technique which uncovers parallelism that static (compile-time) parallelization techniques fail to exploit. This particular case of speculative execution occurs at the thread level as opposed to the instruction level and can be implemented both in hardware as well as in software.


== Further reading ==
Yiapanis, Paraskevas; Brown, Gavin; Lujan, Mikel (2016). ""Compiler-Driven Software Speculation for Thread-Level Parallelism"". ACM Transactions on Programming Languages and Systems (TOPLAS). 38 (2). doi:10.1145/2821505. 
Yiapanis, Paraskevas; Rosas-Ham, Demian; Brown, Gavin; Lujan, Mikel (2013). ""Optimizing Software Runtime Systems for Speculative Parallelization"". ACM Transactions on Architecture and Code Optimization (TACO). 9 (4). doi:10.1145/2400682.2400698. 
Johnson, Nick P.; Kim, Hanjun; Prabhu, Prakash; Zaks, Ayal; August, David I. (2012). ""Speculative separation for privatization and reductions"" (PDF). Proceedings of the 33rd ACM SIGPLAN Conference on Programming Language Design and Implementation. PLDI '12. pp. 359–370. doi:10.1145/2254064.2254107. 
Bhowmik, Anasua; Franklin, Manoj (2002). ""A General Compiler Framework for Speculative Multithreading"". Proceedings of the fourteenth annual ACM symposium on Parallel algorithms and architectures. SPAA '02. pp. 99–108. doi:10.1145/564870.564885. 
Bruening, Derek; Devabhaktuni, Srikrishna; Amarasinghe, Saman (2000). Softspec: Software-based Speculative Parallelism (PDF). FDDO-3. pp. 1–10. 
Chen, Michael K.; Olukotun, Kunle (1998). ""Exploiting Method-Level Parallelism in Single-Threaded Java Programs"". International Conference on Parallel Architectures and Compilation Techniques. PACT 1998. pp. 176–184. doi:10.1109/PACT.1998.727190. 
Chen, Michael K.; Olukotun, Kunle (2003). ""The Jrpm System for Dynamically Parallelizing Java Programs"". Proceedings of the 30th annual international symposium on Computer architecture. ISCA '03. pp. 434–446. doi:10.1145/859618.859668. 
Cintra, Marcelo; Llanos, Diego R. (2003). ""Toward Efficient and Robust Software Speculative Parallelization on Multiprocessors"". Proceedings of the ninth ACM SIGPLAN symposium on Principles and practice of parallel programming. PPoPP '03. pp. 13–24. doi:10.1145/781498.781501. 
Cook, Jonathan J. (2002). ""Reverse Execution of Java Bytecode"". The Computer Journal. 45 (6): 608–619. doi:10.1093/comjnl/45.6.608. 
Quinones, Carlos Garcia; Madriles, Carlos; Sanchez, Jesus; Marcuello, Pedro; Gonzalez, Antonio; Tullsen, Dean M. (2005). ""Mitosis Compiler: An Infrastructure for Speculative Threading Based on Pre-Computation Slices"". Proceedings of the 2005 ACM SIGPLAN conference on Programming language design and implementation. PLDI '05. pp. 269–279. doi:10.1145/1065010.1065043. 
Hu, Shiwen; Bhargava, Ravi; John, Lizy Kurian (2003). ""The Role of Return Value Prediction in Exploiting Speculative Method-Level Parallelism"" (PDF). JILP. 5: 1–21. 
Kazi, Iffat H. (2000). A Dynamically Adaptive Parallelization Model Based on Speculative Multithreading (Ph.D. thesis). University of Minnesota. pp. 1–188. 
Pickett, Christopher J.F.; Verbrugge, Clark (2005). ""SableSpMT: A Software Framework for Analysing Speculative Multithreading in Java"". Proceedings of the 6th ACM SIGPLAN-SIGSOFT workshop on Program analysis for software tools and engineering. PASTE '05. pp. 59–66. doi:10.1145/1108792.1108809. 
Pickett, Christopher J.F.; Verbrugge, Clark (2005). ""Software Thread Level Speculation for the Java Language and Virtual Machine Environment"" (PDF). Proceedings of the 18th international conference on Languages and Compilers for Parallel Computing. LCPC '05. LNCS. 4339. pp. 304–318. doi:10.1007/978-3-540-69330-7_21. 
Porter, Leo; Choi, Bumyong; Tullsen, Dean M. (2009). ""Mapping Out a Path from Hardware Transactional Memory to Speculative Multithreading"". 18th International Conference on Parallel Architectures and Compilation Techniques. PACT '09. pp. 313–324. doi:10.1109/PACT.2009.37. 
Rundberg, Peter; Stenstrom, Per (2001). ""An All-Software Thread-Level Data Dependence Speculation System for Multiprocessors"" (PDF). JILP. 3: 1–28. 
Steffan, J. Gregory; Colohan, Christopher; Zhai, Antonia; Mowry, Todd C. (2005). ""The STAMPede Approach to Thread-Level Speculation"". TOCS. 23 (3): 253–300. doi:10.1145/1082469.1082471. 
Whaley, John; Kozyrakis, Christos (2005). ""Heuristics for Profile-driven Method-level Speculative Parallelization"". International Conference on Parallel Processing. ICPP 2005. pp. 147–156. doi:10.1109/ICPP.2005.44. 
Renau, Jose; Strauss, Karin; Ceze, Luis; Liu, Wei; Sarangi, Smruti; Tuck, James; Torrellas, Josep (2006). ""Energy-Efficient Thread-Level Speculation"" (PDF). IEEE Micro. 26 (1): 80–91. doi:10.1109/MM.2006.11. 
Yoshizoe, Kazuki; Matsumoto, Takashi; Hiraki, Kei (1998). ""Speculative Parallel Execution on JVM"". UK Workshop on HPNC. pp. 1–20. 
Oancea, Cosmin E.; Mycroft, Alan; Harris, Tim (2009). ""A Lightweight In-Place Implementation for Software Thread-Level Speculation"" (PDF). Proceedings of the twenty-first annual symposium on Parallelism in algorithms and architectures. SPAA '09. pp. 1–10. doi:10.1145/1583991.1584050."
149,Wiener connector,45655492,8897,"In mathematics applied to the study of networks, the Wiener connector, named in honor of chemist Harry Wiener who first introduced the Wiener Index, is a means of maximizing efficiency in connecting specified ""query vertices"" in a network. Given a connected, undirected graph and a set of query vertices in a graph, the minimum Wiener connector is an induced subgraph that connects the query vertices and minimizes the sum of shortest path distances among all pairs of vertices in the subgraph. In combinatorial optimization, the minimum Wiener connector problem is the problem of finding the minimum Wiener connector. It can be thought of as a version of the classic Steiner tree problem (one of Karp's 21 NP-complete problems), where instead of minimizing the size of the tree, the objective is to minimize the distances in the subgraph.
The minimum Wiener connector was first presented by Ruchansky, et al. in 2015.
The minimum Wiener connector has applications in many domains where there is a graph structure and an interest in learning about connections between sets of individuals. For example, given a set of patients infected with a viral disease, which other patients should be checked to find the culprit? Or given a set of proteins of interest, which other proteins participate in pathways with them?


== Problem definition ==
The Wiener index is the sum of shortest path distances in a (sub)graph. Using 
  
    
      
        d
        (
        u
        ,
        v
        )
      
    
    {\displaystyle d(u,v)}
   to denote the shortest path between 
  
    
      
        u
      
    
    {\displaystyle u}
   and 
  
    
      
        v
      
    
    {\displaystyle v}
  , the Wiener index of a (sub)graph 
  
    
      
        S
      
    
    {\displaystyle S}
  , denoted 
  
    
      
        W
        (
        S
        )
      
    
    {\displaystyle W(S)}
  , is defined as

  
    
      
        W
        (
        S
        )
        =
        
          ∑
          
            (
            u
            ,
            v
            )
            ∈
            S
          
        
        d
        (
        u
        ,
        v
        )
      
    
    {\displaystyle W(S)=\sum _{(u,v)\in S}d(u,v)}
  .
The minimum Wiener connector problem is defined as follows. Given an undirected and unweighted graph with vertex set 
  
    
      
        V
      
    
    {\displaystyle V}
   and edge set 
  
    
      
        E
      
    
    {\displaystyle E}
   and a set of query vertices 
  
    
      
        Q
        ⊆
        V
      
    
    {\displaystyle Q\subseteq V}
  , find a connector 
  
    
      
        H
        ⊆
        V
      
    
    {\displaystyle H\subseteq V}
   of minimum Wiener index. More formally, the problem is to compute

  
    
      
        
          *
        
        ⁡
        
          
            a
            r
            g
            
            m
            i
            n
          
          
            H
          
        
        W
        (
        H
        ∪
        Q
        )
      
    
    {\displaystyle \operatorname {*} {arg\,min}_{H}W(H\cup Q)}
  ,
that is, find a connector 
  
    
      
        H
      
    
    {\displaystyle H}
   that minimizes the sum of shortest paths in 
  
    
      
        H
      
    
    {\displaystyle H}
  .


== Relationship to Steiner tree ==

The minimum Wiener connector problem is related to the Steiner tree problem. In the former, the objective function in the minimization is the Wiener index of the connector, whereas in the latter, the objective function is the sum of the weights of the edges in the connector. The optimum solutions to these problems may differ, given the same graph and set of query vertices. In fact, a solution for the Steiner tree problem may be arbitrarily bad for the minimum Wiener connector problem; the graph on the right provides an example.


== Computational complexity ==


=== Hardness ===
The problem is NP-hard, and does not admit a polynomial-time approximation scheme unless P = NP. This can be proven using the inapproximability of vertex cover in bounded degree graphs. Although there is no polynomial-time approximation scheme, there is a polynomial-time constant-factor approximation—an algorithm that finds a connector whose Wiener index is within a constant multiplicative factor of the Wiener index of the optimum connector. In terms of complexity classes, the minimum Wiener connector problem is in APX but is not in PTAS unless P = NP.


=== Exact algorithms ===
An exhaustive search over all possible subsets of vertices to find the one that induces the connector of minimum Wiener index yields an algorithm that finds the optimum solution in 
  
    
      
        
          2
          
            O
            (
            n
            )
          
        
      
    
    {\displaystyle 2^{O(n)}}
   time (that is, exponential time) on graphs with n vertices. In the special case that there are exactly two query vertices, the optimum solution is the shortest path joining the two vertices, so the problem can be solved in polynomial time by computing the shortest path. In fact, for any fixed constant number of query vertices, an optimum solution can be found in polynomial time.


=== Approximation algorithms ===
There is a constant-factor approximation algorithm for the minimum Wiener connector problem that runs in time 
  
    
      
        O
        (
        q
        (
        m
        log
        ⁡
        n
        +
        n
        
          log
          
            2
          
        
        ⁡
        n
        )
        )
      
    
    {\displaystyle O(q(m\log n+n\log ^{2}n))}
   on a graph with n vertices, m edges, and q query vertices, roughly the same time it takes to compute shortest-path distances from the query vertices to every other vertex in the graph. The central approach of this algorithm is to reduce the problem to the vertex-weighted Steiner tree problem, which admits a constant-factor approximation in particular instances related to the minimum Wiener connector problem.


== Behavior ==
The minimum Wiener connector behaves like betweenness centrality.
When the query vertices belong to the same community, the non-query vertices that form the minimum Wiener connector tend to belong to the same community and have high centrality within the community. Such vertices are likely to be influential vertices playing leadership roles in the community. In a social network, these influential vertices might be good users for spreading information or to target in a viral marketing campaign.
When the query vertices belong to different communities, the non-query vertices that form the minimum Wiener connector contain vertices adjacent to edges that bridge the different communities. These vertices span a structural hole in the graph and are important.


== Applications ==
The minimum Wiener connector is useful in applications in which one wishes to learn about the relationship between a set of vertices in a graph. For example,
in biology, it provides insight into how a set of proteins in a protein–protein interaction network are related,
in social networks (like Twitter), it demonstrates the communities to which a set of users belong and how these communities are related,
in computer networks, it may be useful in identifying an efficient way to route a multicast message to a set of destinations.


== References =="
150,Environmental informatics,25893176,8877,"Environmental informatics is the science of information applied to environmental science. As such, it provides the information processing and communication infrastructure to the interdisciplinary field of environmental sciences aiming at data, information and knowledge integration, the application of computational intelligence to environmental data as well as the identification of environmental impacts of information technology. The UK Natural Environment Research Council defines environmental informatics as the ""research and system development focusing on the environmental sciences relating to the creation, collection, storage, processing, modelling, interpretation, display and dissemination of data and information."" Kostas Karatzas defined environmental informatics as the ""creation of a new 'knowledge-paradigm' towards serving environmental management needs."" Karatzas argued further that environmental informatics ""is an integrator of science, methods and techniques and not just the result of using information and software technology methods and tools for serving environmental engineering needs.""
Environmental informatics emerged in early 1990 in Central Europe.
Current initiatives to effectively manage, share, and reuse environmental and ecological data are indicative of the increasing importance of fields like environmental informatics and ecoinformatics to develop the foundations for effectively managing ecological information. Examples of these initiatives are National Science Foundation Datanet projects, DataONE and Data Conservancy.


== Conferences ==
EnviroInfo 2013, 2012
Environmental Information Management 2011, 2008
International Conference on Adaptive and Natural Computing Algorithms ICANNGA
International Conference on ICT for Sustainability (ICT4S) 2014, 2013
International Conference on Information Technologies in Environmental Engineering 2013
International Congress on Environmental Modelling and Software (iEMSs) 2014, 2012, 2010
International Congress on Modelling and Simulation (MODSIM) 2013
International Symposium on Environmental Software Systems (ISESS) 2013, 2011


== Journals ==
ACM Transactions on Sensor Networks
Computers and Electronics in Agriculture
Earth Science Informatics
Earth System Science Data
Environmental Earth Sciences
Environmental Modelling and Software
Environmental Monitoring and Assessment
IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
International Journal of Agricultural and Environmental Information Systems
International Journal of Digital Earth
International Journal of Distributed Sensor Networks
International Journal of Sensor Networks
Journal of Environmental Informatics


== Institutions ==
Aalto University: Environmental Infomatics
Aristotle University of Thessaloniki: Informatics Applications and Systems Group, Dept. of Mechanical Engineering: Teaching and Research on Environmental Informatics and Quality of Life Information Services
Institute for Environmental Analytics: Applied environmental informatics, based at the University of Reading, UK
Lancaster Environment Center: Centre for environmental informatics
Lincoln University: GIS and Environmental Informatics
Masaryk University: Division of environmental informatics and modeling
Northern Arizona University: PhD in Enviromental Informatics
NUI Galway: Environmental informatics
RISSAC: Department of environmental informatics, research institute for soil science and agricultural chemistry, Hungarian academy of sciences
Stanford University: Sustainable development & environmental informatics
Tokyo Institute of Technology: Department of Mechanical and Environmental Informatics
TU Graz: Research focus area in environmental informatics
University of California, Irvine: Bren school environmental informatics research
University of Dayton: Center of excellence for strategic energy and environmental informatics
University of Eastern Finland: Division of environmental informatics within the department of environmental science at Kuopio Campus
University of Hamburg: Research in environmental informatics
University of Las Vegas, Nevada: Environmental informatics undergraduate program
University of Michigan: Environmental informatics GIS and modeling graduate program
University of Oldenburg: Division of environmental informatics
University of Sunderland: Centre for environmental informatics
University of Applied Sciences (HTW) Berlin: Research and undergraduate program
Vienna University of Technology: Doctoral College: Environmental Informatics
Virginia Polytechnic Institute: Undergraduate Program: Environmental Informatics


== Collaborations ==
DataONE: Data Observation Network for Earth
Data Conservancy: Leading the movement to build data management tools and services across institutions and disciplines


== References =="
151,Computational criminology,17104799,8816,"Computational criminology is an interdisciplinary field which uses computing science methods to formally define criminology concepts, improve our understanding of complex phenomena, and generate solutions for related problems.


== Methods ==
Computing science methods being used include:
Algorithms
Data Mining
Data Structures
Formal Methods
Software Development Process


== Areas of usage ==
Computational criminology is interdisciplinary in the sense that both criminologists and computing scientists work together to ensure that computational models properly match their theoretical and real-world counterparts. Areas of criminology for which computational approaches are being used include:
Environmental Criminology
Identity Theft
Justice


=== Forensics ===
Computational forensics (CF) is a quantitative approach to the methodology of the forensic sciences. It involves computer-based modeling, computer simulation, analysis, and recognition in studying and solving problems posed in various forensic disciplines. CF integrates expertise from computational science and forensic sciences.
A broad range of objects, substances and processes are investigated, which are mainly based on pattern evidence, such as toolmarks, fingerprints, shoeprints, documents etc., but also physiological and behavioral patterns, DNA, digital evidence and crime scenes.
Computational methods find a place in the forensic sciences in several ways, as for example:
rigorous quantification of individuality,
definition and establishment of likelihood ratio,
increase of efficiency and effectiveness in daily forensic casework.
Algorithms implemented are from the fields of signal and image processing, computer vision, computer graphics, data visualization, statistical pattern recognition, data mining, machine learning, and robotics.
Computer forensics (also referred to as ""digital forensics"" or ""forensic information technology"") is one specific discipline that could use computational science to study digital evidence. Computational Forensics examines diverse types of evidence.


==== Forensic animation ====
Forensic animation is a branch of forensic science in which audio-visual reconstructions of incidents or accidents are created to aid investigators. Examples include the use of computer animation, stills, and other audio visual aids. Application of computer animation in courtrooms today is becoming more popular.
The first use of forensic animation was in Connors v. United States, both sides used computer re-creations and animations in a case surrounding the crash of Delta Flight 191 on August 2, 1985. The crash resulted in the deaths of 137 people and extensive property damage. In the resulting lawsuit a method was required to explain complicated information and situations to the jury. As part of the plaintiff presentation, a 45-minute computer generated presentation was created to explain the intricacies of the evidence and thus began forensic animation.
The first reported use of computer animation in a U.S. criminal trial was in the 1991 Marin County, CA homicide trial of James Mitchell (of the porno-businessman Mitchell Brothers) The prosecution used the animation to explain the complex details of the shooting incident to the jury. It showed the positions of James Mitchell, Artie Mitchell (the victim), the bullet impact points, and the path taken by bullets as they entered Artie's body. The animation was admitted, over objection by the defense, and the case resulted in a conviction. The use of the animation was upheld on appeal and the success of the forensic animation led to its use in many other trials. In India Prof. T D Dogra at AIIMS New Delhi in 2008 used animation to explain the court of law and investigating agencies first time in two important cases of firearm injuries,case of Murder and Terrorist encounter killings(Batla house encounter case).


== References ==


== External links ==
Admission of Forensic Animation
How to Assess a Forensic Animation - Offensive and Defensive Strategies
Institute for Canadian Urban Research Studies (ICURS)
The Mastermind Project
The digital photo authentication project at CS Dept. University of Wisconsin-Madison
Trial Graphics: The role of forensic animation in the courtroom"
152,Computational particle physics,17156914,8793,"Computational particle physics refers to the methods and computing tools developed in and used by particle physics research. Like computational chemistry or computational biology, it is, for particle physics both a specific branch and an interdisciplinary field relying on computer science, theoretical and experimental particle physics and mathematics. The main fields of computational particle physics are: lattice field theory (numerical computations), automatic calculation of particle interaction or decay (computer algebra) and event generators (stochastic methods).


== Computing tools ==
Computer algebra: Many of the computer algebra languages were developed initially to help particle physics calculations: Reduce, Mathematica, Schoonschip, Form, GiNaC.
Data Grid: The largest planned use of the grid systems will be for the analysis of the LHC - produced data. Large software packages have been developed to support this application like the LHC Computing Grid (LCG) . A similar effort in the wider e-Science community is the GridPP collaboration, a consortium of particle physicists from UK institutions and CERN.
Data Analysis Tools: These tools are motivated by the fact that particle physics experiments and simulations often create large datasets, e.g. see references. Examples include ROOT, Java Analysis Studio and SCaViS.
Software Libraries: Many software libraries are used for particle physics computations. Examples include FreeHEP, CLHEP. Also important are packages that simulate particle physics interactions using Monte Carlo simulation techniques (i.e. event generators). Prominent examples of these include PYTHIA, Geant4 and its Fortran predecessor, GEANT.


== History ==
Particle physics played a role in the early history of the internet, the World-Wide Web was created by Tim Berners-Lee when working at CERN in 1991.


=== Computer Algebra ===
Note: This section contains an excerpt from 'Computer Algebra in Particle Physics' by Stefan Weinzierl
Particle physics is an important field of application for computer algebra and exploits the capabilities of Computer Algebra Systems (CAS). This leads to valuable feed-back for the development of CAS. Looking at the history of computer algebra systems, the first programs date back to the 1960s. The first systems were almost entirely based on LISP (""LISt Programming language""). LISP is an interpreted language and, as the name already indicates, designed for the manipulation of lists. Its importance for symbolic computer programs in the early days has been compared to the importance of FORTRAN for numerical programs in the same period. Already in this first period, the program REDUCE had some special features for the application to high energy physics. An exception to the LISP-based programs was SCHOONSHIP, written in assembler language by Martinus J. G. Veltman and specially designed for applications in particle physics. The use of assembler code lead to an incredible fast program (compared to the interpreted programs at that time) and allowed the calculation of more complex scattering processes in high energy physics. It has been claimed the program's importance was recognized in 1998 by awarding the half of the Nobel prize to Veltman. Also the program MACSYMA deserves to be mentioned explicitly, since it triggered important development with regard to algorithms. In the 1980s new computer algebra systems started to be written in C. This enabled the better exploitation of the resources of the computer (compared to the interpreted language LISP) and at the same time allowed to maintain portability (which would not have been possible in assembler language). This period marked also the appearance of the first commercial computer algebra system, among which Mathematica and Maple are the best known examples. In addition, also a few dedicated programs appeared, an example relevant to particle physics is the program FORM by J. Vermaseren as a (portable) successor to SCHOONSHIP. More recently issues of the maintainability of large projects became more and more important and the overall programming paradigma changed from procedural programming to object-oriented design. In terms of programming languages this was reflected by a move from C to C++. Following this change of paradigma, the library GiNaC was developed. The GiNac library allows symbolic calculations in C++.
Code generation for computer algebra can also be used in this area.


=== Lattice field theory ===
Lattice field theory was created by Kenneth Wilson in 1974. Simulation techniques were later developed from statistical mechanics.
Since the early 1980s, LQCD researchers have pioneered the use of massively parallel computers in large scientific applications, using virtually all available computing systems including traditional main-frames, large PC clusters, and high-performance systems. In addition, it has also been used as a benchmark for high-performance computing, starting with the IBM Blue Gene supercomputer.
Eventually national and regional QCD grids were created: LATFOR (continental Europe), UKQCD and USQCD. The ILDG (International Lattice Data Grid) is an international venture comprising grids from the UK, the US, Australia, Japan and Germany, and was formed in 2002.


== See also ==
Les Houches Accords
CHEP Conference
Computational physics


== References ==


== External links ==
Brown University. Computational High Energy Physics (CHEP) group page
International Research Network for Computational Particle Physics. Center for Computational Sciences, Univ. of Tsukuba, Japan.
History of computing at CERN"
153,Event Processing Technical Society,24736980,8747,"Event Processing Technical Society (EPTS) is an inclusive group of organizations and individuals aiming to increase awareness of event processing, foster topics for future standardization, and establish event processing as a separate academic discipline.


== Motivation ==
The goal of the EPTS is development of shared understanding of event processing terminology. The society believes that through communicating the shared understanding developed within the group it would become a catalyst for emergence of effective interoperation standards, would foster academic research, and creation of training curriculum. In turn it would lead to establishment of event processing as a discipline in its own right. The society is trying to follow example of the Database technology when relational theory provided theoretical foundation and homogenized the technology through introduction of Structured Query Language (SQL). The EPTS members hope that through combination of academic research, vendor experience and customer data they will be able develop a unified glossary, language, and architecture that would homogenize Event Processing in the similar way.


== Organization ==
The EPTS is organized into several working groups.


=== Use Case Working Group ===
The Use Case WG collects and documents variety of usage scenarios of event processing in broad spectrum of applications. in order to classify such applications; The group has already collected use cases from Enterprise Information Technology Management, Fraud Detection, Business Process Management, Health Care, and Stock Trading. They have also created a comprehensive questionnaire to capture various facets of use cases. This data is used as input by the Architecture working group


=== Architecture and Meta-Model Working Group ===
The Architecture WG attempts to build a reference architecture for event processing. Since 2009 the meta-model working group has been merged with the reference architecture working group. The Meta Model WG serves as a liaison to a number of standards bodies. Members of this group are usually members of standards organizations such as OASIS, W3C, RuleML, OMG, DMTF and others - see the Event Processing standards reference model. The first version of the EPTS reference architecture is published.


=== Language Analysis Working Group ===
The Language Analysis WG is collecting and organizing examples of various event processing languages used in industry and research in order to extract the language dimensions.


=== Interoperability Working Group ===
The Interoperability WG is studying requirements for interoperability. Its goal is to get to a set of agreed mechanisms that would allow interoperability between event processing systems produced by different vendors.


=== Glossary Working Group ===
The Glossary WG is developing a glossary of terms for Event Processing. The first version of the glossary is already published.
The glossary working group was led and its output was edited by Roy Schulte and David Luckham.


== History ==
The society started as an informal group in 2005/2006. It was formally launched as a consortium in June 2008. Membership of the consortium is based on a formal agreement defining IP ownership terms and rules of engagement. The society is governed by a Steering Committee consisting of founding members of the organization, representatives of major vendors and scientists. It is partner of the major scientific event processing conference: Distributed Event Based Systems (DEBS), the major scientific rules conference: International Web Rule Symposium (RuleML) and also launched two Dagstuhl seminars on event processing, one in May 2007, and the second was held in May 2010. The event processing community is still active, but the EPTS seems to be in a passive state at the moment. At least the website of the society is not active any more since mid of 2014.


== Conferences ==


== References ==


== External links ==
Event Processing Technical Society
Business Wire press release announcing EPTS
Complex Event Processing Forum
Real-time Business Insight Event Processing in Practice, Event Processing Online Magazine


=== Blogs ===
Complex Event Processing & Real Time Intelligence site with pointer to the society's forum
Blog of the EPTS chair Opher Etzion
Tibco Complex Event Processing Blog
EPTS and standardization"
154,OOPSLA,1157909,8737,"OOPSLA (Object-Oriented Programming, Systems, Languages & Applications) is an annual ACM research conference. OOPSLA mainly takes place in the United States, while the sister conference of OOPSLA, ECOOP, is typically held in Europe. It is operated by the Special Interest Group for Programming Languages (SIGPLAN) group of the Association for Computing Machinery (ACM).
OOPSLA is an annual conference covering topics related to object-oriented programming systems, languages and applications. Like other conferences, OOPSLA offers various tracks and many simultaneous sessions, and thus has a different meaning to different people. It is an academic conference, and draws doctoral students who present peer-reviewed papers. It also draws a number of non-academic attendees, many of whom present experience reports and conduct panels, workshops and tutorials.
OOPSLA has been instrumental in helping object-oriented programming develop into a mainstream programming paradigm. It has also helped incubate a number of related topics, including design patterns, refactoring, aspect-oriented programming, model-driven engineering, agile software development, and domain specific languages.
The first OOPSLA conference was held in Portland, Oregon in 1986. As of 2010, OOPSLA became a part of the SPLASH conference. The website states that ""SPLASH isn't just a new name for our favorite conference—SPLASH has a new charter and mission: To bring together practitioners and researchers who are passionate about software, programming, design, and software engineering to explore the frontiers of software and software practice."" SPLASH stands for Systems, Programming, Languages, and Applications: Software for Humanity. OOPSLA will be a premiere research conference for technical papers and presentations within SPLASH. This change was intended to serve as a framework for organizing and streamlining the efforts so that topics that would traditionally be presented at OOPSLA maintain their focus while allowing other conferences (within SPLASH) to highlight new trends and challenges in the world of software.


== Locations and organizers ==


== References ==


== External links ==
Official SPLASH website
Official website
Official OOPSLA history page"
155,MobileHCI,17511951,8727,"The Conference on Human-Computer Interaction with Mobile Devices and Services (MobileHCI) is a leading series of academic conferences in HCI and is sponsored by ACM SIGCHI, the Special Interest Group on Computer-Human Interaction. MobileHCI has been held annually since 1998 and has been an ACM SIGCHI sponsored conference since 2012 The conference is very competitive, with an acceptance rate of below 20% in 2017  from 25% in 2006 and 21.6% in 2009. MobileHCI 2011 was held in Stockholm, Sweden, and MobileHCI 2012 which was sponsored by SIGCHI held in San Francisco, USA.


== History ==

The MobileHCI series started in 1998 as a stand-alone Workshop on Human Computer Interaction with Mobile Devices organized by Chris Johnson and held at the University of Glasgow. In the following year the workshop was held in conjunction with the Interact conference and was organized by Stephen Brewster and Mark Dunlop. In 2001 MobileHCI was again organized by Brewster and Dunlop in association with a major conference. This was in conjunction with IHM-HCI in Lille, France.
In 2002 MobileHCI was held independently from an associated conference as a stand-alone symposium in Pisa, Italy, organized by Fabio Paternò. In 2003 the conference was organized by Luca Chittaro in Udine, Italy. In 2004 it was again organized by Brewster and Dunlop, this time at the University of Strathclyde. In the following years the conference took place in Austria, Finland, and Singapore. MobileHCI 2008 has been organized by Henri Ter Hofte from the Telematica Instituut in the Netherlands.
For 2008 the conference's steering committee agreed to award a prize for the most influential paper published at MobileHCI ten years ago. The price should recognises the longevity of impact papers from the first MobileHCI have had on the research community. The 2008 prize was awarded to Keith Cheverst for the paper Exploiting Context in HCI Design for Mobile Systems written together with Tom Rodden, Nigel Davies, and Alan Dix.
MobileHCI 2009 was organised by Fraunhofer FIT and University of Siegen, in cooperation with ACM SIGCHI and ACM SIGMOBILE. The general chair was Prof. Dr. Reinhard Oppermann from Fraunhofer Society FIT, and the program chairs were Dr. Markus Eisenhauer, Prof. Dr. Matthias Jarke, and Prof. Dr. Volker Wulf. The 2009 prize for the most influential paper from ten years ago was awarded to Albrecht Schmidt for his paper Implicit human-computer interaction through context. The acceptance rate was 24.2% for full papers and 18.5% for short papers.
The 12th MobileHCI took place in Lisboa, Portugal, from September 7–10, 2010. The conference's general chairs were Marco de Sá and Luís Carriço from the University of Lisboa. The theme of the conference was a mobile world for all. The acceptance rate was 20% for full papers and 22% overall.
MobileHCI 2011 took place in Stockholm, Sweden from 30 August to 2 September 2011. The 13th in the series was chaired by Markus Bylund (Swedish Institute of Computer Science) and Maria Holm (Mobile Life Centre) with Oskar Juhlin and Ylva Fernaeus also from Mobile Life Centre as programme chairs. The full paper acceptance rate was 27% with an overall 23%. The Most influential Paper from MobileHCI 2001 prize was awarded to Simon Holland for his paper AudioGPS: Spatial Audio Navigation with a Minimal Attention Interface.


== Topics ==
In its early years, the conference had a limited number of unspecific topics. The list of topics grew over the years.
Topics considered relevant to date are, for example, audio and speech interaction, designing Web sites for mobile devices, evaluation of mobile devices and services, and multimodal interaction. Examples of topics that emerged in the last years are Wearable Computing, Mobile social networks, and studies on the use of mobile devices for special target groups (e.g. seniors).


== Workshops ==
Since 2002 workshops have been held prior to the main conference. Workshops focus on specific topics related to the conference's main theme. To participate in a workshop it is often necessary to submit a paper and present it during the workshop. Usually around 20 persons participate in a workshop. Besides the presentations there is typically more room for discussions than during the main conference. Successful workshops are often repeated in the following years. Some examples are the workshops on HCI in Mobile Guides, Mobile Interaction with the Real World (MIRW), and Speech in Mobile and Pervasive Environments (SiMPE).


== Tutorials ==
Tutorial days have been held at Mobile HCI 2008 and 2009. After more than 10 years of Mobile HCI, providing an overview of the state of the art becomes more and more challenging. During the tutorial days, a number of well-known researchers in Mobile HCI gave overviews of the state of the art and cover many of the relevant topics. The tutorials also introduced the ""must read"" papers in this domain. The audience varied and included new students starting a PhD in Mobile HCI, practitioners wanting a quick survey of the state of the art and educators wishing to get an overview of Mobile HCI for their own teaching.


== External links ==
Website of the MobileHCI conference series
Website of the MobileHCI 2013 conference
Website of the MobileHCI 2012 conference
Website of the MobileHCI 2011 conference
Website of the MobileHCI 2010 conference
Website of the workshop HCI in Mobile Guides 2005
Website of the workshop Mobile Interaction with the Real World 2009
Mobile HCI 2009 tutorial day slides
Mobile HCI 2008 tutorial day slides
Website of the workshop Speech in Mobile and Pervasive Environments


== Notes and references =="
156,Programming Language Design and Implementation,1226359,8708,"Programming Language Design and Implementation (PLDI) is one of the ACM SIGPLAN's most important conferences. The precursor of PLDI was the Symposium on Compiler Optimization, held July 27–28, 1970 at the University of Illinois at Urbana-Champaign and chaired by Robert S. Northcote. That conference included papers by Frances E. Allen, John Cocke, Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. The first conference in the current PLDI series took place in 1979 under the name SIGPLAN Symposium on Compiler Construction in Denver, Colorado. The next Compiler Construction conference took place in 1982 in Boston, Massachusetts. The Compiler Construction conferences then alternated with SIGPLAN Conferences on Language Issues until 1988, when the conference was renamed to PLDI. From 1982 until 2001, the conference acronym was SIGPLAN 'xx. Starting in 2002, the initialism became PLDI 'xx, and in 2006 PLDI xxxx.


== Conference locations and organizers ==
PLDI 2015 - SIGPLAN Conference on Programming Language Design and Implementation: Portland, OR, United States
Conference Chair: Dave Grove
Program Chair: Steve Blackburn
Part of the Federated Computing Research Conference 2015

PLDI 2014 - SIGPLAN Conference on Programming Language Design and Implementation: Edinburgh, Scotland, United Kingdom
Conference Chair: Michael O'Boyle
Program Chair: Keshav Pingali

PLDI 2013 - SIGPLAN Conference on Programming Language Design and Implementation: Seattle, WA, United States
Conference Chair: Hans-J. Boehm
Program Chair: Cormac Flanagan

PLDI 2012 - SIGPLAN Conference on Programming Language Design and Implementation: Beijing, China
Conference Chairs: Jan Vitek, Haibo Lin
Program Chair: Frank Tip

PLDI 2011 - SIGPLAN Conference on Programming Language Design and Implementation: San Jose, CA, United States
Conference Chair: Mary Hall
Program Chair: David Padua
Part of the Federated Computing Research Conference 2011

PLDI 2010 - SIGPLAN Conference on Programming Language Design and Implementation: Toronto, ON, Canada
Conference Chair: Ben Zorn
Program Chair: Alex Aiken

PLDI 2009 - SIGPLAN Conference on Programming Language Design and Implementation: Dublin, Ireland
Conference Chair: Michael Hind
Program Chair: Amer Diwan

PLDI 2008 - SIGPLAN Conference on Programming Language Design and Implementation: Tucson, Arizona, USA
Conference Chair: Rajiv Gupta
Program Chair: [aman Amarasinghe

PLDI 2007 - SIGPLAN Conference on Programming Language Design and Implementation: San Diego, California, USA
Conference Chair: Jeanne Ferrante
Program Chair: Kathryn S. McKinley
Part of the Federated Computing Research Conference 2007

PLDI 2006 - SIGPLAN Conference on Programming Language Design and Implementation: Ottawa, Ontario, Canada
Conference Chair: Michael Schwartzbach
Program Chair: Thomas Ball

PLDI '05 - SIGPLAN Conference on Programming Language Design and Implementation: Chicago, Illinois, USA
Conference Chair: Vivek Sarkar
Program Chair: Mary Hall

PLDI '04 - SIGPLAN Conference on Programming Language Design and Implementation: Washington, D.C., USA
Conference Chair: William Pugh
Program Chair: Craig Chambers

PLDI 03 - SIGPLAN Conference on Programming Language Design and Implementation: San Diego, California, USA
Conference Chair: Ron Cytron
Program Chair: Rajiv Gupta
Part of the Federated Computing Research Conference 2003

PLDI '02 - SIGPLAN Conference on Programming Language Design and Implementation: Berlin, Germany
Conference Chair: Jens Knoop
Program Chair: Laurie Hendren

SIGPLAN '01 Conference on Programming Language Design and Implementation (PLDI): Snowbird, Utah, USA
Conference Chair: Michael Burke
Program Chair: Mary Lou Soffa

SIGPLAN '00 Conference on Programming Language Design and Implementation (PLDI): Vancouver, British Columbia, Canada
Conference Chair: James Larus
Program Chair: Monica Lam

SIGPLAN '99 Conference on Programming Language Design and Implementation (PLDI): Atlanta, Georgia, USA
Conference Chair: Barbara G. Ryder
Program Chair: Benjamin G. Zorn
Part of the Federated Computing Research Conference 1999

SIGPLAN '98 Conference on Programming Language Design and Implementation (PLDI): Montreal, Quebec, Canada
Conference Chair: Jack W. Davidson
Program Chair: Keith D. Cooper

SIGPLAN '97 Conference on Programming Language Design and Implementation (PLDI): Las Vegas, Nevada, USA
Conference Chair: Marina Chen
Program Chair: Ron K. Cytron

SIGPLAN '96] Conference on Programming Language Design and Implementation (PLDI): Philadelphia, Pennsylvania, USA
Conference Chair: Charles N. Fischer
Program Chair: Michael Burke
Part of the Federated Computing Research Conference 1996

SIGPLAN '95 Conference on Programming Language Design and Implementation (PLDI): La Jolla, California, USA
Conference Chair: David W. Wall
Program Chair: David R. Hanson

SIGPLAN '94 Conference on Programming Language Design and Implementation (PLDI): Orlando, Florida, USA
Conference co-Chairs: Barbara Ryder and Mary Lou Soffa
Program Chair: Vivek Sarkar

SIGPLAN '93 Conference on Programming Language Design and Implementation: Albuquerque, New Mexico, USA
Conference Chair: Robert Cartwright
Program Chair: David W. Wall

SIGPLAN '92 Conference on Programming Language Design and Implementation: San Francisco, California
Conference Chair: Stuart I. Feldman
Program Chair: Christopher W. Fraser

SIGPLAN '91 Conference on Programming Language Design and Implementation: Toronto, Ontario, Canada
Conference Chair: Brent Hailpern
Program Chair: Barbara G. Ryder

SIGPLAN '90 Conference on Programming Language Design and Implementation: White Plains, New York, USA
Conference Chair: Mark Scott Johnson
Program Chair: Bernard Lang

SIGPLAN '89 Conference on Programming Language Design and Implementation: Portland, Oregon, USA
Conference Chair: Bruce Knobe
Program Chair: Charles N. Fischer

SIGPLAN '88 Conference on Programming Language Design and Implementation: Atlanta, Georgia, USA
Conference Chair: David S. Wise
Program Chair: Mayer D. Schwartz

SIGPLAN '87 Symposium on Interpreters and Interpretive Techniques: St. Paul, Minnesota, USA
Conference Chair: Mark Scott Johnson
Program Chair: Thomas Turba

SIGPLAN '86 Symposium on Compiler Construction: Palo Alto, California, USA
Conference Chair: John R. Sopka
Program Chair: Jeanne Ferrante

SIGPLAN '85 Symposium on Language Issues in Programming Environments: Seattle, Washington, USA
Conference Chair: Teri Payton
Program Chair: L. Peter Deutsch

SIGPLAN '84 Symposium on Compiler Construction: Montreal, Quebec, Canada
Conference Chair: Mary Van Deusen
Program Chair: Susan L. Graham

SIGPLAN '83 Symposium on Programming Language Issues in Software Systems: San Francisco, California, USA
Conference Chair: John R. White
Program Chair: Lawrence A. Rowe

SIGPLAN '82 Symposium on Compiler Construction: Boston, Massachusetts, USA
Conference Chair: John R. White
Program Chair: Frances E. Allen

SIGPLAN Symposium on Compiler Construction 1979: Denver, Colorado, USA
SIGPLAN Symposium on Compiler Optimization 1970: Urbana-Champaign, Illinois, USA


== External links ==
Official website
DBLP bibliography for PLDI"
157,Marr Prize,19521901,8687,"The Marr Prize is a biennial conference award in computer vision given by the International Conference on Computer Vision. Named after David Marr, the Marr Prize is one of the top honors for a computer vision researcher [1].


== Papers ==
The list of papers that won the Marr Prize from 1987 are listed below.


=== 1st ICCV, 1987, London, United Kingdom ===
Marr Prize Paper: David Heeger, Optical Flow using Spatiotemporal Filters
Marr Prize Honorable Mention Papers:
John Tsotsos, A `Complexity Level' Analysis of Immediate Vision
Michael Kass, Andrew Witkin, and Demetri Terzopoulos, Snakes: Active Contour Models
Yiannis Aloimonos and Issac Weiss, Active Vision


=== 2nd ICCV, 1988, Tampa, U.S.A. ===
Marr Prize Paper: Brian Funt and Jian Ho, Color from Black and White
Marr Prize Honorable Mention Papers
David Lowe, Organization of Smooth Image Curves at Multiple Scales
Vishvjit Nalwa, Representing Oriented Piecewise C2 Surfaces


=== 3rd ICCV, 1990, Osaka, Japan ===
Marr Prize Paper: Shree K. Nayar, Katsushi Ikeuchi, and Takeo Kanade, Shape from Interreflections


=== 4th ICCV, 1993, Berlin, Germany ===
Charles A. Rothwell, David A. Forsyth, Andrew Zisserman, and Joseph L. Mundy, Extracting Projective Structure from Single Perspective Views of 3D Point Sets


=== 5th ICCV, 1995, Cambridge, U.S.A. ===
Marr Prize Papers
Michael Oren and Shree K. Nayar, A Theory of Specular Surface Geometry
Toshikazu Wada, Hiroyuki Ukida, and Takashi Matsuyama, Shape from Shading with Interreflections under a Proximal Light Source: Distortion-Free Copying of an Unfolded Book

Marr Prize Honorable Mention Papers
Paul Viola and William Wells III, Alignment by Maximization of Mutual Information
Anders Heyden, Reconstruction from Image Sequences by Means of Relative Depths
Yalin Xiong and Steven Shafer, Hypergeometric Filters for Optical Flow and Affine Matching


=== 6th ICCV, 1998, Bombay, India ===
Marr Prize Papers
Marc Pollefeys, Reinhard Koch, and Luc Van Gool, Self-Calibration and Metric Reconstruction in spite of Varying and Unknown Internal Camera Parameters
Phil Torr, Andrew Fitzgibbon, and Andrew Zisserman, The Problem of Degeneracy in Structure and Motion Recovery from Uncalibrated Image Sequences

Marr Prize Honorable Mention Paper: Richard Szeliski and Polina Golland, Stereo Matching with Transparency and Matting


=== 7th ICCV, 1999, Kerkyra, Greece ===
Marr Prize Papers
Kiriakos Kutulakos and Steven Seitz, A Theory of Shape by Space Carving
Yi Ma, Stefano Soatto, Jana Kosecka, and Shankar Sastry, Euclidean Reconstruction and Reprojection up to Subgroups

Marr Prize Honorable Mention Papers
Michael Black and David Fleet, Probabilistic Detection and Tracking of Motion Discontinuities
Ying Nian Wu and Song-Chun Zhu, Equivalence of Texture Modeling and Analysis?


=== 8th ICCV, 2001, Vancouver, Canada ===
Marr Prize Papers
Kentaro Toyama and Andrew Blake, Probabilistic Tracking with Exemplars in a Metric Space
Steven Seitz, The Space of All Stereo Images

Marr Prize Honorable Mention Papers
Yaron Caspi and Michal Irani, Alignment of Non-Overlapping Sequences
Lior Wolf and Amnon Shashua, On Projection Matrices and their Applications in Computer Vision


=== 9th ICCV, 2003, Nice, France ===
Marr Prize Papers
Andrew Fitzgibbon, Yonatan Wexler, and Andrew Zisserman, Image-based Rendering using Image-based Priors
Zhuowen Tu, Xiangrong Chen, Alan L. Yuille, and Song-Chun Zhu, Image Parsing: Unifying Segmentation, Detection and Recognition
Paul Viola, Michael J. Jones, and Daniel Snow, Detecting Pedestrians using Patterns of Motion and Appearance


=== 10th ICCV, 2005, Beijing, China ===
Marr Prize Paper: ""Globally Optimal Estimates for Geometric Reconstruction Problems"", Fredrik Kahl, Didier Henrion
Honorable Mention
""A Theory of Refractive and Specular Shape by Light-Path Triangulation"", Kiriakos N. Kutulakos, Eron Steger
""Detecting Irregularities in Images and in Video"", Oren Boiman, Michal Irani
""On the Spatial Statistics of Optical Flow"", Stefan Roth, Michael J. Black


=== 11th ICCV, 2007, Rio de Janeiro, Brazil ===
Marr Prize Paper: Bradley Davis, P. Thomas Fletcher, Elizabeth Bullitt, Sarang Joshi: Population Shape Regression From Random Design Data
Honorable Mention:
Ying Nian Wu, Zhangzhang Si, Chuck Fleming, Song-Chun Zhu: Deformable Template As Active Basis
Abhijeet Ghosh, Shruthi Achutha, Wolfgang Heidrich, Matthew O'Toole: BRDF Acquisition with Basis Illumination
Manmohan Chandraker, Sameer Agarwal, David Kriegman, Serge Belongie: Globally Optimal Affine and Metric Upgrades in Stratified Autocalibration


=== 12th ICCV, 2009, Kyoto, Japan ===
Marr Prize Paper: Chaitanya Desai, Deva Ramanan, Charless Fowlkes. ""Discriminative models for multi-class object layout""
Marr Prize Honorable Mention Paper: Ahmed Kirmani, Tyler Hutchison, James Davis, Ramesh Raskar. ""Looking Around the corner using Transient Imaging""


=== 13th ICCV, 2011, Barcelona, Spain ===
Marr Prize Paper: Devi Parikh, Kristen Grauman: ""Relative Attributes""


=== 14th ICCV, 2013, Sydney, Australia ===
Marr Prize Paper:
Vicente Ordonez, Jia Deng, Yejin Choi, Alexander C. Berg, and Tamara L. Berg. ""From Large Scale Image Categorization to Entry-Level Categories""

Marr Prize Honorable Mention:
Yuandong Tian and Srinivasa Narasimhan. ""Hierarchical Data-driven Descent for Efficient Optimal Deformation Estimation""
Christoph Vogel, Konrad Schindler, Stefan Roth. ""Piecewise Rigid Scene Flow""


=== 15th ICCV, 2015, Santiago, Chile ===
Marr Prize Paper:
Peter Kontschieder, Madalina Fiterau, Antonio Criminisi, and Samuel Rota Bulò. ""Deep Neural Decision Forests"", supplementary material

Marr Prize Honorable Mention:
Saining Xie and Zhuowen Tu. ""Holistically-Nested Edge Detection""


=== 16th ICCV, 2017, Venice, Italy ===
Marr Prize Paper:
Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. ""Mask R-CNN""

Marr Prize Honorable Mention:
Nicholas Rhinehart and Kris M. Kitani. ""First Person Activity Forecasting with Online Inverse Reinforcement Learning""
Pau Panareda Busto and Juergen Gall ""Open Set Domain Adaptation""
Dylan Campbell, Lars Petersson, Laurent Kneip, and Hongdong Li, Globally-Optimal Inlier Set Maximisation for Simultaneous Camera Pose and Feature Correspondence""


== References ==


== External links ==
Award history
ICCV 2009 prize website"
158,Computational social science,40077102,8670,"Computational social science refers to the academic sub-disciplines concerned with computational approaches to the social sciences. This means that computers are used to model, simulate, and analyze social phenomena. Fields include computational economics, computational sociology, cliodynamics, culturomics, and the automated analysis of contents, in social and traditional media. It focuses on investigating social and behavioral relationships and interactions through social simulation, modeling, network analysis, and media analysis.


== Definitions ==
There are two terminologies that relate to each other: Social Science Computing (SSC) and Computational Social Science (CSS). In literature, CSS is referred to the field of social science that uses the computational approaches in studying the social phenomena. On the other hand, SSC is the field in which computational methodologies are created to assist in explanations of social phenomena.
Computational social science revolutionizes both fundamental legs of the scientific method: empirical research, especially through big data, by analyzing the digital footprint left behind through social online activities; and scientific theory, especially through computer simulation model building through social simulation. It is a multi-disciplinary and integrated approach to social survey focusing on information processing by means of advanced information technology. The computational tasks include the analysis of social networks, social geographic systems, social media content and traditional media content.
Computational social science work increasingly relies on the greater availability of large databases, currently constructed and maintained by a number of interdisciplinary projects, including:
The Seshat: Global History Databank, which systematically collects state-of-the-art accounts of the political and social organization of human groups and how societies have evolved through time into an authoritative databank. Seshat is affiliated also with the Evolution Institute, a non-profit think-tank that ""uses evolutionary science to solve real-world problems.""
D-PLACE: the Database of Places, Languages, Culture and Environment, which provides data on over 1,400 human social formations
The Atlas of Cultural Evolution, an archaeological database created by Peter N. Peregrine
CHIA: The Collaborative Information for Historical Analysis, a multidisciplinary collaborative endeavor hosted by the University of Pittsburgh with the goal of archiving historical information and linking data as well as academic/research institutions around the globe
International Institute of Social History, which collects data on the global social history of labour relations, workers, and labour
Human Relations Area Files eHRAF Archaeology
Human Relations Area Files eHRAF World Cultures
Clio-Infra a database of measures of economic performance and other aspects of societal well-being on a global sample of societies from 1800 CE to the present
The Google Ngram Viewer, an online search engine that charts frequencies of sets of comma-delimited search strings using a yearly count of n-grams as found in the largest online body of human knowledge, the Google Books corpus.
The analysis of vast quantities of historical newspaper content has been pioneered in, while other studies on similar data showed how periodic structures can be automatically discovered in historical newspapers. A similar analysis was performed on social media, again revealing strongly periodic structures.


== See also ==
Cliodynamics
Seshat (project)
Computational cognition
Digital sociology
Social web
Social network analysis

Online content analysis
Predictive analytics


== References ==


== External links ==
ESSA: European Social Simulation Association portal
PAAA: Pan-Asian Association for Agent-based Approach in Social Systems Sciences
CSSSA: Computational Social Science Society of the Americas
International Conference on Computational Social Science 2015
""Life in the network: the coming age of computational social science"". Retrieved June 10, 2015.
Seshat"
159,Bachelor in Information Management,21002817,8539,"A Bachelor in Information Management (BIM) degree is an academic degree in Information technology requiring four years of study to acquire. This degree is a hybrid program with a mix of management and information technology courses with a focus on analytical, problem solving, decision-making and critical thinking skills.


== Objective ==
Prepare IT professionals proficient in the use of computers and computational techniques in order to develop effective information systems to solve real life problems in the organizational milieu.
Develop students' skill in object-oriented software design methods and data management systems.
Provide professional training to students by combining information technology with managerial skills.
Prepare students to proceed on to post graduate level study in information management within and outside the country.
Develop students' skill in object-oriented software design methods and data management systems.
Provide professional training to students by combining information technology with managerial skills.
Prepare students to proceed on to post graduate level study in information management within and outside the country.


== Careers ==
Graduating with a bachelors in Information Management highly beneficial as it opens up a plethora of career options and routes for students. This is manly due to the fact that employers now-a-days show a growing interest for applicants with both a business management education as well as an IT background. Paul Matthews from the Institute of IT Professionals stated that ""Employers tell me the key thing holding them back is the ability to get skilled people"". This degree would be beneficial as it is employable all over world. Every company has an IT department, which even in non-tech firms, play an essential role in the company's daily functions.
The fine combination of IT and management that this degree offers, has shown career prospects in fields such as:
Software Designers and Engineers
Advertising and Marketing Executives
IT Consultants and Planners
Sales and retail assistants
Chartered Accountants
Computer programmers and programmer developers
As well as many others
Graduates from these types of degrees have also found themselves working for some of the world's top organisations including Ernst and Young, Deloitte, Goldman Sachs, and Bruder Mannesmann 


== Post Graduate Programs ==
Keep in mind that although you can start a career straight after your bachelors, you may also be interested in furthering your study by enrolling in a masters program. Employers seem to be finding that business management undergraduates are too keen on starting work immediately, without actually having attained the essential skills need for a professional workplace. This may be in terms of general maturity and also the students ability to 'hit the ground running'. This is where masters students have the greater advantage.
There are many different masters programs available to graduates from an Information Management degree. However at this point, the studies tend to narrow down to more specific areas. As you have a solid background in both Business Management and IT, you have a large range to choose from.
If after studying you seem to be more keen in IT then perhaps you should consider doing a masters in either computer science or information systems.These two programs are some of the best for finding postgraduate jobs with high earning salaries.


== Student life ==
Information Management students enrolled in a full-time degree generally have 8–13 hours a week in class, whether its lectures, tutorials, or seminars. However time must be spend outside of class working on coursework and assignments as well as doing prerequisite readings for certain modules.
One of the hardest parts about being a student is learning how to survive on a budget. To compensate for this, many students choose to get a part-time job while studying. In the United States, about 50% of students have a part-time job whilst enrolled in a full-time degree. However it becomes increasing difficult to balance a part-time job along with course work, classes, good grades, self study time, and social life. Often self study time suffers the most from part-time jobs. Therefore it would be recommended to work no more than 10 hours a week.


== Sources and external links ==


== References ==


== External links =="
160,Enlaces,20670767,8494,"Enlaces is a Chilean educational program designed to create a structural change in Chilean education in order to prepare youth, along with their parents and guardians, to participate in the emergent society of knowledge, and to create networks of communication that help integrate them with the world. Enlaces was started in 1992 by Chile's Ministry of Education.


== Test phase ==
The testing phase of this program began in 12 schools in Santiago, the capital of Chile. After 3 years the program had extended to the Ninth Region, the Araucanía Region, (the part of the country with the largest indigenous population) with more than 100 schools participating in the program. When the government decided that this project was viable, it focused all of its efforts on the spread of this program to all the regions of the country.


== First phase ==
From 1995-2007 (the unofficial First Phase of the Enlaces project), the government spent more than $121 million and international cooperation from organizations such as the Canadian International Development Agency (CIDA) in global education, infrastructure, computers, access to educational recourses, technical assistance, and teacher training. In 2003, in order to continue their efforts to connect the country, the Ministry of Education signed an agreement with local telecommunication companies to offer low-cost broadband connection for educational institutions. Sixty percent of the students in Chile have access to broadband internet because of this agreement. In 2007, the Enlaces Program had 3,372,943 students, and 95% of the students in the country have access to computer.


== Future plans ==
In 2007, the Chilean Ministry of Education presented ExpoEnlaces 2007 where they showcased the latest innovations in Information and Communication Technology (ICT) so that people can understand the abilities of these new technologies and their uses in the classroom. That same year, the Ministry of Education published its new objectives and projects for the coming years. Didier de Saint Pierre, the executive director of Enlaces, divided the program into two phases, the first phase, which is coming to a close, focused on infrastructure in the schools and equipping them to install the necessary programs and administration. The second phase involves developing the technological infrastructure so that it has the capacity to improve the quality of learning in Chile. Once the basic technological infrastructure is in place, the Ministry will extend the technology to the remaining schools in less developed regions, and it will implement new technologies in the more advanced regions.
Examples of the new technologies that Enlaces will implement are electronic whiteboards for math, palm pilots or pocket PCs for physics, and projectors for teaching sciences. Other new strategies will include ICT in the classroom, where they will integrate portable technology and projectors in 3,000 classrooms, benefiting 87,000 students (3% of the total number of students). The goal is to implement this program in 16,000 classrooms by 2010.
The science program is a good example of the direction in which the Enlaces program is headed. A very important aspect of this program is teaching style. Enlaces encourages the use of the HEI (hypothesis, experimentation, instruction) method. The new science classes are being developed in a multimedia format using simulations delivered via the internet.
This new program offers students many benefits. The classes that follow this methodology better prepare students for lessons and projects. When presented with an experiment, students can form a hypothesis, the teacher can demonstrate every aspect of the experiment using a computer, a projection system, and the internet, and the students can follow the other steps of the scientific method. While the use of simulations is not the exactly the same as doing the physical experiment, simulation eliminates the problem of lack of equipment and materials (such as chemicals for chemistry, and animals for dissection for biology) that many schools in Chile face.


=== Adult Education ===
Part of the objective of the Enlaces program is to help the community through adult education. The idea was that children would bring the technology home with them, but this has not worked the way the Ministry had hoped. The majority of parents (especially those in rural areas) recognize the benefits of technology for their children, but feel that they cannot benefit personally. For adults who do want to learn more about technology, the Enlaces program offers an 18-hour digital literacy class where they learn the basic functions of a computer, how to use a word processor, how to use the internet, and how to create and use an email account.


== See also ==
Education in Chile


== References =="
161,Workshop on Information Technologies and Systems,29777138,8485,"The Workshop on Information Technologies and Systems - WITS. is an academic conference for information systems that is held annually in December in conjunction with ICIS (the International Conference on Information Systems). WITS is quantitatively/technically-oriented and is primarily attended by business school professors from leading academic research institutions in North America with growing participation from throughout the world. WITS is incorporated in the state of Georgia (US).


== Origins ==
Richard Wang (MIT) and Sudha Ram (University of Arizona) started WITS in 1991. They co-chaired the first conference held in Boston on December 14–15, 1991. Also involved in the early conferences were Sal March (Vanderbilt University), Prabuddha De (Purdue University), Al Hevner (University of South Florida), Stuart Madnick (MIT), Veda Storey (Georgia State University), Diane Strong (Worcester Polytechnic Institute), Andrew Whinston (University of Texas at Austin), Carson Woo (University of British Columbia), and others.


== WITS Presidents ==
2016-2018: Ram Gopal, (University of Connecticut), USA.
2013-2015: Sumit Sarkar, (University of Texas at Dallas), USA.
2010–2012: Jeffrey Parsons, (Memorial University of Newfoundland), Canada.
2007–2009: Paulo Goes, (University of Arizona), USA.
2004–2006: Carson Woo, (University of British Columbia), Canada.
2001–2003: Sal March,(Vanderbilt University), USA.
1998–2000: Prabuddha De, (Purdue University), USA.
1995–1997: Richard Wang, (Massachusetts Institute of Technology), USA.


== WITS Locations and Conference Chairs ==
WITS 2017: Seoul, South Korea— Raghu Santanam (Arizona State University) & Victoria Yoon (Virginia Commonwealth University)
WITS 2016: Dublin, Ireland— Wolfgang Ketter (Erasmus University) & Balaji Padmanabhan (University of South Florida)
WITS 2015: Dallas, USA— Varghese Jacob (University of Texas at Dallas) & Subodha Kumar (Texas A&M University)
WITS 2014: Auckland, New Zealand— Yong Tan (University of Washington) & Arvind Tripathi (University of Auckland)
WITS 2013: Milano, Italy— Raj Sharman (University at Buffalo) & Sandeep Purao (Pennsylvania State University)
WITS 2012: Orlando, Florida — Haldun Aytug (University of Florida) & Jackie Rees Ulmer (Purdue University)
WITS 2011: Shanghai, China – Roger Chiang (University of Cincinnati) & Andrew Gemino (Simon Fraser University)
WITS 2010: St. Louis, Missouri — Erik Rolland & Raymond A. Patterson (University of Calgary)
WITS 2009: Phoenix, Arizona — Huimin Zhao, (University of Wisconsin-Milwaukee) & Vijay Khatri, (Indiana University)
WITS 2008: Paris, France — Ram Gopal (University of Connecticut) & R. Ramesh
WITS 2007: Montreal, Canada — Kaushal Chari (University of South Florida) & Akhil Kumar (Pennsylvania State University)
WITS 2006: Milwaukee, Wisconsin — Ramesh Venkataraman, (Indiana University) & Atish Sinha
WITS 2005: Las Vegas, Nevada – Kar Yan Tam & J. Leon Zhao
WITS 2004: Washington, DC – Paulo Goes, (University of Arizona) & Amitava Dutta
WITS 2003: Seattle, Washington – Deb Dey (University of Washington) & Ramayya Krishnan (Carnegie Mellon University)
WITS 2002: Barcelona, Spain – Amit Basu (Southern Methodist University) & Soumitra Dutta
WITS 2001: New Orleans, LA — Jeffrey Parsons, (Memorial University of Newfoundland) & Olivia Sheng
WITS 2000: Brisbane, Australia – Paul Bowen and Vijay Mookerjee, (University of Texas at Dallas)
WITS 1999: Charlotte, North Carolina — Sridhar Narasimhan, (Georgia Institute of Technology) & Sumit Sarkar, (University of Texas at Dallas)
WITS 1998: Helsinki, Finland – Janis Bubenko and Sal March,(Vanderbilt University)
WITS 1997: Atlanta, Georgia – Arie Segev & Vijay K. Vaishnavi
WITS 1996: Cleveland, Ohio — Arun Sen (Texas A&M University) & George Ernst
WITS 1995: Amsterdam, The Netherlands – Sudha Ram, (University of Arizona) and Matthias Jarke
WITS 1994: Vancouver, BC, Canada — Prabuddha De, (Purdue University) & Carson Woo
WITS 1993: Orlando, Florida – Al Hevner & Nabil Kamel
WITS 1992: Dallas, Texas — Andrew Whinston (University of Texas at Austin) & Veda Storey
WITS 1991: Cambridge, MA — Richard Wang, (Massachusetts Institute of Technology) & Sudha Ram, (University of Arizona)


== References =="
162,Canadian Information Processing Society,4756951,8338,"The Canadian Information Processing Society (CIPS) is legally recognized as the association that advances and self-regulates the Information Technology profession in Canada. The association certifies professionals against a standard of practice and competency, assigning the designation that recognizes the level of experience of the IT Professional holder in the eyes of the general public:
Beginner: Associate Information Technology Professional (AITP)
Seasoned: Information Systems Professional (I.S.P.) / Business Technology Management
Expert: Information Technology Certified Professional (ITCP)
The society also performs accreditation of computer science, software engineering and Business Technology Management programs, as well as college/technical institute level programs in Computer Systems Technology, Applied Information Technology, and finally, Post-Diploma type programs at Canadian universities. CIPS is responsible for defining the Canadian IT Body of Knowledge, and for enforcing a Code of Ethics and Professional Conduct.


== History ==
Calvin Gotlieb helped found CIPS in 1958, serving as its president from 1960 to 1961. Calvin was elected as founding fellow in 2006. The first President of CIPS was Fred Thomas serving in 1958 to 1959.
Prime Minister of Canada Stephen Harper provided a message to CIPS on the 50th anniversary.


== Organization ==
Each province has a provincial body that administers the legislation or regulation establishing the self-regulating professional body. This consists of the following bodies:
CIPS Alberta & North West Territories
CIPS British Columbia & Yukon
CIPS Manitoba
CIPS New Brunswick
CIPS Newfoundland and Labrador & Nunavut
CIPS Nova Scotia
CIPS Ontario
CIPS Prince Edward Island
CIPS Saskatchewan
In Québec, CIPS is partnered with the Réseau ACTION TI.
CIPS is the founding member organisation of the International Federation for Information Processing (IFIP). IFIP works on establishing international standards for information technology and software engineering. CIPS is also a member of South East Asia Regional Computer Confederation (SEARCC) and a founding member of IFIP IP3. CIPS is also a constiuent member of the ICCP, . which is the Institute for Certification of Computing Professionals, based out of the USA, and dedicated to the establishment of high professional standards for the computer industry across North America.
CIPS is also a member organization of the Federation of Enterprise Architecture Professional Organizations (FEAPO), a worldwide association of professional organizations which have come together to provide a forum to standardize, professionalize, and otherwise advance the discipline of Enterprise Architecture.


== Legislation ==
Alberta: Professional and Occupational Associations Registration Act, R.S.A. 2000, c. P-26, Information Systems Professional Regulation, Alta. Reg. 39/1997
British Columbia: Society Act, R.S.B.C. 1996, s.s. 86-93 (Occupational Titles Protection)
New Brunswick: Canadian Information Processing Society of New Brunswick Act, 2001, c. 49
Nova Scotia: Canadian Information Processing Society of Nova Scotia Act, S.N.S. 2002, c. 3
Ontario: An Act respecting Canadian Information Processing Society of Ontario, 1998, c. Pr21
Saskatchewan: Canadian Information Processing Society of Saskatchewan Act, S.S. 2005, c. C-0.2


== See also ==
Association for Computing Machinery (ACM) (related organisation in USA)
Australian Computer Society (related organisation in Australia)
IEEE Computer Society (IEEE CS)
Institution of Engineering and Technology (IET)
International Federation for Information Processing (IFIP)
Mountbatten Medal
New Zealand Computer Society (related organisation in New Zealand)
Computer Society of Southern Africa (CSSA) (related organisation in South Africa)
Brazilian Computer Society (SBC)
British Computer Society (BCS) (related organisation in United Kingdom)


== References ==


== External links ==
Canadian Information Processing Society website
Réseau ACTION TI website
CIPS National Presidents 1958 to 2008
CIPS Fellows
CIPS Awards and Hall of Fame"
163,Expression problem,22935957,8311,"The expression problem is a term used in discussing strengths and weaknesses of various programming paradigms and programming languages.
Philip Wadler coined the term in response to a discussion with Rice University's Programming Languages Team (PLT):

The expression problem is a new name for an old problem. The goal is to define a datatype by cases, where one can add new cases to the datatype and new functions over the datatype, without recompiling existing code, and while retaining static type safety (e.g., no casts).


== History ==
At ECOOP '98, Krishnamurthi et al. presented a design pattern solution to the problem of simultaneously extending an expression-oriented programming language and its tool-set. They dubbed it the ""expressivity problem"" because they thought programming language designers could use the problem to demonstrate the expressive power of their creations. For PLT, the problem had shown up in the construction of DrScheme, now DrRacket, and they solved it via a rediscovery of mixins. To avoid using a programming language problem in a paper about programming languages, Krishnamurthi et al. used an old geometry programming problem to explain their pattern-oriented solution. In conversations with Felleisen and Krishnamurthi after the ECOOP presentation, Wadler understood the PL-centric nature of the problem and he pointed out that Krishnamurthi's solution used a cast to circumvent Java's type system. The discussion continued on the types mailing list, where Corky Cartwright (Rice) and Kim Bruce (Williams) showed how type systems for OO languages might eliminate this cast. In response Wadler formulated his essay and stated the challenge, ""whether a language can solve the expression problem is a salient indicator of its capacity for expression."" The label ""expression problem"" puns on expression = ""how much can your language express"" and expression = ""the terms you are trying to represent are language expressions"".
Others co-discovered variants of the expression problem around the same time as Rice University's PLT, in particular Thomas Kühne in his dissertation, and Smaragdakis and Batory in a parallel ECOOP 98 article.
Some follow-up work used the expression problem to showcase the power of programming language designs.
The expression problem is also a fundamental problem in multi-dimensional Software Product Line design and in particular as an application or special case of FOSD Program Cubes.


== Solutions ==
There are various solutions to the expression problem. Each solution varies in the amount of code a user must write to implement them, and the language features they require.
Multimethods
Open classes
Coproducts of functors
Type classes
Tagless-final / Object algebras
Polymorphic Variants


== See also ==
Applications of FOSD Program Cubes
Generic programming
POPLmark challenge


== References ==


== Further reading ==
John C. Reynolds (1975). ""User-defined types and procedural data structures as complementary approaches to data abstraction"". New Directions in Algorithmic Languages, pp. 157–168.


== External links ==
The Expression Problem by Philip Wadler.
Lecture: The Expression Problem by Ralf Lämmell.
C9 Lectures: Dr. Ralf Lämmel - Advanced Functional Programming - The Expression Problem at Channel 9,
Independently Extensible Solutions to the Expression Problem, Matthias Zenger and Martin Odersky, EPFL Lausanne"
164,Lempel-Ziv complexity,54061907,8268,"Lempel–Ziv–Welch (LZW) is a universal lossless data compression algorithm created by Abraham Lempel, Jacob Ziv, and Terry Welch. It was published by Welch in 1984 as an improved implementation of the LZ78 algorithm published by Lempel and Ziv in 1978. The algorithm is simple to implement and has the potential for very high throughput in hardware implementations. It is the algorithm of the widely used Unix file compression utility compress and is used in the GIF image format.


== Algorithm ==
The scenario described by Welch's 1984 paper encodes sequences of 8-bit data as fixed-length 12-bit codes. The codes from 0 to 255 represent 1-character sequences consisting of the corresponding 8-bit character, and the codes 256 through 4095 are created in a dictionary for sequences encountered in the data as it is encoded. At each stage in compression, input bytes are gathered into a sequence until the next character would make a sequence for which there is no code yet in the dictionary. The code for the sequence (without that character) is added to the output, and a new code (for the sequence with that character) is added to the dictionary.
The idea was quickly adapted to other situations. In an image based on a color table, for example, the natural character alphabet is the set of color table indexes, and in the 1980s, many images had small color tables (on the order of 16 colors). For such a reduced alphabet, the full 12-bit codes yielded poor compression unless the image was large, so the idea of a variable-width code was introduced: codes typically start one bit wider than the symbols being encoded, and as each code size is used up, the code width increases by 1 bit, up to some prescribed maximum (typically 12 bits). When the maximum code value is reached, encoding proceeds using the existing table, but new codes are not generated for addition to the table.
Further refinements include reserving a code to indicate that the code table should be cleared and restored to its initial state (a ""clear code"", typically the first value immediately after the values for the individual alphabet characters), and a code to indicate the end of data (a ""stop code"", typically one greater than the clear code). The clear code allows the table to be reinitialized after it fills up, which lets the encoding adapt to changing patterns in the input data. Smart encoders can monitor the compression efficiency and clear the table whenever the existing table no longer matches the input well.
Since the codes are added in a manner determined by the data, the decoder mimics building the table as it sees the resulting codes. It is critical that the encoder and decoder agree on which variety of LZW is being used: the size of the alphabet, the maximum table size (and code width), whether variable-width encoding is being used, the initial code size, whether to use the clear and stop codes (and what values they have). Most formats that employ LZW build this information into the format specification or provide explicit fields for them in a compression header for the data.


=== Encoding ===
A high level view of the encoding algorithm is shown here:
Initialize the dictionary to contain all strings of length one.
Find the longest string W in the dictionary that matches the current input.
Emit the dictionary index for W to output and remove W from the input.
Add W followed by the next symbol in the input to the dictionary.
Go to Step 2.
A dictionary is initialized to contain the single-character strings corresponding to all the possible input characters (and nothing else except the clear and stop codes if they're being used). The algorithm works by scanning through the input string for successively longer substrings until it finds one that is not in the dictionary. When such a string is found, the index for the string without the last character (i.e., the longest substring that is in the dictionary) is retrieved from the dictionary and sent to output, and the new string (including the last character) is added to the dictionary with the next available code. The last input character is then used as the next starting point to scan for substrings.
In this way, successively longer strings are registered in the dictionary and made available for subsequent encoding as single output values. The algorithm works best on data with repeated patterns, so the initial parts of a message will see little compression. As the message grows, however, the compression ratio tends asymptotically to the maximum.


=== Decoding ===
The decoding algorithm works by reading a value from the encoded input and outputting the corresponding string from the initialized dictionary. In order to rebuild the dictionary in the same way as it was built during encoding, it also obtains the next value from the input and adds to the dictionary the concatenation of the current string and the first character of the string obtained by decoding the next input value, or the first character of the string just output if the next value can not be decoded (If the next value is unknown to the decoder, then it must be the value that will be added to the dictionary this iteration, and so its first character must be the same as the first character of the current string being sent to decoded output). The decoder then proceeds to the next input value (which was already read in as the ""next value"" in the previous pass) and repeats the process until there is no more input, at which point the final input value is decoded without any more additions to the dictionary.
In this way the decoder builds up a dictionary which is identical to that used by the encoder, and uses it to decode subsequent input values. Thus the full dictionary does not need to be sent with the encoded data; just the initial dictionary containing the single-character strings is sufficient (and is typically defined beforehand within the encoder and decoder rather than being explicitly sent with the encoded data.)


=== Variable-width codes ===
If variable-width codes are being used, the encoder and decoder must be careful to change the width at the same points in the encoded data, or they will disagree about where the boundaries between individual codes fall in the stream. In the standard version, the encoder increases the width from p to p + 1 when a sequence ω + s is encountered that is not in the table (so that a code must be added for it) but the next available code in the table is 2p (the first code requiring p + 1 bits). The encoder emits the code for ω at width p (since that code does not require p + 1 bits), and then increases the code width so that the next code emitted will be p + 1 bits wide.
The decoder is always one code behind the encoder in building the table, so when it sees the code for ω, it will generate an entry for code 2p − 1. Since this is the point where the encoder will increase the code width, the decoder must increase the width here as well: at the point where it generates the largest code that will fit in p bits.
Unfortunately, some early implementations of the encoding algorithm increase the code width and then emit ω at the new width instead of the old width, so that to the decoder it looks like the width changes one code too early. This is called ""early change""; it caused so much confusion that Adobe now allows both versions in PDF files, but includes an explicit flag in the header of each LZW-compressed stream to indicate whether early change is being used. Out of graphics file formats capable of using LZW compression, TIFF uses early change, while GIF and most others don't.
When the table is cleared in response to a clear code, both encoder and decoder change the code width after the clear code back to the initial code width, starting with the code immediately following the clear code.


=== Packing order ===
Since the codes emitted typically do not fall on byte boundaries, the encoder and decoder must agree on how codes are packed into bytes. The two common methods are LSB-first (""least significant bit first"") and MSB-first (""most significant bit first""). In LSB-first packing, the first code is aligned so that the least significant bit of the code falls in the least significant bit of the first stream byte, and if the code has more than 8 bits, the high-order bits left over are aligned with the least significant bits of the next byte; further codes are packed with LSB going into the least significant bit not yet used in the current stream byte, proceeding into further bytes as necessary. MSB-first packing aligns the first code so that its most significant bit falls in the MSB of the first stream byte, with overflow aligned with the MSB of the next byte; further codes are written with MSB going into the most significant bit not yet used in the current stream byte.
GIF files use LSB-first packing order. TIFF files and PDF files use MSB-first packing order.


== Example ==
The following example illustrates the LZW algorithm in action, showing the status of the output and the dictionary at every stage, both in encoding and decoding the data. This example has been constructed to give reasonable compression on a very short message. In real text data, repetition is generally less pronounced, so longer input streams are typically necessary before the compression builds up efficiency.
The plaintext to be encoded (from an alphabet using only the capital letters) is:

TOBEORNOTTOBEORTOBEORNOT#

The # is a marker used to show that the end of the message has been reached. There are thus 26 symbols in the plaintext alphabet (the 26 capital letters A through Z), and the # character represents a stop code. We arbitrarily assign these the values 1 through 26 for the letters, and 0 for '#'. (Most flavors of LZW would put the stop code after the data alphabet, but nothing in the basic algorithm requires that. The encoder and decoder only have to agree what value it has.)
A computer will render these as strings of bits. Five-bit codes are needed to give sufficient combinations to encompass this set of 27 values. The dictionary is initialized with these 27 values. As the dictionary grows, the codes will need to grow in width to accommodate the additional entries. A 5-bit code gives 25 = 32 possible combinations of bits, so when the 33rd dictionary word is created, the algorithm will have to switch at that point from 5-bit strings to 6-bit strings (for all code values, including those which were previously output with only five bits). Note that since the all-zero code 00000 is used, and is labeled ""0"", the 33rd dictionary entry will be labeled 32. (Previously generated output is not affected by the code-width change, but once a 6-bit value is generated in the dictionary, it could conceivably be the next code emitted, so the width for subsequent output shifts to 6 bits to accommodate that.)
The initial dictionary, then, will consist of the following entries:


=== Encoding ===
Buffer input characters in a sequence ω until ω + next character is not in the dictionary. Emit the code for ω, and add ω + next character to the dictionary. Start buffering again with the next character. (The string to be encoded is ""TOBEORNOTTOBEORTOBEORNOT#"".)
Unencoded length = 25 symbols × 5 bits/symbol = 125 bits
Encoded length = (6 codes × 5 bits/code) + (11 codes × 6 bits/code) = 96 bits.
Using LZW has saved 29 bits out of 125, reducing the message by almost 22%. If the message were longer, then the dictionary words would begin to represent longer and longer sections of text, allowing repeated words to be sent very compactly.


=== Decoding ===
To decode an LZW-compressed archive, one needs to know in advance the initial dictionary used, but additional entries can be reconstructed as they are always simply concatenations of previous entries.
At each stage, the decoder receives a code X; it looks X up in the table and outputs the sequence χ it codes, and it conjectures χ + ? as the entry the encoder just added – because the encoder emitted X for χ precisely because χ + ? was not in the table, and the encoder goes ahead and adds it. But what is the missing letter? It is the first letter in the sequence coded by the next code Z that the decoder receives. So the decoder looks up Z, decodes it into the sequence ω and takes the first letter z and tacks it onto the end of χ as the next dictionary entry.
This works as long as the codes received are in the decoder's dictionary, so that they can be decoded into sequences. What happens if the decoder receives a code Z that is not yet in its dictionary? Since the decoder is always just one code behind the encoder, Z can be in the encoder's dictionary only if the encoder just generated it, when emitting the previous code X for χ. Thus Z codes some ω that is χ + ?, and the decoder can determine the unknown character as follows:
The decoder sees X and then Z, where X codes the sequence χ and Z codes some unknown sequence ω.
The decoder knows that the encoder just added Z as a code for χ + some unknown character c, so ω = χ + c.
Since c is the first character in the input stream after χ, and since ω is the string appearing immediately after χ, c must be the first character of the sequence ω.
Since χ is an initial substring of ω, c must also be the first character of χ.
So even though the Z code is not in the table, the decoder is able to infer the unknown sequence and adds χ + (the first character of χ) to the table as the value of Z.
This situation occurs whenever the encoder encounters input of the form cScSc, where c is a single character, S is a string and cS is already in the dictionary, but cSc is not. The encoder emits the code for cS, putting a new code for cSc into the dictionary. Next it sees cSc in the input (starting at the second c of cScSc) and emits the new code it just inserted. The argument above shows that whenever the decoder receives a code not in its dictionary, the situation must look like this.
Although input of form cScSc might seem unlikely, this pattern is fairly common when the input stream is characterized by significant repetition. In particular, long strings of a single character (which are common in the kinds of images LZW is often used to encode) repeatedly generate patterns of this sort.


== Further coding ==
The simple scheme described above focuses on the LZW algorithm itself. Many applications apply further encoding to the sequence of output symbols. Some package the coded stream as printable characters using some form of binary-to-text encoding; this will increase the encoded length and decrease the compression rate. Conversely, increased compression can often be achieved with an adaptive entropy encoder. Such a coder estimates the probability distribution for the value of the next symbol, based on the observed frequencies of values so far. A standard entropy encoding such as Huffman coding or arithmetic coding then uses shorter codes for values with higher probabilities.


== Uses ==
LZW compression became the first widely used universal data compression method on computers. A large English text file can typically be compressed via LZW to about half its original size.
LZW was used in the public-domain program compress, which became a more or less standard utility in Unix systems around 1986. It has since disappeared from many distributions, both because it infringed the LZW patent and because gzip produced better compression ratios using the LZ77-based DEFLATE algorithm, but as of 2008 at least FreeBSD includes both compress and uncompress as a part of the distribution. Several other popular compression utilities also used LZW or closely related methods.
LZW became very widely used when it became part of the GIF image format in 1987. It may also (optionally) be used in TIFF and PDF files. (Although LZW is available in Adobe Acrobat software, Acrobat by default uses DEFLATE for most text and color-table-based image data in PDF files.)


== Patents ==

Various patents have been issued in the United States and other countries for LZW and similar algorithms. LZ78 was covered by U.S. Patent 4,464,650 by Lempel, Ziv, Cohn, and Eastman, assigned to Sperry Corporation, later Unisys Corporation, filed on August 10, 1981. Two US patents were issued for the LZW algorithm: U.S. Patent 4,814,746 by Victor S. Miller and Mark N. Wegman and assigned to IBM, originally filed on June 1, 1983, and U.S. Patent 4,558,302 by Welch, assigned to Sperry Corporation, later Unisys Corporation, filed on June 20, 1983.
In 1993–94, and again in 1999, Unisys Corporation received widespread condemnation when it attempted to enforce licensing fees for LZW in GIF images. The 1993–1994 Unisys-Compuserve (Compuserve being the creator of the GIF format) controversy engendered a Usenet comp.graphics discussion Thoughts on a GIF-replacement file format, which in turn fostered an email exchange that eventually culminated in the creation of the patent-unencumbered Portable Network Graphics (PNG) file format in 1995.
Unisys's US patent on the LZW algorithm expired on June 20, 2003, 20 years after it had been filed. Patents that had been filed in the United Kingdom, France, Germany, Italy, Japan and Canada all expired in 2004, likewise 20 years after they had been filed.


== Variants ==
LZMW (1985, by V. Miller, M. Wegman) – Searches input for the longest string already in the dictionary (the ""current"" match); adds the concatenation of the previous match with the current match to the dictionary. (Dictionary entries thus grow more rapidly; but this scheme is much more complicated to implement.) Miller and Wegman also suggest deleting low-frequency entries from the dictionary when the dictionary fills up.
LZAP (1988, by James Storer) – modification of LZMW: instead of adding just the concatenation of the previous match with the current match to the dictionary, add the concatenations of the previous match with each initial substring of the current match (""AP"" stands for ""all prefixes""). For example, if the previous match is ""wiki"" and current match is ""pedia"", then the LZAP encoder adds 5 new sequences to the dictionary: ""wikip"", ""wikipe"", ""wikiped"", ""wikipedi"", and ""wikipedia"", where the LZMW encoder adds only the one sequence ""wikipedia"". This eliminates some of the complexity of LZMW, at the price of adding more dictionary entries.
LZWL is a syllable-based variant of LZW.


== See also ==
LZ77 and LZ78
LZMA
Lempel–Ziv–Storer–Szymanski
LZJB
Context tree weighting


== References ==


== External links ==
Rosettacode wiki, algorithm in various languages
U.S. Patent 4,558,302, Terry A. Welch, High speed data compression and decompression apparatus and method
SharpLZW – C# open source implementation
MIT OpenCourseWare: Lecture including LZW algorithm
Mark Nelson, LZW Data Compression on Dr. Dobbs Journal (October 1, 1989)"
165,International Symposium on Computer Architecture,10029099,8258,"The International Symposium on Computer Architecture (ISCA) is an annual academic conference on computer architecture, generally viewed as the top-tier in the field. Association for Computing Machinery's Special Interest Group on Computer Architecture (ACM SIGARCH) and Institute of Electrical and Electronics Engineers Computer Society are technical sponsors.
ISCA has participated in the Federated Computing Research Conference in 1993, 1996, 1999, 2003, 2007, 2011 and 2015, every year that the conference has been organized.


== Influential Paper Award ==
The ISCA Influential Paper Award is presented annually at ISCA by SIGARCH and TCCA. The award is given for the paper with the most impact in the field (in the area of research, development, products, or ideas) from the conference 15 years ago.
Prior recipients include:
2017 (2002) - Krisztian Flautner, Nam Sung Kim, Steven Martin, David Blaauw, Trevor Mudge,
2016 (2001) - Brian Fields, Shai Rubin, Rastislav Bodík,
2015 (2000) - David Brooks, Vivek Tiwari, and Margaret Martonosi,
2014 (1999) - Seth Copen Goldstein, Herman Schmit, Matthew Moe, Mihai Budiu, Srihari Cadambi, R. Reed Taylor, and Ronald Laufer
2013 (1998) - Srilatha Manne, Artur Klauser, Dirk Grunwald,
2012 (1997) - Subbarao Palacharla, Norman P. Jouppi, James E. Smith
2011 (1996) - Dean M. Tullsen, Susan J. Eggers, Joel S. Emer, Henry M. Levy, Jack L. Lo, and Rebecca L. Stamm
2010 (1995) - Dean M. Tullsen, Susan J. Eggers, and Henry M. Levy
2009 (1994) - Jeffrey Kuskin, David Ofelt, Mark Heinrich, John Heinlein, Richard Simoni, Kourosh Gharachorloo, John Chapin, David Nakahira, Joel Baxter, Mark Horowitz, Anoop Gupta, Mendel Rosenblum, and John L. Hennessy
2008 (1993) - Maurice Herlihy and J. Eliot B. Moss,
2007 (1992) - Tse-Yu Yeh and Yale N. Patt
2006 (1991) - Pohua P. Chang, Scott A. Mahlke, William Y. Chen, Nancy J. Warter, and Wen-mei W. Hwu
2005 (1990) - Norman P. Jouppi,
2004 (1989) - Steven Przybylski, John L. Hennessy, and Mark Horowitz
2003 (1988) - Jean-Loup Baer and Wen-Hann Wang


== References ==


== External links ==
ISCA proceedings in the ACM digital library
ISCA proceedings information in DBLP
ISCA 2017 web site
Recent symposiums"
166,Machtey Award,19168805,8188,"The Machtey Award is awarded at the annual IEEE Symposium on Foundations of Computer Science (FOCS) to the author(s) of the best student paper(s). A paper qualifies as a student paper if all authors are full-time students at the date of the submission. The award decision is made by the Program Committee.
The award is named after Michael Machtey, who was a researcher in the theoretical computer science community in the 1970s. The counterpart of this award at the ACM Symposium on Theory of Computing (STOC) is the Danny Lewin Best Student Paper Award.


== Past recipients ==
Past recipients of the Machtey award are tabulated below.


== See also ==
Kleene award


== References =="
167,Conference on Human Factors in Computing Systems,13683619,8183,"The ACM Conference on Human Factors in Computing Systems (CHI) series of academic conferences is generally considered the most prestigious in the field of human–computer interaction and is one of the top ranked conferences in computer science. It is hosted by ACM SIGCHI, the Special Interest Group on computer–human interaction. CHI has been held annually since 1982 and attracts thousands of international attendees. CHI 2015 was held in Seoul, South Korea, and CHI 2016 was held in San Jose, United States from May 7 to May 12. CHI 2017 was held in Denver, Colorado from May 6–11, 2017.


== History ==
The CHI conference series started with the Human Factors in Computer Systems conference in Gaithersburg, Maryland, US in 1982, organized by Bill Curtis and Ben Shneiderman. During this meeting the formation of the ACM Special Interest Group on Computer–Human Interaction (SIGCHI) was first publicly announced. ACM SIGCHI became the sponsor of the Conference on Human Factors in Computing Systems. The first CHI conference was held in Boston, Massachusetts, US, in 1983. The second conference took place in San Francisco, in 1985. Since then, CHI conferences have been held annually in spring each year. Until 1992 the conference was held in Canada or the US. In 1993 CHI moved to Europe for the first time and was held in Amsterdam, the Netherlands.
Over the years, CHI has grown in popularity. The 1982 meeting drew 907 attendees. CHI 90 attracted 2,314. Attendance has been fairly stable since then. After the early years CHI became highly selective. Since 1993 the acceptance rate for full papers was consistently below 30 percent. After 1992 the average acceptance rate was around 20 percent. The number of accepted full papers is slowly increasing and reached 157 accepted papers with an acceptance rate of 22 percent in 2008. CHI continues to grow, reaching over 3,300 attendees in 2013 and 3,800 in 2016.


== Tracks ==
The CHI conference consists of multiple tracks, including:
Academic papers and notes (short papers) on a variety of topics, such as (ubiquitous computing, visualization, usability and user experience design)
Posters and demonstrations
Workshops and courses hosted by domain experts
Invited panels on relevant topics
Case studies from industry practitioners


== Past and upcoming CHI conferences ==
Past and future CHI conferences include:


== References ==


== External links ==
ACM SIGCHI website"
168,Grooveshark University,31610936,8122,"Grooveshark was a web-based music streaming service owned and operated by Escape Media Group in the United States. Users could upload digital audio files, which could then be streamed and organized in playlists. The Grooveshark website had a search engine, music streaming features, and a music recommendation system.
The legality of Grooveshark's business model, which permitted users to upload copyrighted music, remains undetermined. The company won a major lawsuit filed by Universal Music Group concerning use of Universal's pre-1972 recordings. Grooveshark was also sued for copyright violations by EMI Music Publishing, Sony Music Entertainment, and Warner Music Group. Concerns about copyrights led Apple and Facebook to remove Grooveshark's applications from the iOS App Store and Facebook platform respectively. However, Grooveshark was available in alternative app stores, such as Cydia, Google Play and BlackBerry World. It was also a default application on Ubuntu Touch.
On April 30, 2015, Grooveshark shut down as part of a settlement between the service and Universal Music Group, Sony Music Entertainment, and Warner Music Group. The site briefly redirected to ScoreBig's website, but as of 2017, the URL leads back to a ""Grooveshark"" -titled homepage with a button to download something of the same name and, to all appearances, function.


== History ==


=== Pre-release (2006–2009) ===
Grooveshark was a service of Escape Media Group Inc. (EMG), based in Gainesville, Florida with additional offices located in New York City.
Grooveshark was founded in March 2006 by three undergraduates at the University of Florida, Andrés Barreto, Josh Greenberg and Sam Tarantino (who became CEO). During its first two years, Grooveshark functioned as a paid downloadable music service, with its content sourced from a proprietary peer-to-peer (P2P) network called “Sharkbyte”. Grooveshark stated that it paid users who uploaded a transacted song a portion of the accounting costs for the song. Grooveshark positioned itself as a legal competitor to other popular P2P networks such as LimeWire, although questions about its legality arose from the beginning.
Grooveshark entered beta in September 2007. In the beta, users bought and sold tracks amongst themselves for 99 cents. Around 70 cents went to the record label, 25 cents to the user selling the track, and 4 cents to Grooveshark. Grooveshark's model had been approved by various small record labels, but not by any of the major record companies.


=== Flash web player (2008–2012) ===
On April 15, 2008, the service launched its web service, enabling users to click and play songs on the site without having to download an application. The new web service was a Flash media player called ""Grooveshark Lite"", and added a playlist autoplay feature. The service rose in popularity, with founders Greenberg and Tarantino named 2008 finalists for Bloomberg Businessweek's list of ""America's Best Young Entrepreneurs"".
As of 2009, Grooveshark had secured almost $1 million in seed funding. Also in 2009, Grooveshark launched its artist platform called Grooveshark Artists, which distributes music to fans interested in similar music. On October 27, 2009, Grooveshark revised its interface, which featured skipping to any point in a song, left-hand navigation, customizable site themes, and drag-and-drop editing of playlists. On December 2, 2010, the site's interface was rewritten for HTML5. Its music player continued to use Adobe Flash. Another update occurred in October 2011.
On January 18, 2012 Grooveshark removed service in Germany, stating that it closed due to the costs of licensing. On November 21, 2011, Grooveshark was a Mashable Awards 2011 Finalist in the Best Music Service or App category. On December 19, 2011, Grooveshark co-founders Sam Tarantino and Josh Greenberg were listed among the Forbes 30 Under 30 in Music.


=== HTML5 web player (2012–2015) ===
On August 28, 2012 Google Play restored Grooveshark's app.
On September 5, 2012 Grooveshark presented its full HTML5 player, effectively nullifying Google's and Apple's decisions to make the service unavailable to mobile apps.
On November 12, 2013, Executive Eddy Vasquez was murdered.
In 2013, Cydia repositories iHackStore, BigBoss Repo, c0caine, and all others brought back the Grooveshark app for the iPhone with the ability to download songs and import them directly to the music app within the Grooveshark app.
As of July 2014, Grooveshark announced that it would accept Bitcoin as a form of payment via Stripe.


=== Shutdown (2015) ===
On April 30, 2015, it was announced that, as part of a settlement of the copyright infringement lawsuits between the service and Universal Music Group, Sony Music Entertainment, and Warner Music Group, Grooveshark would be shut down immediately. Furthermore, the ownership of the Grooveshark service, website, and all of its associated intellectual property would be transferred to the labels. The Grooveshark website was replaced with a message announcing the closure, and pointed users towards licensed music streaming services. The move came after it was disclosed that the company could have been liable for up to $736 million in damages if it were determined that the website's infringement of copyrights was willful.
Shortly after the shutdown, a new Grooveshark-branded website surfaced under a different top-level domain, offering a basic MP3 search engine that claimed to use the site's previous library of music, and promising to restore much of its original functionality. Although the site's anonymous creator claimed to have had a prior ""connection"" to the site and promised future development, it was later found that the ""new"" Grooveshark was simply a re-branded version of an existing MP3 search engine. After the labels were granted a temporary restraining order, the clone's domain name was seized, although the site quickly re-appeared on a new domain. Tools have been created for retrieving Grooveshark playlists, such as playlist.fish, Audiosplitter and StreamSquid.
On July 19, 2015, Grooveshark co-founder Josh Greenberg died in his home at the age of 28, of undetermined causes.


== Features ==
Grooveshark was a rich Internet application that originally ran in Adobe Flash. In December 2010, Grooveshark redesigned its site to provide an HTML5 interface. Grooveshark displayed songs, playlists, and users. Grooveshark had a Java Web Start application that scanned user folders for MP3s, uploading and adding them to the user's online library. The ID3 information of the uploaded song was linked to the user, and the file would be uploaded to Grooveshark, which then would offer on-demand music playback. All content on the service was user-sourced. In 2010 Time's on-line supplement had listed Grooveshark among its 50 Best Websites.
Grooveshark streamed over 1 billion sound files per month, contained over 15 million songs, and had 20 million users. Users could search and find music by song, artist, album, browsing friends’ recent activity, and even through other users’ playlists. The service allowed users to create and edit playlists. Registered users could save playlists to an account, subscribe to other users’ playlists, and share them through e-mail, social media, StumbleUpon, Reddit, or an embeddable widget. Users could listen to radio stations of particular genres or populate their own station via their list of songs. The site would use the song list to stream similar music, and this stream selection would update using user ratings of songs. Grooveshark featured a “Community” section, where users could view the activity of friends by “following” them. Users could also connect other social media accounts.
Users could obtain basic accounts without fees. Grooveshark offered two subscription services that gave users increased features, no banner ads, and playability on mobile devices.


== Critical reception ==
In 2013, Entertainment Weekly compared a number of music services and granted Grooveshark a ""B"", rating, ""Users upload libraries onto cloud servers, which means fewer catalog holes. But there's only an Android app, and the Web interface can get sluggish.""


== Copyright issues ==
CEO Sam Tarantino stated that the company strictly follows the takedown procedures of the US's Online Copyright Infringement Liability Limitation Act, stating that usually Grooveshark expeditiously removes content. However, representatives of the music labels argued that songs that are taken down due to infringement claims often reappear almost immediately. Due to copyright concerns and pressure from record labels, many third party companies distanced themselves from Grooveshark. Apple pulled the Grooveshark app for iOS from App Store on August 16, 2010, shortly after its release in response to a complaint from Universal. On April 1, 2011, the Grooveshark application was pulled from the Android Market. In May 2012, Facebook removed Grooveshark ""due to a copyright infringement complaint"". At the end of April 2013 Google Search started censoring ""grooveshark"" term from its Autocomplete feature. In 2012, the British Phonographic Industry engaged Phonographic Performance Limited regarding Grooveshark's licensing, and as of November 2013, was attempting to have all United Kingdom ISPs block the website.


=== Universal Music Group ===

Universal Music Group filed a copyright infringement lawsuit against Grooveshark on January 6, 2010, alleging that Grooveshark maintained on its servers illegal copies of Universal's pre-1972 catalog. In July 2012, New York State Supreme Court Judge Barbara Kapnick ruled that pre-1972 recordings were covered by the ""safe harbor"" provision of the Digital Millennium Copyright Act In April 2013, the New York State Supreme Court of Appeals reversed the decision, saying that pre-1972 licenses are not covered by the DMCA.
In November 2011, Universal Music Group brought an additional lawsuit against Grooveshark for more than $15 billion. UMG cited internal documents revealing that Grooveshark employees uploaded thousands of illegal copies of UMG-owned recordings. Six individuals were named as personally having uploaded between 1,000 and 40,000 songs each; other employees had uploaded 43,000 songs, according to page eight of the complaint. For each of the 113,777 alleged uploadings, a penalty of US $150,000 was requested by Universal, amounting to an estimated US $17.1 billion. Grooveshark denied all the complaints, complaining there was a ""gross mischaracterisation"" of the documents obtained during the lawsuit's discovery phase. In September 2014, the case was decided in favor of the record companies, with damages not yet determined.
Another major label, EMI, had also signed a license agreement for streaming music with Grooveshark in 2009 after settling a previous copyright lawsuit. However, on January 5, 2012, EMI sued Grooveshark over non-payment of royalties stating in their complaint that Grooveshark failed to provide ""a single accounting statement"". As a result, EMI dropped its licensing agreement with Grooveshark. Much of EMI is now owned by Universal Music Group.


=== Independent labels ===
Grooveshark had licensing deals with a number of independent record labels, such as Sun Records.


== See also ==
List of social networking websites
List of Internet radio stations
List of online music databases
Streaming media


== References =="
169,ACM Computing Classification System,3192516,8103,"The ACM Computing Classification System (CCS) is a subject classification system for computing devised by the Association for Computing Machinery (ACM). The system is comparable to the Mathematics Subject Classification (MSC) in scope, aims, and structure, being used by the various ACM journals to organise subjects by area.


== History ==
The system has gone through seven revisions, the first version being published in 1964, and revised versions appearing in 1982, 1983, 1987, 1991, 1998, and the now current version in 2012.


== Structure ==
The ACM Computing Classification System, version 2012, has a revolutionary change in some areas, for example, in ""Software"" that now is called ""Software and its engineering"" which has three main subjects:
Software organization and properties. This subject addresses properties of the software itself.
Software notations and tools. This subject covers programming languages and other tools for writing programs.
Software creation and management. This subject covers human activities including software management.
It is hierarchically structured in four levels. Thus, for example, one branch of the hierarchy contains:
Computing methodologies
Artificial intelligence
Knowledge representation and reasoning
Ontology engineering


== See also ==

Mathematics Subject Classification (MSC)
Physics and Astronomy Classification Scheme (PACS)
arXiv, a preprint server that uses a somewhat different subdivision of topics in its computer science subject areas but also allows papers to be classified using the ACM system
PhySH (Physics Subject Headings)


== References ==
Coulter, Neal (1997), ""ACM's computing classification system reflects changing times"", Communications of the ACM, New York, NY, USA: ACM, 40 (12): 111–112, doi:10.1145/265563.265579 .
Coulter, Neal (chair); French, James; Glinert, Ephraim; Horton, Thomas; Mead, Nancy; Ralston, Anthony; Rada, Roy; Rodkin, Craig; Rous, Bernard; Tucker, Allen; Wegner, Peter; Weiss, Eric; Wierzbicki, Carol (January 21, 1998), ""Computing Classification System 1998: Current Status and Future Maintenance Report of the CCS Update Committee"" (PDF), Computing Reviews, New York, NY, USA: ACM: 1–5 .
Mirkin, Boris; Nascimento, Susana; Pereira, Luis Moniz (2008), ""Representing a Computer Science Research Organization on the ACM Computing Classification System"", in Eklund, Peter; Haemmerlé, Ollivier, Supplementary Proceedings of the 16th International Conference on Conceptual Structures (ICCS-2008) (PDF), CEUR Workshop Proceedings, 354, RWTH Aachen University, pp. 57–65 .


== External links ==
ACM Computing Classification System is the homepage of the system, including links to four complete versions of the system, for 1964 [1], 1991 [2], 1998 [3], and the current 2012 version [4].
The ACM Computing Research Repository uses a classification scheme that is much coarser than the ACM subject classification, and does not cover all areas of CS, but is intended to better cover active areas of research. In addition, papers in this repository are classified according to the ACM subject classification."
170,Alan Turing Year,22523598,8089,"The Alan Turing Year, 2012, marked the celebration of the life and scientific influence of Alan Turing during the centenary of his birth on 23 June 1912. Turing had an important influence on computing, computer science, artificial intelligence, developmental biology, and the mathematical theory of computability and made important contributions to code-breaking during the Second World War. The Alan Turing Centenary Advisory committee (TCAC) was originally set up by Professor Barry Cooper
The international impact of Turing's work is reflected in the list of countries in which Alan Turing Year was celebrated, including: Bolivia, Brazil, Canada, China, Czech Republic, France, Germany, India, Israel, Italy, Netherlands, Mexico, New Zealand, Norway, Philippines, Portugal, Spain, Switzerland, U.K. and the U.S.A. 41+ countries were involved.


== Events ==

A number of major events took place throughout the year. Some of these were linked to places with special significance in Turing’s life, such as Cambridge University, the University of Manchester, Bletchley Park, Princeton University. The ACM was involved from June to September 2012. Twelve museums were involved including in Germany and Brazil. Artists, musicians and poets took part in the celebrations internationally.
Events included the 2012 Computability in Europe conference, as well as Turing Centenary activities organized or sponsored by the British Computer Society, the Association for Symbolic Logic, British Colloquium for Theoretical Computer Science, the British Society for the History of Mathematics, the Association for Computing Machinery, British Logic Colloquium, Society for the Study of Artificial Intelligence and the Simulation of Behaviour, the Computer Conservation Society, the Computer Society of India, the Bletchley Park Trust, the European Association for Computer Science Logic, the European Association for Theoretical Computer Science, International Association for Computing and Philosophy, the Department of Philosophy at De La Salle University-Manila, the John Templeton Foundation, the Kurt Gödel Society, the IEEE Symposium on Logic in Computer Science, the Science Museum, and Turing100in2012. The Alan Turing Centenary Conference was held at the University of Manchester during June 2012.
Alan Turing Year is known on Twitter as Alan Turing Years. @alanturingyear.


== Organisers ==
The Turing Year was coordinated by the Turing Centenary Advisory Committee (TCAC), representing a range of expertise and organisational involvement in the 2012 celebrations. Members of TCAC include Honorary President, Sir John Dermot Turing; The Chair and founder of the committee, mathematician and author of Alan Turing - His Work and Impact S. Barry Cooper; Turing's biographer Andrew Hodges; Wendy Hall, first person from outside North America elected President of the Association for Computing Machinery (ACM) in July 2008; Simon Singh; Hugh Loebner sponsor of the Loebner Prize for Artificial Intelligence (annual science contest based on the famous Turing test) cyberneticist Kevin Warwick, author of 'March of the Machines' and 'I, Cyborg', and committee member Daniela Derbyshire, who is also handling international co-ordination of marketing and publicity.


== UK publicity ==
Examples include:
The Royal Mail issued a UK commemorative stamp for the Turing Centenary.
The Imitation Game: how Benedict Cumberbatch brought Turing to life The Guardian, Tuesday 7 October 2014
De-coding the Turing family Professor Barry Cooper The Guardian, Tue 17 April 2012
Alan Turing: Centenary accolades keep coming - Yahoo News 3 April 2013
Alan Turing Year - the Establishment still doesn't get it Barry Cooper The Guardian, Tue 22 January 2013
Alan Turing and the bullying of Britain's geeks S Barry Cooper The Guardian, Wed 20 June 2012
Playing Monopoly with Alan Turing S Barry Cooper The Guardian, Mon 24 September 2012
Alan Turing: ""I am building a brain."" Half a century later, its successor beat Kasparov S Barry Cooper The Guardian, Mon 14 May 2012
Google doodle becomes an enigma in honour of Alan Turing The Daily Telegraph, Sat 23 June 2012
Tribute to computing's forefather BBC News, Sat 27 October 2012
The other Turing test: Codebreaker's beloved Monopoly pays him the ultimate compliment The Independent, Sat 8 September 2012
GCHQ Director Iain Lobban pays tribute to Alan Turing Centenary - Oddballs Wanted piece in Daily Mail 5 October 2012
Sunflower maths theory is tested BBC News
How did the leopard get its spots? Codebreaker Alan Turing was right all along The Daily Telegraph
The Queen hails 'genius' of Alan Turing on visit to WWII codebreaking HQ at Bletchley Park menmedia.co.uk, Fri 15 July 2011
Barry Cooper was interviewed on BBC 3 Counties Radio and Sky News in relation to the pardon of Alan Turing on December 24, 2013.


== References ==


== External links ==
Alan Turing Year/TCAC website
[2]
CiE 2012:Turing Centenary Conference website at the University of Cambridge
Turing 2012: The Life and Works of Alan Turing at De La Salle University-Manila
The State of Computing: Turing Centenary Conference in Bangalore, India
Turing 100: Alan Turing Centenary Conference at the University of Manchester"
171,History of Programming Languages,5396226,8064,"The first electronic computers had limited speed and memory capacity, forcing programmers to write programs in assembly language, i.e., the native language of the hardware. Once computer capacity increased it became practical to implement higher level languages.
The first high-level programming language was Plankalkül, created by Konrad Zuse between 1942 and 1945. The first high-level language to have an associated compiler, was created by Corrado Böhm in 1951, for his PhD thesis. The first commercially available language was FORTRAN (FORmula TRANslation); developed in 1956 (first manual appeared in 1956, but first developed in 1954) by John Backus, a worker at IBM.
When FORTRAN was first introduced it was treated with suspicion because of the belief that programs compiled from high-level language would be less efficient than those written directly in machine code. FORTRAN became popular because it provided a means of porting existing code to new computers, in a hardware market that was rapidly evolving. FORTRAN eventually became known for its efficiency. Over the years, FORTRAN had been updated, with standards released for FORTRAN-66, FORTRAN-77 and FORTRAN-92.


== Early history ==
During a nine-month period in 1842–1843, Ada Lovelace translated the memoir of Italian mathematician Luigi Menabrea about Charles Babbage's newest proposed machine, the analytical engine. With the article she appended a set of notes which specified in complete detail a method for calculating Bernoulli numbers with the engine, recognized by some historians as the world's first computer program.
The first computer codes were specialized for their applications. In the first decades of the 20th century, numerical calculations were based on decimal numbers. Eventually it was realized that logic could be represented with numbers, not only with words. For example, Alonzo Church was able to express the lambda calculus in a formulatic way. The Turing machine was an abstraction of the operation of a tape-marking machine, for example, in use at the telephone companies. Turing machines set the basis for storage of programs as data in the von Neumann architecture of computers by representing a machine through a finite number. However, unlike the lambda calculus, Turing's code does not serve well as a basis for higher-level languages—its principal use is in rigorous analyses of algorithmic complexity.
To some people, what was the first modern programming language depends on how much power and human-readability is required before the status of ""programming language"" is granted. Jacquard Looms and Charles Babbage's Difference Engine both had simple, extremely limited languages for describing the actions that these machines should perform.


== First programming languages ==
In the 1940s, the first recognizably modern electrically powered computers were created. The limited speed and memory capacity forced programmers to write hand tuned assembly language programs. It was eventually realized that programming in assembly language required a great deal of intellectual effort.
The first programming languages designed to communicate instructions to a computer were written in the 1950s. An early high-level programming language to be designed for a computer was Plankalkül, developed by the Germans for Z1 by Konrad Zuse between 1943 and 1945. However, it was not implemented until 1998 and 2000.
John Mauchly's Short Code, proposed in 1949, was one of the first high-level languages ever developed for an electronic computer. Unlike machine code, Short Code statements represented mathematical expressions in understandable form. However, the program had to be translated into machine code every time it ran, making the process much slower than running the equivalent machine code.
At the University of Manchester, Alick Glennie developed Autocode in the early 1950s, with the second iteration developed for the Mark 1 by R. A. Brooker in 1954, known as the ""Mark 1 Autocode"". Brooker also developed an autocode for the Ferranti Mercury in the 1950s in conjunction with the University of Manchester. The version for the EDSAC 2 was devised by D. F. Hartley of University of Cambridge Mathematical Laboratory in 1961. Known as EDSAC 2 Autocode, it was a straight development from Mercury Autocode adapted for local circumstances, and was noted for its object code optimisation and source-language diagnostics which were advanced for the time. A contemporary but separate thread of development, Atlas Autocode was developed for the University of Manchester Atlas 1 machine.
In 1954, language FORTRAN was invented at IBM by a team led by John Backus; it was the first widely used high level general purpose programming language to have a functional implementation, as opposed to just a design on paper. It is still a popular language for high-performance computing and is used for programs that benchmark and rank the world's fastest supercomputers.
Another early programming language was devised by Grace Hopper in the US, called FLOW-MATIC. It was developed for the UNIVAC I at Remington Rand during the period from 1955 until 1959. Hopper found that business data processing customers were uncomfortable with mathematical notation, and in early 1955, she and her team wrote a specification for an English programming language and implemented a prototype. The FLOW-MATIC compiler became publicly available in early 1958 and was substantially complete in 1959. Flow-Matic was a major influence in the design of COBOL, since only it and its direct descendent AIMACO were in actual use at the time.
Other languages still in use today include LISP (1958), invented by John McCarthy and COBOL (1959), created by the Short Range Committee. Another milestone in the late 1950s was the publication, by a committee of American and European computer scientists, of ""a new language for algorithms""; the ALGOL 60 Report (the ""ALGOrithmic Language""). This report consolidated many ideas circulating at the time and featured three key language innovations:
nested block structure: code sequences and associated declarations could be grouped into blocks without having to be turned into separate, explicitly named procedures;
lexical scoping: a block could have its own private variables, procedures and functions, invisible to code outside that block, that is, information hiding.
Another innovation, related to this, was in how the language was described:
a mathematically exact notation, Backus–Naur form (BNF), was used to describe the language's syntax. Nearly all subsequent programming languages have used a variant of BNF to describe the context-free portion of their syntax.
Algol 60 was particularly influential in the design of later languages, some of which soon became more popular. The Burroughs large systems were designed to be programmed in an extended subset of Algol.
Algol's key ideas were continued, producing ALGOL 68:
syntax and semantics became even more orthogonal, with anonymous routines, a recursive typing system with higher-order functions, etc.;
not only the context-free part, but the full language syntax and semantics were defined formally, in terms of Van Wijngaarden grammar, a formalism designed specifically for this purpose.
Algol 68's many little-used language features (for example, concurrent and parallel blocks) and its complex system of syntactic shortcuts and automatic type coercions made it unpopular with implementers and gained it a reputation of being difficult. Niklaus Wirth actually walked out of the design committee to create the simpler Pascal language.

Some notable languages that were developed in this period include:


== Establishing fundamental paradigms ==

The period from the late 1960s to the late 1970s brought a major flowering of programming languages. Most of the major language paradigms now in use were invented in this period:
Speakeasy (computational environment), developed in 1964 at Argonne National Laboratory (ANL) by Stanley Cohen, is an OOPS (object-oriented programming, much like the later MATLAB, IDL (programming language) and Mathematica) numerical package. Speakeasy has a clear Fortran foundation syntax. It first addressed efficient physics computation internally at ANL, was modified for research use (as ""Modeleasy"") for the Federal Reserve Board in the early 1970s and then was made available commercially; Speakeasy and Modeleasy are still in use currently.
Simula, invented in the late 1960s by Nygaard and Dahl as a superset of Algol 60, was the first language designed to support object-oriented programming.
C, an early systems programming language, was developed by Dennis Ritchie and Ken Thompson at Bell Labs between 1969 and 1973.
Smalltalk (mid-1970s) provided a complete ground-up design of an object-oriented language.
Prolog, designed in 1972 by Colmerauer, Roussel, and Kowalski, was the first logic programming language.
ML built a polymorphic type system (invented by Robin Milner in 1973) on top of Lisp, pioneering statically typed functional programming languages.
Each of these languages spawned an entire family of descendants, and most modern languages count at least one of them in their ancestry.
The 1960s and 1970s also saw considerable debate over the merits of ""structured programming"", which essentially meant programming without the use of ""goto"". A significant fraction of programmers believed that, even in languages that provide ""goto"", it is bad programming style to use it except in rare circumstances. This debate was closely related to language design: some languages did not include a ""goto"" at all, which forced structured programming on the programmer.
To provide even faster compile times, some languages were structured for ""one-pass compilers"" which expect subordinate routines to be defined first, as with Pascal, where the main routine, or driver function, is the final section of the program listing.
Some notable languages that were developed in this period include:


== 1980s: consolidation, modules, performance ==

The 1980s were years of relative consolidation in imperative languages. Rather than inventing new paradigms, all of these movements elaborated upon the ideas invented in the previous decade. C++ combined object-oriented and systems programming. The United States government standardized Ada, a systems programming language intended for use by defense contractors. In Japan and elsewhere, vast sums were spent investigating so-called fifth-generation programming languages that incorporated logic programming constructs. The functional languages community moved to standardize ML and Lisp. Research in Miranda, a functional language with lazy evaluation, began to take hold in this decade.
One important new trend in language design was an increased focus on programming for large-scale systems through the use of modules, or large-scale organizational units of code. Modula, Ada, and ML all developed notable module systems in the 1980s. Module systems were often wedded to generic programming constructs---generics being, in essence, parametrized modules (see also polymorphism in object-oriented programming).
Although major new paradigms for imperative programming languages did not appear, many researchers expanded on the ideas of prior languages and adapted them to new contexts. For example, the languages of the Argus and Emerald systems adapted object-oriented programming to distributed systems.
The 1980s also brought advances in programming language implementation. The RISC movement in computer architecture postulated that hardware should be designed for compilers rather than for human assembly programmers. Aided by processor speed improvements that enabled increasingly aggressive compilation techniques, the RISC movement sparked greater interest in compilation technology for high-level languages.
Language technology continued along these lines well into the 1990s.
Some notable languages that were developed in this period include:


== 1990s: the Internet age ==

The rapid growth of the Internet in the mid-1990s was the next major historic event in programming languages. By opening up a radically new platform for computer systems, the Internet created an opportunity for new languages to be adopted. In particular, the JavaScript programming language rose to popularity because of its early integration with the Netscape Navigator web browser. Various other scripting languages achieved widespread use in developing customized applications for web servers such as PHP. The 1990s saw no fundamental novelty in imperative languages, but much recombination and maturation of old ideas. This era began the spread of functional languages. A big driving philosophy was programmer productivity. Many ""rapid application development"" (RAD) languages emerged, which usually came with an IDE, garbage collection, and were descendants of older languages. All such languages were object-oriented. These included Object Pascal, Visual Basic, and Java. Java in particular received much attention.
More radical and innovative than the RAD languages were the new scripting languages. These did not directly descend from other languages and featured new syntaxes and more liberal incorporation of features. Many consider these scripting languages to be more productive than even the RAD languages, but often because of choices that make small programs simpler but large programs more difficult to write and maintain. Nevertheless, scripting languages came to be the most prominent ones used in connection with the Web.
Some notable languages that were developed in this period include:


== Current trends ==
Programming language evolution continues, in both industry and research. Some of the recent trends have included:

Increasing support for functional programming in mainstream languages used commercially, including pure functional programming for making code easier to reason about and easier to parallelise (at both micro- and macro- levels)
Constructs to support concurrent and distributed programming.
Mechanisms for adding security and reliability verification to the language: extended static checking, dependent typing, information flow control, static thread safety.
Alternative mechanisms for composability and modularity: mixins, traits, delegates, aspects.
Component-oriented software development.
Metaprogramming, reflection or access to the abstract syntax tree
AOP or Aspect Oriented Programming allowing developers to insert code in another module or class at ""join points""
Domain specific languages and code generation
XML for graphical interface (XUL, XAML)

Increased interest in distribution and mobility.
Integration with databases, including XML and relational databases.
Open source as a developmental philosophy for languages, including the GNU Compiler Collection and languages such as Python, Ruby, and Scala.
Massively parallel languages for coding 2000 processor GPU graphics processing units and supercomputer arrays including OpenCL
Early research into (as-yet-unimplementable) quantum computing programming languages
More interest in visual programming languages like Scratch
Some notable languages developed during this period include:


== Prominent people ==

Some key people who helped develop programming languages:
Alan Cooper, developer of Visual Basic.
Alan Kay, pioneering work on object-oriented programming, and originator of Smalltalk.
Anders Hejlsberg, developer of Turbo Pascal, Delphi, C#, and TypeScript.
Bertrand Meyer, inventor of Eiffel.
Bjarne Stroustrup, developer of C++.
Brian Kernighan, co-author of the first book on the C programming language with Dennis Ritchie, coauthor of the AWK and AMPL programming languages.
Chris Lattner, creator of Swift and LLVM.
Dennis Ritchie, inventor of C. Unix Operating System, Plan 9 Operating System.
Grace Hopper, first to use the term compiler and developer of Flow-Matic, influenced development of COBOL. Popularized machine-independent programming languages and the term ""debugging"".
Guido van Rossum, creator of Python.
James Gosling, lead developer of Java and its precursor, Oak.
Jean Ichbiah, chief designer of Ada, Ada 83.
Jean-Yves Girard, co-inventor of the polymorphic lambda calculus (System F).
Jeff Bezanson, main designer, and one of the core developers of Julia.
Joe Armstrong, creator of Erlang.
John Backus, inventor of Fortran and cooperated in the design of ALGOL 58 and ALGOL 60.
John C. Reynolds, co-inventor of the polymorphic lambda calculus (System F).
John McCarthy, inventor of LISP.
John von Neumann, originator of the operating system concept.
Graydon Hoare, inventor of Rust.
Ken Thompson, inventor of B, Go Programming Language, Inferno Programming Language, and Unix Operating System co-author.
Kenneth E. Iverson, developer of APL, and co-developer of J along with Roger Hui.
Konrad Zuse, designed the first high-level programming language, Plankalkül (which influenced ALGOL 58).
Kristen Nygaard, pioneered object-oriented programming, co-invented Simula.
Larry Wall, creator of the Perl programming language (see Perl and Perl 6).
Martin Odersky, creator of Scala, and previously a contributor to the design of Java.
Nathaniel Rochester, inventor of first assembler (IBM 701).
Niklaus Wirth, inventor of Pascal, Modula and Oberon.
Ole-Johan Dahl, pioneered object-oriented programming, co-invented Simula.
Rasmus Lerdorf, creator of PHP
Rich Hickey, creator of Clojure.
Robin Milner, inventor of ML, and sharing credit for Hindley–Milner polymorphic type inference.
Stephen Wolfram, creator of Mathematica.
Tom Love and Brad Cox, creator of Objective-C.
Walter Bright, creator of D.
Yukihiro Matsumoto, creator of Ruby.


== See also ==


== References ==


== Further reading ==
Rosen, Saul, (editor), Programming Systems and Languages, McGraw-Hill, 1967.
Sammet, Jean E., Programming Languages: History and Fundamentals, Prentice-Hall, 1969.
Sammet, Jean E. (July 1972). ""Programming Languages: History and Future"". Communications of the ACM. 15 (7): 601–610. doi:10.1145/361454.361485. 
Richard L. Wexelblat (ed.): History of Programming Languages, Academic Press 1981.
Thomas J. Bergin and Richard G. Gibson (eds.): History of Programming Languages, Addison Wesley, 1996.


== External links ==
History and evolution of programming languages
Graph of programming language history"
172,Alasdair Turner,33417054,7810,"Patrick Alasdair Fionn Turner (19 October 1969 – 6 October 2011) was a British-born scientist, who played a major role in the VR Centre for the Built Environment and the Space group at the University College London. His contribution had a great impact on the development of space syntax theory. This goes in parallel to his research into introducing a dynamic agent model that derives aggregate spatial analysis from the visual affordances of the built environment. Based on the principles of Turner's theory on Embodied space, his agent model proves to correlate well with natural movement behavior in architectural and urban environments. Turner was born on 19 October 1969, in London. He earned an MA in Natural Sciences from the University of Cambridge, and an MSc in Artificial Intelligence from the University of Edinburgh. Turner was last appointed a reader in Urban and Architectural Computing at the University College London in 2011. Turner died on 6 October 2011 after a long struggle with stomach cancer.


== Research ==
Alasdair is a computer scientist whose interests were focused on the interface between the domain of architecture and urban design and the underlying systems that comprise their socio-physical dimensions. He looked for dynamic models that evolve through the process of structural coupling between the man as a vision-driven agent and the background environment. Agents are seen to have two different patterns of movement; a natural movement and a navigational movement. Natural movement is suggested to be the simple product of Agents’ vision and the spatial affordances of the built environment. Navigational movement is the pattern of movement that agents learn by retrieving their spatial memory and implementing that into their navigational decisions with the purpose to reach a certain destination point. Agents are adapted to learn through introducing a neural network method to control their movement behaviour. Alasdair aimed at implementing these methods into building evolutionary design model to reflect on an emergent Ecomorphic model. The model evolves a relationship between the collective patterns of movement by agents and the formation of the environment. In the model he developed with Alan Penn, the environment has an underlying Exosomatic visual architecture (that is, a paradigm of vision that holds for configurational affordances in the environment and outside the body which guide movement decisions). He understands the spatial affordances of the environment as the possibilities for vision and movement in space. Constrained by these affordances, agents’ movement is driven by vision and enabled by access.
Alasdair’s work on spatial analysis constitutes the foundation of the agent model and supports research in space syntax theory. The theory was initially established by Bill Hillier and Julienne Hanson in their book The Social Logic of Space. Space syntax theory was based on a graph-based representation of the spatial network in buildings and cities and what this implies in terms of the social logic of movement and occupation. The nodes in the graph representation are namely the axial lines and convex spaces. Together with Alan Penn, David O'Sullivan and Maria Doxa, Alasdair has introduced to visibility graph analysis (VGA). A VGA is another representation of space that reflects on intervisibility relationships in a layout. In a VGA model, nodes are considered to be points in a grid that covers all spaces in a layout. Alasdair has also contributed to the development of space syntax methods. Together with Alan Penn and Bill Hillier, he has written an algorithm to generate axial maps automatically. He has also contributed to the introduction of segment graph measures and the development of geometric configurations. Segment analysis was seen by Bill Hillier and Shinichi Iida to have some explanatory power by considering angular and metric graph properties of street networks and their psychological effects. The method was mainly applied to studies on urban systems. Alasdair has written the UCL Depthmap software in which he implemented space syntax methods as well as the agent model.
Alasdair's interests go beyond science and architecture towards art and generative code explorations. Being the founder of the MSc Adaptive Architecture and Computation course at UCL, he taught programming for architecture and creative design. For that, he used Processing; an open source Java-based programming language.


== See also ==
Spatial network
Spatial network analysis software
Visibility graph analysis


== References ==


== External links ==
personal Web page at the VR Centre for the Built Environment
profile for the MSc Adaptive Architecture and Computation course
profile for the Vision@UCL program"
173,Theta graph,2472919,7793,"In computational geometry, the Theta graph, or 
  
    
      
        Θ
      
    
    {\displaystyle \Theta }
  -graph, is a type of geometric spanner similar to a Yao graph. The basic method of construction involves partitioning the space around each vertex into a set of cones, which themselves partition the remaining vertices of the graph. Like Yao Graphs, a 
  
    
      
        Θ
      
    
    {\displaystyle \Theta }
  -graph contains at most one edge per cone; where they differ is how that edge is selected. Whereas Yao Graphs will select the nearest vertex according to the metric space of the graph, the 
  
    
      
        Θ
      
    
    {\displaystyle \Theta }
  -graph defines a fixed ray contained within each cone (conventionally the bisector of the cone) and selects the nearest neighbour with respect to orthogonal projections to that ray. The resulting graph exhibits several good spanner properties .

  
    
      
        Θ
      
    
    {\displaystyle \Theta }
  -graphs were first described by Clarkson in 1987 and independently by Keil in 1988.


== Construction ==

  
    
      
        Θ
      
    
    {\displaystyle \Theta }
  -graphs are specified with a few parameters which determine their construction. The most obvious parameter is 
  
    
      
        k
      
    
    {\displaystyle k}
  , which corresponds to the number of equal angle cones that partition the space around each vertex. In particular, for a vertex 
  
    
      
        p
      
    
    {\displaystyle p}
  , a cone about 
  
    
      
        p
      
    
    {\displaystyle p}
   can be imagined as two infinite rays emanating from it with angle 
  
    
      
        θ
        =
        2
        π
        
          /
        
        k
      
    
    {\displaystyle \theta =2\pi /k}
   between them. With respect to 
  
    
      
        p
      
    
    {\displaystyle p}
  , we can label these cones as 
  
    
      
        
          C
          
            1
          
        
      
    
    {\displaystyle C_{1}}
   through 
  
    
      
        
          C
          
            k
          
        
      
    
    {\displaystyle C_{k}}
   in an counterclockwise pattern from 
  
    
      
        
          C
          
            1
          
        
      
    
    {\displaystyle C_{1}}
  , which conventionally opens so that its bisector has angle 0 with respect to the plane. As these cones partition the plane, they also partition the remaining vertex set of the graph (assuming general position) into the sets 
  
    
      
        
          V
          
            1
          
        
      
    
    {\displaystyle V_{1}}
   through 
  
    
      
        
          V
          
            k
          
        
      
    
    {\displaystyle V_{k}}
  , again with respect to 
  
    
      
        p
      
    
    {\displaystyle p}
  . Every vertex in the graph gets the same number of cones in the same orientation, and we can consider the set of vertices that fall into each.
Considering a single cone, we need to specify another ray emanating from 
  
    
      
        p
      
    
    {\displaystyle p}
  , which we will label 
  
    
      
        l
      
    
    {\displaystyle l}
  . For every vertex in 
  
    
      
        
          V
          
            i
          
        
      
    
    {\displaystyle V_{i}}
  , we consider the orthogonal projection of each 
  
    
      
        v
        ∈
        
          V
          
            i
          
        
      
    
    {\displaystyle v\in V_{i}}
   onto 
  
    
      
        l
      
    
    {\displaystyle l}
  . Suppose that 
  
    
      
        r
      
    
    {\displaystyle r}
   is the vertex with the closest such projection, then the edge 
  
    
      
        {
        p
        ,
        r
        }
      
    
    {\displaystyle \{p,r\}}
   is added to the graph. This is the primary difference from Yao Graphs which always select the nearest vertex; in the example image, a Yao Graph would include the edge 
  
    
      
        {
        p
        ,
        q
        }
      
    
    {\displaystyle \{p,q\}}
   instead.
Construction of a 
  
    
      
        Θ
      
    
    {\displaystyle \Theta }
  -graph is possible with a sweepline algorithm in 
  
    
      
        O
        (
        n
        log
        ⁡
        
          n
        
        )
      
    
    {\displaystyle O(n\log {n})}
   time.


== Properties ==

  
    
      
        Θ
      
    
    {\displaystyle \Theta }
  -graphs exhibit several good geometric spanner properties.
When the parameter 
  
    
      
        k
      
    
    {\displaystyle k}
   is a constant, the 
  
    
      
        Θ
      
    
    {\displaystyle \Theta }
  -graph is a sparse spanner. As each cone generates at most one edge per cone, most vertices will have small degree, and the overall graph will have at most 
  
    
      
        k
        ⋅
        n
        =
        O
        (
        n
        )
      
    
    {\displaystyle k\cdot n=O(n)}
   edges.
The stretch factor between any pair of points in a spanner is defined as the ratio between their metric space distance, and their distance within the spanner (i.e. from following edges of the spanner). The stretch factor of the entire spanner is the maximum stretch factor over all pairs of points within it. Recall from above that 
  
    
      
        θ
        =
        2
        π
        
          /
        
        k
      
    
    {\displaystyle \theta =2\pi /k}
  , then when 
  
    
      
        k
        ≥
        9
      
    
    {\displaystyle k\geq 9}
  , the 
  
    
      
        Θ
      
    
    {\displaystyle \Theta }
  -graph has a stretch factor of at most 
  
    
      
        1
        
          /
        
        (
        cos
        ⁡
        θ
        −
        sin
        ⁡
        θ
        )
      
    
    {\displaystyle 1/(\cos \theta -\sin \theta )}
  . If the orthogonal projection line 
  
    
      
        l
      
    
    {\displaystyle l}
   in each cone is chosen to be the bisector, then for 
  
    
      
        k
        ≥
        7
      
    
    {\displaystyle k\geq 7}
  , the spanning ratio is at most 
  
    
      
        1
        
          /
        
        (
        1
        −
        2
        sin
        ⁡
        (
        π
        
          /
        
        k
        )
        )
      
    
    {\displaystyle 1/(1-2\sin(\pi /k))}
  .
For 
  
    
      
        k
        =
        1
      
    
    {\displaystyle k=1}
  , the 
  
    
      
        Θ
      
    
    {\displaystyle \Theta }
  -graph forms a nearest neighbour graph. For 
  
    
      
        k
        =
        2
      
    
    {\displaystyle k=2}
  , it is easy to see that the graph is connected, as each vertex will connect to something to its left, and something to its right, if they exist. For 
  
    
      
        k
        =
        4
      
    
    {\displaystyle k=4}
  , 
  
    
      
        5
      
    
    {\displaystyle 5}
  , 
  
    
      
        6
      
    
    {\displaystyle 6}
  , and 
  
    
      
        ≥
        7
      
    
    {\displaystyle \geq 7}
  , the 
  
    
      
        Θ
      
    
    {\displaystyle \Theta }
  -graph is known to be connected. As yet unpublished results indicate that 
  
    
      
        Θ
      
    
    {\displaystyle \Theta }
  -graphs are connected for 
  
    
      
        k
        =
        3
      
    
    {\displaystyle k=3}
  , as well. Many of these results also give upper and/or lower bounds on their spanning ratios.
When 
  
    
      
        k
      
    
    {\displaystyle k}
   is an even number, we can create a variant of the 
  
    
      
        
          Θ
          
            k
          
        
      
    
    {\displaystyle \Theta _{k}}
  -graph known as the half-
  
    
      
        
          Θ
          
            k
          
        
      
    
    {\displaystyle \Theta _{k}}
  -graph, where the cones themselves are partitioned into even and odd sets in an alternating fashion, and edges are only considered in the even cones (or, only the odd cones). Half-
  
    
      
        
          Θ
          
            k
          
        
      
    
    {\displaystyle \Theta _{k}}
  -graphs are known to have some very nice properties of their own. For example, the half-
  
    
      
        
          Θ
          
            6
          
        
      
    
    {\displaystyle \Theta _{6}}
  -graph (and, consequently, the 
  
    
      
        
          Θ
          
            6
          
        
      
    
    {\displaystyle \Theta _{6}}
  -graph, which is just the union of two complimentary half-
  
    
      
        
          Θ
          
            6
          
        
      
    
    {\displaystyle \Theta _{6}}
  -graphs) is known to be a 2-spanner.


== Software for drawing Theta graphs ==
A tool written in Java
Cone-based Spanners in Computational Geometry Algorithms Library (CGAL)


== See also ==
Yao graph
Semi-Yao graph
geometric spanner


== References =="
174,String interning,1423287,7641,"In computer science, string interning is a method of storing only one copy of each distinct string value, which must be immutable. Interning strings makes some string processing tasks more time- or space-efficient at the cost of requiring more time when the string is created or interned. The distinct values are stored in a string intern pool.
The single copy of each string is called its intern and is typically looked up by a method of the string class, for example String.intern() in Java. All compile-time constant strings in Java are automatically interned using this method.
String interning is supported by some modern object-oriented programming languages, including Python, PHP (since 5.4), Lua, Ruby (with its symbols), Java, Julia and .NET languages. Lisp, Scheme, and Smalltalk are among the languages with a symbol type that are basically interned strings. The library of the Standard ML of New Jersey contains an atom type that does the same thing. Objective-C's selectors, which are mainly used as method names, are interned strings.
Objects other than strings can be interned. For example, in Java, when primitive values are boxed into a wrapper object, certain values (any boolean, any byte, any char from 0 to 127, and any short or int between −128 and 127) are interned, and any two boxing conversions of one of these values are guaranteed to result in the same object.


== History ==
Lisp introduced the notion of interned strings for its symbols. Historically, the data structure used as a string intern pool was called an oblist (when it was implemented as a linked list) or an obarray (when it was implemented as an array).
Modern Lisp dialects typically distinguish symbols from strings; interning a given string returns an existing symbol or creates a new one, whose name is that string. Symbols often have additional properties that strings do not (such as storage for associated values, or namespacing): the distinction is also useful to prevent accidentally comparing an interned string with a not-necessarily-interned string, which could lead to intermittent failures depending on usage patterns.


== Motivation ==
String interning speeds up string comparisons, which are sometimes a performance bottleneck in applications (such as compilers and dynamic programming language runtimes) that rely heavily on associative arrays with string keys to look up the attributes and methods of an object. Without interning, checking that two different strings are equal involves examining every character of both strings (in the case where the strings are equal - when they differ the character comparison only has to proceed to the point at which either different characters are found, or to the end of the shortest string). This is slow for several reasons: it is inherently O(n) in the length of the strings; it typically requires reads from several regions of memory, which take time; and the reads fill up the processor cache, meaning there is less cache available for other needs. With interned strings, a simple object identity test suffices after the original intern operation; this is typically implemented as a pointer equality test, normally just a single machine instruction with no memory reference at all.
String interning also reduces memory usage if there are many instances of the same string value; for instance, it is read from a network or from storage. Such strings may include magic numbers or network protocol information. For example, XML parsers may intern names of tags and attributes to save memory. Network transfer of objects over Java RMI serialization object streams can transfer strings that are interned more efficiently, as the String object's handle is used in place of duplicate objects upon serialization.


== Issues ==


=== Multithreading ===
One source of drawbacks is that string interning may be problematic when mixed with multithreading. In many systems, string interns are required to be global across all threads within an address space (or across any contexts which may share pointers), thus the intern pool(s) are global resources that should be synchronized for safe concurrent access. While this only affects string creation (where the intern pool must be checked and modified if necessary), and double-checked locking may be used on platforms where this is a safe optimization, the need for mutual exclusion when modifying the intern pool can be expensive.
Contention can also be reduced by partitioning the string space into multiple pools, which can be synchronized independently of one another.


=== Reclaiming unused interned strings ===
Many implementations of interned strings do not attempt to reclaim (manually or otherwise) strings that are no longer used. For applications where the number of interned strings is small or fixed, or which are short-lived, the loss of system resources may be tolerable. But for long-running systems where large numbers of string interns are created at runtime, the need to reclaim unused interns may arise. This task can be handled by a garbage collector, though for this to work correctly weak references to string interns must be stored in the intern pool.


== See also ==
Flyweight pattern


== References ==


== External links ==
Visual J# String class
.NET String Class
Guava Java Library - Interner - Non-permgen String.intern and supports other immutable types with weak and strong referenced implementations"
175,Co-simulation,42389862,7550,"In co-simulation the different subsystems which form a coupled problem are modeled and simulated in a distributed manner. Hence, the modeling is done on the subsystem level without having the coupled problem in mind. Furthermore, the coupled simulation is carried out by running the subsystems in a black-box manner. During the simulation the subsystems will exchange data. Co-simulation can be considered as the joint simulation of the already well-established tools and semantics; when they are simulated with their suitable solvers . Co-simulation proves its advantage in validation of multi-domain and cyber physical system by offering a flexible solution which allows consideration of multiple domains with different time steps, at the same time. As the calculation load is shared among simulators, co-simulation also enables the possibility of large scale system assessment.


== Abstraction layers of co-simulation framework ==
The following introduction and structuration is proposed in .
Establishing a co-simulation framework can be a challenging and complex task because it requires a strong interoperability among the participating elements, especially in case of multiple-formalism co-simulation. Harmonization, adaptation and eventually changes of actual employed standards and protocols in individual models need to be done to be able to integrate into the holistic framework. The generic layered structuration of co-simulation framework  highlights the intersection of domains and the issues that need to be solved in the process of designing a co-simulation framework. In general, a co-simulation framework constitutes of five abstraction layers:
From conceptual structuration, the architecture on which the co-simulation framework is developed and the formal semantic relations/syntactic formulation are defined. The detailed technical implementation and synchronization techniques are covered in dynamic and technical layers.


== Problem Partitioning - Architecture of co-simulation ==
The partitioning procedure identifies the process of spatial separation of the coupled problem into multiple partitioned subsystems. Information is exchanged through either ad-hoc interfaces or via intermediate buffer governed by a master algorithm. Master algorithm (where exists) is responsible for instantiating the simulators and for orchestrating the information exchange (simulator-simulator or simulator-orchestrator).


== Coupling methods ==
Co-simulation coupling methods can be classified into operational integration and formal integration, depending on abstraction layers. In general, operational integration is used in co-simulation for a specific problem and aims for interoperability at dynamic and technical layers (i.e. signal exchange). On the other hand, formal integration allows interoperability in semantic and syntactic level via either model coupling or simulator coupling. Formal integration often involves a master federate to orchestrate the semantic and syntactic of the interaction among simulators.
From a dynamic and technical point of view, it is necessary to consider the synchronization techniques and communication patterns in the process of implementation.


=== Communication Patterns ===


==== Gauss-Seidel (serial) ====


==== Jacobi (parallel) ====


=== Synchronization techniques ===


==== Conservative ====


==== Optimistic ====


=== Synchronization techniques for continuous vs. discrete event simulators ===


== Standards and Software realization ==


=== High Level Architecture ===


=== Functional Mock-up Interface ===
For signals co-simulation can be performed with a standardized interface called Functional Mock-up Interface.


=== Agent-based model ===
Agent-based model is a modeling approach of complex systems. Each simulator is seen as an agent and ""behaves"" according to its associated simulator. The agents interact, exchange data, between each other in a network. The simulation environment Mecysco is an implementation of this approach [1]


== Examples ==


=== 1D Spring ===


==== Steady State Problem ====


===== Interface Constraints =====


===== Stability =====


=== 1D/3D Fluid Dynamics ===


== References =="
176,O'Reilly Open Source Award,33121710,7525,"The O'Reilly Open Source Award is presented to individuals for dedication, innovation, leadership and outstanding contribution to open source. From 2005 to 2009 the award was known as the Google–O'Reilly Open Source Award but since 2010 the awards have only carried the O'Reilly name.


== Award winners ==
This is a list of the winners of individuals that won the annual O'Reilly Open Source Awards.


== 2005 ==
Best Communicator: Doc Searls (co-author of ""The Cluetrain Manifesto"" and Senior Editor for Linux Journal)
Best Evangelist: Jeff Waugh (Ubuntu Linux and Gnome desktop environment)
Best Diplomat: Geir Magnusson Jr
Best Integrator: D. Richard Hipp (SQLite)
Best Hacker: David Heinemeier Hansson (Ruby on Rails and 37Signals)


== 2006 ==
Best Legal Eagle: Cliff Schmidt (Apache License)
Best Community Activist: Gervase Markham (programmer) (Firefox)
Best Toolmaker: Julian Seward (Valgrind)
Best Corporate Liaison: Stefan Taxhet (OpenOffice.org)
Best All-around Developer: Peter Lundblad (Subversion)


== 2007 ==
Best Community Builder: Karl Fogel
Best FUD Fighter: Pamela Jones
Best Accessibility Architect: Aaron Leventhal
Best Strategist: David Recordon
Best Outstanding Lifetime Contributions: Paul Vixie


== 2008 ==
Best Community Amplifier: Chris Messina - BarCamp, Microformats and Spread Firefox
Best Contributor: Angela Byron - Drupal
Best Education Enabler: Martin Dougiamas - Moodle
Best Interoperator: Andrew Tridgell - Samba and Rsync
Defender of Rights: Harald Welte - gpl-violations.org


== 2009 ==
Best Open Source Database Hacker: Brian Aker - Drizzle and MySQL
Database Jedi Master: Bruce Momjian - PostgreSQL
Best Community Builder: Clay Johnson - Sunlight Labs
Best Social Networking Hacker: Evan Prodromou - identi.ca and Laconica
Best Education Hacker: Penny Leach - Mahara and Moodle


== 2010 ==
Jeremy Allison - Samba
Deborah Bryant
Brad Fitzpatrick - memcached, Gearman, MogileFS, and OpenID
Leslie Hawthorn - Google's Summer of Code
Greg Stein - Subversion, Apache, Python


== 2011 ==
Fabrice Bellard - QEMU, FFmpeg
Karen Sandler - SFLC, licensing
Keith Packard - X Window System
Ryan Dahl - Node.js
Kohsuke Kawaguchi - Jenkins


== 2012 ==
Massimo Banzi
Jim Jagielski
Christie Koehler
Bradley M. Kuhn
Elizabeth Krumbach


== 2013 ==
Behdad Esfahbod - HarfBuzz
Jessica McKellar - Python Software Foundation
Limor Fried - Adafruit Industries
Valerie Aurora - Ada Initiative
Paul Fenwick - Perl
Martin Michlmayr - Debian Project


== 2014 ==
Sage Weil - Ceph
Deb Nicholson - MediaGoblin and OpenHatch.org
John ""Warthog9"" Hawley - gitweb and Linux kernel site kernel.org
Erin Petersen - Outercurve Foundation and Girl Develop It
Patrick Volkerding - Slackware Linux


== 2015 ==
Doug Cutting
Sarah Mei
Christopher Webber
Stefano Zacchiroli
Marina Zhurakhinskaya


== 2016 ==
Sage Sharp
Rikki Endsley
VM (Vicky) Brasseur
Máirín Duffy
Marijn Haverbeke


== 2017 ==
William John Sullivan, Executive Director, Free Software Foundation.
Nithya Ruff, Senior Director, Open Source Practice Comcast Director, Linux Foundation, Boards of Directors.
Tony Sebro, General Counsel, Software Freedom Conservancy; Outreachy coordinator.
Katie McLaughlin, BeeWare / KatieConf.
Juan González Gómez, R&D Engineer & Member of the CloneWars and FPGAwars communities


== References ==


== External links ==
Google-O'Reilly Open Source Awards - Hall of Fame
O'Reilly Open Source Awards 2010
OSCON 2010: O'Reilly Open Source Awards (photo)
OSCON 2011: O'Reilly Open Source Awards
OSCON 2011: O'Reilly Open Source Awards (video)
Geek Feminism Wiki: Google O'Reilly Open Source Award"
177,MPT8080,23215202,7494,"The MPT8080 ""Microtutor"" is a microprocessor trainer based on the Intel 8080 processor, developed by Limrose Electronics. It was designed in the mid-1970s to assist in the understanding of the then-new microprocessors.
Users of the MPT8080 enter assembly language programs via binary switches or a hexadecimal keypad. While the code executes, the user can observe what is happening on the address, data, and control signals of the microprocessor. The MPT8080 acts like a simulator, in that code can be stepped through one instruction—or each cycle of each individual instruction—at a time to observe what is happening.
The MPT8080 has simple input and output, consisting of eight LEDs and eight switches. The input port allows code to sense the state of external switches, whilst the output port can display information on one of its eight LEDs. The input and outputs port also have connectors, allowing them to be connected to external signals through accessory patch kits, allowing the MPT8080 to control and monitor other circuitry.


== History ==
Initially, a Motorola 6800-based trainer was developed alongside the Intel 8080–based model, but due to technical and operational issues the 6800 trainer was abandoned.
As recently as 2012, the MPT8080 remained in academic use at King's College London, as part of a course in practical physics.  As of 2011, the MPT8080 was still available for sale.


=== MPT8080 Version 1 ===
The initial version of the MPT8080 was designed by Dr. Ravi Raizada, the chief executive officer of Limrose Electronics. It was first marketed in 1977.
Details of version 1 of the microtutor are included in the book : Small Systems Computer Sourcebook, author: J C Boonham
This version used eight binary switches and a load button for program entry.


=== MPT8080 Version 2 ===
The second version of the MPT8080, introduced around 1980, replaced the binary switch input with a 16-key hexadecimal keypad. Although more than half of the trainer's circuitry was redesigned for this version, it remained compatible with the first version. A trace mode was added to allow single-cycle execution as well as machine cycles; this allowed the user to quickly step through code until reaching the portion of the program in which the user was interested. Latest UK price £495+vat (source: Limrose U.K.Price List 1st Sept 2010).


== Operating the MPT8080 ==
Programs are entered in the MPT8080 in data entry mode (""MMLE""). Each byte of the program is entered, either by toggling the binary switches and pressing the load button, or by entering the byte on the hexadecimal keypad. The program can then be executed.
The program can control the eight output port lines, turning the associated LEDs on or off.
By selecting single-step and single-cycle mode and stepping through a program with the step button, the user can see exactly what happens during every instruction cycle, observing the program counter, data bus and control signals on their corresponding LEDs.
By selecting single-instruction mode, rather than single-cycle mode, each press of the step button will execute a complete instruction, rather than a single cycle. This is useful for quickly advancing to a specific address.
The input/output ports can be used to read external signals and drive output devices like motors and buzzers with little or no additional hardware.
The system is programmed directly in 8080 machine code:

Address       Code           Instruction                     Comments
0000          DB 00          IN  0                           Read from switches
0002          D3 00          OUT 0                           write to LEDs
0004          C3 00 00       JMP 0                           return to start of code


== See also ==
List of early microcomputers
8080


== References ==

Notes
A cached copy of the Department of Physics (2008-10-06). ""Machine code programming"" can be found on the internet archive at:
2ndyearmanual2008-2009lab1.pdf at the Wayback Machine (archived February 16, 2010)


== Further reading ==
Raizada, Ravi S. (1979). Introduction to Microprocessors, Volume One.  (printed training manual provided with MPT8080)


== External links ==
Official website
Web site on old computers, includes some pictures of version 1 of the Limrose MPT-8080 microprocessor trainer."
178,Digital Humanities conference,27990947,7403,"The Digital Humanities conference is an academic conference for the field of digital humanities. It is hosted by Alliance of Digital Humanities Organizations and has been held annually since 1989.


== History ==
The first joint conference was held in 1989, at the University of Toronto—but that was the 16th annual meeting of ALLC, and the ninth annual meeting of the ACH-sponsored International Conference on Computers and the Humanities (ICCH).
The Chronicle of Higher Education has called the conference ""highly competitive"" but ""worth the price of admission,"" praising its participants' focus on best practices, the intellectual community it has fostered, and the tendency of its organizers to sponsor attendance of early-career scholars (important given the relative expense of attending it, as compared to other academic conferences).
An analysis of the Digital Humanities conference abstracts between 2004 and 2014 highlights some trends evident in the evolution of the conference (such as the increasing rate of new authors entering the field, and the continuing disproportional predominance of authors from North America represented in the abstracts). An extended study (2000-2015) offer a feminist and critical engagement of Digital Humanities conferences with solutions for a more inclusive culture. Scott Weingart has also published detailed analyses of submissions to Digital Humanities 2013, 2014, 2015, and 2016 on his blog.


== Conferences ==


== References ==


== External links ==
Alliance of Digital Humanities Organizations official website"
179,Dataism,56221934,7385,"Dataism is a term that has been used to describe the mindset or philosophy created by the emerging significance of Big Data. It was first used by David Brooks in the New York Times in 2013. More recently, the term has been expanded to describe what social scientist Yuval Noah Harari has called an emerging ideology or even a new form of religion, in which 'information flow' is the 'supreme value'.


== History ==
""If you asked me to describe the rising philosophy of the day, I'd say it is Data-ism"", wrote David Brooks in the New York Times in February 2013. Brooks argued that in a world of increasing complexity, relying on data can reduce cognitive biases and ""illuminate patterns of behavior we haven't yet noticed"".
In 2015, Steve Lohr's book 'Data-ism' looked at how Big Data is transforming society, using the term to describe the Big Data revolution.


=== Development by Harari ===
In his 2016 book, Homo Deus, Yuval Noah Harari takes the idea of Dataism further, putting it into a historical context. He argues that all competing political or social structures can be seen as data processing systems: ""Dataism declares that the universe consists of data flows, and the value of any phenomenon or entity is determined by its contribution to data processing"".

Harari posits that ""we may interpret the entire human species as a single data processing system, with individual humans serving as its chips."" He then argues that the whole of human history can be read as a process of improving the efficiency of this system by increasing the number and variety of processors/chips (humans) in the system, increasing the number of connections between the processors and increasing the freedom of movement along existing connections. A condensed form of this argument can be read in Harari's Wired article from 2016.
Harari goes on to argue that Dataism, like any other religion, has practical commandments. A Dataist should want to ""maximise dataflow by connecting to more and more media"", and believes that freedom of information is ""the greatest good of all"". Harari also argues that Aaron Swartz, who took his life in 2013 after being prosecuted for releasing hundreds of thousands of scientific papers from the JSTOR archive online for free, could be called the ""first martyr"" of Dataism.
Writing in the Financial Times, Harari argued that Dataism presents an existential challenge to the dominant moral ideology of Humanism, which sees human feelings as the ultimate authority in the world: ""humanism is now facing an existential challenge and the idea of “free will” is under threat... Once Big Data systems know me better than I know myself, authority will shift from humans to algorithms."" Harari predicts that the logical conclusion of this process is that eventually humans will give algorithms the authority to make the most important decisions in their lives, such as who to marry.


== Criticism ==
Commenting on Harari's characterisation of Dataism, security analyst Daniel Meissler believes that Dataism does not present the challenge to the ideology of liberal humanism that Harari claims, because humans will simultaneously be able to believe in their own importance and that of data.
Harari himself raises some criticisms, such as the problem of consciousness, which Dataism is unlikely to illuminate. Humans may also find out that organisms are not algorithms, he suggests.
Other analysts such as Terry Ortleib have looked at the extent to which Dataism poses a dystopian threat to humanity.


== References ==


== External links ==
Techopedia definition of Dataism
Wired: 'Homo sapiens is an obsolete algorithm': by Yuval Noah Harari
Steve Lohr on Data-ism"
180,Johan Bollen,39753732,7352,"Johan Lambert Trudo Maria Bollen (born 1971) is a scientist that currently investigates complex systems and networks, the relation between social media and a variety of socio-economic phenomena such as the financial markets, public health, and social well-being, as well as Science of Science with a focus on impact metrics derived from usage data. He presently works as associate professor at the Indiana University School of Informatics of Indiana University Bloomington and a fellow at the SparcS Institute of Wageningen University and Research Centre in the Netherlands. He is best known for his work on scholarly impact metrics, measuring public well-being from large-scale socia media data, and correlating Twitter mood to stock market prices. He has taught courses on data mining, information retrieval, and digital libraries. His research has been funded by The Andrew W. Mellon Foundation, National Science Foundation, Library of Congress, National Aeronautics and Space Administration and the Los Alamos National Laboratory. In his free time, he DJs at the Root Cellar Lounge in Bloomington, Indiana. He specializes in Deep House and Techno.


== Biography ==
Bollen received his MS in Experimental Psychology from the Free University of Brussels in 1993 after a master thesis ""Learning to Select Activities: a Conditionable System for an Autonomous Robot that Learns to Use Drive Reduction as Reinforcement."" He defended his Ph.D. in Psychology from the Free University of Brussels in October 2001 on the subject of cognitive models of human hypertext navigation. From 2002 to 2005 he was an Assistant Professor at the Department of Computer Science of Old Dominion University. He was a Staff Scientist at the Los Alamos National Laboratory from 2005-2009. He is currently an Associate Professor at the Indiana University School of Informatics and Computing.


== Work ==
Bollen's work has been extensively covered in the press and popular media and has received over 6,900 citations (2016 h-index 33). Bollen was awarded two patents: one for ""Usage based indicators to assess the impact of scholarly works: architecture and method"" (US 8135662 B2) and ""Predicting economic trends via network communication mood tracking"" (US 8380607 B2). His work has found widespread application, e.g. in systems for digital library recommendations systems (Ex Libris bX Recommender Service) and in systems that use social media information to generate financial trading signals. As a result on his work on predicting the stock market based on Twitter mood, Derwent Capital Markets started the Absolute Return fund, the worlds's first Twitter hedge fund. However, it later shut down after thirteen months.


== Selected academic works ==
Bollen has published over 70 papers. A selection of the highly cited ones:
2011, Twitter mood predicts the stock market.
2005, Co-authorship networks in the digital library research community.
2009, Modeling public mood and emotion: Twitter sentiment and socio-economic phenomena 
2006: Bollen, Johan; Rodriquez, Marko A.; Van de Sompel, Herbert (2006). ""Journal status"". Scientometrics. 69 (3): 669–687.
2009, A Principal Component Analysis of 39 Scientific Impact Measures
2005, Toward alternative metrics of journal impact: A comparison of download and citation data


== References =="
181,Information Systems Research in Scandinavia,30292493,7271,"The IRIS (Information Systems Research in Scandinavia) Association is a non-profit organization aiming to promote research and research education in the use, development and management of information systems in Scandinavia, and making that research known in the international research community and among practitioners. The IRIS association publishes the Scandinavian Journal of Information Systems (SJIS) and holds the IRIS Conference as well as the Scandinavian Conference on Information Systems Conference (SCIS). Its current president is Judith Molka-Danielsen (2011-2013).


== IRIS Conferences ==
In 1978 the IRIS started holding annual conferences and is the oldest consecutive IS conference in the world. The conference is organized as an annual working seminar for Scandinavian researchers and PhD students. After its first few meetings in Finland, the locations now alternate between different Scandinavian countries. Among the subjects discussed at the IRIS conferences is Information and Communications Technology, Business Process Management and Participatory Design. In 2005 IRIS had a Steering Committee the chairman of which was Jan Pries-Heje.
In 2006 Judith Molka-Danielsen, and her associates published a paper that identified the most prolific authors who had successfully submitted papers to IRIS. The top ten researchers were: Lars Mathiassen, Markku Nurminen, Pertti Järvinen, Carsten Sørensen, Per Flensburg, Karl Heinz Kautz, Peter Axel Nielsen, Lars Svensson, Urban Nuldén, and Ole Hanseth.
The next IRIS conference will be held in Oulu, Finland, in August 2015.


== Past Conference Locations ==
2014: IRIS37 Ringsted, Denmark
2013: IRIS36 Gran, Norway
2012: IRIS35 Sigtuna, Sweden
2011: IRIS34 Turku, Finland
2010: IRIS33 Comwell Rebild Bakker, Denmark
2009: IRIS32 Molde, Norway
2008: IRIS31 Åre, Sweden
2007: IRIS30 Tampere, Finland
2006: IRIS29 Helsingør, Denmark
2005: IRIS28 Kristiansand, Norway
2004: IRIS27 Falkenberg, Sweden
2003: IRIS26 Porvoo, Finland
2002: IRIS25 Bautahøj, Denmark
2001: IRIS24 Bergen, Norway
2000: IRIS23 Uddevalla, Sweden
1999: IRIS22 Keuruu, Finland
1998: IRIS21 Sæby, Denmark
1997: IRIS20 Hankø, Norway
1996: IRIS19 Lökeberg, Sweden
1995: IRIS18 Gjern, Denmark
1994: IRIS17 Syöte, Finland
1993: IRIS16 Copenhagen, Denmark
1992: IRIS15 Larkollen, Norway
1991: IRIS14 Umeå-Lövånger, Sweden
1990: IRIS13 Turku, Finland
1989: IRIS12 Skagen, Denmark
1988: IRIS11 Røros, Norway
1987: IRIS10 Tampere, Finland
1986: IRIS9 Lund, Sweden
1985: IRIS8 Århus, Denmark
1984: IRIS7 Helsinki, Finland
1983: IRIS6 Øystese, Norway
1982: IRIS5 Stockholm, Sweden
1981: IRIS4 Oulu, Finland
1980: IRIS3 Saarijärvi, Finland
1979: IRIS2 Dragsfjärd, Finland
1978: IRIS1 Tampere, Finland


== See also ==
Information Systems Research
University of Turku
Roskilde University


== References ==


== External links ==
""Welcome to the IRIS Association"". http://iris.cs.aau.dk: IRIS. Retrieved 8 June 2011. If you are a researcher or practitioner in the field of information systems interested in your own development and in the building of knowledge in organizations and society - you are most welcome to become a member of IRIS Association."
182,Concatenation,64474,7264,"In formal language theory and computer programming, string concatenation is the operation of joining character strings end-to-end. For example, the concatenation of ""snow"" and ""ball"" is ""snowball"". In some but not all formalisations of concatenation theory, also called string theory, string concatenation is a primitive notion.


== Syntax ==
In many programming languages, string concatenation is a binary infix operator. The + (plus) operator is often overloaded to denote concatenation for string arguments: ""Hello, "" + ""World"" has the value ""Hello, World"". In other languages there is a separate operator, particularly to specify implicit type conversion to string, as opposed to more complicated behavior for generic plus. Examples include . in Edinburgh IMP, Perl, and PHP, .. in Lua, and & in Ada and Visual Basic. Other syntax exists, like || in PL/I and Oracle Database SQL.
In a few languages, notably C, C++, and Python, there is string literal concatenation, meaning that adjacent string literals are concatenated, without any operator: ""Hello, "" ""World"" has the value ""Hello, World"". In other languages, concatenation of string literals with an operator is evaluated at compile time, via constant folding.


== Implementation ==
In programming, string concatenation generally occurs at run time, as string values are not in general known until run time. However, in the case of string literals, the values are known at compile time, and thus string concatenation can be done at compile time, either via string literal concatenation or via constant folding.


== Concatenation of sets of strings ==
In formal language theory and pattern matching (including regular expressions), the concatenation operation on strings is generalised to an operation on sets of strings as follows:
For two sets of strings S1 and S2, the concatenation S1S2 consists of all strings of the form vw where v is a string from S1 and w is a string from S2, or formally S1S2 = { vw : v ∈ S1, w ∈ S2 }. Many authors also use concatenation of a string set and a single string, and vice versa, which are defined similarly by S1w = { vw : v ∈ S1 } and vS2 = { vw : w ∈ S2 }. In these definitions, the string vw is the ordinary concatenation of strings v and w as defined in the introductory section.
For example, if F = { a, b, c, d, e, f, g, h }, and R = { 1, 2, 3, 4, 5, 6, 7, 8 }, then FR denotes the set of all chess board coordinates in algebraic notation, while eR denotes the set of all coordinates of the kings' file.
In this context, sets of strings are often referred to as formal languages. The concatenation operator is usually expressed as simple juxtaposition (as with multiplication).


== Algebraic properties ==
The strings over an alphabet, with the concatenation operation, form an associative algebraic structure with identity element the null string—a free monoid.
Sets of strings with concatenation and alternation form a semiring, with concatenation (*) distributing over alternation (+); 0 is the empty set and 1 the set consisting of just the null string.


== Applications ==


=== Audio/telephony ===
In programming for telephony, concatenation is used to provide dynamic audio feedback to a user. For example, in a ""time of day"" speaking clock, concatenation is used to give the correct time by playing the appropriate recordings concatenated together. For example:
""At the tone the time will be""
""Eight""
""Thirty""
""Five""
""and""
""Twenty""
""Two""
""Seconds""
The recordings themselves exist separately, but playing them one after the other provides a grammatically correct sentence to the listener.
This technique is also used in number change announcements, voice mail systems, or most telephony applications that provide dynamic feedback to the caller (e.g. moviefone, tellme, and others).
Programming for any kind of computerised public address system can also employ concatenation for dynamic public announcements (for example, flights in an airport). The system would archive recorded speech of numbers, routes or airlines, destinations, times, etc. and play them back in a specific sequence to produce a grammatically correct sentence that is announced throughout the facility.


=== Database theory ===
One of the principles of relational database design is that the fields of data tables should reflect a single characteristic of the table's subject, which means that they should not contain concatenated strings. When concatenation is desired in a report, it should be provided at the time of running the report. For example, to display the physical address of a certain customer, the data might include building number, street name, building sub-unit number, city name, state/province name, postal code, and country name, e.g., ""123 Fake St Apt 4, Boulder, CO 80302, USA"", which combines seven fields. However, the customers data table should not use one field to store that concatenated string; rather, the concatenation of the seven fields should happen upon running the report. The reason for such principles is that without them, the entry and updating of large volumes of data becomes error-prone and labor-intensive. Separately entering the city, state, ZIP code, and nation allows data-entry validation (such as detecting an invalid state abbreviation). Then those separate items can be used for sorting or indexing the records, such as all with ""Boulder"" as the city name.


== See also ==
Rope (data structure)


== References =="
183,Comparison of programming languages (strings),7703569,7186,"This comparison of programming languages (strings) compares the features of string data structures or text-string processing for over 52 various computer programming languages.


== Concatenation ==
Different languages use different symbols for the concatenation operator. Many languages use the ""+"" symbol, though several deviate from this.


=== Common variants ===


=== Unique variants ===
Awk uses the empty string: two expressions adjacent to each other are concatenated. This is called juxtaposition. Unix shells have a similar syntax. Rexx uses this syntax for concatenation including an intervening space.
C (along with Python) allows juxtaposition for string literals, however, for strings stored as character arrays, the strcat function must be used.
COBOL uses the STRING statement to concatenate string variables.
MATLAB and Octave use the syntax ""[x y]"" to concatenate x and y.
Visual Basic Versions 1 to 6 can also use the ""+"" sign but, this leads to ambiguity if a string representing a number and a number is added together.
Microsoft Excel allows both ""&"" and the function ""=CONCATENATE(X,Y)"".


== String literals ==
This section compares styles for declaring a string literal.


=== Quoted raw ===
""Raw"" meaning that the interpreter/compiler does not recognize any variable or constant identifiers located inside the string and the content of the identifier will not replace the identifier in the string.


=== Quoted interpolated ===
""Interpolated"" means that the interpreter/compiler does recognize a variable or constant identifier located inside the string and the content of the identifier will replace the identifier in the string.


=== Escaped quotes ===
""Escaped"" quotes means that a 'flag' symbol is used to warn that the character after the flag is used in the string rather than ending the string.


=== Dual quoting ===
""Dual quoting"" means that whenever a quote is used in a string, it is used twice, and one of them is discarded and the single quote is then used within the string.


=== Multiple quoting ===


=== Here document ===


=== Unique quoting variants ==="
184,K-server problem,7767038,7142,"The k-server problem is a problem of theoretical computer science in the category of online algorithms, one of two abstract problems on metric spaces that are central to the theory of competitive analysis (the other being metrical task systems). In this problem, an online algorithm must control the movement of a set of k servers, represented as points in a metric space, and handle requests that are also in the form of points in the space. As each request arrives, the algorithm must determine which server to move to the requested point. The goal of the algorithm is to keep the total distance all servers move small, relative to the total distance the servers could have moved by an optimal adversary who knows in advance the entire sequence of requests.
The problem was first posed by Mark Manasse, Lyle A. McGeoch and Daniel Sleator (1990). The most prominent open question concerning the k-server problem is the so-called k-server conjecture, also posed by Manasse et al. This conjecture states that there is an algorithm for solving the k-server problem in an arbitrary metric space and for any number k of servers that has competitive ratio exactly k. Manasse et al. were able to prove their conjecture when k = 2, and for more general values of k when the metric space is restricted to have exactly k+1 points. Chrobak and Larmore (1991) proved the conjecture for tree metrics. The special case of metrics in which all distances are equal is called the paging problem because it models the problem of page replacement algorithms in memory caches, and was also already known to have a k-competitive algorithm (Sleator and Tarjan 1985). Fiat et al. (1990) first proved that there exists an algorithm with finite competitive ratio for any constant k and any metric space, and finally Koutsoupias and Papadimitriou (1995) proved that Work Function Algorithm (WFA) has competitive ratio 2k - 1. However, despite the efforts of many other researchers, reducing the competitive ratio to k or providing an improved lower bound remains open as of 2014. The most common believed scenario is that the Work Function Algorithm is k-competitive. To this direction, in 2000 Bartal and Koutsoupias showed that this is true for some special cases (if the metric space is a line, a weighted star or any metric of k+2 points).
In 2011, a randomized algorithm with competitive bound Õ(log2k log3n) was found.


== Example ==
To make the problem more concrete, imagine sending customer support technicians to customers when they have trouble with their equipment. In our example problem there are two technicians, Mary and Noah, serving three customers, in San Francisco, California; Washington, DC; and Baltimore, Maryland. As a k-server problem, the servers are the technicians, so k = 2 and this is a 2-server problem. Washington and Baltimore are 35 miles (56 km) apart, while San Francisco is 3,000 miles (4,800 km) away from both, and initially Mary and Noah are both in San Francisco.
Consider an algorithm for assigning servers to requests that always assigns the closest server to the request, and suppose that each weekday morning the customer in Washington needs assistance while each weekday afternoon the customer in Baltimore needs assistance, and that the customer in San Francisco never needs assistance. Then, our algorithm will assign one of the servers (say Mary) to the Washington area, after which she will always be the closest server and always be assigned to all customer requests. Thus, every day our algorithm incurs the cost of traveling between Washington and Baltimore and back, 70 miles (110 km). After a year of this request pattern, the algorithm will have incurred 20,500 miles (33,000 km) travel: 3000 to send Mary to the East Coast, and 17,500 for the trips between Washington and Baltimore. On the other hand, an optimal adversary who knows the future request schedule could have sent both Mary and Noah to Washington and Baltimore respectively, paying 6,000 miles (9,700 km) of travel once but then avoiding any future travel costs. The competitive ratio of our algorithm on this input is 20,500/6000 or approximately 3.4, and by adjusting the parameters of this example the competitive ratio of this algorithm can be made arbitrarily large.
Thus we see that always assigning the closest server can be far from optimal. On the other hand, it seems foolish for an algorithm that does not know future requests to send both of its technicians away from San Francisco, as the next request could be in that city and it would have to send someone back immediately. So it seems that it is difficult or impossible for a k-server algorithm to perform well relative to its adversary. However, for the 2-server problem there exists an algorithm that always has a total travel distance of at most twice the adversary's distance. The k-server conjecture states that similar solutions exist for problems with any larger number of technicians.


== References ==

Chrobak, Marek; Larmore, Lawrence L. (1991). ""An optimal on-line algorithm for K-servers on trees"". SIAM Journal on Computing. 20 (1): 144–148. doi:10.1137/0220008. CS1 maint: Multiple names: authors list (link)
Fiat, A.; Rabani, Y.; Ravid, Y. (1990). ""Competitive k-server algorithms"". Proceedings of the 31st Annual IEEE Symposium on Foundations of Computer Science. pp. 454–463. 
Koutsoupias, Elias; Papadimitriou, Christos H. (1995). ""On the k-server conjecture"". Journal of the ACM. 42 (5): 971–983. doi:10.1145/210118.210128. CS1 maint: Multiple names: authors list (link)
Manasse, Mark; McGeoch, Lyle A.; Sleator, Daniel D. (1990). ""Competitive algorithms for server problems"". Journal of Algorithms. 11 (2): 208–230. doi:10.1016/0196-6774(90)90003-W. CS1 maint: Multiple names: authors list (link)
Sleator, Daniel D.; Tarjan, Robert E. (1985). ""Amortized efficiency of list update and paging rules"". Communications of the ACM. 28 (2): 202–208. doi:10.1145/2786.2793. CS1 maint: Multiple names: authors list (link)"
185,Jaro–Winkler distance,6782835,7124,"In computer science and statistics, the Jaro–Winkler distance is a string metric for measuring the edit distance between two sequences. It is a variant proposed in 1990 by William E. Winkler of the Jaro distance metric (1989, Matthew A. Jaro). Informally, the Jaro distance between two words is the minimum number of single-character transpositions required to change one word into the other.
The Jaro–Winkler distance uses a prefix scale 
  
    
      
        p
      
    
    {\displaystyle p}
   which gives more favourable ratings to strings that match from the beginning for a set prefix length 
  
    
      
        ℓ
      
    
    {\displaystyle \ell }
  .
The lower the Jaro–Winkler distance for two strings is, the more similar the strings are. The score is normalized such that 0 equates to no similarity and 1 is an exact match. The Jaro–Winkler similarity is given by 1 − Jaro–Winkler distance.
Although often referred to as a distance metric, the Jaro–Winkler distance is not a metric in the mathematical sense of that term because it does not obey the triangle inequality.


== Definition ==


=== Jaro Similarity ===
The Jaro Similarity 
  
    
      
        s
        i
        
          m
          
            j
          
        
      
    
    {\displaystyle sim_{j}}
   of two given strings 
  
    
      
        
          s
          
            1
          
        
      
    
    {\displaystyle s_{1}}
   and 
  
    
      
        
          s
          
            2
          
        
      
    
    {\displaystyle s_{2}}
   is

  
    
      
        s
        i
        
          m
          
            j
          
        
        =
        
          {
          
            
              
                
                  0
                
                
                  
                    if 
                  
                  m
                  =
                  0
                
              
              
                
                  
                    
                      1
                      3
                    
                  
                  
                    (
                    
                      
                        
                          m
                          
                            
                              |
                            
                            
                              s
                              
                                1
                              
                            
                            
                              |
                            
                          
                        
                      
                      +
                      
                        
                          m
                          
                            
                              |
                            
                            
                              s
                              
                                2
                              
                            
                            
                              |
                            
                          
                        
                      
                      +
                      
                        
                          
                            m
                            −
                            t
                          
                          m
                        
                      
                    
                    )
                  
                
                
                  
                    otherwise
                  
                
              
            
          
          
        
      
    
    {\displaystyle sim_{j}=\left\{{\begin{array}{l l}0&{\text{if }}m=0\\{\frac {1}{3}}\left({\frac {m}{|s_{1}|}}+{\frac {m}{|s_{2}|}}+{\frac {m-t}{m}}\right)&{\text{otherwise}}\end{array}}\right.}
  
Where:

  
    
      
        
          |
        
        
          s
          
            i
          
        
        
          |
        
      
    
    {\displaystyle |s_{i}|}
   is the length of the string 
  
    
      
        
          s
          
            i
          
        
      
    
    {\displaystyle s_{i}}
  ;

  
    
      
        m
      
    
    {\displaystyle m}
   is the number of matching characters (see below);

  
    
      
        t
      
    
    {\displaystyle t}
   is half the number of transpositions (see below).
Two characters from 
  
    
      
        
          s
          
            1
          
        
      
    
    {\displaystyle s_{1}}
   and 
  
    
      
        
          s
          
            2
          
        
      
    
    {\displaystyle s_{2}}
   respectively, are considered matching only if they are the same and not farther than 
  
    
      
        
          ⌊
          
            
              
                max
                (
                
                  |
                
                
                  s
                  
                    1
                  
                
                
                  |
                
                ,
                
                  |
                
                
                  s
                  
                    2
                  
                
                
                  |
                
                )
              
              2
            
          
          ⌋
        
        −
        1
      
    
    {\displaystyle \left\lfloor {\frac {\max(|s_{1}|,|s_{2}|)}{2}}\right\rfloor -1}
  .
Each character of 
  
    
      
        
          s
          
            1
          
        
      
    
    {\displaystyle s_{1}}
   is compared with all its matching characters in 
  
    
      
        
          s
          
            2
          
        
      
    
    {\displaystyle s_{2}}
  . The number of matching (but different sequence order) characters divided by 2 defines the number of transpositions. For example, in comparing CRATE with TRACE, only 'R' 'A' 'E' are the matching characters, i.e. m=3. Although 'C', 'T' appear in both strings, they are farther than 1 (the result of 
  
    
      
        ⌊
        
          
            
              5
              2
            
          
        
        ⌋
        −
        1
      
    
    {\displaystyle \lfloor {\tfrac {5}{2}}\rfloor -1}
  ). Therefore, t=0 . In DwAyNE versus DuANE the matching letters are already in the same order D-A-N-E, so no transpositions are needed.


=== Jaro–Winkler Similarity ===
Jaro–Winkler similarity uses a prefix scale 
  
    
      
        p
      
    
    {\displaystyle p}
   which gives more favorable ratings to strings that match from the beginning for a set prefix length 
  
    
      
        ℓ
      
    
    {\displaystyle \ell }
  . Given two strings 
  
    
      
        
          s
          
            1
          
        
      
    
    {\displaystyle s_{1}}
   and 
  
    
      
        
          s
          
            2
          
        
      
    
    {\displaystyle s_{2}}
  , their Jaro–Winkler similarity 
  
    
      
        s
        i
        
          m
          
            w
          
        
      
    
    {\displaystyle sim_{w}}
   is:

  
    
      
        s
        i
        
          m
          
            w
          
        
        =
        s
        i
        
          m
          
            j
          
        
        +
        (
        ℓ
        p
        (
        1
        −
        s
        i
        
          m
          
            j
          
        
        )
        )
        ,
      
    
    {\displaystyle sim_{w}=sim_{j}+(\ell p(1-sim_{j})),}
  
where:

  
    
      
        s
        i
        
          m
          
            j
          
        
      
    
    {\displaystyle sim_{j}}
   is the Jaro similarity for strings 
  
    
      
        
          s
          
            1
          
        
      
    
    {\displaystyle s_{1}}
   and 
  
    
      
        
          s
          
            2
          
        
      
    
    {\displaystyle s_{2}}
  

  
    
      
        ℓ
      
    
    {\displaystyle \ell }
   is the length of common prefix at the start of the string up to a maximum of four characters

  
    
      
        p
      
    
    {\displaystyle p}
   is a constant scaling factor for how much the score is adjusted upwards for having common prefixes. 
  
    
      
        p
      
    
    {\displaystyle p}
   should not exceed 0.25, otherwise the distance can become larger than 1. The standard value for this constant in Winkler's work is 
  
    
      
        p
        =
        0.1
      
    
    {\displaystyle p=0.1}
  
The Jaro-Winkler distance 
  
    
      
        
          d
          
            w
          
        
      
    
    {\displaystyle d_{w}}
   is defined as 
  
    
      
        
          d
          
            w
          
        
        =
        1
        −
        s
        i
        
          m
          
            w
          
        
      
    
    {\displaystyle d_{w}=1-sim_{w}}
  .
Although often referred to as a distance metric, the Jaro–Winkler distance is not a metric in the mathematical sense of that term because it does not obey the triangle inequality. The Jaro–Winkler distance also does not satisfy that axiom that states that 
  
    
      
        d
        (
        x
        ,
        y
        )
        =
        0
        ↔
        x
        =
        y
      
    
    {\displaystyle d(x,y)=0\leftrightarrow x=y}
  .


== Relationship with other edit distance metrics ==

There are other popular measures of edit distance, which are calculated using a different set of allowable edit operations. For instance,
the Levenshtein distance allows deletion, insertion and substitution;
the Damerau–Levenshtein distance allows insertion, deletion, substitution, and the transposition of two adjacent characters;
the longest common subsequence (LCS) distance allows only insertion and deletion, not substitution;
the Hamming distance allows only substitution, hence, it only applies to strings of the same length.
Edit distance is usually defined as a parameterizable metric calculated with a specific set of allowed edit operations, and each operation is assigned a cost (possibly infinite). This is further generalized by DNA sequence alignment algorithms such as the Smith–Waterman algorithm, which make an operation's cost depend on where it is applied.


== See also ==
Record linkage
Census


== Footnotes ==


== References ==
Cohen, W. W.; Ravikumar, P.; Fienberg, S. E. (2003). ""A comparison of string distance metrics for name-matching tasks"" (PDF). KDD Workshop on Data Cleaning and Object Consolidation. 3: 73–8. 
Jaro, M. A. (1989). ""Advances in record linkage methodology as applied to the 1985 census of Tampa Florida"". Journal of the American Statistical Association. 84 (406): 414–20. doi:10.1080/01621459.1989.10478785. 
Jaro, M. A. (1995). ""Probabilistic linkage of large public health data file"". Statistics in Medicine. 14 (5–7): 491–8. doi:10.1002/sim.4780140510. PMID 7792443. 
Winkler, W. E. (1990). ""String Comparator Metrics and Enhanced Decision Rules in the Fellegi-Sunter Model of Record Linkage"" (PDF). Proceedings of the Section on Survey Research Methods. American Statistical Association: 354–359. 
Winkler, W. E. (2006). ""Overview of Record Linkage and Current Research Directions"" (PDF). Research Report Series, RRS. 


== External links ==
strcmp.c - Original C implementation by the author of the algorithm"
186,SaferVPN,55440919,7072,"SaferVPN is a Virtual Private Network utility developed by Safer Social, Ltd. The network protects user data from Wi-Fi security risks through end-to-end encryption of user connections. SaferVPN has provided free accounts to dissidents in Turkey, Iran, and Bangladesh.


== History ==
SaferVPN was released in 2013 by cybersecurity software developers, including founders Amit Bareket and Sagi Gidali. SaferVPN’s parent company began raising capital after creating and patenting a system created to aid law enforcement in identifying and catching car thieves. In the Microsoft Imagine Cup competition the first application cofounders Amit Bareket and Sagi Gidali assembled together made second place.


=== #UnblockTheWeb ===
In March 2014 when Turkey attempted to block Twitter, SaferVPN launched the #UnblockTheWeb initiative to give individuals blocked by government censorship free access to a VPN. In August, 2016, Unblock the Web provided a free VPN account to a coalition of Bangladeshi bloggers allowing them to express themselves safely and anonymously. In 2016, Apple approved SaferVPN to implement automatic Wi-Fi Security. Its patented technology alerts users when they connect to an unsecured Wi-Fi network even if their device is locked, preventing harmful data leaks.


== Technology ==
SaferVPN uses protocols to secure data that is transmitted over its network. Each protocol varies in how the data is secured. SaferVPN’s supported protocols include:
OpenVPN, the commonly recommended protocol due to its performance and security level.
Point-To-Point Tunneling Protocol (PPTP), a commonly used VPN protocol that uses basic encryption giving users fast connection speeds.
Layer 2 Tunneling Protocol (L2TP/IPSec), secure but slower than other protocols. L2TP is a good option if OpenVPN or IKEv2 aren’t available. Unlike PPTP, L2TP/IPSec requires a shared key or the use of certificates.
IKEv2, the newest protocol available. Fastest of all protocols, it is secure and stable but not supported on all platforms.


== See also ==
Comparison of virtual private network services


== References =="
187,Ramesh Sitaraman,38426358,7049,"Ramesh Sitaraman is an Indian American computer scientist currently in the computer science department at University of Massachusetts Amherst. He is known for his work on distributed algorithms, content delivery networks, streaming video delivery, and application delivery networks.


== Biography ==
Ramesh Sitaraman received a B.Tech in Electrical Engineering from the Indian Institute of Technology, Madras and a Ph.D in computer science from Princeton University. In the past, he helped build Akamai's high-performance network for delivering web and media content  and is an Akamai Fellow. Currently, he is at the computer science department at University of Massachusetts, Amherst.


== Research Interests ==
Sitaraman's early research centered on algorithms for building reliable parallel networks from unreliable components by emulating a virtual overlay network on top of an underlying unreliable parallel network. Later, serving as a principal architect, he helped build the Akamai network, a large overlay network that currently delivers 15-30% of all web traffic using 190,000 servers in 110 countries in over 1,100 networks. He is known for helping pioneer Iarge distributed networks for web content delivery, streaming media delivery, and application delivery on the Internet. His current research is focused on energy efficiency of Internet-scale distributed networks. He is also known for his early work in building large-scale video delivery networks, measuring their performance, and more recently studying the impact of streaming video performance on users.


== References =="
188,"List of important publications in concurrent, parallel, and distributed computing",24096816,7038,"This is a list of important publications in concurrent, parallel, and distributed computing, organized by field.
Some reasons why a particular publication might be regarded as important:
Topic creator – A publication that created a new topic
Breakthrough – A publication that changed scientific knowledge significantly
Influence – A publication which has significantly influenced the world or has had a massive impact on the teaching of concurrent, parallel, and distributed computing.


== Consensus, synchronization, and mutual exclusion ==
Synchronizing concurrent processes. Achieving consensus in a distributed system in the presence of faulty nodes, or in a wait-free manner. Mutual exclusion in concurrent systems.
Dijkstra: “Solution of a problem in concurrent programming control”
Dijkstra, E. W. (1965). ""Solution of a problem in concurrent programming control"". Communications of the ACM. 8 (9): 569. doi:10.1145/365559.365617. 
This paper presented the first solution to the mutual exclusion problem. Leslie Lamport writes that this work “started the field of concurrent and distributed algorithms”.
Pease, Shostak, Lamport: “Reaching agreement in the presence of faults”Lamport, Shostak, Pease: “The Byzantine generals problem”
Pease, Marshall; Shostak, Robert; Lamport, Leslie (1980), ""Reaching agreement in the presence of faults"", Journal of the ACM, 27 (1): 228–234, doi:10.1145/322186.322188 .
Lamport, Leslie; Shostak, Robert; Pease, Marshall (1982), ""The Byzantine generals problem"", ACM Transactions on Programming Languages and Systems, 4 (3): 382–401, doi:10.1145/357172.357176 .
These two papers introduced and studied the problem that is nowadays known as Byzantine fault tolerance. The 1980 paper presented the classical lower bound that agreement is impossible if at least 1/3 of the nodes are faulty; it received the Edsger W. Dijkstra Prize in Distributed Computing in 2005. The highly cited 1982 paper gave the problem its present name, and also presented algorithms for solving the problem.
Herlihy, Shavit: “The topological structure of asynchronous computation”Saks, Zaharoglou: “Wait-free k-set agreement is impossible …”
Herlihy, Maurice; Shavit, Nir (1999), ""The topological structure of asynchronous computation"" (PDF), Journal of the ACM, 46 (6): 858–923, doi:10.1145/331524.331529 . Gödel prize lecture.
Saks, Michael; Zaharoglou, Fotios (2000), ""Wait-free k-set agreement is impossible: The topology of public knowledge"", SIAM Journal on Computing, 29 (5): 1449–1483, doi:10.1137/S0097539796307698 .
These two papers study wait-free algorithms for generalizations of the consensus problem, and showed that these problems can be analyzed by using topological properties and arguments. Both papers received the Gödel Prize in 2004.


== Foundations of distributed systems ==
Fundamental concepts such as time and knowledge in distributed systems.
Halpern, Moses: “Knowledge and common knowledge in a distributed environment”
Halpern, Joseph; Moses, Yoram (1990), ""Knowledge and common knowledge in a distributed environment"", Journal of the ACM, 37 (3): 549–587, doi:10.1145/79147.79161 .
This paper formalized the notion of “knowledge” in distributed systems, demonstrated the importance of the concept of “common knowledge” in distributed systems, and also proved that common knowledge cannot be achieved if communication is not guaranteed. The paper received the Gödel Prize in 1997 and the Edsger W. Dijkstra Prize in Distributed Computing in 2009.


== Notes ==


== External links ==
""Top-ranked Papers in ""Distributed and Parallel Computing"""", Microsoft Academic Search, archived from the original on 7 December 2009"
189,Rachel Sibande,52515228,7033,"Rachel Chimwemwe Sibande, born January 9, 1986, is a Malawian technology expert, computer scientist, STEM educator, and entrepreneur.


== Early life and education ==
Sibande was born in 1986 in Lilongwe, Malawi, the daughter of Ethel (née Muyaba) and Ben Chavula, and the oldest of five children. Her father is an accountant and her mother was first a home economics teacher at vocational college and later a rural development specialist.
Sibande attended Our Lady of Wisdom Secondary School, a Catholic school in Blantyre. When she was 15, she was selected to attend the University of Malawi’s Chancellor College. In 2006, Sibande graduated with a Bachelors, majoring in Computer Science with a credit.
Shortly afterwards, she took up her first job as a developer at Globe Computer Systems. She then moved into education, teaching ICT at Malawi’s elite high school, Kamuzu Academy, known as ""the Eton of Africa"". She left the Academy to pursue a Master of Science in Information Theory, Coding and Cryptography at Mzuzu University in 2007. There, she also taught Statistics as an Adjunct lecturer in the Department of Mathematics. She graduated with a Master of Science degree with distinction average of 80% in 2010.
In 2010, Sibande got a fully funded PhD scholarship from Institute Markets Technologies (IMT Lucca), but had to forego the opportunity as it coincided with the birth of her first child. She later took on another PhD opportunity in 2015 from the Computer Science Department at Rhodes University in South Africa.


== Tech career ==
Sibande founded Malawi’s first technology hub, mHub. It nurtures young technology enthusiasts with technical and entrepreneurship skills. Its software development unit empowers young Malawians to champion the development of local technology and provide local solutions. Through the hub, Sibande established a Children's Coding Club and a Girls' Coding Club (cf Code Club) to show young people how to develop games, animations and mobile technology applications.
In 2012, Sibande became an alumna of President Obama’s Young African Leaders Initiative. In 2015, she received the Anita Borg Scholarship from Google, now called the TechWomen Scholarship programme. The scholarship is given to outstanding female computer science students from around the world. In 2016, Rachel became Malawi’s Ambassador of the Next Einstein Forum Initiative which promotes science, technology engineering and mathematics (STEM).
Sibande delivered a TED talk in 2013 on the subject of using technology for agricultural development. In 2016, she became the first local licensee for TEDxLilongwe.
In 2016, Forbes Magazine named Sibande as one of Africa’s 30 most promising entrepreneurs under the age of 30.


== Working in elections ==
For the 2014 Malawian general election, Sibande worked as a technology expert on the team that developed and deployed a mobile and online voter verification system and a citizen journalism for real time monitoring of elections by independent observers. Almost 400,000 citizens verified voter IDs through digital platform. Over 1,500,000 messages sent through citizens' reports and from observers on incidences related to the electoral process.
Sibande offered technical support on a similar initiative for the 2015 Tanzanian general election. She was the lead ICT expert on the 2016 Zambian general election.


== Honours and awards ==
Listed by FORBES as one of the 30 most promising young entrepreneurs in Africa (2016)
Listed as one of the top 40 innovators under 40 in Africa (2016)
Named Next Einstein Ambassador for Malawi at the Next Einstein Forum (2016)
Recipient from Southern Africa of the Google Anita Borg Scholarship; a prestigious scholarship given to female outstanding computer science students from around the world. (2015)


== Personal life ==
Sibande is married to Chrispine Sibande, a Malawian human rights lawyer. Chrispine has an LLM in Sexual Reproductive Health Rights from University of Free State in South Africa. He has been an ardent champion for minority rights in Malawi and is currently working on abortion law reforms. Together, they have three children; Uwemi born in 2010; Uzengi born in 2012 and Unenesko born in 2013.


== References =="
190,TOOLS conference series,28290748,7001,"The TOOLS conference series was a long-running conferences on object technology, component-based development, model-based development and other advanced software technologies. The name originally stood for ""Technology of Object-Oriented Languages and Systems"" although later it was usually no longer expanded, the conference being known simply as ""the TOOLS conference"".
The first conference was held in Paris in 1988, organized by Eiffel Software. The TOOLS conference series started next year, in 1989. Over the following 15 years it held 45 sessions: TOOLS EUROPE (usually in Paris), TOOLS USA (usually in Santa Barbara, California), TOOLS PACIFIC (usually Sydney), TOOLS ASIA (Peking) and TOOLS EASTERN EUROPE.
TOOLS has played a major role in the development of object technology; many seminal software concepts now taken for granted were first discussed at TOOLS. Invited speakers have included countless luminaries of science and industry such as Kent Beck, Robert Binder, Peter Coad, Alistair Cockburn, Steve Cook, James Coplien, Brad Cox, Miguel de Icaza, John Dvorak, Martin Fowler, Erich Gamma, Adele Goldberg, Richard Helms, Tony Hoare, Ivar Jacobson, Philippe Kahn, Alan Kay, Bertrand Meyer, Jim Miller, Robin Milner, David Parnas, Trygve Reenskaug, Michael Stal, Dave Thomas, David Taylor, Tony Wasserman and many others.
In 2007, after an interruption of four years, TOOLS started again as an annual conference with an extended scope, encompassing not only objects and components but all modern approaches to software technology. For the last editions of the conference, see the TOOLS page. In 2012, in its 50th edition, The Triumph of Objects was declared and the series closed.
In 2013, after the end of TOOLS, Software Technologies: Applications and Foundations (STAF) emerged as an umbrella conference that aims to establish continuity, with its participating conferences focusing on diverse software technology areas such as software technology foundations, testing and formal analysis, graph and model transformations, model-driven engineering, and tools.


== Goals and scope ==
TOOLS is devoted to the study and application of advanced software development methods, techniques, tools and languages, with a special emphasis on object-oriented techniques, component-based development, model-based development and design patterns. The conference distinguishes itself by combining selective peer review process, as in academic conferences, with a strong practical slant and concern about relevance to industry.


== Hosted conferences ==
The TOOLS federated conference has hosted the following associated conferences in recent years:
Software Composition (SC): since 2007.
Test and Proofs (TAP): since 2007.
Software Engineering Advances For Outsourced and Offshore Development (SEAFOOD): 2008 - 2009.
International Conference on Model Transformation (ICMT): since 2008.


== List of past conferences ==
This list includes a reference to the proceedings when known.
TOOLS 50: TOOLS EUROPE 2012 The Triumph of Objects: Prague (Czech Republic), May 29–31, 2012, http://toolseurope2012.fit.cvut.cz/. Conference chair: Bertrand Meyer. Proceedings: Springer Verlag LNCS no. 7304, ed. Carlo A. Furia, Sebastian Nanz.
TOOLS 49: TOOLS EUROPE 2011: Zurich(Switzerland), http://toolseurope2011.lcc.uma.es/
TOOLS 48: TOOLS EUROPE 2010: Málaga (Spain), June 28-July 2, 2010. Program chair: Jan Vitek; Organizing chair: Antonio Valecillo. Proceedings: Springer Verlag LNCS no. 6141, ed. Jan Vitek.
TOOLS 47: TOOLS EUROPE 2009: Zurich (Switzerland), June 29-July 3, 2009. Program chair: Manuel Oriol; Conference chair: Bertrand Meyer. Proceedings: Springer Verlag LNBIP no. 33, ed. Manuel Oriol, Bertrand Meyer.
TOOLS 46: TOOLS EUROPE 2008: Zurich (Switzerland), June 30-July 4, 2008. Program chair: Richard Paige; Conference chair: Bertrand Meyer. Proceedings: Springer Verlag LNBIP no. 11, ed. Richard Paige.
TOOLS 45: TOOLS EUROPE 2007: Zurich (Switzerland), June 25–27, 2007. Program chair: Jean Bézivin. Proceedings published in the Journal of Object Technology, Vol 6, No. 9, July 2007. http://tools.ethz.ch/tools2007/
TOOLS 19: TOOLS EUROPE 1996: Paris(France); program chair: Richard Mitchell. Proceedings edited by Prentice Hall. ISBN 0-13-491903-3
TOOLS 17: TOOLS USA 1995: Santa Barbara (USA); Program chair: Raimund Ege. Proceedings edited by Prentice Hall, ISBN 0-13-461484-4
TOOLS 16: TOOLS EUROPE 1995: Versailles (France); Program chairs: Ian Graham & Boris Magnuusson. Proceedings edited by Prentice Hall, ISBN 0-13-443128-6
TOOLS 15: TOOLS PACIFIC 1994: Melbourne (Australia); Program chair: Christine Mingins. Proceedings edited by Prentice Hall, ISBN 0-13-438292-7
TOOLS 13: TOOLS EUROPE 1994: Versailles (France); Program chairs: Boris Magnusson & Jean-François Perrot. Proceedings edited by Prentice Hall, ISBN 0-13-350539-1
TOOLS 12: TOOLS PACIFIC 1993: Melbourne (Australia); Program chair: William Haebich. Proceedings edited by Prentice Hall, ISBN 978-0-13-124512-9
TOOLS 11: TOOLS USA 1993: Santa Barbara (USA); Program chair: Raimund Ege. Proceedings edited by Prentice Hall, ISBN 0-13-103979-2
TOOLS 10: TOOLS EUROPE 1993: Versailles (France); Program chairs: Boris Magnusson & Jean-François Perrot. Proceedings edited by Prentice Hall, ISBN 0-13-443128-6
TOOLS 9: TOOLS PACIFIC 1992: Sydney (Australia); Program chair: John Potter. Proceedings edited by Prentice Hall, ISBN 978-0-13-124512-9
TOOLS 4: TOOLS EUROPE 1991: Paris (France); Program chair: Jean Bézivin. Proceedings edited by Prentice Hall, ISBN 0-13-923160-9
TOOLS 3: TOOLS PACIFIC 1990: Sydney (Australia); Program chair: John Potter.
TOOLS 2: TOOLS EUROPE 1990: Paris (France); Program chair: Jean Bézivin. Proceedings edited by Angkor, ISBN 2-87892-000-7
TOOLS 1: TOOLS EUROPE 1988: Paris (France); Program chair: Jean Bézivin.


== References ==


== External links ==
TOOLS series page
2010 conference (Málaga)
Past TOOLS Conferences"
191,Digital Preservation Award,13478498,6956,"The Digital Preservation Award is an international award sponsored by the Digital Preservation Coalition. The award 'recognises the many new initiatives being undertaken in the challenging field of digital preservation'.[1] It was inaugurated in 2004. It was initially presented as part of the Institute of ConservationConservation Awards. Since 2012 the prize is presented independently. The prize includes a trophy and a cheque. Awards ceremonies have taken place at the British Library, the British Museum and the Wellcome Trust.


== Winners and shortlisted entries ==


=== 2004 ===


==== Winner ====
The Digital Archive: The National Archives of the United Kingdom


==== Shortlisted ====
The CAMiLEON Project: University of Leeds & University of Michigan (Special Commendation)
JISC Continuing Access and Digital Preservation Strategy: Jisc
Preservation Metadata Extraction Tool: National Library of New Zealand
Wellcome Library/JISC Web Archiving Project: Wellcome Library & Jisc


=== 2005 ===


==== Winner ====
PREMIS (Preservation Metadata: Implementation Strategies): PREMIS Working Group


==== Shortlisted ====
Choosing the optimal digital preservation strategy: Vienna University of Technology
Digital Preservation Testbed: National Archives of the Netherlands
Reverse Standards Conversion: British Broadcasting Corporation
UK Web Archiving Consortium


=== 2007 ===


==== Winner ====
Active Preservation at The National Archives - PRONOM Technical Registry and DROID file format identification tool: The National Archives of the United Kingdom


==== Shortlisted ====
LIFE: British Library
Web Curator Tool software development project: National Library of New Zealand & British Library
PARADIGM (The Personal Archives Accessible in Digital Media): Bodleian Library, University of Oxford, & John Rylands University Library, University of Manchester
Digital Repository Audit and Certification: Center for Research Libraries, RLG-OCLC, NARA, Digital Curation Centre, Digital Preservation Europe and NESTOR


=== 2010 ===


==== Winner ====
The Memento Project: Time Travel for the Web : Old Dominion University and the Los Alamos National Laboratory in the United States


==== Shortlisted ====
Web Continuity: ensuring access to online government information, from The National Archives UK
PLATO 3: Preservation Planning made simple from Vienna University of Technology and the PLANETS Project
The Blue Ribbon Task Force on Sustainable Digital Preservation and Access
Preserving Virtual Worlds, University of Illinois at Urbana Champaign with Rochester Institute of Technology, University of Maryland, Stanford University and Linden Lab in the United States


=== 2012 ===


==== Winner - outstanding contribution to teaching and communication in digital preservation in the last 2 years ====
The Digital Preservation Training Programme, University of London Computing Centre


==== Shortlisted - outstanding contribution to teaching and communication in digital preservation in the last 2 years ====
The Signal, Library of Congress
Keeping Research Data Safe Project, Charles Beagrie Ltd and partners
Digital Archaeology Exhibition, Story Worldwide Ltd


==== Winner - outstanding contribution to research and innovation in digital preservation in the last two years ====
The PLANETS Project  Preservation and Long-term Access through Networked Services, The Open Planets Foundation and partners


==== Shortlisted - outstanding contribution to research and innovation in digital preservation in the last two years ====
Data Management Planning Toolkit, The Digital Curation Centre and partners
TOTEM Trustworthy Online Technical Environment Metadata Registry, University of Portsmouth and partners
The KEEP Emulation Framework, Koninklijke Bibliotheek (National Library of the Netherlands) and partners


==== Winner - most outstanding contribution to digital preservation in the last decade ====
The Archaeology Data Service at the University of York


==== Shortlisted - most outstanding contribution to digital preservation in the last decade ====
The International Internet Preservation Consortium
The National Archives for the PRONOM and DROID services
The PREMIS Preservation Metadata Working Group for the PREMIS Standard


=== 2014 ===


==== Winner - OPF Award for Research and Innovation ====
bwFLA Functional Long Term Archiving and Access by the University of Freiburg and partners


==== Shortlisted - OPF Award for Research and Innovation ====
Jpylyzer by the KB (Royal Library of the Netherlands) and partners
The SPRUCE Project by The University of Leeds and partners


==== Winner - NCDD Award for Teaching and Communications ====
Practical Digital Preservation: a how to guide for organizations of any size  by Adrian Brown


==== Shortlisted - NCDD Award for Teaching and Communications ====
Skilling the Information Professional by Aberystwyth University
Introduction to Digital Curation: An open online UCLeXtend Course by University College London


==== Winner - Award for the Most Distinguished Student Work in Digital Preservation ====
Game Preservation in the UK by Alasdair Bachell, University of Glasgow


==== Shortlisted - Award for the Most Distinguished Student Work in Digital Preservation ====
Voices from a Disused Quarry by Kerry Evans, Ann MacDonald and Sarah Vaughan, University of Aberystwyth and partners
Emulation v Format Conversion by Victoria Sloyan, University College London


==== Winner - Award for Safeguarding the Digital Legacy ====
Carcanet Press Email Archive, University of Manchester


==== Shortlisted - Award for Safeguarding the Digital Legacy ====
Conservation and Re-enactment of Digital Art Ready-Made, by the University of Freiburg and Rhizome
Inspiring Ireland, Digital Repository of Ireland and Partners
The Cloud and the Cow, Archives and Records Council of Wales


== See also ==
Digital preservation
Digital Preservation Coalition


== References ==


== External links ==
Digital Preservation Awards website
Conservation Awards website"
192,IEEE Visualization,53704008,6945,"The IEEE Visualization Conference (VIS) is an annual conference on scientific visualization, information visualization, and visual analytics administrated by the IEEE Computer Society Technical Committee on Visualization and Graphics. As ranked by Google Scholar's h-index metric in 2016, VIS is the highest rated venue for visualization research and the second-highest rated conference for computer graphics over all. It has an 'A' rating from the Australian Ranking of ICT Conferences and an 'A' rating from the Brazilian ministry of education. The conference is highly selective with generally < 25% acceptance rates for all papers.


== Location ==
The conference is held in October and rotates around the US generally West, Central and East.
Past conferences:
 2017: Phoenix, Arizona
 2016: Baltimore, Maryland
 2015: Chicago, Illinois,
 2014: Paris, France
 2013: Atlanta, Georgia
 2012: Seattle, Washington
 2011: Providence, Rhode Island
 2010: Salt Lake City, Utah
 2009: Atlantic City, New Jersey
 2008: Columbus, Ohio
 2007: Sacramento, California
 2006: Baltimore, Maryland
 2005: Minneapolis, Minnesota
 2004: Austin, Texas
 2003: Seattle, Washington
 2002: Boston, Massachusetts
 2001: San Diego, California
 2000: Salt Lake City, Utah
 1999: San Francisco, California
 1998: Research Triangle Park, North Carolina
 1997: Phoenix, Arizona
 1996: San Francisco, California
Future conferences:
 2018: Berlin, Germany


== Awards ==


=== VIS Best Paper Award ===
2016:
VAST
An Analysis of Machine- and Human-Analytics in Classification, Gary K.L. Tam, Vivek Kothari, Min Chen

InfoVis
Vega-Lite: A Grammar of Interactive Graphics, Arvind Satyanarayan, Dominik Moritz, Kanit Wongsuphasawat, and Jeffrey Heer

SciVis
Jacobi Fiber Surfaces for Bivariate Reeb Space Computation, Julien Tierny and Hamish Carr

2015
VAST
Reducing Snapshots to Points: A Visual Analytics Approach to Dynamic Network Exploration, Stef van den Elzen, Danny Holten, Jorik Blaas, Jarke van Wijk

InfoVis
HOLA: Human-like Orthogonal Network Layout, Steve Kieffer, Tim Dwyer, Kim Marriott, Michael Wybrow

SciVis
Visualization-by-Sketching: An Artist’s Interface for Creating Multivariate Time-Varying Data, David Schroeder, Daniel Keefe

2014
VAST
Supporting Communication and Coordination in Collaborative Sensemaking, Narges Mahyar, Melanie Tory

InfoVis
Multivariate Network Exploration and Presentation: From Detail to Overview via Selections and Aggregations, Stef van den Elzen, Jarke van Wijk

SciVis
Visualization of Brain Microstructure through Spherical Harmonics Illumination of High Fidelity Spatio-Angular Fields, Sujal Bista, Jiachen Zhou, Rao Gullapalli, Amitabh Varshney

2013
VAST
A Partition-Based Framework for Building and Validating Regression Models, Thomas Muhlbacher, Harald Piringer

InfoVis
LineUp: Visual Analysis of Multi-Attribute Rankings, Samuel Gratzl, Alexander Lex, Nils Gehlenborg, Hanspeter Pfister, Marc Streit

SciVis
Comparative Visual Analysis of Lagrangian Transport in CFD Ensembles, Mathias Hummel, Harald Obermaier, Christoph Garth, Kenneth I. Joy


=== Technical Achievement Award ===
Past recipients:
2017 - Jeffrey Heer
2016 - David Ebert
2015 - Tamara Munzner
2014 - Claudio T. Silva
2013 - Kwan-Liu Ma
2012 - John Stasko
2011 - Daniel A. Keim
2010 - Hanspeter Pfister
2009 - Jock D. Mackinlay
2008 - David Laidlaw
2007 - Jarke van Wijk
2006 - Thomas Ertl
2005 - Charles D. Hansen
2004 - Amitabh Varshney


=== Career Award ===
To earn the IEEE VGTC Visualization Career Award, an individual must demonstrate that their research and service has had broad impacts on the field over a long period of time.
Past recipients:
2017 - Charles D. Hansen
2016 - John C. Dill
2015 - Markus Gross
2014 - Kenneth Joy
2013 - Gregory M. Nielson
2012 - Ben Shneiderman
2011 - Frits Post
2010 - Christopher R. Johnson
2009 - Hans Hagen
2008 - Lawrence J. Rosenblum
2007 - Stuart Card
2006 - Pat Hanrahan
2005 - Arie Kaufman
2004 - Bill Lorensen


== References =="
193,Museum informatics,32613583,6902,"Museum informatics is an interdisciplinary field of study that refers to the theory and application of informatics by museums. It is in essence a sub-field of cultural informatics at the intersection of culture, digital technology, and information science. In the context of the digital age facilitating growing commonalities across museums, libraries and archives, its place in academe has grown substantially and also has connections with digital humanities.


== History ==
The earliest references to museum informatics in English are from Archives and Museum Informatics a newsletter and journal published on the subject from 1987–1996. In the early 1990s, museum informatics projects and services developed at numerous American universities. Cultural informatics was introduced into library and information science education in 2000 at the Pratt Institute School of Library and Information Science in New York. Graduate courses devoted to museum informatics were offered from at least 2001. PhD theses were using ""museum informatics"" in the title by 2004. By 2007, an academic reader, Museum Informatics: People, Information, and Technology in Museums, edited by Paul F. Marty and Katherine Burton Jones, was published as part of the Routledge Studies in Library and Information Science.


== Overview ==
Museum informatics is an emerging field of academic study focused on the intersection between information technologies, museums and their staff members, and online museum data and services. The more general cultural informatics deals with, for example, information design and interaction, digital curation, cultural heritage description and access, social media, and the application of digital tools. Museums have embraced the application of museum informatics which has been supported by US federal grants and in particular by the Institute of Museum and Library Services (IMLS). The older term ""museum studies"" refers more to traditional curatorial perspectives rather than relating to the use of information science and information technology.
Archives and Museum Informatics is a leading journal in the field of museum informatics. University courses relating to museology include a component on museum informatics. The Museum Computer Network (MCN) in the United States holds an annual conference and runs the MCN-L electronic mailing list. The Museums Computer Group (MCG) in the United Kingdom also holds meetings relevant to museum informatics. The ICHIM conference series in Europe and the Museums and the Web conference series in North America cover aspects of museum informatics. Other relevant conferences include the EVA Conferences. Books are available on the subject.
There have been a number of collaborative projects in the field of museum informatics such as AMICO, Artstor, the Museum Informatics Project (MIP), and steve.museum. The International Council of Museums (ICOM), through Cary Karp, was instrumental in initiating the "".museum"" top-level domain for museums on the Internet. Companies such as Archives & Museum Informatics in Canada and Cogapp in the United Kingdom help museums in using information technology effectively.


== See also ==
Archival informatics
Library science


== References ==


== Bibliography ==
Marty, P.F., Rayward, W.B., and Twidale, M.B. (2003). Museum Informatics. Annual Review of Information Science and Technology, 37, 259–294.
Marty, P.F. (2003). Museum Informatics. In Encyclopedia of Library and Information Science (pp. 1906–1913). New York: Marcel Dekker.


== External links ==
Museum Informatics people on Academia.edu
Museum Informatics papers on Academia.edu"
194,List of ACM-W chapters,49860998,6885,"This is a list of chapters of the Association of Computing Machinery Council on Women in Computing (ACM-W). Chapters are listed by country and if applicable, by state.


== Azerbaijan ==


== Canada ==


== Cyprus ==


== Greece ==


== Hong Kong ==


== India ==


== Ireland ==


== Nigeria ==


== Pakistan ==


== Saudi Arabia ==


== Spain ==


== Sweden ==


== Turkey ==


== United States ==


== United Kingdom ==


== References ==


== External links ==
Official website
All ACM-W Chapters"
195,International Conference on Advances in ICT for Emerging Regions,38034988,6879,"International Conference on Advances in ICT for Emerging Regions (ICTer) is the successor to the seminal International Information Technology Conference (IITC) held in Sri Lanka since 1998. It provides a platform where research done in ICT is presented by both local and foreign Computer Scientists and IT Professionals. In order to get wider international participation and to promote computing research in the fast emerging regions of the world especially in Asia-Pacific, it was decided to broadbase the IITC conference and link it with the related International Journal ICTer (www.icter.org).


== Overview ==
Conference is attended by about 200 researchers and students each year. Topics of interest have included Technology, Applications, Human Computer Interaction, Development Processes and Social, Legal and Ethical Issues. The conference usually includes pre-conference and/or post-conference high quality tutorials/workshops in areas of current interest in Information and Communication Technology.
Conference was initiated to mark the year of IT declared by the Government of Sri Lanka in 1998. Initially it was organised by Infotel Lanka Society and managed by the Council for Information Technology (CINTEC). Management was taken over by the University of Colombo School of Computing (UCSC) in 2003. Infotel Lanka Society organised by conference until 2006. Currently the UCSC organise and manage the conference.


== Conferences ==


== Management ==
University of Colombo School of Computing 35, Reid Avenue Colombo 00700 Sri Lanka


== See also ==
List of computer science conference acronyms


== References ==


== External links ==
[1] ICTer Conference web page."
196,Visual computing,45350085,6873,"Visual computing is a generic term for all computer science disciplines handling with images and 3D models, i.e. computer graphics, image processing, visualization, computer vision, virtual and augmented reality, video processing, but also includes aspects of pattern recognition, human computer interaction, machine learning and digital libraries. The core challenges are the acquisition, processing, analysis and rendering of visual information (mainly images and video). Application areas include industrial quality control, medical image processing and visualization, surveying, robotics, multimedia systems, virtual heritage, special effects in movies and television, and computer games.


== History and overview ==
Visual computing is a relatively newly coined term, which got its current meaning around 2005, when the established computer science disciplines computer graphics, image processing, computer vision and others noticed that their methods and applications overlapped more and more, so that a new generic term was needed. Many of the used mathematical and algorithmic methods are the same in all areas dealing with images: image formats, filtering methods, color models, image metrics and others. And also the programming methods on graphics hardware, the manipulation tricks to handle huge data, textbooks and conferences, the scientific communities of these disciplines and working groups at companies intermixed more and more.
Furthermore, applications increasingly needed techniques from more than one of these fields concurrently. To generate very detailed models of complex objects you need image recognition, 3D sensors and reconstruction algorithms, and to display these models believably you need realistic rendering techniques with complex lighting simulation. Real-time graphics is the basis for usable virtual and augmented reality software. A good segmentation of the organs is the basis for interactive manipulation of 3D visualizations of medical scans. Robot control needs the recognition of objects just as a model of its environment. And all devices (computers) need ergonomic graphical user interfaces.
Although many problems are considered solved within the scientific communities of the sub-disciplines making up visual computing (mostly under idealistic assumptions), one major challenge of visual computing as a whole is the integration of these partial solutions into applicable products. This includes dealing with many practical problems like addressing a multitude of hardware, the use of real data (that is often erroneous and/or gigantic in size), and the operation by untrained users. In this respect, Visual computing is more than just the sum of its sub-disciplines, it is the next step towards systems fit for real use in all areas using images or 3D objects on the computer.


== Visual computing disciplines ==
At least the following disciplines are sub-fields of visual computing. More detailed descriptions of each of these fields can be found on the linked special pages.
Computer graphics and computer animation
Computer graphics is a general term for all techniques that produce images as result with the help of a computer. To transform the description of objects to nice images is called rendering which is always a compromise between image quality and run-time.
Image analysis and computer vision
Techniques that can extract content information from images are called image analysis techniques. Computer vision is the ability of computers (or of robots) to recognize their environment and to interpret it correctly.
Visualization and visual analytics
Visualization is used to produce images that shall communicate messages. Data may be abstract or concrete, often with no a priori geometrical components. Visual analytics describes the discipline of interactive visual analysis of data, also described as “the science of analytical reasoning supported by the interactive visual interface”.
Geometric modeling and 3D-printing
To represent objects for rendering it needs special methods and data structures, which subsumed with the term geometric modeling. In addition to describing and interactive geometric techniques, sensor data are more and more used to reconstruct geometrical models. Algorithms for the efficient control of 3D printers also belong to the field of visual computing.
Image processing and image editing
In contrast to image analysis image processing manipulates images to produce better images. “Better” can have very different meanings subject to the respective application. Also, it has to be discriminated from image editing which describes interactive manipulation of images based on human validation.
Virtual and augmented reality
Techniques that produce the feeling of immersion into a fictive world are called virtual reality (VR). Requirements for VR include head-mounted displays, real-time tracking, and high-quality real-time rendering. Augmented reality enables the user to see the real environment in addition to the virtual objects, which augment this reality. Accuracy requirements on rendering speed and tracking precision are significantly higher here.
Human computer interaction
The planning, design and uses of interfaces between people and computers is not only part of every system involving images. Due to the high bandwidth of the human visual channel (eye), images are also a preferred part of ergonomic user interfaces in any system, so that human-computer interaction is also an integral part of visual computing.


== Footnotes ==


== External links ==
Microsoft Research Group Visual Computing
Visual Computing at NVidia
Visual Computing Group at Harvard University
Visual Computing Center at KAUST
Applied Research in Visual Computing (Fraunhofer IGD)
Institute of Visual Computing (Hochschule Bonn-Rhein-Sieg, Sankt Augustin)
VRVis Research Center for Virtual Reality and Visualisation (Vienna, Austria)
Visual Computing Group @ HTW Berlin (Germany)"
197,Computer Society of Sri Lanka,24499692,6848,"The Computer Society of Sri Lanka (CSSL) is a professional body and learned society in the field of Computing and Information Technology in Sri Lanka. It was established in 1976.
The CSSL was established in 1976 in Sri Lanka for the purpose of promoting Information and Communication Technology and professionalism among those engaged in the field of Information and Communication Technology (ICT), and of maintaining the highest professional standards among the Information and Communication Technology fraternity.
It is the apex body for ICT Professionals in Sri Lanka.


== Membership ==
There are six categories of membership to the Society.
Honorary Fellow Members
Members (MCS)
Associate Members
Student Members
Affiliate Members


== Objectives of the Society ==
The Objectives of the CSSL are as follows: 1. The primary objectives of the society are: To promote and develop the science of Information Technology and foster and maintain investigations and research into the best means and methods of developing and applying such science and to encourage, increase, disseminate and promote knowledge, education and training and the exchange of information and ideas in respect of all questions relating thereto or connected therewith: 2. To provide an Organization for professionals in Information Technology and by means of examinations and other methods of assessment to test the skills and knowledge of persons desiring to enter the profession. CSSL has played a nationally important role for over 35 years now and is considered the apex body for ICT professionals in Sri Lanka.


== The CSSL Council ==
The CSSL is governed by an Executive Council elected out of members. The election is held after calling for nominations via the AGM announcement. If there are multiple nominations for the same post, an election is held at the AGM.
The last AGM was held on 28 November 2016 at the Taj Samudra Hotel The new Executive Council that was elected consists of the following Members.
President - Mr. Yasas Vishuddhi Abeywickrama, Vice President - Mr. W K Prabath Samindra, Secretary - Mr. Damith Hettihewa, Treasurer - Dr. Ajantha Athukorala, Asst. Secretary - Dr Malitha Wijesundara, Asst. Treasurer - Mr. Parakum Amaranga Pathirana, Student Counselor - Dr. T A Samantha Thelijjagoda, Publication Secretary - Mr. Enosh Praveen, Council Member - Dr. Dayan Rajapakse, Council Member - Mr. Heshan Karunaratne, Council Member - Mr Buddhika Senasekara, Council Member - Mr Sajith Sameera
Yasas Vishuddhi Abeywickrama is the President of Computer Society of Sri Lanka (CSSL), the apex body for IT Professionals in Sri Lanka. Yasas was recognized in 2003 by CIMA (UK) as an up-and-coming business leader for the future. In 2009 he was named the Young Professional of the Year by Professions Australia. In 2011, Yasas was recognized with a TOYP award (Ten Outstanding Young Persons). He started his career at Virtusa in 2003, a US-based global provider of software development and ICT services in Sri Lanka. Yasas began in Software Engineering and later moved into Business Analysis roles, which saw him travelling around the world with postings in the USA and UK. Later Yasas worked for Accenture in Australia, the world’s biggest ICT consulting company. In 2010 he co-founded Lanka BPO Academy, which is Sri Lanka's pioneering organisation for training people for the IT Enabled Services and Business Process Outsourcing (BPO) industry. He is a Director of this organisation. Currently, Yasas is also the Country Manager of Quarrio Corporation - Sri Lanka Branch. Quarrio is an Artificial Intelligence product company from the USA with offshore branches in Pakistan and Sri Lanka. He is a proud product of Ananda College Colombo. He is a graduate of the University of Colombo, where he obtained an honours degree in Computer Science (2004). Yasas holds a Masters in Entrepreneurship and Innovation from Swinburne University of Technology in Australia (2010). He is a Board Member of IFIP (International Federation of Information Processing). IFIP is the global professional federation of societies and associations for people working in Information and Communications Technologies. Established under the auspices of UNESCO in 1960 and recognised by the United Nations, IFIP represents ICT professional associations from more than 50 countries and regions with a total membership of over half a million.


== Projects organised by the CSSL ==
The CSSL organises regular events for the benefit of the ICT community of Sri Lanka, and key among them are the following projects
National IT Conference (NITC)
National Schools' Software Competition (NSSC)
IT MasterMind National Schools' Quiz Competition
IT Blast / Membership Night, a Dinner Dance for the ICT Fraternity
CSSL Awards - An Annual Award Ceremony where key individuals and contributors are awarded and recognised.
In 2017, the CSSL will organize Sri Lanka's biggest ever International ICT event, as it hosts IFIP General Assembly, SEARCC Conference (South East Asian Computer Confederation's conference) and also Sri Lanka's National IT Conference (NITC) all on the same platform from 11 September 2017 to 15 September 2017.
At the AGM held in November, the new CSSL President Yasas Vishuddhi Abeywickrama presented the vision for CSSL in front of its valued members. This vision involves enhancing - Representation & Engagement, Activeness, Recognition, Knowledge Platform and Being Vocal & Present.


== Affiliations ==
Information and Communication Technology Agency of Sri Lanka (ICTA)
International Computer Driving Licence (ICDL)
South East Asian Regional Computer Confederation (SEARCC)
Organisation for Professional Associations OPA (OPA)


== External links ==
Official web site of Computer Society of Sri Lank"
198,Nicolai Petkov,40982733,6847,"Nicolai Petkov (born 1965) is Dutch computer scientist, and professor of Intelligent Systems and Computer Science at the University of Groningen, known for his contributions in the fields of brain-inspired computing, pattern recognition, machine learning, and parallel computing.


== Life and work ==
Petkov received his doctoral degree at the Dresden University of Technology in Germany. After graduation he worked at several universities and in 1991 he was appointed Professor of Computer Science (chair of Intelligent Systems and Parallel Computing) at the University of Groningen. He was PhD thesis director (promotor) of Michael Wilkinson (1995), Henk Bekker (1996), Marc Lankhorst (1996), Frank Schnorrenberg (1998), Thomas A. Lippert (1998), Peter Kruizinga (1999), Michel Westenberg (2001), Simona E. Grigorescu (2003), Cosmin Grigorescu (2004), Anarta Ghosh (2007), Gisela Klette (2007), Lidia Sanchez Gonzalez (2007), Erik Urbach (2008), Easwar Subramanian (2008), Giuseppe Papari (2009), Georgeos Ouzounis (2009), Florence Tushabe (2010), Panchalee Sukjit (2011), George Azzopardi (2013), Ioannis E. Giotis (2013), Fred N. Kiwanuka (2013), Ando C. Emerencia (2014), Ugo Moschini (2016), Nicola Strisciuglio (2016), Andreas Neocleous (2016). At the University of Groningen he was scientific director of the Institute for Mathematics and Computer Science (now Johann Bernoulli Institute) from 1998 to 2009, and he is member of the University Council and chairman of the Science Faction since 2011.
Petkov is associate editor of several scientific journals (e.g. J. Image and Vision Computing). He co-organised and co-chaired the 10th International Conference of Computer Analysis of Images and Patterns CAIP 2003 in Groningen, the 13th CAIP 2009 in Münster, Germany, the 16th CAIP 2015 in Valletta, Malta, and the Workshops Braincomp 2013 and 2015 on Brain-Inspired Computing in Cetraro, Italy.
Petkov's initial research in the late 1980s was in the field of systolic parallel algorithms. His current research interests are in the field of development of pattern recognition and machine learning algorithms that he applies to various types of big data: image, video, audio, text, genetic, phenotype, medical, sensor, financial, web, and heterogeneous. He develops methods for the generation of intelligent programs that are automatically configured using training examples of events and patterns of interest.


== Selected publications ==
Petkov is author and editor of several books and more than 150 other scientific publications.
Books:
N. Petkov. Systolische Algorithmen und Arrays. Berlin: Akademie-Verlag, 1989.
N. Petkov. Systolic Parallel Processing. Amsterdam: North-Holland, Elsevier Sci. Publ., 1993
Edited books:
G. Azzopardi and N. Petkov (Eds.). Computer Analysis of Images and Patterns: 16th International Conference, CAIP 2015, Valletta, Malta, September 2–4, 2015, Proceedings. Parts I and II, LNCS 9256 and 9257, Springer.
L. Grandinetti, T. A. Lippert and N. Petkov (Eds.). Brain-Inspired Computing (International Workshop, BrainComp 2013, Cetraro, Italy, July 8–11, 2013, Revised Selected Papers), LNCS 8603, Springer.
X. Jiang, Nicolai Petkov (Eds.). Computer Analysis of Images and Patterns: 13th International Conference, CAIP 2009, Münster, Germany, September 2–4, 2009, Proceedings. LNCS 5702, Springer.
N. Petkov, and M. A. Westenberg (Eds.). Computer Analysis of Images and Patterns: 10th International Conference, CAIP 2003, Groningen, The Netherlands, August 25–27, 2003, Proceedings. LNCS 2756, Springer.
Articles, a selection:
G Azzopardi, N Strisciuglio, M Vento, N Petkov: Trainable COSFIRE filters for vessel delineation with application to retinal images. Medical image analysis 19 (1), 2015: 46-57
G. Azzopardi, N. Petkov: Trainable COSFIRE filters for keypoint detection and pattern recognition. IEEE Trans. on Pattern Analysis and Machine Intelligence, 35 (2), 2013: 490–503.
G. Papari and N. Petkov. Edge and line oriented contour detection: State of the art. Image and Vision Computing, 29 (2-3), 2011: 79-103.
N Petkov, E Subramanian: Motion detection, noise reduction, texture suppression, and contour enhancement by spatiotemporal Gabor filters with surround inhibition. Biological Cybernetics 97 (5-6), 2007: 423-439
A Ghosh, N Petkov: Robustness of shape descriptors to incomplete contour representations. IEEE transactions on pattern analysis and machine intelligence 27 (11), 2005: 1793 -1804
C Grigorescu, N Petkov, MA Westenberg: Contour and boundary detection improved by surround suppression of texture edges. Image and Vision Computing 22 (8), 2004: 609-622
C. Grigorescu, N. Petkov, and M. A. Westenberg. Contour detection based on nonclassical receptive field inhibition. IEEE Transactions on Image Processing, 12 (7), 2003: 729-739.
C Grigorescu, N Petkov: Distance sets for shape filters and shape recognition. IEEE Transactions on Image Processing 12 (10), 2003: 1274-1286
N Petkov, MA Westenberg: Suppression of contour perception by band-limited noise and its relation to nonclassical receptive field inhibition. Biological Cybernetics 88 (3), 2003: 236-246
S.E. Grigorescu, N. Petkov, and P. Kruizinga. Comparison of texture features based on Gabor filters. IEEE Transactions on Image Processing, 11 (10), 2002: 1160-1167.
P Kruizinga and N Petkov: Non-linear operator for oriented texture. IEEE Trans. on Image Processing 8 (10), 1999: 1395-1407
N. Petkov and P. Kruizinga: Computational models of visual neurons specialised in the detection of periodic and aperiodic oriented visual stimuli: bar and grating cells, Biological Cybernetics, 76 (2), 1997: 83-96.


== References ==


== External links ==
Nicolai Petkov homepage at rug.nl"
199,Computational journalism,25531875,6830,"Computational Journalism can be defined as the application of computation to the activities of journalism such as information gathering, organization, sensemaking, communication and dissemination of news information, while upholding values of journalism such as accuracy and verifiability. The field draws on technical aspects of computer science including artificial intelligence, content analysis (NLP, vision, audition), visualization, personalization and recommender systems as well as aspects of social computing and information science.


== History of the Field ==
The field emerged at Georgia Institute of Technology in 2006 where a course in the subject was taught by professor Irfan Essa. In February 2008 Georgia Tech hosted a Symposium on Computation and Journalism which convened several hundred computing researchers and journalists in Atlanta, GA. In July 2009, The Center for Advanced Study in the Behavioral Sciences (CASBS) at Stanford University hosted a workshop to push the field forward.
Since 2012, Columbia Journalism School has offered a course called Frontiers of Computational Journalism for the students enrolled in their dual degree in CS and journalism. The course covers many computer science topics from the perspective of journalism, including document vector space representation, algorithmic and social story selection (recommendation algorithms), language topic models, information visualization, knowledge representation and reasoning, social network analysis, quantitative and qualitative inference, and information security. The Knight Foundation awarded $3,000,000 to Columbia University's Tow Center to continue its computational journalism program.
Syracuse University launched a masters in computational journalism in 2015, with a mission of preparing students ""to be data journalists, able to work with big data sets to organize and communicate the compelling and important news stories that might be hidden in the numbers.""
Stanford University launched a Computational Journalism Lab in 2015, as well as a course titled, Computational Journalism.
In 2017, the Associated Press published a guide for newsrooms to deploy artificial intelligence and computational methods, a report developed by media strategist Francesco Marconi.


=== Computational Journalism conferences ===
In February 2013, the Georgia Institute of Technology held the Computational Journalism Symposium once again in Atlanta, GA.
In 2014 and 2015, Columbia University hosted the Computation + Journalism Symposium.
In 2016, Stanford University hosted the Computation + Journalism Symposium.
The Google News Lab has sponsored ""Computational Journalism Research Awards"" within the United States and in Europe.


== Related fields ==
Database journalism
Computer-assisted reporting
Data-driven journalism (extending the focus of data investigation to a workflow from data analysis to data visualization to storytelling based on the findings)


== Resources ==
Columbia University Computational Journalism course
DocumentCloud project
Computational+Journalism courses at Georgia Tech
A computational journalism reading list by Jonathan Stray of the Associated Press
Cultivating the Landscape of Innovation in Computational Journalism
Communications of the ACM, October 2011, ""Computational Journalism""
How Artificial Intelligence Will Impact Journalism by Francesco Marconi of the Associated Press


== References =="
200,Software incompatibility,14743352,6775,"Software incompatibility is a characteristic of software components or systems which cannot operate satisfactorily together on the same computer, or on different computers linked by a computer network. They may be components or systems which are intended to operate cooperatively or independently. Software compatibility is a characteristic of software components or systems which can operate satisfactorily together on the same computer, or on different computers linked by a computer network. It is possible that some software components or systems may be compatible in one environment and incompatible in another.


== Examples ==


=== Deadlocks ===
Consider sequential programs of the form:

Request resource A
Request resource B
Perform action using A and B
Release resource B
Release resource A

A particular program might use a printer (resource A) and a file (resource B) in order to print the file.
If several such programs P1,P2,P3 ... operate at the same time, then the first one to execute will block the others until the resources are released, and the programs will execute in turn. There will be no problem. It makes no difference whether a uni-processor or a multi-processor system is used, as it's the allocation of the resources which determines the order of execution.
Note, however, that programmers are, in general, not constrained to write programs in a particular way, or even if there are guidelines, then some may differ from the guidelines. A variant of the previous program may be:

Request resource B
Request resource A
Perform action using A and B
Release resource A
Release resource B

The resources A and B are the same as in the previous example – not simply dummy variables, as otherwise the programs are identical.
As before, if there are several such programs, Q1,Q2,Q3 which run at the same time using resources as before, there will be no problem.
However, if several of the Ps are set to run at the same time as several of the Qs, then a deadlock condition can arise. Note that the deadlock need not arise, but may.

P: Request resource A
Q: Request resource B
Q: Request resource A (blocked by P)
P: Request resource B (blocked by Q)
...

Now neither P nor Q can proceed1.
This is one kind of example where programs may demonstrate incompatibility.


=== Interface incompatibility ===
Another example of a different kind would be where one software component provides service to another. The incompatibility could be as simple as a change in the order of parameters between the software component requesting service, and the component providing the service. This would be a kind of interface incompatibility. This might be considered a bug, but could be very hard to detect in some systems. Some interface incompatibilities can easily be detected during the build stage, particularly for strongly typed systems, others may be hard to find and may only be detected at run time, while others may be almost impossible to detect without a detailed program analysis.
Consider the following example:

 Component P calls component Q with parameters x and y. For this example, y may be an integer.

 Q returns f(x) which is desired and never zero, and ignores y.

A variant of Q, Q' has similar behaviour, with the following differences:

 if y = 100, then Q' does not terminate.

If P never calls Q with y set to 100, then using Q' instead is a compatible computation. However if P calls Q with y set to 100, then using Q' instead will lead to a non-terminating computation.
If we assume further that f(x) has a numeric value, then component Q'' defined as:

 Q behaves as Q except that
 if y = 100 then Q'' does not terminate
 if y = 101 then Q'' returns 0.9 * f(x)
 if y = 102 then Q'' returns a random value
 if y = 103 then Q'' returns 0.

may cause problem behaviour. If P now calls Q'' with = 101, then the results of the computation will be incorrect, but may not cause a program failure. If P calls Q'' with y = 102 then the results are unpredictable, and failure may arise, possibly due to divide by zero or other errors such as arithmetic overflow. If P calls Q'' with y= 103 then in the event that P uses the result in a division operation, then a divide by zero failure may occur.
This example shows how one program P1 may be always compatible with another Q1, but that there can be constructed other programs Q1' and Q1'' such that P1 and Q' are sometimes incompatible, and P1 and Q1'' are always incompatible.


=== Performance incompatibility ===
Sometimes programs P and Q can be running on the same computer, and the presence of one will inhibit the performance of the other. This can particularly happen where the computer uses virtual memory. The result may be that disk thrashing occurs, and one or both programs will have significantly reduced performance. This form of incompatibility can occur if P and Q are intended to cooperate, but can also occur if P and Q are completely unrelated but just happen to run at the same time. An example might be if P is a program which produces large output files, which happen to be stored in main memory, and Q is an anti-virus program which scans many files on the hard disk. If a memory cache is used for virtual memory, then it is possible for the two programs to interact adversely and the performance of each will be drastically reduced.
For some programs P and Q their performance compatibility may depend on the environment in which they are run. They may be substantially incompatible if they are run on a computer with limited main memory, yet it may be possible to run them satisfactorily on a machine with more memory. Some programs may be performance incompatible in almost any environment.


== See also ==
Backward compatibility
Forward compatibility


== References ==
C. M. Krishna, K. G. Shin, Real-Time Systems, McGraw-Hill, 1997"
201,Sloan Research Fellowship,12637991,6748,"The Sloan Research Fellowships are awarded annually by the Alfred P. Sloan Foundation since 1955 to ""provide support and recognition to early-career scientists and scholars"".
Fellowships were initially awarded in physics, chemistry, and mathematics. Awards were later added in neuroscience (1972), economics (1980), computer science (1993), and computational and evolutionary molecular biology (2002). These two-year fellowships are awarded to 126 researchers yearly. Since the beginning of the program in 1955, 43 fellows have won a Nobel Prize, and 16 have won the Fields Medal in mathematics.


== Eligibility requirements ==
The foundation has been supportive of scientists who are parents by allowing them extra time after their doctorate during which they remain eligible for the award:

""Candidates for Sloan Research Fellowships are required to hold the Ph.D. (or equivalent) in chemistry, physics, mathematics, computer science, economics, neuroscience or computational and evolutionary molecular biology, or in a related interdisciplinary field, and must be members of the regular faculty (i.e., tenure track) of a college or university in the United States or Canada. They may be no more than six years from completion of the most recent Ph.D. or equivalent as of the year of their nomination, unless special circumstances such as military service, a change of field, or child rearing are involved or unless they have held a faculty appointment for less than two years. If any of the above circumstances apply, the letter of nomination (see below) should provide a clear explanation. While Fellows are expected to be at an early stage of their research careers, there should be strong evidence of independent research accomplishments. Candidates in all fields are normally below the rank of associate professor and do not hold tenure, but these are not strict requirements. The Alfred P. Sloan Foundation welcomes nominations of all candidates who meet the traditional high standards of the program, and strongly encourages the participation of women and members of underrepresented minority groups.""


== Award recipients ==


=== Nobel Prize in Physics winners who received Sloan Fellowships ===
1965 Richard Feynman (SRF 1955)
1969 Murray Gell-Mann (SRF 1957)
1972 Leon N. Cooper (SRF 1959)
1979 Sheldon L. Glashow (SRF 1962)
1979 Steven Weinberg (SRF 1961)
1980 Val L. Fitch ( SRF 1960)
1980 James W. Cronin (SRF 1962)
1982 Kenneth G. Wilson (SRF 1963)
1988 Jack Steinberger (SRF 1958)
1988 Melvin Schwartz (SRF 1959)
1995 Frederick Reines (SRF 1959)
2000 Alan J. Heeger (SRF 1963 – chemistry)
2001 Carl E. Wieman (SRF 1984)
2004 David J. Gross (SRF 1970)
2004 H. David Politzer (SRF 1977)
2004 Frank Wilczek (SRF 1976)
2005 Theodor W. Hänsch (SRF 1973)


=== Nobel Prize in Chemistry winners who received Sloan Fellowships ===
1981 Roald Hoffmann (SRF 1966)
1986 Dudley R. Herschbach (SRF 1959)
1986 Yuan T. Lee (SRF1969)
1986 John C. Polanyi (SRF1959)
1990 Elias J. Corey (SRF 1955)
1992 Rudolph A. Marcus (SRF 1960)
1995 Mario J. Molina (SRF 1976)
1996 Robert F. Curl, Jr. (SRF 1961)
1996 Richard E. Smalley (SRF 1978)
1999 Ahmed H. Zewail (SRF1978)
2000 Alan G. MacDiarmid (SRF 1959)
2001 K. Barry Sharpless (SRF 1973)
2005 Robert H. Grubbs (SRF 1974)
2005 Richard R. Schrock (SRF 1976)
2013 Martin Karplus (SRF 1959)
2013 Arieh Warshel (SRF 1978)


=== Nobel Memorial Prize in Economics winners who received Sloan Fellowships ===
1994 John Forbes Nash (Sloan Research Fellowship 1956 in Mathematics)
2007 Eric S. Maskin (SRF 1983)
2007 Roger B. Myerson (SRF 1984)
2012 Alvin E. Roth (SRF 1984)
2013 Lars Peter Hansen (SRF 1982)
2014 Jean Tirole (SRF 1985)


=== Nobel Prize in Medicine winners who received Sloan Fellowships ===
1997 Stanley Prusiner (Sloan Research Fellowship 1976 in Neuroscience)
2003 Paul C. Lauterbur (Sloan Research Fellowship 1965 in Chemistry)
2004 Linda B. Buck (Sloan Research Fellowship 1992 in Neuroscience)


=== Fields Medalists who received Sloan Fellowships ===
1962 John Milnor (SRF 1955)
1966 Paul Cohen (SRF 1962)
1966 Stephen Smale (SRF 1960)
1970 Heisuke Hironaka (SRF 1962)
1970 John G. Thompson (SRF 1961)
1974 David Mumford (SRF 1962)
1978 Charles L. Fefferman (SRF 1970)
1978 Daniel G. Quillen (SRF 1967)
1982 William Thurston (SRF 1974)
1982 Shing-Tung Yau (SRF 1974)
1986 Michael H. Freedman (SRF 1980)
1990 Vaughan F. R. Jones (SRF 1983)
1998 Curtis T. McMullen (SRF 1988)
2002 Vladimir Voevodsky (SRF 1997)
2006 Andrei Okounkov (SRF 2000)
2006 Terence Tao (SRF 1999)


== References ==


== External links ==
Sloan Research Fellowships official site
2015 Sloan Research Fellowships brochure"
202,Social cloud computing,50218929,6699,"Social cloud computing, also peer-to-peer social cloud computing, is an area of computer science that generalizes cloud computing to include the sharing, bartering and renting of computing resources across peers whose owners and operators are verified through a social network or reputation system. It expands cloud computing past the confines of formal commercial data centers operated by cloud providers to include anyone interested in participating within the cloud services sharing economy. This in turn leads to more options, greater economies of scale, while bearing additional advantages for hosting data and computing services closer to the edge where they may be needed most.


== Research ==
Peer-to-peer (P2P) computing and networking to enable decentralized cloud computing has been an area of research for sometime. Social cloud computing intersects peer-to-peer cloud computing with social computing to verify peer and peer owner reputation thus providing security and quality of service assurances to users. On demand computing environments may be constructed and altered statically or dynamically across peers on the Internet based on their available resources and verified reputation to provide such assurances.


== Applications ==
Social cloud computing has been highlighted as a potential benefit to large-scale computing, video gaming, and media streaming. The tenets of social cloud computing has been most famously employed in the Berkeley Open Infrastructure for Network Computing (BOINC), making the service the largest computing grid in the world. Another service that uses social cloud computing is Subutai Social. Subutai allows peer-to-peer sharing of hardware resources as well as files globally or within a small network.


== Challenges ==
Many challenges arise when moving from a traditional cloud infrastructure, to a social cloud environment.


=== Availability of computational resources ===
In the case of traditional cloud computing, availability on demand is essential for many cloud customers. Social Cloud Computing doesn't provide this availability guarantee because in a P2P environment, peers are mobile devices which may enter or leave the P2P network at any time, or PCs which have a primary purpose that can override the P2P computation at any time. The only relatively successful use cases as of recent years are those which do not require real time results, only computation power for a small subset or module of a larger algorithm or data set.


=== Trust and security ===
Unlike large scale data centers and company brand image, people may be less likely to trust peers vs. a large company like Google or Amazon. Running some sort of computation with sensitive information would then need to be encrypted properly and the overhead of that encryption may reduce the usefulness of the P2P offloading. When resources are distributed in small pieces to many peers for computations, inherent trust must be placed in the client, regardless of the encryption that may be promised to the client.


=== Reliability ===
Similar to availability, reliability of computations must be consistent and uniform. If computations offloaded to the client are continuously interrupted, some mechanism for detecting this must be in place such that the client may know the computation is tainted or needs to be completely re-run. In P2P social computing, reliable expected computation power is difficult to achieve because the speed of the client calculation may depend on how much the client is using the end device. Some ways of overcoming this may be to only allow computations to occur at night, or during specified times the client resources will not be in use.


== See also ==
Berkeley Open Infrastructure for Network Computing
Subutai Social


== References =="
203,Computational lexicology,11209252,6636,"Computational lexicology is a branch of computational linguistics, which is concerned with the use of computers in the study of lexicon. It has been more narrowly described by some scholars (Amsler, 1980) as the use of computers in the study of machine-readable dictionaries. It is distinguished from computational lexicography, which more properly would be the use of computers in the construction of dictionaries, though some researchers have used computational lexicography as synonymous.


== History ==
Computational lexicology emerged as a separate discipline within computational linguistics with the appearance of machine-readable dictionaries, starting with the creation of the machine-readable tapes of the Merriam-Webster Seventh Collegiate Dictionary and the Merriam-Webster New Pocket Dictionary in the 1960s by John Olney et al. at System Development Corporation. Today, computational lexicology is best known through the creation and applications of WordNet. As the computational processing of the researchers increased over time, the use of computational lexicology has been applied ubiquitously in the text analysis. In 1987, amongst others Byrd, Calzolari, Chodorow have developed computational tools for text analysis. In particular the model was designed for coordinating the associations involving the senses of polysemous words.


=== Study of lexicon ===
Computational lexicology has contributed to the understanding of the content and limitations of print dictionaries for computational purposes (i.e. it clarified that the previous work of lexicography was not sufficient for the needs of computational linguistics). Through the work of computational lexicologists almost every portion of a print dictionary entry has been studied ranging from:
what constitutes a headword - used to generate spelling correction lists;
what variants and inflections the headword forms - used to empirically understand morphology;
how the headword is delimited into syllables;
how the headword is pronounced - used in speech generation systems;
the parts of speech the headword takes on - used for POS taggers;
any special subject or usage codes assigned to the headword - used to identify text document subject matter;
the headword's definitions and their syntax - used as an aid to disambiguation of word in context;
the etymology of the headword and its use to characterize vocabulary by languages of origin - used to characterize text vocabulary as to its languages of origin;
the example sentences;
the run-ons (additional words and multi-word expressions that are formed from the headword); and
related words such as synonyms and antonyms.
Many computational linguists were disenchanted with the print dictionaries as a resource for computational linguistics because they lacked sufficient syntactic and semantic information for computer programs. The work on computational lexicology quickly led to efforts in two additional directions.


=== Successors to Computational Lexicology ===
First, collaborative activities between computational linguists and lexicographers led to an understanding of the role that corpora played in creating dictionaries. Most computational lexicologists moved on to build large corpora to gather the basic data that lexicographers had used to create dictionaries. The ACL/DCI (Data Collection Initiative) and the LDC (Linguistic Data Consortium) went down this path. The advent of markup languages led to the creation of tagged corpora that could be more easily analyzed to create computational linguistic systems. Part-of-speech tagged corpora and semantically tagged corpora were created in order to test and develop POS taggers and word semantic disambiguation technology.
The second direction was toward the creation of Lexical Knowledge Bases (LKBs). A Lexical Knowledge Base was deemed to be what a dictionary should be for computational linguistic purposes, especially for computational lexical semantic purposes. It was to have the same information as in a print dictionary, but totally explicated as to the meanings of the words and the appropriate links between senses. Many began creating the resources they wished dictionaries were, if they had been created for use in computational analysis. WordNet can be considered to be such a development, as can the newer efforts at describing syntactic and semantic information such as the FrameNet work of Fillmore. Outside of computational linguistics, the Ontology work of artificial intelligence can be seen as an evolutionary effort to build a lexical knowledge base for AI applications.


== Standardization ==
Optimizing the production, maintenance and extension of computational lexicons is one of the crucial aspects impacting NLP. The main problem is the interoperability: various lexicons are frequently incompatible. The most frequent situation is: how to merge two lexicons, or fragments of lexicons? A secondary problem is that a lexicon is usually specifically tailored to a specific NLP program and has difficulties being used within other NLP programs or applications.
To this respect, the various data models of Computational lexicons are studied by ISO/TC37 since 2003 within the project lexical markup framework leading to an ISO standard in 2008.


== References ==

Amsler, Robert A. 1980. Ph.D. Dissertation, ""The Structure of the Merriam-Webster Pocket Dictionary"". The University of Texas at Austin.


== External links ==
Computational lexicology issue in ACL Wiki
1.ACL Wiki
2.Association for Computational Linguistics, Official page

Computational lexicography
Lexical Markup Framework (LMF)"
204,Reduction Operator,51669182,6628,"In computer science, the reduction operator is a special type of operator that is both associative and commutative.  This type of operator is commonly used in parallel programming to reduce the elements of an array into a single result.


== Background ==
In shared memory parallel programming, the main criteria is to have parallel threads in the program. In order to run sections in parallel, they shouldn't have a dependence relationship, as sections with dependence relationships cannot run in parallel. If there's a R/W Conflicting dependence in variable reduction, an operation can be performed to remove the dependency and to make private copies of the variable.
A reduction operator can help break down a task into various partial tasks by calculating partial results which can be used to obtain a final result. It allows certain serial operations to be performed in parallel, thereby reducing the number of steps required for certain operations. A reduction operator breaks a serial task into various partial tasks and stores the result into a private copy of the variable. These private copies are then merged into a shared copy at the end.
An operator can be a reduction operator if:
It can reduce an array into a single scalar value. (Refer to example below)
The final result obtained should be from the partial tasks that were created.
These two requirements are satisfied for elements if the operations performed are ""Commutative"" and ""Associative"" (See the examples section).
These properties are described as:
Commutative: An operator is commutative if it satisfies the condition:
a ° b = b ° a
Associative: An operator is associative if it satisfies the condition:
(a ° b) ° c = a ° (b ° c)
Some operators which satisfy this are sum (addition), product (multiplication), and logical operators (and, or, etc.). An OpenMP user can specify the reduction operator and the variable being reduced using the reduction clause. If an operator is not in OpenMP, the code will need to be modified and the operator should be expressed as a reduction operator.
Reduction helps to remove the data dependencies in the section and can allow multiple threads to run at the same time thus parallelizing the task.


== Examples ==


=== Addition ===
Suppose we have an array 
  
    
      
        [
        5
        ,
        7
        ,
        1
        ,
        4
        ,
        6
        ,
        8
        ,
        2
        ,
        3
        ]
      
    
    {\displaystyle [5,7,1,4,6,8,2,3]}
  . The sum of this array can be computed serially by sequentially reducing the array into a single sum using the '+' operator. Starting the summation from the beginning of the array yields:

  
    
      
        (
        (
        (
        (
        (
        (
        5
        +
        7
        )
        +
        1
        )
        +
        4
        )
        +
        6
        )
        +
        8
        )
        +
        2
        )
        +
        3
        =
        36
      
    
    {\displaystyle ((((((5+7)+1)+4)+6)+8)+2)+3=36}
  
Since '+' is both commutative and associative, it is a reduction operator. Therefore this reduction can be performed in parallel using several cores, where each core computes the sum of a subset of the array, and the reduction operator merges the results. Using a binary tree reduction would allow 4 cores to compute 
  
    
      
        (
        5
        +
        7
        )
      
    
    {\displaystyle (5+7)}
  , 
  
    
      
        (
        1
        +
        4
        )
      
    
    {\displaystyle (1+4)}
  , 
  
    
      
        (
        6
        +
        8
        )
      
    
    {\displaystyle (6+8)}
  , and 
  
    
      
        (
        2
        +
        3
        )
      
    
    {\displaystyle (2+3)}
  . Then two cores can compute 
  
    
      
        (
        12
        +
        5
        )
      
    
    {\displaystyle (12+5)}
   and 
  
    
      
        (
        14
        +
        5
        )
      
    
    {\displaystyle (14+5)}
  , and lastly a single core computes 
  
    
      
        (
        17
        +
        19
        )
        =
        36
      
    
    {\displaystyle (17+19)=36}
  . So a total of 4 cores can be used to compute the sum in 
  
    
      
        
          log
          
            2
          
        
        ⁡
        8
        =
        3
      
    
    {\displaystyle \log _{2}8=3}
   steps instead of the 
  
    
      
        7
      
    
    {\displaystyle 7}
   steps required for the serial version. This parallel binary tree technique computes 
  
    
      
        (
        (
        5
        +
        7
        )
        +
        (
        1
        +
        4
        )
        )
        +
        (
        (
        6
        +
        8
        )
        +
        (
        2
        +
        3
        )
        )
      
    
    {\displaystyle ((5+7)+(1+4))+((6+8)+(2+3))}
  . Of course the result is the same, but only because of the associativity of the reduction operator. The commutativity of the reduction operator would be important if there were a master core distributing work to several processors, since then the results could arrive back to the master processor in any order. The property of commutativity guarantees that the result will be the same.
The pseudo-code for a basic binary tree parallel reduction algorithm is shown here:

Note that this algorithm stores the partial sums by overwriting portions of the original array, and the final result is found in the first element.


=== Multiplication ===
Real number multiplication (
  
    
      
        ×
      
    
    {\displaystyle \times }
  ) is also a reduction operator since it is both associative and commutative. Thus a similar procedure as shown for addition can be performed in order to find the product of every element of a numeric array in parallel.


== Nonexample ==


=== Matrix Multiplication ===
Matrix multiplication is not a reduction operator since the operation is not commutative. If processes were allowed to return their matrix multiplication results in any order to the master process, the final result that the master computes will likely be incorrect if the results arrived out of order. However, note that matrix multiplication is associative, and therefore the result would be correct as long as the proper ordering were enforced, as in the binary tree reduction technique.


== References ==


== Books ==
Chandra, Rohit (2001). Parallel Programming in OpenMP. Morgan Kaufmann. pp. 59–77. ISBN 1558606718. 
Solihin, Yan (2016). Fundamentals of Parallel Multicore Architecture. CRC Press. p. 75. ISBN 978-1-4822-1118-4. 


== External links ==
Reduction Clause, Reference to reduction clause"
205,Docstring,4225907,6616,"In programming, a docstring is a string literal specified in source code that is used, like a comment, to document a specific segment of code. Unlike conventional source code comments, or even specifically formatted comments like Javadoc documentation, docstrings are not stripped from the source tree when it is parsed, but are retained throughout the runtime of the program. This allows the programmer to inspect these comments at run time, for instance as an interactive help system, or as metadata.
Languages that support docstrings include Python, Lisp, Elixir, Clojure, Gherkin, and Julia.


== Implementation examples ==


=== Elixir ===
Documentation is supported at language level, in the form of docstrings. Markdown is Elixir's de facto markup language of choice for use in docstrings:


=== Lisp ===
In Lisp, docstrings are known as documentation strings. The Common Lisp standard states that a particular implementation may choose to discard docstrings whenever they want, for whatever reason. When they are kept, docstrings may be viewed (and changed) using the DOCUMENTATION function. For instance,


=== Python ===
The common practice of documenting a code object at the head of its definition is captured by the addition of docstring syntax in the Python language.
The docstring for a Python code object (a module, class, or function) is the first statement of that code object, immediately following the definition (the 'def' or 'class' statement). The statement must be a bare string literal, not any other kind of expression. The docstring for the code object is available on that code object's __doc__ attribute and through the help function.
The following Python file shows the declaration of docstrings within a Python source file:

Assuming that the above code was saved as mymodule.py, the following is an interactive session showing how the docstrings may be accessed:


==== Content of Python docstrings ====
The docstring of a script (a stand-alone program) should be usable as its ""usage"" message, printed when the script is invoked with incorrect or missing arguments (or perhaps with a ""-h"" option, for ""help""). Such a docstring should document the script's purpose, command line parameters and any dependencies. Usage messages can be fairly elaborate (several screens full) and should be sufficient for a new user to use the script properly, as well as provide a complete quick reference to all options and arguments for the sophisticated user.
If the stand-alone script uses another module for handling options, such as the argparse module, then option information is moved from the docstring to the module's utilities.
The docstring for a module should generally list all classes, exceptions, functions and objects that are exported by the module, with a one-line summary of each. (These summaries generally give less detail than the summary line in each object's docstring.)
The docstring for a package (i.e., the docstring of the package's __init__.py module) should also list the modules and subpackages exported by the package.
The docstring for a class should summarize its behavior and list the public methods and instance variables. If the class is intended to be subclassed, and has an additional interface for subclasses, this interface should be listed separately in the docstring.
The docstring for a function or a method should generally be an imperative sentence ending in a period, prescribing its behavior (e.g. ""Do this ...""), rather than a description (e.g. ""The function does this ...""). When the function is not trivial, it should be a multiline-string, with the imperative sentence being followed by a more in-depth summary of the function's behavior and a documentation of its arguments (including default values and other relevant details), return value(s), side effects, exceptions raised, and any restrictions on when it can be called (all if applicable).
There are conventions for the placement of quotation marks and newlines.


== Tools using docstrings ==
cobra -doc (Cobra)
doctest (Python)
Epydoc (Python)
Pydoc (Python)
Sphinx (Python)


== See also ==
Literate programming – alternative code commenting paradigm
Plain Old Documentation – Perl documentation


== References ==


== External links ==
Python Docstrings at Epydoc's Sourceforge page
Documentation in GNU Emacs Lisp
Section from the doxygen documentation about Python docstrings"
206,Convex volume approximation,22721042,6578,"In the analysis of algorithms, several authors have studied the computation of the volume of high-dimensional convex bodies, a problem that can also be used to model many other problems in combinatorial enumeration. Often these works use a black box model of computation in which the input is given by a subroutine for testing whether a point is inside or outside of the convex body, rather than by an explicit listing of the vertices or faces of a convex polytope. It is known that, in this model, no deterministic algorithm can achieve an accurate approximation, and even for an explicit listing of faces or vertices the problem is #P-hard. However, a joint work by Martin Dyer, Alan M. Frieze and Ravindran Kannan provided a randomized polynomial time approximation scheme for the problem, providing a sharp contrast between the capabilities of randomized and deterministic algorithms.
The main result of the paper is a randomized algorithm for finding an 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
   approximation to the volume of a convex body 
  
    
      
        K
      
    
    {\displaystyle K}
   in 
  
    
      
        n
      
    
    {\displaystyle n}
  -dimensional Euclidean space by assuming the existence of a membership oracle. The algorithm takes time bounded by a polynomial in 
  
    
      
        n
      
    
    {\displaystyle n}
  , the dimension of 
  
    
      
        K
      
    
    {\displaystyle K}
   and 
  
    
      
        1
        
          /
        
        ε
      
    
    {\displaystyle 1/\varepsilon }
  . The algorithm combines two ideas:
By using a Markov chain Monte Carlo (MCMC) method, it is possible to generate points that are nearly uniformly randomly distributed within a given convex body. The basic scheme of the algorithm is a nearly uniform sampling from within 
  
    
      
        K
      
    
    {\displaystyle K}
   by placing a grid consisting 
  
    
      
        n
      
    
    {\displaystyle n}
  -dimensional cubes and doing a random walk over these cubes. By using the theory of rapidly mixing Markov chains, they show that it takes a polynomial time for the random walk to settle down to being a nearly uniform distribution.
By using rejection sampling, it is possible to compare the volumes of two convex bodies, one nested within another, when their volumes are within a small factor of each other. The basic idea is to generate random points within the outer of the two bodies, and to count how often those points are also within the inner body.
The given convex body can be approximated by a sequence of nested bodies, eventually reaching one of known volume (a hypersphere), with this approach used to estimate the factor by which the volume changes at each step of this sequence. Multiplying these factors gives the approximate volume of the original body.
This work earned its authors the 1991 Fulkerson Prize. Although the time for this algorithm is polynomial, it has a high exponent. Subsequent authors improved the running time of this method by providing more quickly mixing Markov chains for the same problem.


== References =="
207,Poornima Vijayashanker,47719401,6548,"Poornima Vijayashanker is an engineer and entrepreneur. She is also a teacher at various tech hubs in San Francisco such as General Assembly, Parisoma and Hackbright. She has also started Femgineer, a blog and teaching platform.


== Biography ==
During Vijayashanker's childhood, electronics were routinely ""taken apart for fun"" at her house. Vijayashanker took apart her first computer at age 14. The men in her family were all engineers, but as a child, Vijayashanker wanted to be a lawyer because being an engineer didn't seem ""glamorous or exciting."" Vijayashanker began to write computer code right after high school. After starting college at Duke University, she changed her mind, deciding that engineering would be ""a cool and intense field."" She double-majored in Electrical Engineering and Computer Science.
After graduating, Vijayashanker started work as a research and development engineer at Synopsys. Vijayashanker was a founding engineer at Mint.com when it was a start-up company, where she was also the only woman on staff. Vijayashanker was instrumental in building software for Mint from scratch. While at Mint, she began Femgineer in 2007, in order to write about her experiences and show that ""women can be sophisticated, cultured, and still be engineers, Femgineers!"" After Mint was acquired by Intuit, she left to start her own company, BizeeBee, which is a platform for fitness studios which tracks attendance, revenue and controls marketing for these types of businesses.
In 2014, she gave her first TEDx talk where she discusses why tinkering is useful. In 2015, she released an eBook, titled How to Transform Your Ideas Into Software Products. In 2015, she co-authored ""Present! A Techie's Guide to Public Speaking"" with Karen Catlin.
Vijayashanker has said that if society wants to see more women in technology careers, then it is important to have more role models for women in technology. She also emphasizes the importance of encouraging women to pursue ""hard-core engineering or finance roles"" because these jobs are more likely to lead to top positions in the company.
Vijayashanker lives in Palo Alto, California.


== References ==


== External links ==
Femgineer"
208,Computational musicology,24782516,6545,"Computational musicology is defined as the study of music with computational modelling and simulation. It saw its beginning in the 1950s and originally did not use computers, but more of statistical and mathematical methods. Nowadays computational musicology depends mostly on complex algorithms to either go through vast amounts of information or produce music using given parameters. Several alternative names and subdisciplines of the field include mathematical music theory, computer music, systematic musicology, music information retrieval, computational musicology, digital musicology, sound and music computing and music informatics.


== History ==
Lejaren Hiller acted as one of the foremost pioneers by creating one of the first musical compositions with a computer in 1957. In the 1960s research continued using statistical and mathematical methods, and started to use computers in an increasing manner as their capabilities grew. 1970s and 1980s were especially significant times for computational musicology as many discoveries were made. Since then the field has suffered a general lack of interest.


== Methods ==
Most of the work in computational musicology is done with computers that run specifically designed programs. Commonly they employ the theory and methods statistical science, mathematics and music theory. Comprehension of the physics of hearing and sound are also required in analysis of raw audio data.


== Applications ==


=== Music databases ===
One of the earliest applications in computational musicology was the creation and use of musical databases. Input, usage and analysis of large amounts of data can be very troublesome using manual methods while usage of computers can make such tasks considerably easier.


=== Analysis of music ===
Different computer programs have been developed to analyze musical data. Data formats vary from standard notation to raw audio. Analysis of formats that are based on storing all properties of each note, for example MIDI, were used originally and are still among the most common methods. Significant advances in analysis of raw audio data have been made only recently.


=== Artificial production of music ===
Different algorithms can be used to both create complete compositions and improvise music. One of the methods by which a program can learn improvisation is analysis of choices a human player makes while improvising. Artificial neural networks are used extensively in such applications.


=== Historical change and music ===
One developing sociomusicological theory in computational musicology is the ""Discursive Hypothesis"" proposed by Kristoffer Jensen and David G. Hebert, which suggests that ""because both music and language are cultural discourses (which may reflect social reality in similarly limited ways), a relationship may be identifiable between the trajectories of significant features of musical sound and linguistic discourse regarding social data."" According to this perspective, analyses of ""big data"" may improve our understandings of how particular features of music and society are interrelated and change similarly across time, as significant correlations are increasingly identified within the musico-linguistic spectrum of human auditory communication.


=== Non-western music ===
Strategies from computational musicology are recently being applied for analysis of music in various parts of the world. For example, professors affiliated with the Birla Institute of Technology in India have produced studies of harmonic and melodic tendencies (in the raga structure) of Hindustani classical music.


== Research ==
RISM's (Répertoire International des Sources Musicales) database is one of the world's largest music databases, containing over 700,000 references to musical manuscripts. Anyone can use its search engine to find compositions.
The Centre for History and Analysis of Recorded Music (CHARM) has developed the Mazurka Project, which offers ""downloadable recordings . . . analytical software and training materials, and a variety of resources relating to the history of recording.""


== See also ==
Music cognition
Cognitive musicology
Musicology
Artificial neural network
MIDI
JFugue


== References ==


== External links ==
Computational Musicology: A Survey on Methodologies and Applications
Towards the compleat musicologist?"
209,Lighthouse Labs,46827997,6520,"Lighthouse Labs is a coding bootcamp for web and mobile software development in multiple cities across Canada. The establishment organizes an annual free learn-to-code event, The HTML500, in partnership with Telus. More recently, the bootcamp became the first organization to offer a remote-hybrid web development program in Whitehorse through a partnership with the Government of Yukon.


== Locations ==


=== Vancouver ===
Opened in 2013, Lighthouse Labs' first immersive bootcamp facility is based out of Launch Academy, an incubator in the heart of the city's Gastown neighborhood. Students learn front and back-end development through the bootcamp's Web and iOS programs. The iOS program, launched in July 2014, is arguably Canada's first immersive iOS immersive program.


=== Toronto ===
As part of a national partnership with Highline, a seed stage investment platform, Lighthouse Labs announced the launch of its Toronto operations durin The HTML500 in February 2015. Located in Highline's office in downtown Toronto, the facility offers enrollment in any of the bootcamp's full-time or part-time programs. It joins existing Toronto-based bootcamps such as Bitmaker Labs, BrainStation and HackerYou as one of several developer bootcamps that offer in-person learning opportunities to individuals in the Greater Toronto Area.


=== Calgary ===
In April 2015, Lighthouse Labs launched a 'pop-up' program in Calgary through its national partnership with Highline, a co-venture platform that helps early stage digital startups. Lighthouse Labs in Calgary currently offers students the opportunity to enroll in a 6-week part-time program focused primarily on learning the fundamentals of web development.


=== Whitehorse ===
In partnership with the Government of Yukon, Lighthouse Labs launched the territory's first web development bootcamp via a remote-hybrid program. Students remotely attend lectures from the bootcamp's Vancouver location and have access to on-site assistance through Mentors in the facility 


== Scholarships ==


=== Simon Fraser University ===
The 'Lighthouse Labs Prize' is an annual scholarship open to students of Simon Fraser University's Faculty of Communication, Art & Technology. Worth over $7000, the scholarship awards one student a spot in the bootcamp's web-development program in Vancouver and is currently in its second year of institution.


== See also ==
Coursera
The Data Incubator
Codecademy
Khan Academy
General Assembly
Lynda.com
App Academy
Bloc (code school)


== References ==


== External links ==
Lighthouse Labs website"
210,Leslie Blackett Wilson,34651870,6453,"Leslie Blackett Wilson (born 1930) was chair of Computing Science at the University of Stirling, appointed on August, 1979. Previously, he was a Senior Lecturer in Computer Science at the Computing Laboratory of the University of Newcastle upon Tyne. He joined the Computing Laboratory in 1964. Before that, since 1951, he was a Senior Scientific Officer at the Naval Construction Research Establishment at Dunfermline.
He has written four books in computer science and combinatorics. His book Comparative Programming Languages was regarded among the major textbooks on programming languages and has received positive reviews since its first edition. This book was translated into French in its second edition. As a researcher, he is best known for his contributions to extensions of the stable marriage problem.
He was the doctoral advisor of Jayme Luiz Szwarcfiter.


== Education ==
Leslie Blackett Wilson got a B.Sc. in Mathematics from Durham University in 1951 and a D.Sc. degree from the University of Newcastle upon Tyne in 1980.


== Books ==
Wilson, Leslie Blackett; Clark, Robert George (1993). Comparative programming languages (second ed.). Boston, MA, USA: Addison-Wesley. p. 374. ISBN 0-201-56885-3. . Translated into French.
Page, Ewan Stafford; Wilson, Leslie Blackett (1983). Information Representation and Manipulation Using Pascal. Cambridge Computer Science Texts. 15. New York, NY, USA: Cambridge University Press. p. 284. ISBN 0-521-24954-6. 
Page, Ewan Stafford; Wilson, Leslie Blackett (1979). An introduction to computational combinatorics. Cambridge Computer Science Texts. 9. New York, NY, USA: Cambridge University Press. p. 228. ISBN 0-521-29492-4. 
Page, Ewan Stafford; Wilson, Leslie Blackett (1978). Information, Representation and Manipulation in a Computer (second ed.). New York, NY, USA: Cambridge University Press. ISBN 0-521-29357-X. 


== References =="
211,Computing Research Association,2106993,6389,"The Computing Research Association (CRA) is a 501(c)3 non-profit association of North American academic departments of computer science, computer engineering, and related fields; laboratories and centers in industry, government, and academia engaging in basic computing research; and affiliated professional societies. CRA was formed in 1972 and is based in Washington, D.C., United States.


== Mission and Activities ==
CRA's mission is to enhance innovation by joining with industry, government and academia to strengthen research and advanced education in computing. CRA executes this mission by leading the computing research community, informing policymakers and the public, and facilitating the development of strong, diverse talent in the field.


=== Policy ===
CRA assists policymakers who seek to understand the issues confronting the federal Networking and Information Technology Research and Development (NITRD) program, a thirteen-agency, $4-billion-a-year federal effort to support computing research. CRA works to educate Members of Congress and provide policy makers with expert testimony in areas associated with computer science research. CRA and their Computing Community Consortium (CCC) sponsored the Leadership in Science Policy Institute, a one and half day workshop that took place in Washington, D.C.. CRA also maintains a Government Affairs website and a Computing Research Policy Blog.


=== Professional Development ===
CRA works to support computing researchers throughout their careers to help ensure that the need for a continuous supply of talented and well-educated computing researchers and advanced practitioners is met. CRA assists with leadership development within the computing research community, promotes needed changes in advanced education, and encourages participation by members of underrepresented groups. CRA offers Academic Careers Workshops, supports the CRA-W: CRA's Committee on the Status of Women in Computing Research, and runs the DREU: Distributed Research Experiences for Undergraduates Project.


=== Leadership ===
CRA supports leadership development in the research community to support researchers in broadening the scope of computing research and increasing its impact on society and works to promote cooperation among various elements of the computing research community. CRA supports the CRA Conference at Snowbird, a biennial conference where leadership in computing research departments gather to network and address common issues in the field. CRA also supports the Computing Leadership Summit.


=== Information Collection and Dissemination ===
CRA collects and disseminates information to the research and policy-making communities information about the importance and state of computing research and related policy. CRA works to develop relevant information and make the information available to the public, policy makers, and computing research community.
CRA publishes the Taulbee Survey, a key source of information on the enrollment, production, and employment of Ph.D.s in computer science and computer engineering (CS & CE) and in providing salary and demographic data for faculty in CS & CE in North America. Statistics given include gender and ethnicity breakdowns. CRA also provides the Forsythe List, Computing Research News published ten times annually for computing researchers, and the CRA Bulletin to share news, information about CRA initiatives, and items of interest to the general community.


== See also ==

Association for Computing Machinery
CRA was the main organiser of the first Federated Computing Research Conference in 1993.
CRA-W
Informatics Europe is a similar organization to the CRA for Europe.


== References ==


== External links ==
Official website
CRA Members
CRA Board of Directors
CRA History
CRA Events"
212,Requirements Engineering Specialist Group,33556483,6246,"The Requirements Engineering Specialist Group (RESG) is a Specialist Group of the British Computer Society. It runs events on all aspects of Requirements.


== Mission of the RESG ==
The RESG's stated purpose is ""to provide a forum for interaction between the many disciplines involved"" in Requirements Engineering, which it explains is ""a key activity in the development of software systems and is concerned with the identification of the goals of stakeholders and their elaboration into precise statements of desired services and behaviour."" The RESG describes Requirements engineering as ""the elicitation, definition, modelling, analysis, specification and validation of what is needed from a system."". The RESG's stated mission is to attempt to bridge the gap between industry and research, as it ""welcomes members from, and organises events for practitioners, academics and students"" in the field.


== History of the RESG ==
The RESG was founded in 1994 by Bashar Nuseibeh of Imperial College (now professor at the Open University), Neil Maiden of City University (also now professor), Paul Gough of Philips Labs, Sara Jones of the University of Hertfordshire, Steve Easterbrook of the University of Sussex, and Orlena Gotel of Imperial College. It has run events and published its newsletter ever since.
The RESG's first Chairman was Bashar Nuseibeh. He was succeeded by Pete Sawyer, Ian Alexander and Emmanuel Letier.


== Events ==
The RESG has run ""workshops, seminars and tutorials on all aspects of requirements engineering"", held ""in a variety of locations in the UK, including London, Manchester, York and Edinburgh"". In 1998, the RESG, with the RENOIR project, ran a 2-day Conference on European Industrial Requirements Engineering (CEIRE'98) in London. Events have included a 'Goals day'; workshops on 'Agile Requirements' and 'Self-Adaptive Systems'; an 'i* Showcase'; special events for post-doctoral researchers; Scenarios days; an event on software services; annual events on 'Careers in RE'; informal 'birds of a feather' pub meetings; book launches; and workshops at the annual international RE conference on 'RE Education and Training' 


== Newsletter ==
The RESG published a newsletter since its foundation. The first 35 issues were titled ""Requirenautics Quarterly"", with Issue 1 published in October 1994. Since Issue 36, the newsletter has been titled ""Requirements Quarterly"". The founding editor was Steve Easterbrook, then at Sussex University, succeeded by Pete Sawyer of Lancaster University, Ian Alexander of Scenario Plus Ltd, Simon Hutton of Headmark Analysis, and William Heaven.


== Patron ==
The RESG's patron is Professor Michael A. Jackson, the creator of JSP and JSD, and the inventor of the Problem Frames approach to requirements.


== References ==


== External links ==
RESG Official Website
BCS page on RESG
RESG LinkedIn Discussion Group
Guest Editorial - RE Journal 1996"
213,SIGCSE,1999738,6226,"SIGCSE is the Association for Computing Machinery's Special Interest Group on Computer Science Education, which provides a forum for educators to discuss issues related to the development, implementation, and/or evaluation of computing programs, curricula, and courses, as well as syllabi, laboratories, and other elements of teaching and pedagogy. It is also the name of one of the three annual events organized by SIGCSE. This event, the ACM SIGCSE Technical Symposium on Computer Science Education, is held annually in February or March in the USA. SIGCSE also sponsors an annual conference in or around Europe, named ITiCSE (Innovation and Technology in Computer Science Education), usually in late June, and an annual research workshop named ICER (International Computing Education Research), at varied locations, usually in August.
The main focus of SIGCSE is higher education, and discussions include improving computer science education at high school level and below. The membership level has held steady at around 3300 members for several years. The current chair of SIGCSE is Amber Settle for July 1, 2016 to June 30, 2019.


== Conferences ==
SIGCSE has three annual conferences.
The Technical Symposium on Computer Science Education conference is held in the United States with an average annual attendance of approximately 1300. The next conference will be held February 27 through March 2, 2019 in Minneapolis, Minnesota.
The annual conference on Innovation and Technology in Computer Science Education (ITiCSE). The next one will be held Monday 2 July through Wednesday 4 July 2018 in Larnaca, Cyprus. This conference is attended by about 200-300 and is mainly held in Europe, but has also been held in countries outside of Europe (Turkey - 2010), (Israel - 2012), and (Peru - 2016).
The International Computing Education Research (ICER) Conference. This conference has about 70 attendees and is held in the United States every other year. On the alternate years it rotates between Europe and Australasia. The next conference will be held August 13 through August 15, 2018 in Espoo, Finland.


== Newsletter/Bulletin ==
The SIGCSE Bulletin is a quarterly newsletter that was first published in 1969. It evolved from an informal gathering of news and ideas to a venue for columns, editor-reviewed articles, research announcements, editorials, symposium proceedings, etc.
In 2010, with the inception of ACM Inroads magazine, the Bulletin was transformed into an electronic newsletter sent to all SIGCSE members providing communications about SIGCSE: announcing activities, publicizing events, and highlighting topics of interest. In other words, it has returned to its roots.


== Awards ==
SIGCSE has two main awards that are given out annually.


=== Outstanding Contribution Award ===
The SIGCSE Award for Outstanding Contribution to Computer Science Education is given annually since 1981. Winners are the following:
Gail Chapman, 2017
Janice E. Cuny, 2016
Mark Allen Weiss, 2015
Robert Panoff, 2014
Michael Kolling, 2013
Harold Abelson, 2012
Matthias Felleisen, 2011
Sally Fincher, 2010
Elliott Koffman, 2009
Randy Pausch, 2008
Judith Gal-Ezer, 2007
John Hughes, 2007
Richard Pattis, 2006
Kim Bruce, 2005
Mordechai Ben-Ari, 2004
Eric S. Roberts, 2003
Elliot Soloway, 2002
Allen B. Tucker, 2001
Andries van Dam, 2000
Peter J. Denning, 1999
William Wulf, 1998
Andrew Tanenbaum, 1997
Nell B. Dale, 1996
Robert Aiken, 1995
Norman Gibbs, 1994
Alan Kay, 1993
Daniel McCracken, 1992
David Gries, 1991
Curriculum 1968 Committee, 1990
Edsger Dijkstra, 1989
Grace Murray Hopper, 1988
Niklaus Wirth, 1987
Donald Knuth, 1986
Elliot Organick, 1985
Karl Karlstrom, 1984
Alan Perlis, 1982
William Atchison, 1981


=== Lifetime Service Award ===
The SIGCSE Award for Lifetime Service to the Computer Science Education Community is given annually since 1997. Winners are the following:
Mats Daniels, 2017
Barbara Boucher Owens, 2016
Frank Young, 2015
Andrea Lawrence, 2014
Henry Walker, 2013
Jane Prey, 2012
Gordon Davies, 2011
Peter J. Denning, 2010
Michael Clancy, 2009
Dennis J. Frailey, 2008
John Impagliazzo, 2007
Joyce Currie Little, 2006
Andrew McGettrick, 2005
Bruce Klein, 2004
Harriet Taylor, 2003
A. Joe Turner, 2002
Lillian N. (Boots) Cassel, 2001
James Miller, 2000
Bob Aiken, 1999
Della Bonnette, 1998
Dick Austing, 1997


== SIGCSE Board ==
The current SIGCSE Board for July 1, 2016 – June 30, 2019 is:
Amber Settle, Chair
Susan H. Rodger, Past Chair
Judith Sheard, Vice-Chair
Adrienne Decker, Treasurer
Sue Fitzgerald, Secretary
Michelle Craig, at-large member
Briana Morrison, at-large member
Mark Allen Weiss, at-large member
SIGCSE Chairs over the years:
Susan H. Rodger, 2013–16
Renee McCauley, 2010-2013
Barbara Boucher Owens, 2007–10
Henry Walker, 2001-2007
Bruce Klein, 1997-01
Lillian N. (Boots) Cassel, 1993–97
Nell B. Dale, 1991–93


== External links ==
SIGCSE


== References =="
214,Variable elimination,39269581,6215,"Variable elimination (VE) is a simple and general exact inference algorithm in probabilistic graphical models, such as Bayesian networks and Markov random fields. It can be used for inference of maximum a posteriori (MAP) state or estimation of conditional or marginal distributions over a subset of variables. The algorithm has exponential time complexity, but could be efficient in practice for the low-treewidth graphs, if the proper elimination order is used.


== Factors ==
Enabling a key reduction in algorithmic complexity, a factor 
  
    
      
        f
      
    
    {\displaystyle f}
  , also known as a potential, of variables 
  
    
      
        V
      
    
    {\displaystyle V}
   is a relation between each instantiation of 
  
    
      
        v
      
    
    {\displaystyle v}
   of variables 
  
    
      
        f
      
    
    {\displaystyle f}
   to a non-negative number, commonly denoted as 
  
    
      
        f
        (
        x
        )
      
    
    {\displaystyle f(x)}
  . A factor does not necessarily have a set interpretation. One may perform operations on factors of different representations such as a probability distribution or conditional distribution. Joint distributions often become too large to handle as the complexity of this operation is exponential. Thus variable elimination becomes more feasible when computing factorized entities.


== Basic Operations ==


=== Variable Summation ===
Algorithm 1, called sum-out (SO), or marginalization, eliminates a single variable 
  
    
      
        v
      
    
    {\displaystyle v}
   from a set 
  
    
      
        ϕ
      
    
    {\displaystyle \phi }
   of factors, and returns the resulting set of factors. The algorithm collect-relevant simply returns those factors in 
  
    
      
        ϕ
      
    
    {\displaystyle \phi }
   involving variable 
  
    
      
        v
      
    
    {\displaystyle v}
  .
Algorithm 1 sum-out(
  
    
      
        v
      
    
    {\displaystyle v}
  ,
  
    
      
        ϕ
      
    
    {\displaystyle \phi }
  )

  
    
      
        Φ
      
    
    {\displaystyle \Phi }
   = collect factors relevant to 
  
    
      
        v
      
    
    {\displaystyle v}
  

  
    
      
        Ψ
      
    
    {\displaystyle \Psi }
   = the product of all factors in 
  
    
      
        Φ
      
    
    {\displaystyle \Phi }
  

  
    
      
        τ
        =
        
          ∑
          
            v
          
        
        Ψ
      
    
    {\displaystyle \tau =\sum _{v}\Psi }
  
return 
  
    
      
        (
        ϕ
        −
        Ψ
        )
        ∪
        {
        τ
        }
      
    
    {\displaystyle (\phi -\Psi )\cup \{\tau \}}
  
Example
Here we have a joint probability distribution. A variable, 
  
    
      
        v
      
    
    {\displaystyle v}
   can be summed out between a set of instantiations where the set 
  
    
      
        V
        −
        v
      
    
    {\displaystyle V-v}
   at minimum must agree over the remaining variables. The value of 
  
    
      
        v
      
    
    {\displaystyle v}
   is irrelevant when it is the variable to be summed out. 
After eliminating 
  
    
      
        
          V
          
            1
          
        
      
    
    {\displaystyle V_{1}}
  , its reference is excluded and we are left with a distribution only over the remaining variables and the sum of each instantiation.
The resulting distribution which follows the sum-out operation only helps to answer queries that do not mention 
  
    
      
        
          V
          
            1
          
        
      
    
    {\displaystyle V_{1}}
  . Also worthy to note, the summing-out operation is commutative.


=== Factor Multiplication ===
Computing a product between multiple factors results in a factor compatible with a single instantiation in each factor.
Algorithm 2 mult-factors(
  
    
      
        v
      
    
    {\displaystyle v}
  ,
  
    
      
        ϕ
      
    
    {\displaystyle \phi }
  )

  
    
      
        Z
      
    
    {\displaystyle Z}
   = Union of all variables between product of factors 
  
    
      
        
          f
          
            1
          
        
        (
        
          X
          
            1
          
        
        )
        ,
        .
        .
        .
        ,
        
          f
          
            m
          
        
        (
        
          X
          
            m
          
        
        )
      
    
    {\displaystyle f_{1}(X_{1}),...,f_{m}(X_{m})}
  

  
    
      
        f
      
    
    {\displaystyle f}
   = a factor over 
  
    
      
        f
      
    
    {\displaystyle f}
   where 
  
    
      
        f
      
    
    {\displaystyle f}
   for all 
  
    
      
        f
      
    
    {\displaystyle f}
  
For each instantiation 
  
    
      
        z
      
    
    {\displaystyle z}
  
For 1 to 
  
    
      
        m
      
    
    {\displaystyle m}
  

  
    
      
        
          x
          
            1
          
        
        =
      
    
    {\displaystyle x_{1}=}
   instantiation of variables 
  
    
      
        
          X
          
            1
          
        
      
    
    {\displaystyle X_{1}}
   consistent with 
  
    
      
        z
      
    
    {\displaystyle z}
  

  
    
      
        f
        (
        z
        )
        =
        f
        (
        z
        )
        
          f
          
            i
          
        
        (
        
          x
          
            i
          
        
        )
      
    
    {\displaystyle f(z)=f(z)f_{i}(x_{i})}
  

return 
  
    
      
        f
      
    
    {\displaystyle f}
  
Factor multiplication is not only commutative but also associative.


== Inference ==
The most common query type is in the form 
  
    
      
        p
        (
        X
        
          |
        
        E
        =
        e
        )
      
    
    {\displaystyle p(X|E=e)}
   where 
  
    
      
        X
      
    
    {\displaystyle X}
   and 
  
    
      
        E
      
    
    {\displaystyle E}
   are disjoint subsets of 
  
    
      
        U
      
    
    {\displaystyle U}
  , and 
  
    
      
        E
      
    
    {\displaystyle E}
   is observed taking value 
  
    
      
        e
      
    
    {\displaystyle e}
  . A basic algorithm to computing p(X|E = e) is called variable elimination (VE), first put forth in.
Taken from, this algorithm computes 
  
    
      
        p
        (
        X
        
          |
        
        E
        =
        e
        )
      
    
    {\displaystyle p(X|E=e)}
   from a discrete Bayesian network B. VE calls SO to eliminate variables one by one. More specifically, in Algorithm 2, 
  
    
      
        ϕ
      
    
    {\displaystyle \phi }
   is the set C of conditional probability tables (henceforth ""CPTs"") for B, 
  
    
      
        X
      
    
    {\displaystyle X}
   is a list of query variables, 
  
    
      
        E
      
    
    {\displaystyle E}
   is a list of observed variables, 
  
    
      
        e
      
    
    {\displaystyle e}
   is the corresponding list of observed values, and 
  
    
      
        σ
      
    
    {\displaystyle \sigma }
   is an elimination ordering for variables 
  
    
      
        U
        −
        X
        E
      
    
    {\displaystyle U-XE}
  , where 
  
    
      
        X
        E
      
    
    {\displaystyle XE}
   denotes 
  
    
      
        X
        ∪
        E
      
    
    {\displaystyle X\cup E}
  .
Variable Elimination Algorithm VE(
  
    
      
        ϕ
        ,
        X
        ,
        E
        ,
        e
        ,
        σ
      
    
    {\displaystyle \phi ,X,E,e,\sigma }
  )
Multiply factors with appropriate CPTs While σ is not empty
Remove the first variable 
  
    
      
        v
      
    
    {\displaystyle v}
   from 
  
    
      
        σ
      
    
    {\displaystyle \sigma }
  

  
    
      
        ϕ
      
    
    {\displaystyle \phi }
   = sum-out
  
    
      
        (
        v
        ,
        ϕ
        )
      
    
    {\displaystyle (v,\phi )}
  

  
    
      
        p
        (
        X
        ,
        E
        =
        e
        )
      
    
    {\displaystyle p(X,E=e)}
   = the product of all factors 
  
    
      
        Ψ
        ∈
        ϕ
      
    
    {\displaystyle \Psi \in \phi }
  
return 
  
    
      
        p
        (
        X
        ,
        E
        =
        e
        )
        
          /
        
        
          ∑
          
            X
          
        
        p
        (
        X
        ,
        E
        =
        e
        )
      
    
    {\displaystyle p(X,E=e)/\sum _{X}p(X,E=e)}
  


== Ordering ==
Finding the optimal order in which to eliminate variables is an NP-hard problem. As such there are heuristics one may follow to better optimize performance by order:
Minimum Degree: Eliminate the variable which results in constructing the smallest factor possible.
Minimum Fill: By constructing an undirected graph showing variable relations expressed by all CPTs, eliminate the variable which would result in the least edges to be added post elimination.


== References =="
215,VLDB,5202532,6163,"VLDB is an annual conference held by the non-profit Very Large Data Base Endowment Inc. The mission of VLDB is to promote and exchange scholarly work in databases and related fields throughout the world. The VLDB conference began in 1975 and is now closely associated with SIGMOD and SIGKDD.


== Venues ==


== External links ==
VLDB Endowment Inc."
216,Computer Science Teachers Association,50032073,6150,"The Computer Science Teachers Association (CSTA) is a professional association that supports and encourages education in the field of computer science and related areas. Started in 2004, CSTA supports computer science education in elementary schools, middle schools, high schools, higher education, and industry.


== Awards ==
Together with the Association for Computing Machinery (ACM), the CSTA offers the ACM/CSTA Cutler-Bell Prize in High School Computing. The award provides four $10,000 scholarships to each of four winners along with travel to a reception each February.


== Computer Science Education Standards ==
CSTA publishes a set of recommended Computer Science Standards for kindergarten through high school. CSTA the Association for Computing Machinery publish an interactive State-By-State map showing the degree to which the recommended computer science standards have been included in the state educational standards.
CSTA recommendations for computer science education include beginning introductory lessons as early as kindergarten. A recent report by Association for Computing Machinery and the CSTA, Running on Empty: The Failure to Teach K-12 Computer Science in the Digital Age, found that in the United States, most high schools count computer science as an elective and most secondary schools have few educational standards related to computer science.


== Code.org Advocacy Coalition ==
CSTA is one of the participating organizations in the Code.org Advocacy Coalition (previously called Computing in the Core (CinC)). The Code.org Advocacy Coalition is a group of organizations that works on public outreach and advocacy to encourage additional support for computer science in the core curriculum and includes members such as Microsoft, Google, Facebook, Anita Borg Institute for Women and Technology, Computing Research Association, and others.


== Chapters ==
CSTA has more than 50 chapters in the United States and international affiliates in Israel, New Zealand, and the United Kingdom.


== Newsletters ==
CSTA publishes a bi-monthly newsletter, the CSTA Voice, that highlights issues related to computer science education.


== Governance ==
CSTA Board of Directors include:
Dave Reed, Chair
Deborah Seehorn, Past Chair
Laura Blankenship and Stephanie Hoeppner, 9-12 Teacher Representatives
Fred Martin, University Faculty Representative
Tammy Pirmann, School District Representative
Mina Theofilatou, International Representative
Myra Deister and Alfred Thompson, At-Large Representatives
Sheena Vaidynathan, K-8 Representative
Aman Yadav, Teacher Education Representative
CSTA Committees include:


== See also ==

Association for Computing Machinery
Computer science
Education


== References ==


== External links ==
New official website (in progress)
Official website
Association for Computing Machinery
Computer Science Education Week
ACM/CSTA Cutler-Bell Prize in High School Computing"
217,Jürgen Schmidhuber,405484,6122,"Jürgen Schmidhuber (born 17 January 1963) is a computer scientist who works in the field of artificial intelligence. He is a co-director of the Dalle Molle Institute for Artificial Intelligence Research in Manno, in the district of Lugano, in Ticino in southern Switzerland.
Schmidhuber did his undergraduate studies at the Technische Universität München in Munich, Germany. He taught there from 2004 until 2009 when he became a professor of artificial intelligence at the Università della Svizzera Italiana in Lugano, Switzerland.


== Work ==
In 1997, Schmidhuber and Sepp Hochreiter published a paper on a type of recurrent neural network which they called Long short-term memory. In 2015, this was used in a new implementation of speech recognition in Google's software for smartphones.
In 2014, Schmidhuber formed a company, Nnaisense, to work on commercial applications of artificial intelligence in fields such as finance, heavy industry and self-driving cars. Sepp Hochreiter, Jaan Tallinn, and Marcus Hutter are advisers to the company. Sales were under 11 million USD in 2016; however, Schmidhuber states that the current emphasis is on research and not revenue. Nnaisense raised its first round of capital funding in January 2017. Schumidhuber's overall goal is to create an all-purpose AI by training a single AI in sequence on a variety of narrow tasks; however, skeptics point out that companies such as Arago GmbH and IBM have applied AI to various different projects for years without showing any signs of artificial general intelligence.


== Views ==
Schmidhuber frequently and harshly criticizes modern deep learning researchers such as Yann LeCun for allegedly understating the contributions of Schmidhuber and other early machine learning pioneers. LeCun denies the charge, stating instead that Schmidhuber ""keeps claiming credit he doesn't deserve"".
Schmidhuber is a techno-optimist who argues that trends of accelerating change and advances in neural networks will, in a matter of years or decades, result in sentient and cooperative superintelligence that will dominate the world and will ""pay about as much attention to us as we do to ants"".


== Recognition ==
Schmidhuber received the Helmholtz Award of the International Neural Network Society in 2013, and the Neural Networks Pioneer Award of the IEEE Computational Intelligence Society in 2016. He is a member of the European Academy of Sciences and Arts.


== References =="
218,Empty string,835827,6084,"In formal language theory, the empty string, or empty word is the unique string of length zero.


== Formal theory ==
Formally, a string is a finite, ordered sequence of characters such as letters, digits or spaces. The empty string is the special case where the sequence has length zero, so there are no symbols in the string. There is only one empty string, because two strings are only different if they have different lengths or a different sequence of symbols. In formal treatments, the empty string is denoted with ε or sometimes Λ or λ.
The empty string should not be confused with the empty language ∅, which is a formal language (i.e. a set of strings) that contains no strings, not even the empty string.
The empty string has several properties:
|ε| = 0. Its string length is zero.
ε ⋅ s = s ⋅ ε = s. The empty string is the identity element of the concatenation operation. The set of all strings forms a free monoid with respect to ⋅ and ε.
εR = ε. Reversal of the empty string produces the empty string.
The empty string precedes any other string under lexicographical order, because it is the shortest of all strings.


== Use in programming languages ==
In most programming languages, strings are a data type. Individual strings are typically stored in consecutive memory locations. This means that the same string (for example, the empty string) could be stored in two different places in memory. (Note that even a string of length zero can require memory to store it, depending on the format being used.) In this way there could be multiple empty strings in memory, in contrast with the formal theory definition, for which there is only one possible empty string. However, a string comparison function would indicate that all of these empty strings are equal to each other.
In most programming languages, the empty string is distinct from a null reference (or null pointer) because a null reference does not point to any string at all, not even the empty string. The empty string is a legitimate string, upon which most string operations should work. Some languages treat some or all of the following in similar ways, which can lessen the danger: empty strings, null references, the integer 0, the floating point number 0, the boolean value false, the ASCII character NUL, or other such values.
The empty string is usually represented similarly to other strings. In implementations with string terminating character (null-terminated strings or plain text lines), the empty string is indicated by the immediate use of this terminating character.


=== Examples of empty strings ===
The empty string is a syntactically valid representation of zero in positional notation (in any base), which does not contain leading zeros. Since the empty string does not have a standard visual representation outside of formal language theory, the number zero is traditionally represented by a single decimal digit 0 instead.
Zero-filled memory area, interpreted as a null-terminated string, is an empty string.
Empty lines of text show the empty string. This can occur from two consecutive EOLs, as often occur in text files, and this is sometimes used in text processing to separate paragraphs, e.g. in MediaWiki.


== See also ==
Empty set
Null-terminated string
Concatenation theory
Concatenation


== References =="
219,International Conference on Information Systems Security and Privacy,51911702,6066,"The International Conference on Information Systems Security and Privacy – ICISSP – will have the third edition in 2017 in conjunction with the International Conference on Model-Driven Engineering and Software Development - MODELSWARD - and the International Conference on Sensor Networks - SENSORNETS. This conference aims to create a meeting point for practitioners and researchers interested in security and privacy challenges that concern information systems covering technological and social issues.
The format of this conference counts on technical sessions, poster sessions, tutorials, doctoral consortiums, panels, industrial tracks and keynote lectures. The papers presented in the conference are made available at the SCITEPRESS digital library, published in the conference proceedings and some of the best papers are invited to a post-publication with Springer, in a CCIS Series book.
ICISSP also counts on keynote talks. Elisa Bertino (Purdue University, United States), Nancy Cam-Winget (Cisco Systems, United States) and Bart Preneel (KU Leuven, Belgium) are the names announced in the conference website as the invited speakers for the next conference edition.


== Conference Topics ==
Security awareness and Education
Security Testing
Security and Privacy in Cloud and Pervasive Computing
Phishing
Content protection and Digital Rights Management
Vulnerability analysis and Countermeasures
Biometric Technologies and Applications
Database security
Information Hiding and Anonymity
Risk and Reputation management
Data Mining and Knowledge discovery
Intrusion Detection and Response
Phishing
Identity and Trust management
Web Applications and Services
Intrusion Detection and Response
Software Security Assurance


== Event Chairs ==


=== Conference Chair ===
Olivier Camp, MODESTE/ESEO, France


=== Program Co-Chairs ===
Steven Furnell, Plymouth University, United Kingdom
Paolo Mori, Istituto di Informatica e Telematica, Consiglio Nazionale delle Ricerche, Italy


== Editions and Proceedings ==


=== ICISSP 2016 - Rome, Italy ===
Proceedings of the 2nd International Conference on Information Systems Security and Privacy. 


=== ICISSP 2015 - ESEO, Angers, Loire Valley, France ===
Proceedings of the 1st International Conference on Information Systems Security and Privacy . ISBN 978-989-758-081-9 


== Best Paper Awards ==
2016Best Paper Award – Christoph Kerschbaumer, Sid Stamm and Stefan Brunthaler. “Injecting CSP for Fun and Security” 
Best Student Paper Award - Kexin Qiao, Lei Hu and Siwei Sun, “Differential Security Evaluation of Simeck with Dynamic Key-guessing Techniques” 
2015Best Paper Award - Fabian Knirsch, Dominik Engel, Christian Neureiter, Marc Frincu and Viktor Prasanna. ""Model-driven Privacy Assessment in the Smart Grid""
Best Student Paper Award - Carsten Büttner and Sorin A. Huss. ""A Novel Anonymous Authenticated Key Agreement Protocol for Vehicular Ad Hoc Networks""


== External links ==
Science and Technology Events
Conference website
Science and Technology Publications
Event management system
WikiCfp call for papers


== References =="
220,List of unsolved problems in computer science,1101069,6045,"This article is a list of unsolved problems in computer science. A problem in computer science is considered unsolved when no solution is known, or when experts in the field disagree about proposed solutions.


== Computational complexity ==

P versus NP problem (occasionally written erroneously as ""P = NP"")
What is the relationship between BQP and NP?
NC = P problem
NP = co-NP problem
P = BPP problem
P = PSPACE problem
L = NL problem
PH = PSPACE problem
L = P problem
L = RL problem
Unique games conjecture
Is the exponential time hypothesis true?
Do one-way functions exist?
Is public-key cryptography possible?


== Polynomial versus non-polynomial time for specific algorithmic problems ==

Can integer factorization be done in polynomial time on a classical (non-quantum) computer?
Is integer factorization NP-complete?
Can clustered planar drawings be found in polynomial time?
Can the discrete logarithm be computed in polynomial time?
Can the graph isomorphism problem be solved in polynomial time?
Can leaf powers and k-leaf powers be recognized in polynomial time?
Can parity games be solved in polynomial time?
Can the rotation distance between two binary trees be computed in polynomial time?
Can graphs of bounded clique-width be recognized in polynomial time?
Can one find a simple closed quasigeodesic on a convex polyhedron in polynomial time?
Can a simultaneous embedding with fixed edges for two given graphs be found in polynomial time?


== Other algorithmic problems ==
What is the fastest algorithm for multiplication of two n-digit numbers?
What is the fastest algorithm for matrix multiplication?
Can the Schwartz–Zippel lemma for polynomial identity testing be derandomized?
Can a depth-first search tree be constructed in NC?
Does linear programming admit a strongly polynomial-time algorithm? This is problem #9 in Smale's list of problems.
What is the lower bound on the complexity of fast Fourier transform algorithms? Can they be faster than O(N log N)?
The dynamic optimality conjecture: do splay trees have a bounded competitive ratio?
Can we compute the edit distance between two strings of length n in strongly sub-quadratic time, i.e., in time O(n2−ϵ) for some ϵ>0 ?
Is there a k-competitive online algorithm for the k-server problem?
Can X + Y sorting be done in strictly less than O(n2 log n) time?
How many queries are required for envy-free cake-cutting?
What is the lowest possible average-case time complexity of Shellsort with a deterministic, fixed gap sequence?


== Programming language theory ==

POPLmark
Barendregt–Geuvers–Klop conjecture


== Other problems ==
Aanderaa–Karp–Rosenberg conjecture
Generalized star height problem
Separating words problem
Possibility of hypercomputation


== References ==


== External links ==
Open problems around exact algorithms by Gerhard J. Woeginger, Discrete Applied Mathematics 156 (2008) 397–405.
The RTA list of open problems – open problems in rewriting.
The TLCA List of Open Problems – open problems in area typed lambda calculus."
221,Arvind Gupta (academic),43813002,6019,"Arvind Gupta (b. circa 1961) is an Indo-Canadian computer scientist who was the 13th President of the University of British Columbia (UBC) and the former CEO of Mitacs Canada.


== Early life and education ==
Gupta was born in Jalandhar in the Indian state of Punjab. Both his parents were academics. His mother was one of the first women to teach mathematics at a college in the Indian state of Uttar Pradesh.
Gupta lived in India and spoke Punjabi for his first five years until his family moved to Detroit where his father, a chemistry professor, had started a fellowship at Wayne State University. He then learned to speak English. Within two years, they moved to Timmins, Ontario after his father earned a job as a pollution chemist with a mining company.
He obtained a bachelor's degree in mathematics at McMaster University in Hamilton, Ontario before earning a master's and a PhD at the University of Toronto. His family knew some of the victims killed in the 1985 bombing of Air India Flight 182.


== Academia ==
Gupta spent 18 years in the School of Computing Science at Simon Fraser University before being recruited by UBC in 2009 as a professor of computer science.
In 2012, he joined the federal government’s Science, Technology and Innovation Council.
From 2000 until his appointment as President of UBC in 2014, Gupta served as CEO and scientific director of Mitacs Canada, a national non-profit that worked with government and industry to fund student researchers.
In his inauguration, Gupta committed to increasing UBC's focus on research. Gupta resigned abruptly from his position as President of UBC on August 7, 2015, after 13 months of service. The reasons for his resignation were not revealed and caused some public controversies.
In October 2015, the University of Toronto announced Gupta's joining them as a distinguished visiting professor for one academic year.


== References ==


== External links ==
Arvind Gupta - UBC Department of Computer Science"
222,Weight-balanced tree,4849574,5997,"In computer science, weight-balanced binary trees (WBTs) are a type self-balancing binary search trees that can be used to implement dynamic sets, dictionaries (maps) and sequences. These trees were introduced by Nievergelt and Reingold in the 1970s as trees of bounded balance, or BB[α] trees. Their more common name is due to Knuth.
Like other self-balancing trees, WBTs store bookkeeping information pertaining to balance in their nodes and perform rotations to restore balance when it is disturbed by insertion or deletion operations. Specifically, each node stores the size of the subtree rooted at the node, and the sizes of left and right subtrees are kept within some factor of each other. Unlike the balance information in AVL trees (which store the height of subtrees) and red-black trees (which store a fictional ""color"" bit), the bookkeeping information in a WBT is an actually useful property for applications: the number of elements in a tree is equal to the size of its root, and the size information is exactly the information needed to implement the operations of an order statistic tree, viz., getting the n'th largest element in a set or determining an element's index in sorted order.
Weight-balanced trees are popular in the functional programming community and are used to implement sets and maps in MIT Scheme, SLIB and implementations of Haskell.


== Description ==
A weight-balanced tree is a binary search tree that stores the sizes of subtrees in the nodes. That is, a node has fields
key, of any ordered type
value (optional, only for mappings)
left, right, pointer to node
size, of type integer.
By definition, the size of a leaf (typically represented by a nil pointer) is zero. The size of an internal node is the sum of sizes of its two children, plus one (size[n] = size[n.left] + size[n.right] + 1). Based on the size, one defines the weight as weight[n] = size[n] + 1.

Operations that modify the tree must make sure that the weight of the left and right subtrees of every node remain within some factor α of each other, using the same rebalancing operations used in AVL trees: rotations and double rotations. Formally, node balance is defined as follows:
A node is α-weight-balanced if weight[n.left] ≥ α·weight[n] and weight[n.right] ≥ α·weight[n].
Here, α is a numerical parameter to be determined when implementing weight balanced trees. Larger values of α produce ""more balanced"" trees, but not all values of α are appropriate; Nievergelt and Reingold proved that

  
    
      
        α
        <
        1
        −
        
          
            1
            
              2
            
          
        
      
    
    {\displaystyle \alpha <1-{\frac {1}{\sqrt {2}}}}
  
is a necessary condition for the balancing algorithm to work. Later work showed a lower bound of ​2⁄11 for α, although it can be made arbitrarily small if a custom (and more complicated) rebalancing algorithm is used.
Applying balancing correctly guarantees a tree of n elements will have height

  
    
      
        h
        ≤
        
          log
          
            
              1
              
                1
                −
                α
              
            
          
        
        ⁡
        n
        =
        
          
            
              
                log
                
                  2
                
              
              ⁡
              n
            
            
              
                log
                
                  2
                
              
              ⁡
              
                (
                
                  
                    1
                    
                      1
                      −
                      α
                    
                  
                
                )
              
            
          
        
        =
        O
        (
        log
        ⁡
        n
        )
      
    
    {\displaystyle h\leq \log _{\frac {1}{1-\alpha }}n={\frac {\log _{2}n}{\log _{2}\left({\frac {1}{1-\alpha }}\right)}}=O(\log n)}
  
The number of balancing operations required in a sequence of n insertions and deletions is linear in n, i.e., balancing takes a constant amount of overhead in an amortized sense.


== Notes ==


== References =="
223,Substring,2696560,5993,"A substring of a string 
  
    
      
        S
      
    
    {\displaystyle S}
   is a string 
  
    
      
        
          S
          ′
        
      
    
    {\displaystyle S'}
   that occurs ""in"" 
  
    
      
        S
      
    
    {\displaystyle S}
  . For example, ""the best of"" is a substring of ""It was the best of times"". This is not to be confused with subsequence, which is a generalization of substring. For example, ""Itwastimes"" is a subsequence of ""It was the best of times"", but not a substring.
Prefix and suffix are special cases of substring. A prefix of a string 
  
    
      
        S
      
    
    {\displaystyle S}
   is a substring of 
  
    
      
        S
      
    
    {\displaystyle S}
   that occurs at the beginning of 
  
    
      
        S
      
    
    {\displaystyle S}
  . A suffix of a string 
  
    
      
        S
      
    
    {\displaystyle S}
   is a substring that occurs at the end of 
  
    
      
        S
      
    
    {\displaystyle S}
  .
The list of all substrings of the string ""apple"" would be ""apple"", ""appl"", ""pple"", ""app"", ""ppl"", ""ple"", ""ap"", ""pp"", ""pl"", ""le"", ""a"", ""p"", ""l"", ""e"", """".


== Substring ==
A substring (or factor) of a string 
  
    
      
        T
        =
        
          t
          
            1
          
        
        …
        
          t
          
            n
          
        
      
    
    {\displaystyle T=t_{1}\dots t_{n}}
   is a string 
  
    
      
        
          
            
              T
              ^
            
          
        
        =
        
          t
          
            1
            +
            i
          
        
        …
        
          t
          
            m
            +
            i
          
        
      
    
    {\displaystyle {\hat {T}}=t_{1+i}\dots t_{m+i}}
  , where 
  
    
      
        0
        ≤
        i
      
    
    {\displaystyle 0\leq i}
   and 
  
    
      
        m
        +
        i
        ≤
        n
      
    
    {\displaystyle m+i\leq n}
  . A substring of a string is a prefix of a suffix of the string, and equivalently a suffix of a prefix. If 
  
    
      
        
          
            
              T
              ^
            
          
        
      
    
    {\displaystyle {\hat {T}}}
   is a substring of 
  
    
      
        T
      
    
    {\displaystyle T}
  , it is also a subsequence, which is a more general concept. Given a pattern 
  
    
      
        P
      
    
    {\displaystyle P}
  , you can find its occurrences in a string 
  
    
      
        T
      
    
    {\displaystyle T}
   with a string searching algorithm. Finding the longest string which is equal to a substring of two or more strings is known as the longest common substring problem.
Example: The string ana is equal to substrings (and subsequences) of banana at two different offsets:

banana
 |||||
 ana||
   |||
   ana

In the mathematical literature, substrings are also called subwords (in America) or factors (in Europe).
Not including the empty substring, the number of substrings of a string of length 
  
    
      
        n
      
    
    {\displaystyle n}
   where symbols only occur once, is the number of ways to choose two distinct places between symbols to start/end the substring. Including the very beginning and very end of the string, there are 
  
    
      
        n
        +
        1
      
    
    {\displaystyle n+1}
   such places. So there are 
  
    
      
        
          
            
              
                (
              
              
                
                  n
                  +
                  1
                
                2
              
              
                )
              
            
          
        
        =
        
          
            
              
                n
                (
                n
                +
                1
                )
              
              2
            
          
        
      
    
    {\displaystyle {\tbinom {n+1}{2}}={\tfrac {n(n+1)}{2}}}
   non-empty substrings.


== Prefix ==
A prefix of a string 
  
    
      
        T
        =
        
          t
          
            1
          
        
        …
        
          t
          
            n
          
        
      
    
    {\displaystyle T=t_{1}\dots t_{n}}
   is a string 
  
    
      
        
          
            
              T
              ^
            
          
        
        =
        
          t
          
            1
          
        
        …
        
          t
          
            m
          
        
      
    
    {\displaystyle {\widehat {T}}=t_{1}\dots t_{m}}
  , where 
  
    
      
        m
        ≤
        n
      
    
    {\displaystyle m\leq n}
  . A proper prefix of a string is not equal to the string itself (
  
    
      
        0
        ≤
        m
        <
        n
      
    
    {\displaystyle 0\leq m<n}
  ); some sources in addition restrict a proper prefix to be non-empty (
  
    
      
        0
        <
        m
        <
        n
      
    
    {\displaystyle 0<m<n}
  ). A prefix can be seen as a special case of a substring.
Example: The string ban is equal to a prefix (and substring and subsequence) of the string banana:

banana
|||
ban

The square subset symbol is sometimes used to indicate a prefix, so that 
  
    
      
        
          
            
              T
              ^
            
          
        
        ⊑
        T
      
    
    {\displaystyle {\widehat {T}}\sqsubseteq T}
   denotes that 
  
    
      
        
          
            
              T
              ^
            
          
        
      
    
    {\displaystyle {\widehat {T}}}
   is a prefix of 
  
    
      
        T
      
    
    {\displaystyle T}
  . This defines a binary relation on strings, called the prefix relation, which is a particular kind of prefix order.
In formal language theory, the term prefix of a string is also commonly understood to be the set of all prefixes of a string, with respect to that language. See the article on string functions for more details.


== Suffix ==
A suffix of a string is any substring of the string which includes its last letter, including itself. A proper suffix of a string is not equal to the string itself. A more restricted interpretation is that it is also not empty[1]. A suffix can be seen as a special case of a substring.
Example: The string nana is equal to a suffix (and substring and subsequence) of the string banana:

banana
  ||||
  nana

A suffix tree for a string is a trie data structure that represents all of its suffixes. Suffix trees have large numbers of applications in string algorithms. The suffix array is a simplified version of this data structure that lists the start positions of the suffixes in alphabetically sorted order; it has many of the same applications.


== Border ==
A border is suffix and prefix of the same string, e.g. ""bab"" is a border of ""babab"" (and also of ""babooneatingakebab"").


== Superstring ==
Given a set of 
  
    
      
        k
      
    
    {\displaystyle k}
   strings 
  
    
      
        P
        =
        {
        
          s
          
            1
          
        
        ,
        
          s
          
            2
          
        
        ,
        
          s
          
            3
          
        
        ,
        …
        
          s
          
            k
          
        
        }
      
    
    {\displaystyle P=\{s_{1},s_{2},s_{3},\dots s_{k}\}}
  , a superstring of the set 
  
    
      
        P
      
    
    {\displaystyle P}
   is single string that contains every string in 
  
    
      
        P
      
    
    {\displaystyle P}
   as a substring. For example, a concatenation of the strings of 
  
    
      
        P
      
    
    {\displaystyle P}
   in any order gives a trivial superstring of 
  
    
      
        P
      
    
    {\displaystyle P}
  . For a more interesting example, let 
  
    
      
        P
        =
        {
        
          abcc
        
        ,
        
          efab
        
        ,
        
          bccla
        
        }
      
    
    {\displaystyle P=\{{\text{abcc}},{\text{efab}},{\text{bccla}}\}}
  . Then 
  
    
      
        
          bcclabccefab
        
      
    
    {\displaystyle {\text{bcclabccefab}}}
   is a superstring of 
  
    
      
        P
      
    
    {\displaystyle P}
  , and 
  
    
      
        
          efabccla
        
      
    
    {\displaystyle {\text{efabccla}}}
   is another, shorter superstring of 
  
    
      
        P
      
    
    {\displaystyle P}
  . Generally, we are interested in finding superstrings whose length is small.


== See also ==
Brace notation
Substring index
Suffix automaton


== References =="
224,Cheney's algorithm,4422941,5968,"Cheney's algorithm, first described in a 1970 ACM paper by C.J. Cheney, is a stop and copy method of tracing garbage collection in computer software systems. In this scheme, the heap is divided into two equal halves, only one of which is in use at any one time. Garbage collection is performed by copying live objects from one semispace (the from-space) to the other (the to-space), which then becomes the new heap. The entire old heap is then discarded in one piece. It is an improvement on the previous stop and copy technique.
Cheney's algorithm reclaims items as follows:
Object references on the stack. Object references on the stack are checked. One of the two following actions is taken for each object reference that points to an object in from-space:
If the object has not yet been moved to the to-space, this is done by creating an identical copy in the to-space, and then replacing the from-space version with a forwarding pointer to the to-space copy. Then update the object reference to refer to the new version in to-space.
If the object has already been moved to the to-space, simply update the reference from the forwarding pointer in from-space.

Objects in the to-space. The garbage collector examines all object references in the objects that have been migrated to the to-space, and performs one of the above two actions on the referenced objects.
Once all to-space references have been examined and updated, garbage collection is complete.
The algorithm needs no stack and only two pointers outside of the from-space and to-space: a pointer to the beginning of free space in the to-space, and a pointer to the next word in to-space that needs to be examined. For this reason, it is sometimes called a ""two-finger"" collector—it only needs ""two fingers"" pointing into the to-space to keep track of its state. The data between the two fingers represents work remaining for it to do.
The forwarding pointer (sometimes called a ""broken heart"") is used only during the garbage collection process; when a reference to an object already in to-space (thus having a forwarding pointer in from-space) is found, the reference can be updated quickly simply by updating its pointer to match the forwarding pointer.
Because the strategy is to exhaust all live references, and then all references in referenced objects, this is known as a breadth-first list copying garbage collection scheme.


== Sample algorithm ==

initialize() =
    tospace   = N/2
    fromspace = 0
    allocPtr  = fromspace
    scanPtr   = whatever -- only used during collection

allocate(n) =
    If allocPtr + n > fromspace+ N/2
        collect()
    EndIf
    If allocPtr + n > fromspace+ N/2
        fail “insufficient memory”
    EndIf
    o = allocPtr
    allocPtr = allocPtr + n
    return o

collect() =
    swap(fromspace, tospace)
    allocPtr = fromspace
    scanPtr  = fromspace

    -- scan every root you've got
    ForEach root in the stack -- or elsewhere
        root = copy(root)
    EndForEach
    
    -- scan objects in the heap (including objects added by this loop)
    While scanPtr < allocPtr
        ForEach reference r from o (pointed to by scanPtr)
            r = copy(r)
        EndForEach
        scanPtr = scanPtr  + o.size() -- points to the next object in the heap, if any
    EndWhile

copy(o) =
    If o has no forwarding address
        o' = allocPtr
        allocPtr = allocPtr + size(o)
        copy the contents of o to o'
        forwarding-address(o) = o'
    EndIf
    return forwarding-address(o)


== Semispace ==
Cheney based his work on the semispace garbage collector, which was published a year earlier by R.R. Fenichel and J.C. Yochelson.


== Equivalence to tri-color abstraction ==
Cheney's algorithm is an example of a tri-color marking garbage collector. The first member of the gray set is the stack itself. Objects referenced on the stack are copied into the to-space, which contains members of the black and gray sets.
The algorithm moves any white objects (equivalent to objects in the from-space without forwarding pointers) to the gray set by copying them to the to-space. Objects that are between the scanning pointer and the free-space pointer on the to-space area are members of the gray set still to be scanned. Objects below the scanning pointer belong to the black set. Objects are moved to the black set by simply moving the scanning pointer over them.
When the scanning pointer reaches the free-space pointer, the gray set is empty, and the algorithm ends.


== References ==
Cheney, C.J. (November 1970). ""A Nonrecursive List Compacting Algorithm"". Communications of the ACM. 13 (11): 677–678. doi:10.1145/362790.362798. 
Fenichel, R.R.; Yochelson, Jerome C. (1969). ""A LISP garbage-collector for virtual-memory computer systems"". Communications of the ACM. 12 (11): 611–612. doi:10.1145/363269.363280. 
Byers, Rick (2007). ""Garbage Collection Algorithms"" (PDF). Garbage Collection Algorithms. 12 (11): 3–4. 
Tutorial at the University of Maryland, College Park"
225,Swiss Informatics Society,39708960,5967,"The Swiss Informatics Society (Schweizer Informatik Gesellschaft), short ""SI"", is a Swiss organization of computer science educators, researchers, and professionals.
The Swiss Informatics Society was founded in 1983. Helmar Burkhart was the president from 1990 to 1992, Bernhard M. Hämmerli has been president from 2009 and since 2014 Jürg Gutknecht is the president. SI has about 2000 members, academics in research and science and representatives from business, administration and education. It is the largest organization of IT specialists in Switzerland. A focus of SI is the exchange of IT education on university and college level and IT practice in companies. It maintains working groups, including computer science education, computer graphics and computer security. A working group, coordinated by the ETH Zurich, deals with data bases, their theory and application, including aspects such as ""web information systems, ontologies, XML data management, service-oriented architectures and information retrieval systems"". SI serves as a network for its members and represents their interests in politics and education.
SI collaborates with the US Association for Computing Machinery (ACM) and the German Gesellschaft für Informatik (GI). The organization is a member of the Council of European Professional Informatics Societies (CEPIS). It is the Swiss partner for the international certificate European Computer Driving License (ECDL). Collaborations include for example a conference on the ""role of Information and Communication Technologies (ICT) in energy efficiency and sustainable development"" in Zurich in 2013.
SI celebrated a history of 30 years on 25 June 2013 at the Fachhochschule Westschweiz in Fribourg. The program includes events of working groups, lectures by scientists from the University of Zurich and others, prizes for bachelor's and master's theses, and a final discussion on the future of IT education in Switzerland.


== References ==


== External links ==
The Swiss Informatics Society SI Official website
Professional Groups Information Systems Security Association (ISSA)"
227,Creative computing,49870344,5861,"Creative computing covers the interdisciplinary area at the cross-over of the creative arts and computing. Issues of creativity include knowledge discovery, for example.


== Overview ==
The International Journal of Creative Computing describes creative computing as follows:

Creative computing refers to a meta-technology to coalesce knowledge in computing and other disciplines. People use computers as aids to creativity and creative-computing topics may reshape the world as we know it. Applications are seen in arts, entertainment/games, mobile applications, multimedia, product/web design and other interactive systems.

Creative computing is interdisciplinary in nature and topics relating to it include applications, development method, evaluation, modeling, philosophy, principles, support environment, and theory.
The term ""creative computing"" is used both in the United Kingdom and the United States (e.g., at Harvard University and MIT).


== Degree programmes ==
A number of university degree programmes in Creative Computing exist, for example at:

University of West London
Bath Spa University
Falmouth University
Goldsmiths, University of London
Queen Mary, University of London
Wrexham Glyndŵr University


== Journal ==
The International Journal of Creative Computing is a quarterly peer-reviewed scientific journal published by Inderscience Publishers, covering creativity in computing and the other way around. The editor-in-chief is Hongji Yang (Bath Spa University).
The journal was established in 2013 and is abstracted and indexed in CSA, ProQuest, and DBLP databases.


== See also ==
Computational creativity


== References =="
228,International Symposium on Wearable Computers,24212401,5798,"The International Symposium on Wearable Computers or ISWC (pronounced ""iz-wic"") is one of the most prominent academic conferences on wearable computing and ubiquitous computing.  Its first edition was held in 1997 in Cambridge, MA, USA. Proceedings from every edition are published by IEEE Press.


== Overview ==


== References ==


== External links ==
Official website"
229,Computer scientist,328784,5766,"A computer scientist is a scientist who has acquired the knowledge of computer science, the study of the theoretical foundations of information and computation and their application.
Computer scientists typically work on the theoretical side of computer systems, as opposed to the hardware side that computer engineers mainly focus on (although there is overlap). Although computer scientists can also focus their work and research on specific areas (such as algorithm and data structure development and design, software engineering, information theory, database theory, computational complexity theory, numerical analysis, programming language theory, computer graphics, and computer vision), their foundation is the theoretical study of computing from which these other fields derive.
A primary goal of computer scientists is to develop and validate models, often mathematical in nature, to estimate the properties of computer-based systems (processors, programs, computers interacting with people, computers interacting with other computers, etc.) with an overall objective of discovering designs that allow improved performance (faster, smaller, cheaper, more precise, etc.).


== Education ==
Most computer scientists are required to possess a Ph.D., M.S., or B.S. in computer science, or other similar fields like Information and Computer Science (CIS), or a closely related discipline such as mathematics or physics. A strong aptitude for mathematics is important for a computer scientist.
Good communication skills are also important for a computer scientist, since a key part of being a good scientist is conveying results for use by others; generally via well-crafted publications and presentations. Further, since computer scientists often work in teams on real-world projects, they must be able to communicate effectively with computer personnel, such as programmers and managers, and with users or other staff who may have no technical computer background.


=== Areas of specialization ===
Theoretical computer science – including data structures and algorithms, theory of computation, information theory and coding theory, programming language theory, and formal methods
Computer systems – including computer architecture and computer engineering, computer performance analysis, concurrency, and distributed computing, computer networks, computer security and cryptography, and databases.
Computer applications – including computer graphics and visualization, human–computer interaction, scientific computing, and artificial intelligence.
Software engineering


== Employment ==
Computer scientists are often hired by software publishing firms, scientific research and development organizations where they develop the theories that allow new technologies to be developed. Computer scientists are also employed by educational institutions such as universities.
Computer scientists can follow more practical applications of their knowledge, doing things such as software engineering. They can also be found in the field of information technology consulting, and may be seen as a type of mathematician, given how much of the field depends on mathematics.
Computer scientists employed in industry may eventually advance into managerial or project leadership positions.
Employment prospects for computer scientists are said to be excellent. Such prospects seem to be attributed, in part, to very rapid growth in computer systems design and related services industry, and the software publishing industry, which are projected to be among the fastest growing industries in the U.S. economy.


== See also ==


== References =="
230,Code mobility,15851275,5760,"In distributed computing, code mobility is the ability for running programs, code or objects to be migrated (or moved) from one machine or application to another. This is the process of moving mobile code across the nodes of a network as opposed to distributed computation where the data is moved.
It is common practice in distributed systems to require the movement of code or processes between parts of the system, instead of data.
Examples of code mobility include scripts downloaded over a network (for example JavaScript, VBScript), Java applets, ActiveX controls, Flash animations, Shockwave movies (and Xtras), and macros embedded within Microsoft Office documents.


== Overview ==
The purpose of code mobility is to support sophisticated operations. For example, an application can send an object to another machine, and the object can resume executing inside the application on the remote machine with the same state as it had in the originating application.
According to a classification proposed by Fuggetta, Picco and Vigna, code mobility can be either strong or weak: strong code mobility involves moving both the code, data and the execution state from one host to another, notably via a process image (this is important in cases where the running application needs to maintain its state as it migrates from host to host), while weak code mobility involves moving the code and the data only. Therefore, it may be necessary to restart the execution of the program at the destination host.
Several paradigms, or architectural styles, exist within code mobility:
Remote evaluation — A client sends code to a remote machine for execution.
Code on demand — A client downloads code from a remote machine to execute locally.
Mobile agents — Objects or code with the ability to migrate between machines autonomously.


== Implementations ==
Within code mobility, the Mobile Agent paradigm has conventionally attracted the most interest and research, however some recent work has produced general purpose implementations.
Mobile agent frameworks
Aglets — Mobile agent framework, Java
Java Agent Development Framework — Mobile agent framework, Java
Mobile-C — Mobile agent platform, C/C++ 
General purpose
Mobility-RPC — Mobile agent, remote evaluation, code on demand, RPC, Java 
Mobile code can also be encapsulated or embedded in other file formats not traditionally associated with executable code. An example of this form of encapsulation is the presence of JavaScript in a PDF.


== Viruses ==
Mobile code can also download and execute in the client workstation via email. Mobile code may download via an email attachment (e.g., macro in a Word file) or via an HTML email body (e.g., JavaScript). For example, the ILOVEYOU, TRUELOVE, and AnnaK email viruses/worms all were implemented as mobile code (VBScript in a .vbs email attachment that executed in Windows Scripting Host). In almost all situations, the user is not aware that mobile code is downloading and executing in their workstation.


== Renting code ==
Mobile code also refers to code ""used for rent"", a way of making software packages more affordable. i.e. to use on demand. This is specially relevant to the mobile devices being developed which are cellular phones, PDAs, etc. all in one. Instead of installing software packages, they can be ""leased"" and paid for on a per-usage basis.


== See also ==
Remote evaluation
Code on demand
Mobile agent


== References =="
231,Jim Coplien,653567,5758,"James O. Coplien, also known as Cope, is a writer, lecturer, and researcher in the field of computer science. He held the 2003–4 Vloeberghs Leerstoel (Vloeberghs Chair) at Vrije Universiteit Brussel and has been a visiting professor at University of Manchester.
He is known for his involvement in founding the pattern movement as part of the Hillside Group, organizing events in the Pattern Languages of Programs conference series, and his writings on software design patterns and organizational patterns.


== Career ==
His ongoing work with Liping Zhao includes a monograph entitled ""A Generalized Formal Design Theory"" which explores the foundations of symmetry and symmetry-breaking in design in general, and in patterns in particular.
Cope was a founding Member of Hillside Group with Kent Beck, Grady Booch, Ward Cunningham, Ralph Johnson, Ken Auer and Hal Hildebrand. He has started up several of the conferences in the Pattern Languages of Programs (PLoP) conference series and is a longstanding pattern author and PLoP shepherd. His pattern form, the ""Coplien Form,"" is a simplified way to structure a pattern in preparation for writing a more literate version in Alexandrian form. Together with Trygve Reenskaug, he was a principal in the design of the data, context and interaction (DCI) paradigm.
He was also Program Chair of Object-Oriented Programming, Systems, Languages & Applications conference (OOPSLA) in 1996, and has been a co-founder and sometime chair of many software pattern conferences.


=== Books ===
Books he has written or co-written include:
James O. Coplien (September 1991). Advanced C++ Programming Styles and Idioms. ISBN 978-0-201-54855-6. 
James O. Coplien, Douglas C. Schmidt (May 1995). Pattern Languages of Program Design. ISBN 978-0-201-60734-5. 
John M. Vlissides; James O. Coplien; Norman L. Kerth (June 1996). Pattern Languages of Program Design 2 (v. 2). ISBN 978-0-201-89527-8. 
James O. Coplien (June 1996). Software Patterns. ISBN 978-1-884842-50-4. 
James O. Coplien (October 1998). Multi-Paradigm Design for C++. ISBN 978-0-201-82467-4. 
James O. Coplien, Neil B. Harrison (July 2004). Organizational Patterns of Agile Software Development. ISBN 978-0-13-146740-8. 
James O. Coplien, Gertrud Bjørnvig (August 2010). Lean Software Architecture for Agile Software Development. ISBN 978-0-470-68420-7. 


=== Research ===
His early work on C++ idioms was one of the three primary sources of the popular Design Patterns. He also named the curiously recurring template pattern C++ idiom.[1] His work on organizational patterns was an inspiration for both extreme programming[2] and for Scrum daily standups.[3] In Organizational Patterns of Agile Software Development book he co-presented an alternative version of Conway's law.


=== Presenter ===
Coplien has presented several times in the UK at the ACCU conference:
ACCU2010 Lean Architecture and Agile Software Development
ACCU2008 Five practical solutions to Agile myths
ACCU2008 Organizational Patterns: The Foundations of Agile
ACCU2007 A balanced Agile design approach
He has given several conference keynotes, including the recent presentations ""Reflections on Reflection"" at SPLASH 2013, ""Kaizen and Certification"" at the 2013 Scrum Alliance Regional Conference in Tokyo, and ""Objects of the people, by the people, and for the people"" at the AOSD Conference in Berlin in 2012.


== References ==

^ Coplien, James O. (February 1995). ""Curiously Recurring Template Patterns"". C++ Report: 24–27. 
^ Fraser, Steven, Kent Beck, Bill Caputo, Tim Mackinnon, James Newkirk and Charlie Pool. “Test Driven Development (TDD).” In M. Marchesi and G. Succi, eds., XP 2003, LNCS 2675, pp. 459–462, 2003. ©Springer-Verlag, Berlin and Heidelberg, 2003.
^ Sutherland, Jeff. Origins of Scrum. Web page [4], accessed 31 January 2010. July 5, 2007.


== External links ==
Jim's Homepage
Jim's blog"
232,SMAWK algorithm,40077712,5753,"The SMAWK algorithm is an algorithm for finding the minimum value in each row of an implicitly-defined totally monotone matrix. It is named after the initials of its five inventors, Peter Shor, Shlomo Moran, Alok Aggarwal, Robert Wilber, and Maria Klawe.
For the purposes of this algorithm, a matrix is defined to be monotone if each row's minimum value occurs in a column which is equal to or greater than the column of the previous row's minimum. It is totally monotone if the same property is true for every submatrix (defined by an arbitrary subset of the rows and columns of the given matrix). Equivalently, a matrix is totally monotone if there does not exist a 2×2 submatrix whose row minima are in the top right and bottom left corners. Every Monge array is totally monotone, but not necessarily vice versa.
For the SMAWK algorithm, the matrix to be searched should be defined as a function, and this function is given as input to the algorithm (together with the dimensions of the matrix). The algorithm then evaluates the function whenever it needs to know the value of a particular matrix cell. If this evaluation takes O(1), then, for a matrix with r rows and c columns, the running time and number of function evaluations are both O(c(1 + log(r/c))). This is much faster than the O(r c) time of a naive algorithm that evaluates all matrix cells.
The basic idea of the algorithm is to follow a prune and search strategy in which the problem to be solved is reduced to a single recursive subproblem of the same type whose size is smaller by a constant factor. To do so, the algorithm first preprocesses the matrix to remove some of its columns that cannot contain a row-minimum, using a stack-based algorithm similar to the one in the Graham scan and all nearest smaller values algorithms. After this phase of the algorithm, the number of remaining columns will at most equal the number of rows. Next, the algorithm calls itself recursively to find the row minima of the even-numbered rows of the matrix. Finally, by searching the columns between the positions of consecutive even-row minima, the algorithm fills out the remaining minima in the odd rows.
The main applications of this method presented in the original paper by Aggarwal et al. were in computational geometry, in finding the farthest point from each point of a convex polygon, and in finding optimal enclosing polygons. Subsequent research found applications of the same algorithm in breaking paragraphs into lines, RNA secondary structure prediction, DNA and protein sequence alignment, the construction of prefix codes, and image thresholding, among others.


== References =="
233,Visibility graph,1448291,5742,"In computational geometry and robot motion planning, a visibility graph is a graph of intervisible locations, typically for a set of points and obstacles in the Euclidean plane. Each node in the graph represents a point location, and each edge represents a visible connection between them. That is, if the line segment connecting two locations does not pass through any obstacle, an edge is drawn between them in the graph. When the set of locations lies in a line, this can be understood as an ordered series. Visibility graphs have therefore been extended to the realm of time series analysis.


== Applications ==
Visibility graphs may be used to find Euclidean shortest paths among a set of polygonal obstacles in the plane: the shortest path between two obstacles follows straight line segments except at the vertices of the obstacles, where it may turn, so the Euclidean shortest path is the shortest path in a visibility graph that has as its nodes the start and destination points and the vertices of the obstacles. Therefore, the Euclidean shortest path problem may be decomposed into two simpler subproblems: constructing the visibility graph, and applying a shortest path algorithm such as Dijkstra's algorithm to the graph. For planning the motion of a robot that has non-negligible size compared to the obstacles, a similar approach may be used after expanding the obstacles to compensate for the size of the robot. Lozano-Pérez & Wesley (1979) attribute the visibility graph method for Euclidean shortest paths to research in 1969 by Nils Nilsson on motion planning for Shakey the robot, and also cite a 1973 description of this method by Russian mathematicians M. B. Ignat'yev, F. M. Kulakov, and A. M. Pokrovskiy.
Visibility graphs may also be used to calculate the placement of radio antennas, or as a tool used within architecture and urban planning through visibility graph analysis.
The visibility graph of a set of locations that lie in a line can be interpreted as a graph-theoretical representation of a time series. This particular case builds a bridge between time series, dynamical systems and graph theory.


== Characterization ==
The visibility graph of a simple polygon has the polygon's vertices as its point locations, and the exterior of the polygon as the only obstacle. Visibility graphs of simple polygons must be Hamiltonian graphs: the boundary of the polygon forms a Hamiltonian cycle in the visibility graph. It is known that not all visibility graphs induce a simple polygon. In fact, visibility graphs of simple polygons do not possess the characteristics of a few special classes of graphs.


== Related problems ==
The art gallery problem is the problem of finding a small set of points such that all other non-obstacle points are visible from this set. Certain forms of the art gallery problem may be interpreted as finding a dominating set in a visibility graph.
The bitangents of a system of polygons or curves are lines that touch two of them without penetrating them at their points of contact. The bitangents of a set of polygons form a subset of the visibility graph that has the polygon's vertices as its nodes and the polygons themselves as the obstacles. The visibility graph approach to the Euclidean shortest path problem may be sped up by forming a graph from the bitangents instead of using all visibility edges, since a Euclidean shortest path may only enter or leave the boundary of an obstacle along a bitangent.


== See also ==
Visibility graph analysis
Fuzzy architectural spatial analysis
Space syntax


== Notes ==


== References ==
de Berg, Mark; van Kreveld, Marc; Overmars, Mark; Schwarzkopf, Otfried (2000), ""Chapter 15: Visibility Graphs"", Computational Geometry (2nd ed.), Springer-Verlag, pp. 307–317, ISBN 3-540-65620-0 .
Lozano-Pérez, Tomás; Wesley, Michael A. (1979), ""An algorithm for planning collision-free paths among polyhedral obstacles"", Communications of the ACM, 22 (10): 560–570, doi:10.1145/359156.359164 .


== External links ==
VisiLibity: A free open source C++ library of floating-point visibility algorithms and supporting data types. This software can be used for calculating visibility graphs of polygonal environments with polygonal holes. A Matlab interface is also included."
234,RoboMind,10434801,5740,"RoboMind is a simple educational programming environment with its own scripting language that allows beginners to learn the basics of computer science by programming a simulated robot. In addition to introducing common programming techniques, it also aims at offering insights in robotics and artificial intelligence. RoboMind is available as stand-alone application for Windows, Linux and Mac OS X. It was first released in 2005 and was originally developed by Arvid Halma, a student of the University of Amsterdam at that time. Since 2011 RoboMind is published by Research Kitchen.


== The simulation environment ==
The application is built around a two-dimensional grid world in which a robot can move around, observe neighboring cells, or mark them by leaving a paint trail. The world may also contain so-called beacons that can be carried around by the robot in order to clear its way.
Since version 4.0, it is possible to export RoboMind scripts to robots in the real world directly. Currently, Lego Mindstorms NXT 2.0 are supported.


== The scripting language ==
RoboMind offers a basic scripting language that consists of a concise set of rules. Apart from commands to make the robot perform basic movement instructions, the control flow can be modified by conditional branching (if-then-else), loops (while) and calls to custom procedures.
Example script to draw square:

paintWhite
repeat(4) {
    forward(2)
    right
}

Recursive line follower example:

follow

procedure follow{
    if(frontIsWhite){
              forward(1)                
    }
    else if(rightIsWhite){
              right
    }
    else if(leftIsWhite){
         left
    }
    else{
         end
    }
    follow
}

The programming environment offers an integrated text editor to write these scripts, with syntax highlighting, autocompletion and line numbering.
Modifications to the environment, such as painting grid cells, are used to store a runtime state. This shows the robot in its environment is directly related to 2D Turing machines. Since version 5.0, the language does allow the declaration of variables and functions (procedures that return values).
The scripting language itself is currently available in 22 languages: Arabic, Catalan, Chinese, Czech, Dutch, English, French, German, Greek, Hungarian, Indonesian, Korean, Polish, Brazilian Portuguese, Russian, Slovak, Slovenian, Spanish, Swedish, Thai, Turkish and Ukrainian. All instructions and keywords can be translated. This makes it easier to learn for non-English speakers than most other programming languages that are constrained to English syntax and Latin alphabets.


== Relation to other educational software ==
RoboMind is somewhat similar to Karel the Robot but its syntax is closer to C/C++ while Karel is closer to Pascal.
RoboMind can be related to the Logo, at which a turtle can be moved around to create geometric shapes. The syntax of RoboMind however is different and corresponds more directly to mainstream scripting languages, such as JavaScript. In RoboMind perceiving and changing the environment are of equal importance, where Logo focuses mostly on the latter. This makes RoboMind more suitable to demonstrate real life applications. In Logo, on the other hand, users have more freedom to create visual effects.
Other free educational programming languages, such as Alice and Scratch focus on the wider domain of interactive story telling.


== See also ==
Educational programming language
Karel the Robot (programming language)
RUR-PLE
Microsoft Small Basic
Minibloq
Logo (programming language)
Alice (software)
Scratch (programming language)
Kodu Game Lab
TouchDevelop


== References ==


== External links ==
Official website
Online RoboMind with complete computational thinking curriculums"
235,European Conference on Object-Oriented Programming,5094367,5718,"In computer science, an object can be a variable, a data structure, a function, or a method, and as such, is a location in memory having a value and referenced by an identifier.
In the class-based object-oriented programming paradigm, ""object"" refers to a particular instance of a class where the object can be a combination of variables, functions, and data structures.
In relational database management, an object can be a table or column, or an association between data and a database entity (such as relating a person's age to a specific person).


== Object-based languages ==

An important distinction in programming languages is the difference between an object-oriented language and an object-based language. A language is usually considered object-based if it includes the basic capabilities for an object: identity, properties, and attributes. A language is considered object-oriented if it is object-based and also has the capability of polymorphism and inheritance. Polymorphism refers to the ability to overload the name of a function with multiple behaviors based on which object(s) are passed to it. Conventional message passing discriminates only on the first object and considers that to be ""sending a message"" to that object. However, some OOP languages such as Flavors and the Common Lisp Object System (CLOS) enable discriminating on more than the first parameter of the function. Inheritance is the ability to subclass an object class, to create a new class that is a subclass of an existing one and inherits all the data constraints and behaviors of its parents but also adds new and/or changes one or more of them.


== Object-oriented programming ==

Object-oriented programming is an approach to designing modular reusable software systems. The object-oriented approach is an evolution of good design practices that go back to the very beginning of computer programming. Object-orientation is simply the logical extension of older techniques such as structured programming and abstract data types. An object is an abstract data type with the addition of polymorphism and inheritance.
Rather than structure programs as code and data, an object-oriented system integrates the two using the concept of an ""object"". An object has state (data) and behavior (code). Objects can correspond to things found in the real world. So for example, a graphics program will have objects such as circle, square, menu. An online shopping system will have objects such as shopping cart, customer, product. The shopping system will support behaviors such as place order, make payment, and offer discount. The objects are designed as class hierarchies. So for example with the shopping system there might be high level classes such as electronics product, kitchen product, and book. There may be further refinements for example under electronic products: CD Player, DVD player, etc. These classes and subclasses correspond to sets and subsets in mathematical logic.


== Specialized objects ==
An important concept for objects is the design pattern. A design pattern provides a reusable template to address a common problem. The following object descriptions are examples of some of the most common design patterns for objects.
Function object: an object with a single method (in C++, this method would be the function operator, ""operator()"") that acts much like a function (like a C/C++ pointer to a function).
Immutable object: an object set up with a fixed state at creation time and which does not change afterward.
First-class object: an object that can be used without restriction.
Container object: an object that can contain other objects.
Factory object: an object whose purpose is to create other objects.
Metaobject: an object from which other objects can be created (compare with a class, which is not necessarily an object).
Prototype object: a specialized metaobject from which other objects can be created by copying
God object: an object that knows or does too much (it is an example of an anti-pattern).
Singleton object: an object that is the only instance of its class during the lifetime of the program.
Filter object.


== Distributed objects ==

The object-oriented approach is not just a programming model. It can be used equally well as an interface definition language for distributed systems. The objects in a distributed computing model tend to be larger grained, longer lasting, and more service-oriented than programming objects.
A standard method to package distributed objects is via an Interface Definition Language (IDL). An IDL shields the client of all of the details of the distributed server object. Details such as which computer the object resides on, what programming language it uses, what operating system, and other platform specific issues. The IDL is also usually part of a distributed environment that provides services such as transactions and persistence to all objects in a uniform manner. Two of the most popular standards for distributed objects are the Object Management Group's CORBA standard and Microsoft's DCOM.
In addition to distributed objects, a number of other extensions to the basic concept of an object have been proposed to enable distributed computing:
Protocol objects are components of a protocol stack that enclose network communication within an object-oriented interface.
Replicated objects are groups of distributed objects (called replicas) that run a distributed multi-party protocol to achieve high consistency between their internal states, and that respond to requests in a coordinated way. Examples include fault-tolerant CORBA objects.
Live distributed objects (or simply live objects) generalize the replicated object concept to groups of replicas that might internally use any distributed protocol, perhaps resulting in only a weak consistency between their local states.
Some of these extensions, such as distributed objects and protocol objects, are domain-specific terms for special types of ""ordinary"" objects used in a certain context (such as remote method invocation or protocol composition). Others, such as replicated objects and live distributed objects, are more non-standard, in that they abandon the usual case that an object resides in a single location at a time, and apply the concept to groups of entities (replicas) that might span across multiple locations, might have only weakly consistent state, and whose membership might dynamically change.


== The Semantic Web ==
The Semantic Web is essentially a distributed objects framework. Two key technologies in the Semantic Web are the Web Ontology Language (OWL) and the Resource Description Framework (RDF). RDF provides the capability to define basic objects—names, properties, attributes, relations—that are accessible via the Internet. OWL adds a richer object model, based on set theory, that provides additional modeling capabilities such as multiple inheritance.
OWL objects are not like standard large grained distributed objects accessed via an Interface Definition Language. Such an approach would not be appropriate for the Internet because the Internet is constantly evolving and standardization on one set of interfaces is difficult to achieve. OWL objects tend to be similar to the kind of objects used to define application domain models in programming languages such as Java and C++.
However, there are important distinctions between OWL objects and traditional object-oriented programming objects. Where as traditional objects get compiled into static hierarchies usually with single inheritance, OWL objects are dynamic. An OWL object can change its structure at run time and can become an instance of new or different classes.
Another critical difference is the way the model treats information that is currently not in the system. Programming objects and most database systems use the ""closed-world assumption"". If a fact is not known to the system that fact is assumed to be false. Semantic Web objects use the open-world assumption, a statement is only considered false if there is actual relevant information that it is false, otherwise it is assumed to be unknown, neither true nor false.
OWL objects are actually most like objects in artificial intelligence frame languages such as KL-ONE and Loom.
The following table contrasts traditional objects from Object-Oriented programming languages such as Java or C++ with Semantic Web Objects:


== See also ==
Object lifetime
Object copy
Design pattern (computer science)
Business object (computer science)
Actor model


== References ==


== External links ==
What Is an Object? from The Java Tutorials
THE COMPUTER OBJECTS LOOKING FOR THEIR SOCIAL AND ORGANIZATIONAL IMPLICATIONS. http://revistas.face.ufmg.br/index.php/farol/article/view/2709"
236,ACCU (organisation),18949316,5704,"ACCU, previously known as Association of C and C++ Users, is a non-profit user group of people interested in software development, dedicated to raising the standard of computer programming. The ACCU publishes two journals and organizes an annual conference.


== History ==
ACCU was formed in 1987 by Martin Houston. The original name of the organisation was C Users' Group (UK) and this remained the formal name of the organisation until 2011, although it adopted the public name Association of C and C++ Users for the period 1993–2003, and adopted the shorter form ACCU from 2003 onward. As the formal name suggests, the organisation was originally created for people in the United Kingdom. However, the membership is worldwide, predominantly European and North American, but also with members from central and southern America, Australasia, Africa and Asia. Originally, the voluntary association was mainly for C programmers, but it has expanded over time to include all programming languages, especially C++, C#, Java, Perl and Python.


== Publications ==
The ACCU currently publishes two journals:
C Vu is a members-only journal which acts as the association's news letter and carries book reviews, articles on software development and a number of regular columns such as Student Code Critique and Professionalism in Programming. It was edited by Phil Stubbington from its first issue until 1991.
Overload aims to carry more in-depth articles aimed at professional software developers. Topics range from programming and design through to process and management. Overload is available online to members and non-members free of charge.
Other journals have been published by ACCU in the past. Accent was the news letter of the Silicon Valley chapter and CAUGers was the news letter of the Acorn special interest group. Overload was originally the journal of ACCU's C++ special interest group, but is no longer language-specific.


== Local groups ==
The Silicon Valley chapter organized local meetings in San Jose. Local groups were formed in London, Bristol & Bath, Oxford, Cambridge, North East England, Southern England and Zurich.


== Conference ==
The ACCU is operated by a volunteer committee, elected at an Annual General Meeting during the annual conference each Spring which from 1997 to 2012 took place in Oxford, and for the first time in Bristol in 2013. It attracts speakers from the computing community including David Abrahams, Andrei Alexandrescu, Ross J. Anderson, James Coplien, Tom Gilb, Kevlin Henney, Andrew Koenig, Simon Peyton-Jones, Eric S. Raymond, Guido van Rossum, Greg Stein, Bjarne Stroustrup (the designer and original implementor of C++), Herb Sutter and Daveed Vandevoorde.
The UK Python Conference, for the Python programming language, originally started out as a track at the ACCU conference.


== Standardisation ==
ACCU supports the standardisation process for computer programming languages. ACCU provided financial sponsorship of meetings in the UK for both the International Organization for Standardization (ISO) C programming language working group and the ISO C++ working groups and helped finance travel to ECMA meetings in mainland Europe.


== Mailing lists ==
The ACCU operates mailing lists, some of which are also open to non-members. These lists allow for general programming-orientated discussions, but also for mentored discussions. Mentored groups have included Effective C++, Python, software patterns, functional programming and XML. They are often based around study of a book.


== References ==


== External links ==
ACCU Official Site
The C Acorn User Group (with back issues of CAUGers)
CUG
ACCU Silicon Valley Chapter"
237,Geoinformatics,1404353,5678,"Geoinformatics is the science and the technology which develops and uses information science infrastructure to address the problems of geography, cartography, geosciences and related branches of science and engineering.


== Overview ==
Geoinformatics has been described as ""the science and technology dealing with the structure and character of spatial information, its capture, its classification and qualification, its storage, processing, portrayal and dissemination, including the infrastructure necessary to secure optimal use of this information"" or ""the art, science or technology dealing with the acquisition, storage, processing production, presentation and dissemination of geoinformation"".
Geomatics is a similarly used term which encompasses geoinformatics, but geomatics focuses more so on surveying. Geoinformatics has at its core the technologies supporting the processes of acquiring, analyzing and visualizing spatial data. Both geomatics and geoinformatics include and rely heavily upon the theory and practical implications of geodesy.
Geography and earth science increasingly rely on digital spatial data acquired from remotely sensed images analyzed by geographical information systems (GIS) and visualized on paper or the computer screen.
Geoinformatics combines geospatial analysis and modeling, development of geospatial databases, information systems design, human-computer interaction and both wired and wireless networking technologies. Geoinformatics uses geocomputation and geovisualization for analyzing geoinformation.
Branches of geoinformatics include:


== Research ==
Research in this field is used to support global and local environmental, energy and security programs. The Geographic Information Science and Technology group of Oak Ridge National Laboratory is supported by various government departments and agencies including the United States Department of Energy. It is currently the only group in the United States Department of Energy National Laboratory System to focus on advanced theory and application research in this field. There are also a lot of interdiscipline research involved in geoinformatics fields including computer science, information technology, software engineering, biogeography, geography, conservation, architecture, spatial analysis and reinformacement learning.


== Applications ==
Many fields benefit from geoinformatics, including urban planning and land use management, in-car navigation systems, virtual globes, public health, local and national gazetteer management, environmental modeling and analysis, military, transport network planning and management, agriculture, meteorology and climate change, oceanography and coupled ocean and atmosphere modelling, business location planning, architecture and archeological reconstruction, telecommunications, criminology and crime simulation, aviation, biodiversity conservation and maritime transport. The importance of the spatial dimension in assessing, monitoring and modelling various issues and problems related to sustainable management of natural resources is recognized all over the world. Geoinformatics becomes very important technology to decision-makers across a wide range of disciplines, industries, commercial sector, environmental agencies, local and national government, research, and academia, national survey and mapping organisations, International organisations, United Nations, emergency services, public health and epidemiology, crime mapping, transportation and infrastructure, information technology industries, GIS consulting firms, environmental management agencies), tourist industry, utility companies, market analysis and e-commerce, mineral exploration, etc. Many government and non government agencies started to use spatial data for managing their day-to-day activities.


== See also ==
Geographic information science
Geographical information systems
Geographic information systems software
GeoComputation
International Coalition for GeoInformatics (iGeoInfo)
Cartography
Symbiosis Institute of Geoinformatics
Urban informatics
Crowdmapping


== References ==


== External links ==
Open Geospatial Consortium
International Cartographic Association (ICA), the world body for mapping and GIScience professionals
International Society for Photogrammetry and Remote Sensing
International Union of Geodesy and Geophysics (IUGG)"
238,Responsible disclosure,6989858,5651,"In computer security or elsewhere, responsible disclosure is a vulnerability disclosure model in which a vulnerability or issue is disclosed only after a period of time that allows for the vulnerability or issue to be patched or mended. This period distinguishes the model from full disclosure.
Developers of hardware and software often require time and resources to repair their mistakes. Hackers and computer security scientists have the opinion that it is their social responsibility to make the public aware of vulnerabilities with a high impact. Hiding these problems could cause a feeling of false security. To avoid this, the involved parties join forces and agree on a period of time for repairing the vulnerability and preventing any future damage. Depending on the potential impact of the vulnerability, the expected time needed for an emergency fix or workaround to be developed and applied and other factors, this period may vary between a few days and several months. It is easier to patch software by using the Internet as a distribution channel.
Responsible disclosure fails to satisfy security researchers who expect to be financially compensated, while reporting vulnerabilities to the vendor with the expectation of compensation might be viewed as extortion. While a market for vulnerabilities has developed, vulnerability commercialization remains a hotly debated topic tied to the concept of vulnerability disclosure. Today, the two primary players in the commercial vulnerability market are iDefense, which started their vulnerability contributor program (VCP) in 2003, and TippingPoint, with their zero-day initiative (ZDI) started in 2005. These organisations follow the responsible disclosure process with the material bought. Between March 2003 and December 2007 an average 7.5% of the vulnerabilities affecting Microsoft and Apple were processed by either VCP or ZDI. Independent firms financially supporting responsible disclosure by paying bug bounties include Facebook, Google, Mozilla, and Barracuda Networks.
Vendor-sec was a responsible disclosure mailing list. Many, if not all, of the CERT groups coordinate responsible disclosures.


== Examples ==
Selected security vulnerabilities resolved by applying responsible disclosure:
MD5 collision attack that shows how to create false CA certificates, 1 week
Starbucks gift card double-spending/race condition to create free extra credits, 10 days (Egor Homakov)
Dan Kaminsky discovery of DNS cache poisoning, 5 months
MBTA vs. Anderson, MIT students find vulnerability in the Massachusetts subway security, 5 months
Radboud University Nijmegen breaks the security of the MIFARE Classic cards, 6 months
The Meltdown vulnerability, hardware vulnerability affecting Intel x86 microprocessors and some ARM-based microprocessors, 7 months.
The Spectre vulnerability, hardware vulnerability with implementations of branch prediction affecting modern microprocessors with speculative execution, allowing malicious processes access to the contents of other programs' mapped memory, 7 months.
The ROCA vulnerability, affecting RSA keys generated by an Infineon library and Yubikeys, 8 months.


== See also ==
Whistleblowing
Information sensitivity
White hat (computer security)
Proactive cyber defence
Computer emergency response team
Critical infrastructure protection


== References =="
239,Double compare-and-swap,2057712,5624,"In computer science, compare-and-swap (CAS) is an atomic instruction used in multithreading to achieve synchronization. It compares the contents of a memory location with a given value and, only if they are the same, modifies the contents of that memory location to a new given value. This is done as a single atomic operation. The atomicity guarantees that the new value is calculated based on up-to-date information; if the value had been updated by another thread in the meantime, the write would fail. The result of the operation must indicate whether it performed the substitution; this can be done either with a simple boolean response (this variant is often called compare-and-set), or by returning the value read from the memory location (not the value written to it).


== Overview ==
A compare-and-set operation is an atomic version of the following pseudocode, where * denotes access through a pointer:

function cas(p : pointer to int, old : int, new : int) returns bool {
    if *p ≠ old {
        return false
    }
    *p ← new
    return true
}

This operation is used to implement synchronization primitives like semaphores and mutexes, as well as more sophisticated lock-free and wait-free algorithms. Maurice Herlihy (1991) proved that CAS can implement more of these algorithms than atomic read, write, or fetch-and-add, and assuming a fairly large  amount of memory, that it can implement all of them. CAS is equivalent to load-link/store-conditional, in the sense that a constant number of invocations of either primitive can be used to implement the other one in a wait-free manner.
Algorithms built around CAS typically read some key memory location and remember the old value. Based on that old value, they compute some new value. Then they try to swap in the new value using CAS, where the comparison checks for the location still being equal to the old value. If CAS indicates that the attempt has failed, it has to be repeated from the beginning: the location is re-read, a new value is re-computed and the CAS is tried again.
Instead of immediately retrying after a CAS fails, researchers have found that total system performance can be improved—in multiprocessor systems where many threads constantly update some particular shared variable—if threads that see their CAS fail use exponential backoff—in other words, wait a little before retrying the CAS.


=== Example application: atomic adder ===
As an example use case of compare-and-swap, here is an algorithm for atomically incrementing or decrementing an integer. This is useful in a variety of applications that use counters. The function add performs the action *p ← *p + a, atomically (again denoting pointer indirection by *, as in C) and returns the final value stored in the counter. Unlike in the cas pseudocode above, there is no requirement that any sequence of operations is atomic except for cas.

function add(p : pointer to int, a : int) returns int {
    done ← false
    while not done {
        value ← *p  // Even this operation doesn't need to be atomic.
        done ← cas(p, value, value + a)
    }
    return value + a
}

In this algorithm, if the value of *p changes after (or while!) it is fetched and before the CAS does the store, CAS will notice and report this fact, causing the algorithm to retry.


=== ABA problem ===

Some CAS-based algorithms are affected by and must handle the problem of a false positive match, or the ABA problem. It is possible that between the time the old value is read and the time CAS is attempted, some other processors or threads change the memory location two or more times such that it acquires a bit pattern which matches the old value. The problem arises if this new bit pattern, which looks exactly like the old value, has a different meaning: for instance, it could be a recycled address, or a wrapped version counter.
A general solution to this is to use a double-length CAS (e.g. on a 32-bit system, a 64-bit CAS). The second half is used to hold a counter. The compare part of the operation compares the previously read value of the pointer and the counter, with the current pointer and counter. If they match, the swap occurs - the new value is written - but the new value has an incremented counter. This means that if ABA has occurred, although the pointer value will be the same, the counter is exceedingly unlikely to be the same (for a 32-bit value, a multiple of 232 operations would have to have occurred, causing the counter to wrap and at that moment, the pointer value would have to also by chance be the same).
An alternative form of this (useful on CPUs which lack DCAS) is to use an index into a freelist, rather than a full pointer, e.g. with a 32-bit CAS, use a 16-bit index and a 16-bit counter. However, the reduced counter lengths begin to make ABA - especially at modern CPU speeds - likely.
One simple technique which helps alleviate this problem is to store an ABA counter in each data structure element, rather than using a single ABA counter for the whole data structure.
A more complicated but more effective solution is to implement safe memory reclamation (SMR). This is in effect lock-free garbage collection. The advantage of using SMR is the assurance a given pointer will exist only once at any one time in the data structure, thus the ABA problem is completely solved. (Without SMR, something like a freelist will be in use, to ensure that all data elements can be accessed safely (no memory access violations) even when they are no longer present in the data structure. With SMR, only elements actually currently in the data structure will be accessed).


=== Costs and benefits ===
CAS, and other atomic instructions, are sometimes thought to be unnecessary in uniprocessor systems, because the atomicity of any sequence of instructions can be achieved by disabling interrupts while executing it. However, disabling interrupts has numerous downsides. For example, code that is allowed to do so must be trusted not to be malicious and monopolize the CPU, as well as to be correct and not accidentally hang the machine in an infinite loop or page fault. Further, disabling interrupts is often deemed too expensive to be practical. Thus, even programs only intended to run on uniprocessor machines will benefit from atomic instructions, as in the case of Linux's futexes.
In multiprocessor systems, it is usually impossible to disable interrupts on all processors at the same time. Even if it were possible, two or more processors could be attempting to access the same semaphore's memory at the same time, and thus atomicity would not be achieved. The compare-and-swap instruction allows any processor to atomically test and modify a memory location, preventing such multiple-processor collisions.
On server-grade multi-processor architectures of the 2010s, compare-and-swap is cheap relative to a simple load that is not served from cache. A 2013 paper points out that a CAS is only 1.15 times more expensive than a non-cached load on Intel Xeon (Westmere-EX) and 1.35 times on AMD Opteron (Magny-Cours).


== Implementations ==
Compare-and-swap (and compare-and-swap-double) has been an integral part of the IBM 370 (and all successor) architectures since 1970. The operating systems that run on these architectures make extensive use of this instruction to facilitate process (i.e., system and user tasks) and processor (i.e., central processors) parallelism while eliminating, to the greatest degree possible, the ""disabled spin locks"" which had been employed in earlier IBM operating systems. Similarly, the use of test-and-set was also eliminated. In these operating systems, new units of work may be instantiated ""globally"", into the global service priority list, or ""locally"", into the local service priority list, by the execution of a single compare-and-swap instruction. This substantially improved the responsiveness of these operating systems.
In the x86 (since 80486) and Itanium architectures this is implemented as the compare and exchange (CMPXCHG) instruction (on a multiprocessor the LOCK prefix must be used).
As of 2013, most multiprocessor architectures support CAS in hardware, and the compare-and-swap operation is the most popular synchronization primitive for implementing both lock-based and non-blocking concurrent data structures.
The atomic counter and atomic bitmask operations in the Linux kernel typically use a compare-and-swap instruction in their implementation. The SPARC 32 and PA-RISC architectures are two of the very few recent architectures that do not support CAS in hardware; the Linux port to these architectures uses a spinlock.


=== Implementation in C ===
Many C compilers support using compare-and-swap either with the C11 <stdatomic.h> functions, or some non-standard C extension of that particular C compiler, or by calling a function written directly in assembly language using the compare-and-swap instruction.
The following C function shows the basic behavior of a compare-and-swap variant that returns the old value of the specified memory location; however, this version does not provide the crucial guarantees of atomicity that a real compare-and-swap operation would:

old_reg_val is always returned, but it can be tested following the compare_and_swap operation to see if it matches oldval, as it may be different, meaning that another process has managed to succeed in a competing compare_and_swap to change the reg value from oldval.
For example, an election protocol can be implemented such that every process checks the result of compare_and_swap against its own PID (= newval). The winning process finds the compare_and_swap returning the initial non-PID value (e.g., zero). For the losers it will return the winning PID.

This is the logic in the Intel Software Manual Vol 2A.


== Extensions ==
Since CAS operates on a single pointer-sized memory location, while most lock-free and wait-free algorithms need to modify multiple locations, several extensions have been implemented.
Double compare-and-swap (DCAS) 
Compares two unrelated memory locations with two expected values, and if they're equal, sets both locations to new values. The generalization of DCAS to multiple (non-adjacent) words is called MCAS or CASN. DCAS and MCAS are of practical interest in the convenient (concurrent) implementation of some data structures like dequeues or binary search trees. DCAS and MCAS may be implemented however using the more expressive hardware transactional memory present in some recent processors such as IBM POWER8 or in Intel processors supporting Transactional Synchronization Extensions (TSX).
Double-wide compare-and-swap 
Operates on two adjacent pointer-sized locations (or, equivalently, one location twice as big as a pointer). On later x86 processors, the CMPXCHG8B and CMPXCHG16B instructions serve this role, although early 64-bit AMD CPUs did not support CMPXCHG16B (modern AMD CPUs do). Some Intel motherboards from the Core 2 era also hamper its use, even though the processors support it. These issues came into the spotlight at the launch of Windows 8.1 because it required hardware support for CMPXCHG16B.
Single compare, double swap 
Compares one pointer but writes two. The Itanium's cmp8xchg16 instruction implements this, where the two written pointers are adjacent.
Multi-word compare-and-swap 
Is a generalisation of normal compare-and-swap. It can be used to atomically swap an arbitrary number of arbitrarily located memory locations. Usually, multi-word compare-and-swap is implemented in software using normal double-wide compare-and-swap operations. The drawback of this approach is a lack of scalability.


== See also ==
Conditional Put and Delete
Fetch-and-add
Load-link/store-conditional
Non-blocking synchronization
Test-and-set
Transactional memory


== References ==


== External links ==


=== Basic algorithms implemented using CAS ===
Sundell, Håkan; Tsigas, Philippas. ""Lock-Free and Practical Deques using Single-Word Compare-And-Swap"" (PDF). 
Valois, John D. ""Lock-Free Linked Lists Using Compare-and-Swap"". CiteSeerX 10.1.1.41.9506 . 
Prakash, S.; Lee, Yann Hang; Johnson, T. ""A Nonblocking Algorithm for Shared Queues Using Compare-and-Swap"". 
2003 discussion ""Lock-Free using cmpxchg8b..."" on Intel x86 with pointers to various papers and source code


=== Implementations of CAS ===
AIX compare_and_swap Kernel Service
Java package java.util.concurrent.atomic implements `compareAndSet` in various classes
.NET Class methods Interlocked::CompareExchange
Windows API InterlockedCompareExchange"
240,Large Installation System Administration Conference,649640,5622,"LISA is the Large Installation System Administration Conference, co-sponsored by the computing professional organizations USENIX and its LISA special interest group (formerly known as SAGE).
The word ""large"" was dropped from the title of the 6th conference in 1992 (though retaining the ""LISA"" name). The full acronym was restored in the title of the 2003 conference and remains in use today. The definition of ""large"" was originally understood to mean sites with over 100 users or over 100 terabytes of storage.


== History ==
The LISA conference were first held in 1986. The USENIX web site lists proceedings as far back as 1987, though only those proceedings from 1993 onward are available online. Attendance has recently been in the 1000-2000 range.


== Content ==
The conference is typically held in the fall in a conference center hotel somewhere in the United States. Between 1987 and 2008, roughly half of them were somewhere in California. It generally runs six days: six days of full-day and half-day tutorial training sessions, three days of technical sessions, and a two-day vendor exhibition. The technical sessions usually include multiple tracks, including a peer reviewed refereed paper track, invited talks, and a ""Guru-Is-In"" Q&A track.
For many years, the conference often ended with a LISA quiz show trivia contest, but that has been replaced with a plenary session to close out the week.


== Proceedings ==
The refereed papers are published in a proceedings volume. Many important topics in system administration were first disseminated publicly via LISA papers.


== External links ==
LISA '15 - 29th conference November 8-13 2015, Washington, D.C.
LISA ‘14 - 28th Conference November 9-14, 2014, Seattle, WA
LISA ‘13 - 27th Conference November 3-8, 2013, Washington, DC
LISA ‘12 - 26th Conference December 9–14, 2012, San Diego, CA
LISA ‘11 – 25th Conference December 4–9, 2011, Boston, MA
LISA ‘10 – 24th Conference November 7–12, 2010, San Jose, CA
LISA ‘09 – 23rd Conference November 1–6, 2009, Baltimore, MD
LISA '08 – 22nd Conference November 9–14, 2008, San Diego, CA
LISA '07 – 21st Conference November 11–16, 2007, Dallas, TX
LISA '06 – 20th Conference December 3–8, 2006, Washington, DC
LISA '05 – 19th Conference December 4–9, 2005, San Diego, CA
The LISA '05 Conference Blog
LISA '04 - 18th Conference November 14-19, 2004, Atlanta, GA
LISA '03 - 17th Conference October 26-31, 2003, San Diego, CA
LISA '02 - 16th Conference November 3-8, 2002, Philadelphia, PA
LISA '01 - 15th Conference December 2-7, 2001, San Diego, CA
LISA '00 - 14th Conference December 3-8, 2000, New Orleans, LA
LISA '99 - 13th Conference November 7-12, 1999, Seattle, WA
LISA '98 - 12th Conference December 6-11, 1998, Boston, MA
LISA '97 - 11th Conference October 26-31, 1997, San Diego, CA
LISA '96 - 10th Conference September 29 - October 4, 1996, Chicago, IL
LISA '95 - 9th Conference September 18-22, 1995, Monterey, CA
LISA '94 - 8th Conference September 19-23, 1994, San Diego, CA
LISA '93 - 7th Conference November 1-5, 1993, Monterey, CA


== References =="
241,International Conference on Bioinformatics,12848292,5599,"The International Conference on Bioinformatics (InCoB) is a scientific conference on bioinformatics aimed at scientists in the Asia Pacific region. It has been held annually since 2002. Originally organised by coordination between the Asia Pacific Bioinformatics Network (APBioNet) and the Thailand National Center for Genetic Engineering and Biotechnology (BIOTEC) in 2002, the meeting has since been the flagship conference of the APBioNet, where APBioNet's Annual General Meeting is held.


== Scientific publications ==
Since 2006, InCoB has been partnering with BMC Bioinformatics to publish an InCoB Special Conference Issue of top papers presented at the conference. In 2007, an additional tie-up with the Bioinformation journal was established in addition to the BMC Bioinformatics issue.


== Technological placeshifting ==
Since 2007, InCoB held in Hong Kong University of Science and Technology, has been placeshifted in an additional location in a developing country venue, namely the Vietnam National University, Hanoi (VNU) through the advanced videoconferencing project of APAN and TEIN2. In 2015, InCoB was organised jointly with the International Conference on Genome Informatics in an attempt to increase effectiveness and scalability.


== Satellite training workshops ==
Since 2007, at the VNU site coordinated by the Institute of Biotechnology Hanoi (IBT), InCoB coordinated with the International Union for Biochemists and Molecular Biologists (IUBMB), the Federation of Asian Oceanian Biochemists and Molecular Biologists (FAOBMB) and APBioNet to hold a two-week bioinformatics training course with course faculty from Karolinska Institutet, NCBI and National University of Singapore, supported by the S* Alliance for Bioinformatics Education and BioSlax, a software development project hosted at NUS as part of an ASEAN SubCommittee on Biotechnology (SCB) project. This collaboration with IUBMB and FAOBMB continues in 2008 with a bioinformatics education workshop in Taipei, Taiwan, where the main meeting of InCoB 2008 will be situated.


== Past and present conferences ==


== External links ==
APBioNet Website
InCoB Website


== References =="
242,Ashley Morris (blogger),28448721,5561,"Ashley Morris (1963 – April 2, 2008) was a notable New Orleans cultural and political blogger and a professor of computer science at DePaul University in Chicago (commuting between the two cities during the school semester). Morris was a prolific blogger, commenting on New Orleans culture and politics, often critical of the status quo. Morris became popular through a series of post-Katrina blog posts that dealt with the destruction caused by the hurricane and the efforts to rebuild New Orleans. One post in particular, entitled ""Fuck You You Fucking Fucks"" earned Morris a great deal of notoriety and inspired an ""FYYFF"" T-shirt.
Morris was a very big fan of David Simon and The Wire and wrote a blog, along with Ray Shea, devoted to the show. Simon learned of Morris' writings on The Wire and of his writings on New Orleans while Morris was still alive, and has called Morris ""a very passionate, very blunt, very funny, but very honest voice about the anger and the isolation that the people in New Orleans felt."" Simon later used Morris as the inspiration for the character Creighton Bernette, an English professor and blogger played by John Goodman, in his series Treme, even quoting Morris' blog for some of Goodman's dialogue.
Morris died of a heart attack on April 2, 2008 while in Florida settling some minor legal issues.


== References ==


== External links ==
Ashley Morris: the blog (currently maintained by his wife).
Got That New Package! (Morris' blog on The Wire)
Treme homepage"
243,Kestrel Institute,56382262,5491,"The Kestrel Institute is a nonprofit computer science research center located in Palo Alto's Stanford Research Park. Cordell Green, who founded Kestrel in 1981, is its Director and Chief Scientist. Its mission is to make it easier to write good, high-quality software and employs computer scientists like Lambert Meertens.
In the 1980s, Kestrel described its research focus as ""knowledge-based software environments"" to make it easier to write software (""normalize and mechanize the programming process""). In addition, a 2002 MIT Technology Review article described one of Kestrel's projects as a way to ""almost force coders to write reliable programs"". A 2005 Newsweek article discussed one Kestrel technology that developed software to help the U.S. military schedule cargo deployment by ""translating a description of a problem into guidelines a computer can understand"".
Nearly all of Kestrel's funding comes from government grants, from organizations such as the U.S. Department of Defense, DARPA, Intelligence Advanced Research Projects Activity (IARPA), Air Force Research Laboratory (AFRL), AFOSR, Office of Naval Research (ONR), NASA, and the National Science Foundation (NSF). In 2015 it received $4.9 million in grants and contributions, down from the previous year's $6.6 million.


== References ==


== External links ==
Official website"
244,Alpha shape,33029175,5465,"In computational geometry, an alpha shape, or α-shape, is a family of piecewise linear simple curves in the Euclidean plane associated with the shape of a finite set of points. They were first defined by Edelsbrunner, Kirkpatrick & Seidel (1983). The alpha-shape associated with a set of points is a generalization of the concept of the convex hull, i.e. every convex hull is an alpha-shape but not every alpha shape is a convex hull.


== Characterization ==
For each real number α, define the concept of a generalized disk of radius 1/α as follows:
If α = 0, it is a closed half-plane;
If α > 0, it is closed disk of radius 1/α;
If α < 0, it is the closure of the complement of a disk of radius −1/α.
Then an edge of the alpha-shape is drawn between two members of the finite point set whenever there exists a generalized disk of radius 1/α containing the entire point set and which has the property that the two points lie on its boundary.
If α = 0, then the alpha-shape associated with the finite point set is its ordinary convex hull.


== Alpha complex ==
Alpha shapes are closely related to alpha complexes, subcomplexes of the Delaunay triangulation of the point set.
Each edge or triangle of the Delaunay triangulation may be associated with a characteristic radius, the radius of the smallest empty circle containing the edge or triangle. For each real number α, the α-complex of the given set of points is the simplicial complex formed by the set of edges and triangles whose radii are at most 1/α.
The union of the edges and triangles in the α-complex forms a shape closely resembling the α-shape; however it differs in that it has polygonal edges rather than edges formed from arcs of circles. More specifically, Edelsbrunner (1995) showed that the two shapes are homotopy equivalent. (In this later work, Edelsbrunner used the name ""α-shape"" to refer to the union of the cells in the α-complex, and instead called the related curvilinear shape an α-body.)


== Examples ==
This technique can be employed to reconstruct a Fermi surface from the electronic Bloch spectral function evaluated at the Fermi level, as obtained from the Green function in a generalised ab-initio study of the problem. The Fermi surface is then defined as the set of reciprocal space points within the first Brillouin zone, where the signal is highest. The definition has the advantage of covering also cases of various forms of disorder.


== See also ==
Beta skeleton


== References ==
N. Akkiraju, H. Edelsbrunner, M. Facello, P. Fu, E. P. Mucke, and C. Varela. ""Alpha shapes: definition and software"". In Proc. Internat. Comput. Geom. Software Workshop 1995, Minneapolis.
Edelsbrunner, Herbert (1995), ""Smooth surfaces for multi-scale shape representation"", Foundations of software technology and theoretical computer science (Bangalore, 1995), Lecture Notes in Comput. Sci., 1026, Berlin: Springer, pp. 391–412, MR 1458090 .
Edelsbrunner, Herbert; Kirkpatrick, David G.; Seidel, Raimund (1983), ""On the shape of a set of points in the plane"", IEEE Transactions on Information Theory, 29 (4): 551–559, doi:10.1109/TIT.1983.1056714 .


== External links ==
2D Alpha Shapes and 3D Alpha Shapes in CGAL the Computational Geometry Algorithms Library
Alpha Complex in the GUDHI library.
Description and implementation by Duke University
Everything You Always Wanted to Know About Alpha Shapes But Were Afraid to Ask – with illustrations and interactive demonstration
Implementation of the 3D alpha-shape for the reconstruction of 3D sets from a point cloud in R
Description of the implementation details for alpha shapes - lecture providing a description of the formal and intuitive aspects of alpha shape implementation
Alpha Hulls, Shapes, and Weighted things - lecture slides by Robert Pless at the Washington University"
245,John von Neumann Theory Prize,16461,5458,"The John von Neumann Theory Prize of the Institute for Operations Research and the Management Sciences (INFORMS) is awarded annually to an individual (or sometimes a group) who has made fundamental and sustained contributions to theory in operations research and the management sciences.
The Prize named after mathematician John von Neumann is awarded for a body of work, rather than a single piece. The Prize was intended to reflect contributions that have stood the test of time. The criteria include significance, innovation, depth, and scientific excellence.
The award is $5,000, a medallion and a citation.
The Prize has been awarded since 1975. The first recipient was George B. Dantzig for his work on linear programming.


== List of recipients ==
2017 Donald Goldfarb and Jorge Nocedal
for seminal contributions to the theory and applications of nonlinear optimization over the past several decades.

2016 Martin I. Reiman and Ruth J. Williams
for seminal research contributions over the past several decades, to the theory and applications of “stochastic networks/systems” and their “heavy traffic approximations.”

2015 Vašek Chvátal and Jean Bernard Lasserre
for seminal and profound contributions to the theoretical foundations of optimization.

2014 Nimrod Megiddo
for fundamental contributions across a broad range of areas of operations research and management science, most notably in linear programming, combinatorial optimization, and algorithmic game theory.

2013 Michel Balinski.
2012 George Nemhauser and Laurence Wolsey. 
2011 Gérard Cornuéjols, IBM University Professor of Operations Research at Carnegie Mellon University’s Tepper School of Business
for his fundamental and broad contributions to discrete optimization including his deep research on balanced and ideal matrices, perfect graphs and cutting planes for mixed-integer optimization.

2010 Søren Asmussen and Peter W. Glynn
2009 Yurii Nesterov and Yinyu Ye
2008 Frank Kelly
2007 Arthur F. Veinott, Jr.
for his profound contributions to three major areas of operations research and management science: inventory theory, dynamic programming and lattice programming.

2006 Martin Grötschel, László Lovász and Alexander Schrijver
for their fundamental path-breaking work in combinatorial optimization.

2005 Robert J. Aumann
in recognition of his fundamental contributions to game theory and related areas

2004 J. Michael Harrison
for his profound contributions to two major areas of operations research and management science: stochastic networks and mathematical finance.

2003 Arkadi Nemirovski and Michael J. Todd
for their seminal and profound contributions in continuous optimization.

2002 Donald L. Iglehart and Cyrus Derman
for their fundamental contributions to performance analysis and optimization of stochastic systems

2001 Ward Whitt
for his contributions to queueing theory, applied probability and stochastic modelling

2000 Ellis L. Johnson and Manfred W. Padberg
1999 R. Tyrrell Rockafellar
1998 Fred W. Glover
1997 Peter Whittle
1996 Peter C. Fishburn
1995 Egon Balas
1994 Lajos Takacs
1993 Robert Herman
1992 Alan J. Hoffman and Philip Wolfe
1991 Richard E. Barlow and Frank Proschan
1990 Richard Karp
1989 Harry M. Markowitz
1988 Herbert A. Simon
1987 Samuel Karlin
1986 Kenneth J. Arrow
1985 Jack Edmonds
1984 Ralph Gomory
1983 Herbert Scarf
1982 Abraham Charnes, William W. Cooper, and Richard J. Duffin
1981 Lloyd Shapley
1980 David Gale, Harold W. Kuhn, and Albert W. Tucker
1979 David Blackwell
1978 John F. Nash and Carlton E. Lemke
1977 Felix Pollaczek
1976 Richard Bellman
1975 George B. Dantzig for his work on linear programming
There is also an IEEE John von Neumann Medal awarded by the IEEE annually ""for outstanding achievements in computer-related science and technology"".


== See also ==
IEEE John von Neumann Medal
List of science and technology awards
Prizes named after people


== References ==


== External links ==
Official website"
246,Matt Welsh (computer scientist),2630919,5441,"Matthew David ""Matt"" Welsh is a computer scientist and software engineer at Google. He was the Gordon McKay Professor of Computer Science at Harvard University and author of several books about the Linux operating system, several Linux HOWTOs, the LinuxDoc format and articles in the Linux Journal.
In November 2010, five months after being granted tenure, Welsh announced that he was leaving Harvard.
He is a 1992 graduate of the North Carolina School of Science and Mathematics.
Welsh received a BS from Cornell University in 1996 and MS and PhD degrees from the University of California, Berkeley in 1999 and 2002, respectively. He spent the 1996-7 school year at the University of Cambridge Computer Laboratory and at the University of Glasgow.


== The Social Network ==
Welsh taught the operating system class at Harvard in which Mark Zuckerberg was a student. Welsh was later portrayed by actor Brian Palermo in the movie The Social Network featuring Zuckerberg and the founding of Facebook. Welsh was reportedly paid $200 for his powerpoint slides used in the movie.


== Publications ==
Dalheimer, Matthias Kalle; Welsh, Matt (2005). Running Linux (5th ed.). O'Reilly Media. ISBN 978-0596007607. Retrieved 2013-08-23. 
Welsh, Matt; Hughes, Phil; Bandel, David; Beletsky, Boris; Dreilinger, Sean; Kiesling, Robert; Liebovitch, Evan; Pierce, Henry (1998) [1992-1996]. Linux Installation and Getting Started (2nd ed.). Specialized Systems Consultants. ISBN 1-57831-001-6. Retrieved 2009-08-14. 
See footnotes below


== References ==


== External links ==
http://www.mdw.la"
247,"Single instruction, multiple threads",43320329,5359,"Single instruction, multiple thread (SIMT) is an execution model used in parallel computing where single instruction, multiple data (SIMD) is combined with multithreading.


== Overview ==
The processors, say a number p of them, seem to execute many more than p tasks. This is achieved by each processor having multiple ""threads"" (or ""work-items"" or ""Sequence of SIMD Lane operations""), which execute in lock-step, and are analogous to SIMD lanes.
The SIMT execution model has been implemented on several GPUs and is relevant for general-purpose computing on graphics processing units (GPGPU), e.g. some supercomputers combine CPUs with GPUs.
SIMT was introduced by Nvidia:

Nvidia's Tesla GPU microarchitecture (first available November 8, 2006 as implemented in the ""G80"" GPU chip) introduced the single-instruction multiple-thread (SIMT) execution model where multiple independent threads execute concurrently using a single instruction.

ATI Technologies (now AMD) released a competing product slightly later on May 14, 2007, the TeraScale 1-based ""R600"" GPU chip.
As access time of all the widespread RAM types (e.g. DDR SDRAM, GDDR SDRAM, XDR DRAM, etc.) is still relatively high, engineers came up with the idea to hide the latency that inevitably comes with each memory access. Strictly, the latency-hiding is a feature of the zero-overhead scheduling implemented by modern GPUs. This might or might not be considered to be a property of 'SIMT' itself.
SIMT is intended to limit instruction fetching overhead, i.e. the latency that comes with memory access, and is used in modern GPUs (such as those of Nvidia and AMD) in combination with 'latency hiding' to enable high-performance execution despite considerable latency in memory-access operations. This is where the processor is oversubscribed with computation tasks, and is able to quickly switch between tasks when it would otherwise have to wait on memory. This strategy is comparable to multithreading in CPUs (not to be confused with multi-core).
A downside of SIMT execution is the fact that thread-specific control-flow is performed using ""masking"", leading to poor utilization where a processor's threads follow different control-flow paths. For instance, to handle an IF-ELSE block where various threads of a processor execute different paths, all threads must actually process both paths (as all threads of a processor always execute in lock-step), but masking is used to disable and enable the various threads as appropriate. Masking is avoided when control flow is coherent for the threads of a processor, i.e. they all follow the same path of execution. The masking strategy is what distinguishes SIMT from ordinary SIMD, and has the benefit of inexpensive synchronization between the threads of a processor.


== See also ==
General-purpose computing on graphics processing units


== References =="
248,On the Cruelty of Really Teaching Computer Science,624625,5344,"“On the Cruelty of Really Teaching Computing Science” is a 1988 paper by E. W. Dijkstra which argues that computer programming should be understood as a branch of mathematics, and that the formal provability of a program is a major criterion for correctness.
Despite the title, most of the article is on Dijkstra’s attempt to put computer science into a wider perspective within science, teaching being addressed as a corollary at the end. Specifically, Dijkstra made a “proposal for an introductory programming course for freshmen” that consisted of Hoare logic as an uninterpreted formal system.


== Debate over feasibility ==
Since the term ""software engineering"" was coined, formal verification has almost always been considered too resource-intensive to be feasible. In complex applications, the difficulty of correctly specifying what the program should do in the first place is also a common source of error. Other methods of software testing are generally employed to try to eliminate bugs and many other factors are considered in the measurement of software quality.
The notion that cost of production of hardware should be a constraint in programming was foreign to Dijkstra. He viewed the cost controls as artifacts that could become excuses and the controls of nature as nonexistent in digital systems, which above the level of circuits guarantee a second, constructed nature.
Until the end of his life, Dijkstra maintained that the central challenges of computing hadn’t been met to his satisfaction, due to an insufficient emphasis on program correctness (though not obviating other requirements, such as maintainability and efficiency).


== Pedagogical legacy ==
Computer science as taught today does not follow all of Dijkstra's advice. The curricula generally emphasize techniques for managing complexity and preparing for future changes, following Dijkstra's earlier writings. These include abstraction, programming by contract, and design patterns. Programming techniques to avoid bugs and conventional software testing methods are taught as basic requirements, and students are exposed to certain mathematical tools, but formal verification methods are not included in the curriculum except perhaps as an advanced topic. So in some ways, Dijkstra's ideas have been adhered to; however, the ideas he felt most strongly about have not been.
Newly formed curricula in software engineering have adopted Dijkstra's recommendations. The focus of these programs is the formal specification of software requirements and design in order to facilitate the formal validation of system correctness. In Canada, they are often accredited engineering degrees with similar core competencies in physics-based engineering.
There is also greater emphasis on the social aspects of programming, such as learning how to program as part of a team, and how to write code that is easily re-used by other people, or ""borrowing"" code from other programs' source code, which was not considered immoral or illegal at the time. Some institutions focus more on pleasing the computing industry by teaching the most popular programming languages, or teaching the use of commonly available development tools, than they do on imparting the foundational concepts of computing science.


== References =="
249,Pedro Domingos,47790413,5319,"Pedro Domingos is Professor at University of Washington. He is a researcher in machine learning and known for markov logic network enabling uncertain inference.


== Biography ==
Domingos received an undergraduate degree and M.S. from Instituto Superior Técnico (IST). And then at University of California, Irvine, he received an M.S. and Ph.D. After spending two years as an assistant professor at IST, he joined University of Washington in 1999 and now is a professor.


== Research ==


=== Markov logic network ===


== Awards and honors ==
2014. ACM SIGKDD Innovation Award.
for his foundational research in data stream analysis, cost-sensitive classification, adversarial learning, and Markov logic networks, as well as applications in viral marketing and information integration.
2010. AAAI Fellow.
For significant contributions to the field of machine learning and to the unification of first-order logic and probability.
2003. Sloan Fellowship
Fulbright Scholarship


== Selected works ==


=== Books ===
DOMINGOS, PEDRO (2015). The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World. New York: Basic Books. ISBN 978-0-465-06570-7. 


=== Articles ===
2015. (with Abram Friesen). Recursive Decomposition for Nonconvex Optimization. IJCAI 2015 Distinguished Paper Award.
2011. (with Hoifung Poon). Sum-Product Networks: A New Deep Architecture. UAI 2011 Best Paper Award..
2009. (with Hoifung Poon). Unsupervised Semantic Parsing. EMNLP 2009 Best Paper Award.
2005. (with Parag Singla). Object Identification with Attribute-Mediated Dependences. PKDD 2005 Best Paper Award.
1999. (with Parag Singla). MetaCost: A General Method for Making Classifiers Cost-Sensitive. SIGKDD 1999 Best Paper Award for Fundamental Research.
1998. (with Parag Singla). Occam's Two Razors: The Sharp and the Blunt. SIGKDD 2005 Best Paper Award for Fundamental Research.


== References ==


== External links ==
Google Scholar, h-index is 73."
250,Device tree,40312837,5297,"In computing, a device tree (also written devicetree) is a data structure describing the hardware components of a particular computer so that the operating system's kernel can use and manage those components, including the CPU or CPUs, the memory, the buses and the peripherals.
The device tree was derived from SPARC-based workstations and servers via the Open Firmware project. The current Devicetree specification is targeted at smaller systems, but is still used with some server-class systems (for instance, those described by the Power Architecture Platform Reference including some Apple Macintoshes).
Personal computers with the x86 architecture generally do not use device trees, relying instead on various auto configuration protocols to discover hardware. Systems which use device trees usually pass a static device tree (perhaps stored in ROM) to the operating system, but can also generate a device tree in the early stages of booting. As an example, Das U-Boot and kexec can pass a device tree when launching a new operating system. On systems with a boot loader that does not support device trees, a static device tree may be installed along with the operating system; the Linux kernel supports this approach.
The Devicetree specification is currently managed by a community named devicetree.org, which is associated with, among others, Linaro and ARM.


== Device Tree formats ==
A device tree can hold any kind of data as internally it is a tree of named nodes and properties. Nodes contain properties and child nodes, while properties are name–value pairs.
Device trees have both a binary format for operating systems to use and a textual format for convenient editing and management.


== Usage in Linux ==
Given the correct device tree, the same compiled kernel can support different hardware configurations within a wider architecture family. The Linux kernel for the ARM, x86, MicroBlaze, PowerPC, and SPARC architectures reads device tree information; on ARM, device trees have been mandatory for all new SoCs since 2012. This can be seen as a remedy to the vast number of forks (of Linux and Das U-boot) that has historically been created to support (marginally) different ARM boards. The purpose is to move a significant part of the hardware description out of the kernel binary, and into the compiled device tree blob, which is handed to the kernel by the boot loader, replacing a range of board-specific C source files and compile-time options in the kernel.
It has been customary for ARM-based Linux distributions to include a boot loader, that necessarily was customised for specific boards, for example Raspberry Pi or Hackberry A10. This has created problems for the creators of Linux distributions as some part of the operating system must be compiled specifically for every board variant, or updated to support new boards. However, some modern SoCs (for example, Freescale i.MX6) have a vendor-provided boot loader with device tree on a separate chip from the operating system.
A proprietary configuration file format used for similar purposes, the FEX file format, is a de facto standard among Allwinner SoCs.


== See also ==

Differentiated System Description Table –  part of ACPI firmware that describes power events to an OS.
PCI configuration space  –  how one bus found in most PCs is auto configured.


== References ==


== External links ==
devicetree.org website
Device Tree Reference –  eLinux.org
Device Tree –  OMAPpedia
Embedded Power Architecture Platform Requirements (ePAPR)
About The Device Tree"
251,Munindar P. Singh,39215970,5278,"Munindar P. Singh is Alumni Distinguished Graduate Professor and a full professor in the Department of Computer Science at North Carolina State University. He is a AAAI fellow and an IEEE Fellow. 


== Education ==
Singh received his B.Tech. in Computer Science & Engineering from the Indian Institute of Technology Delhi in 1986. He obtained a Ph.D. in Computer Science from the University of Texas at Austin in 1993 under the supervision of E. Allen Emerson and Nicholas M. Asher.


== Research ==
Singh's research interests include multiagent systems, service-oriented computing, software engineering, artificial intelligence, and social networks. He has made several important contributions to the understanding of interaction and norms in multiagent systems. He introduced to artificial intelligence the distinction between social commitment (a norm) and psychological commitment (a mental attitude). Singh also introduced the idea that interaction among autonomous social principals (e.g., between two or more organizations) must have a social semantics. This idea has proved to be highly influential within multiagent systems research. In recognition of Singh's contribution, the paper in which he introduced this idea was awarded the IFAAMAS 2016 Influential Paper Award. Taking this line of thinking further, Singh, in joint work with his Ph.D. student pInar Yolum, introduced the abstraction of commitment protocols.
Singh has also made important contributions to social networks, trust, and distributed computing. His Blindingly Simple Protocol Language (BSPL) introduces the idea that message ordering in interaction protocols fall out automatically from information flow requirements. Therefore, one need not model specify control flow at all in interaction protocols.


== References ==


== External links ==
Munindar P. Singh's home page
Multiagent Systems and Service-Oriented Computing Laboratory"
252,Tucker Prize,56125271,5263,"The Tucker Prize for outstanding theses in the area of discrete mathematics is sponsored by the Mathematical Optimization Society (MOS). Up to three finalists are presented at each (triennial) International Symposium of the MOS. The winner will receive an award of $1000 and a certificate. The Albert W. Tucker Prize was established by the Society in 1985, and was first awarded at the Thirteenth International Symposium on Mathematical Programming in 1988.


== Winners and finalists ==
1988:
Andrew V. Goldberg for ""Efficient graph algorithms for sequential and parallel computers"".

1991:
Michel Goemans for ""Analysis of Linear Programming Relaxations for a Class of Connectivity Problems"".
Other Finalists: Leslie Hall and Mark Hartmann

1994:
David P. Williamson for ""On the Design of Approximation Algorithms for a Class of Graph Problems"".
Other Finalists: Dick Den Hertog and Jiming Liu

1997:
David Karger for ""Random Sampling in Graph Optimization Problems"".
Other Finalists: Jim Geelen and Luis Nunes Vicente

2000:
Bertrand Guenin for his PhD thesis.
Other Finalists: Kamal Jain and Fabian Chudak

2003:
Tim Roughgarden for ""Selfish Routing"".
Other Finalists: Pablo Parrilo and Jiming Peng

2006:
Uday V. Shanbhag for ""Decomposition and Sampling Methods for Stochastic Equilibrium Problems"".
Other Finalists: José Rafael Correa and Dion Gijswijt

2009:
Mohit Singh for ""Iterative Methods in Combinatorial Optimization"".
Other Finalists: Tobias Achterberg and Jiawang Nie

2012:
Oliver Friedmann for ""Exponential Lower Bounds for Solving Infinitary Payoff Games and Linear Programs"".
Other Finalists: Amitabh Basu and Guanghui Lan

2015:
Daniel Dadush for ""Integer Programming, Lattice Algorithms, and Deterministic Volume Computation"".
Other Finalists: Dmitriy Drusvyatskiy and Marika Karbstein


== References ==


== External links ==
Official web page (MOS)"
253,Left-leaning red–black tree,35239409,5259,"A left-leaning red–black (LLRB) tree is a type of self-balancing binary search tree. It is a variant of the red–black tree and guarantees the same asymptotic complexity for operations, but is designed to be easier to implement.


== Properties of Left Leaning RB ==
All of the red-black tree algorithms that have been proposed are characterized by a worst-case search time bounded by a small constant multiple of log N in a tree of N keys, and the behavior observed in practice is typically that same multiple faster than the worst-case bound, close to the optimal log N nodes examined that would be observed in a perfectly balanced tree.
Specifically, in a left-leaning red-black 2-3 tree built from N random keys:
A random successful search examines log2 N − 0.5 nodes.
The average tree height is about 2 log2 N
The average size of left subtree exhibits log-oscillating behavior.


== External links ==


=== Papers ===
Robert Sedgewick. Left-leaning Red–Black Trees. Direct link to PDF.
Robert Sedgewick. Left-Leaning Red–Black Trees (slides). Two versions:
Robert Sedgewick. Left-Leaning Red–Black Trees (slides), from seminar at Dagstuhl in February 2008. Outdated.
Robert Sedgewick. Left-Leaning Red–Black Trees (slides), from April 2008; updated

Linus Ek, Ola Holmström and Stevan Andjelkovic. May 19, 2009. Formalizing Arne Andersson trees and Left-leaning Red–Black trees in Agda
Julien Oster. March 22, 2011. An Agda implementation of deletion in Left-leaning Red–Black trees
Kazu Yamamoto. 2011.10.19. Purely Functional Left-Leaning Red–Black Trees


=== Implementations ===


=== Other ===
Robert Sedgewick. 20 Apr 2008. Animations of LLRB operations
Open Data Structures - Section 9.2.2 - Left-Leaning Red–Black Trees
Left-Leaning Red-Black Trees Considered Harmful"
254,Computational magnetohydrodynamics,9441268,5235,"Computational magnetohydrodynamics (CMHD) is a rapidly developing branch of magnetohydrodynamics that uses numerical methods and algorithms to solve and analyze problems that involve electrically conducting fluids. Most of the methods used in CMHD are borrowed from the well established techniques employed in Computational fluid dynamics. The complexity mainly arises due to the presence of a magnetic field and its coupling with the fluid. One of the important issues is to numerically maintain the 
  
    
      
        ∇
        ⋅
        
          
            B
          
        
        =
        0
      
    
    {\displaystyle \nabla \cdot {\mathbf {B} }=0}
   (conservation of magnetic flux) condition, from Maxwell's equations, to avoid any unphysical effects.


== Open-source MHD codes ==
Pencil Code
Compressible resistive MHD, intrinsically divergence free, embedded particles module, finite-difference explicit scheme, high-order derivatives, Fortran95 and C, parallelized up to hundreds of thousands cores. Source code is available.
RAMSES
RAMSES is an open source code to model astrophysical systems, featuring self-gravitating, magnetised, compressible, radiative fluid flows. It is based on the Adaptive Mesh Refinement (AMR) technique on a fully threaded graded octree. RAMSES is written in Fortran 90 and is making intensive use of the Message Passing Interface (MPI) library. Source code is available.
RamsesGPU
RamsesGPU is a MHD Code written in C++, based on the original RAMSES but only for regular grid (no AMR). The code has been designed to run on large clusters of GPU (NVIDIA graphics processors), so parallelization relies on MPI for distributed memory processing, as well as the programing language CUDA for efficient usage of GPU resources. Static Gravity Fields are supported. Different finite volume methods are implemented. Source code is available.
Athena
Athena is a grid-based code for astrophysical magnetohydrodynamics (MHD). It was developed primarily for studies of the interstellar medium, star formation, and accretion flows. Source code is available.


== Commercial MHD codes ==
USim
MACH2


== See also ==
Magnetohydrodynamic turbulence
Magnetic flow meter
Plasma modeling


== References ==

Brio, M., Wu, C. C.(1988), ""An upwind differencing scheme for the equations of ideal magnetohydrodynamics"", Journal of Computational Physics, 75, 400–422.
Henri-Marie Damevin and Klaus A. Hoffmann(2002), ""Development of a Runge-Kutta Scheme with TVD for Magnetogasdynamics"", Journal of Spacecraft and Rockets, 34,No.4, 624–632.
Robert W. MacCormack(1999), ""An upwind conservation form method for ideal magnetohydrodynamics equations"", AIAA-99-3609.
Robert W. MacCormack(2001), ""A conservation form method for magneto-fluid dynamics"", AIAA-2001-0195.


== Further reading ==
Toro, E. F. (1999), Riemann Solvers and Numerical Methods for Fluid Dynamics, Springer-Verlag.
Ledvina, S. A.; Y.-J. Ma; E. Kallio (2008). ""Modeling and Simulating Flowing Plasmas and Related Phenomena"". Space Science Reviews. 139. Bibcode:2008SSRv..139..143L. doi:10.1007/s11214-008-9384-6. 


== External links ==
NCBI"
255,ACM Symposium on User Interface Software and Technology,38721413,5230,"The ACM Symposium on User Interface Software and Technology (UIST) is an annual conference for technical innovations in human–computer interfaces. UIST is sponsored by ACM SIGCHI and ACM SIGGRAPH. By impact factor, it is the most impactful conference in the field of human–computer interaction. Scott Hudson is the current chair of the UIST community, which organizes the UIST conference.
UIST 2014 was held in Honolulu, Hawaii, USA, 5–8 October 2014. UIST 2015 will be held in Charlotte, NC USA, 8–11 November 2015.


== Overview ==
UIST is a highly selective conference, with an acceptance rate of 20.3% over the last five years.


== History ==
 Through 2013, UIST was well known for its intimate single-track format. UIST 2014 introduced a new dual-track format.


== Past Conferences ==
Past and future UIST conferences include:


== References =="
256,Algebraic Logic Functional programming language,11868019,5170,"Algebraic Logic Functional programming language, also known as ALF, is a programming language which combines functional and logic programming techniques. Its foundation is Horn clause logic with equality which consists of predicates and Horn clauses for logic programming, and functions and equations for functional programming.
ALF was designed to be genuine integration of both programming paradigms, and thus any functional expression can be used in a goal literal and arbitrary predicates can occur in conditions of equations. ALF's operational semantics is based on the resolution rule to solve literals and narrowing to evaluate functional expressions. In order to reduce the number of possible narrowing steps, a leftmost-innermost basic narrowing strategy is used which, it is claimed, can be efficiently implemented. Terms are simplified by rewriting before a narrowing step is applied and equations are rejected if the two sides have different constructors at the top. Rewriting and rejection are supposed to result in a large reduction of the search tree and produce an operational semantics that is more efficient than Prolog's resolution strategy. Similarly to Prolog, ALF uses a backtracking strategy corresponding to a depth-first search in the derivation tree.
The ALF system was designed to be an efficient implementation of the combination of resolution, narrowing, rewriting, and rejection. ALF programs are compiled into instructions of an abstract machine. The abstract machine is based on the Warren Abstract Machine (WAM) with several extensions to implement narrowing and rewriting. In the current ALF implementation programs of this abstract machine are executed by an emulator written in C.
In the Carnegie Mellon University Artificial Intelligence Repository, ALF is included as an AI programming language, in particular as a functional/logic programming language Prolog implementation. A user manual describing the language and the use of the system is available. The ALF System runs under Unix and is free.


== References ==


== External links ==
Publications of Michael Hanus, including many articles relevant to the design and theory of ALF
Information about getting and installing the ALF system"
257,Computational mechanics,5205878,5170,"Computational mechanics is the discipline concerned with the use of computational methods to study phenomena governed by the principles of mechanics. Before the emergence of computational science (also called scientific computing) as a ""third way"" besides theoretical and experimental sciences, computational mechanics was widely considered to be a sub-discipline of applied mechanics. It is now considered to be a sub-discipline within computational science.


== Overview ==
Computational mechanics (CM) is interdisciplinary. Its three pillars are mechanics, mathematics, and computer science.


=== Mechanics ===
Computational fluid dynamics, computational thermodynamics, computational electromagnetics, computational solid mechanics are some of the many specializations within CM.


=== Mathematics ===
The areas of mathematics most related to computational mechanics are partial differential equations, linear algebra and numerical analysis. The most popular numerical methods used are the finite element, finite difference, and boundary element methods in order of dominance. In solid mechanics finite element methods are far more prevalent than finite difference methods, whereas in fluid mechanics, thermodynamics, and electromagnetism, finite difference methods are almost equally applicable. The boundary element technique is in general less popular, but has a niche in certain areas including acoustics engineering, for example.


=== Computer Science ===
With regard to computing, computer programming, algorithms, and parallel computing play a major role in CM. The most widely used programming language in the scientific community, including computational mechanics, is Fortran. Recently, C++ has increased in popularity. The scientific computing community has been slow in adopting C++ as the lingua franca. Because of its very natural way of expressing mathematical computations, and its built-in visualization capacities, the proprietary language/environment MATLAB is also widely used, especially for rapid application development and model verification.


== Process ==
Scientists within the field of computational mechanics follow a list of tasks to analyze their target mechanical process:
A mathematical model of the physical phenomenon is made. This usually involves expressing the natural or engineering system in terms of partial differential equations. This step uses physics to formalize a complex system.
The mathematical equations are converted into forms which are suitable for digital computation. This step is called discretization because it involves creating an approximate discrete model from the original continuous model. In particular, it typically translates a partial differential equation (or a system thereof) into a system of algebraic equations. The processes involved in this step are studied in the field of numerical analysis.
Computer programs are made to solve the discretized equations using direct methods (which are single step methods resulting in the solution) or iterative methods (which start with a trial solution and arrive at the actual solution by successive refinement). Depending on the nature of the problem, supercomputers or parallel computers may be used at this stage.
The mathematical model, numerical procedures, and the computer codes are verified using either experimental results or simplified models for which exact analytical solutions are available. Quite frequently, new numerical or computational techniques are verified by comparing their result with those of existing well-established numerical methods. In many cases, benchmark problems are also available. The numerical results also have to be visualized and often physical interpretations will be given to the results.


== Applications ==
Some examples where computational mechanics have been put to practical use are vehicle crash simulation, petroleum reservoir modeling, biomechanics, glass manufacturing, and semiconductor modeling.
Complex systems that would be very difficult or impossible to treat using analytical methods have been successfully simulated using the tools provided by computational mechanics.


== See also ==
Scientific computing
Dynamical systems theory
Movable cellular automaton


== External links ==
United States Association for Computational Mechanics
Santa Fe Institute Comp Mech Publications"
258,Hutter Prize,7687846,5167,"The Hutter Prize is a cash prize funded by Marcus Hutter which rewards data compression improvements on a specific 100 MB English text file. Specifically, the prize awards 500 euros for each one percent improvement (with 50,000 euros total funding) in the compressed size of the file enwik8, which is the smaller of two files used in the Large Text Compression Benchmark; enwik8 is the first 100,000,000 characters of a specific version of English Wikipedia. The ongoing competition is organized by Hutter, Matt Mahoney, and Jim Bowery.


== Goals ==
The goal of the Hutter Prize is to encourage research in artificial intelligence (AI). The organizers believe that text compression and AI are equivalent problems. Hutter proved that the optimal behavior of a goal seeking agent in an unknown but computable environment is to guess at each step that the environment is probably controlled by one of the shortest programs consistent with all interaction so far. However, there is no general solution because Kolmogorov complexity is not computable. Hutter proved that in the restricted case (called AIXItl) where the environment is restricted to time t and space l, a solution can be computed in time O(t2l), which is still intractable.
The organizers further believe that compressing natural language text is a hard AI problem, equivalent to passing the Turing test. Thus, progress toward one goal represents progress toward the other. They argue that predicting which characters are most likely to occur next in a text sequence requires vast real-world knowledge. A text compressor must solve the same problem in order to assign the shortest codes to the most likely text sequences.


== Rules ==
The contest is open-ended. It is open to everyone. To enter, a competitor must submit a compression program and a decompressor that decompresses to the file enwik8. It is also possible to submit a compressed file instead of the compression program. The total size of the compressed file and decompressor (as a Win32 or Linux executable) must not be larger than 99% of the previous prize winning entry. For each one percent improvement, the competitor wins 500 euros. The decompression program must also meet execution time and memory constraints, currently 10 hours on a 2 GHz Pentium 4 with 1 GB memory. These constraints may be relaxed in the future.
Submissions must be published in order to allow independent verification. There is a 30-day waiting period for public comment before awarding a prize. The rules do not require the release of source code, unless such release is required by the code's license (as in the case of PAQ, which is licensed under GPL).


== History ==
The prize was announced on August 6, 2006. The prize baseline was 18,324,887 bytes, achieved by PAQ8F.
On August 16, Rudi Cilibrasi submitted a modified version of PAQ8F called RAQ8G that added parenthesis modeling. However it failed to meet the 1% threshold.
On the same day, but a few hours later Dmitry Shkarin submitted a modified version of his DURILCA compressor called DURILCA 0.5h, which improved compression by 1.5%. However it was disqualified for using 1.75 GB of memory. The decision to disqualify was controversial because the memory limits were not clearly specified in the rules at the time.
On August 20, Alexander Ratushnyak submitted PAQ8HKCC, a modified version of PAQ8H, which improved compression by 2.6% over PAQ8F. He continued to improve the compression to 3.0% with PAQ8HP1 on August 21, 4% with PAQ8HP2 on August 28, 4.9% with PAQ8HP3 on September 3, 5.9% with PAQ8HP4 on September 10, and 5.9% with PAQ8HP5 on September 25. At that point he was declared the first winner of the Hutter prize, awarded 3416 euros, and the new baseline was set to 17,073,018 bytes.
Ratushnyak has since broken his record multiple times, becoming the second (on May 14, 2007, with PAQ8HP12 compressing enwik8 to 16,481,655 bytes, and winning 1732 euros), third (on May 23, 2009, with decomp8 compressing the file to 15,949,688 bytes, and winning 1614 euros), and fourth (on Nov 4, 2017, with phda compressing the file to 15,284,944 bytes, and winning 2085 euros) winner of the Hutter prize.


== References ==


== External links ==
Website of the Hutter Prize"
259,Critical code studies,34549363,5155,"Critical Code Studies (CCS) is an emerging academic subfield, related to software studies, digital humanities, cultural studies, computer science, human-computer interface, and the DIY do-it-yourself maker culture. Its primary focus is on the cultural significance of computer code, without excluding or focusing solely upon the code's functional purpose.
As introduced by Mark C. Marino (""Critical Code Studies,"" Electronic Book Review ), critical code studies was initially a method by which scholars ""can read and explicate code the way we might explicate a work of literature,"" but the concept also draws upon Espen Aarseth's conception of a cybertext as a ""mechanical device for the production and consumption of verbal signs"" (Cybertext, 21), arguing that in order to understand a digital artifact we must also understand the constraints and capabilities of the authoring tools used by the creator of the artifact, as well as the memory storage and interface required for the user to experience the digital artifact.
Evidence that CCS has gained momentum since 2006 include an article by Matthew Kirschenbaum in the Chronicle of Higher Education, CCS sessions at the Modern Language Association in 2011 that were ""packed"" with attendees, several academic conferences devoted wholly to critical code studies, and a book devoted to the explication of a single line of computer code, titled 10 PRINT CHR$(205.5+RND(1)); : GOTO 10 (Montfort et al., MIT Press).


== See also ==
Software studies


== References ==


== Bibliography ==
Black, M. J, (2002) The Art of Code. PhD dissertation, University of Pennsylvania.
Berry, D. M. (2011) The Philosophy of Software: Code and Mediation in the Digital Age, Basingstoke: Palgrave Macmillan. [1]
Berry, D. M. (2008) Copy, Rip, Burn: The Politics of Copyleft and Open Source, London: Pluto Press.
Chopra, S. and Dexter, S. (2008) Decoding Liberation: The Promise of Free and Open Source Software. Oxford: Routledge.
Chun, W. H. K. (2008) ‘On “Sourcery,” or Code as Fetish’, Configurations, 16:299–324.
Chun, W. H. K. (2011) Programmed Visions: Software and Memory, MIT Press.
Fuller, M. (2003) Behind the Blip: Essays on the Culture of Software. London: Autonomedia.
Fuller, M. (2008) Software Studies\A Lexicon. London: MIT Press.
Hayles, N. K. (2004) ‘Print Is Flat, Code Is Deep: The Importance of Media-Specific Analysis’, Poetics Today, 25(1): 67–90.
Heim, M. (1987) Electric Language: A Philosophical Discussion of Word Processing. London: Yale University Press.
Kirschenbaum, M. (2004) ‘Extreme Inscription: Towards a Grammatology of the Hard Drive’, TEXT Technology, No. 2, pp. 91–125.
Kirschenbaum, M. (2008) Mechanisms: New Media and the Forensic Imagination, MIT Press.
Kitchin, R. and Dodge, M. (2011) Code/Space: Software and Everyday Life, MIT Press.
Kittler, F. (1997). Literature, Media, Information Systems, Johnston, J. (ed.). Amsterdam: OPA.
Kittler, F. (1999) Gramophone, Film, Typewriter. Stanford: Stanford University Press.
Mackenzie, A. (2003) The problem of computer code: Leviathan or common power, retrieved 13/03/2010 from http://www.lancs.ac.uk/staff/mackenza/papers/code-leviathan.pdf
Mackenzie, A. (2006) Cutting Code: Software and Sociality, Oxford: Peter Lang.
Manovich, L. (2001) The Language of New Media. London: MIT Press.
Manovich, L. (2008) Software takes Command, retrieved 03/05/2010 from https://web.archive.org/web/20110127183751/http://lab.softwarestudies.com/2008/11/softbook.html
Manovich, L. and Douglas, J. (2009) Visualizing Temporal Patterns In Visual Media: Computer Graphics as a Research Method, retrieved 10/10/09 from http://softwarestudies.com/cultural_analytics/visualizing_temporal_patterns.pdf
Marino, M. C. (2006) Critical Code Studies, Electronic Book Review, accessed 16 Sept 2011, http://www.electronicbookreview.com/thread/electropoetics/codology
Montfort, N. and Bogost, I. (2009) Racing the Beam: The Atari Video Computer System, London: MIT Press.
Wardrip-Fruin, N. (2011) Expressive Processing. London: MIT Press."
260,Real RAM,20748371,5136,"In computing, especially computational geometry, a real RAM (random access machine) is a mathematical model of a computer that can compute with exact real numbers instead of the binary numbers used by most actual computers. The real RAM was formulated by Michael Ian Shamos in his 1978 Ph.D. dissertation.


== Model ==
The ""RAM"" part of the real RAM model name stands for ""random access machine"". This is a model of computing that resembles a simplified version of a standard computer architecture. It consists of a stored program, a computer memory unit consisting of an array of cells, and a central processing unit with a bounded number of registers. Each memory cell or register can store a real number. Under the control of the program, the real RAM can transfer real numbers between memory and registers, and perform arithmetic operations on the values stored in the registers.
The allowed operations typically include addition, subtraction, multiplication, and division, as well as comparisons, but not modulus or rounding to integers. The reason for avoiding integer rounding and modulus operations is that allowing these operations could give the real RAM unreasonable amounts of computational power, enabling it to solve PSPACE-complete problems in polynomial time.
When analyzing algorithms for the real RAM, each allowed operation is typically assumed to take constant time.


== Implementation ==
Software libraries such as LEDA have been developed which allow programmers to write computer programs that work as if they were running on a real RAM. These libraries represent real values using data structures which allow them to perform arithmetic and comparisons with the same results as a real RAM would produce. The time analysis of the underlying real RAM algorithm can be interpreted as counting the number of library calls needed by a given algorithm.


== Related models ==
The real RAM closely resembles the later Blum–Shub–Smale machine, which however lacks the memory unit that gives the real RAM the ""RAM"" part of its name. However, the real RAM is typically used for the analysis of concrete algorithms in computational geometry, while the Blum–Shub–Smale machine instead forms the basis for extensions of the theory of NP-completeness to real-number computation.
An alternative to the real RAM is the word RAM, in which both the inputs to a problem and the values stored in memory and registers are assumed to be integers with a fixed number of bits. The word RAM model can perform some operations more quickly than the real RAM; for instance, it allows fast integer sorting algorithms, while sorting on the real RAM must be done with slower comparison sorting algorithms. However, some computational geometry problems have inputs or outputs that cannot be represented exactly using integer coordinates; see for instance the Perles configuration, an arrangement of points and line segments that has no integer-coordinate representation.


== References ==


== External links ==
Feasible Real Random Access Machines References
Geometric Computing The Science of Making Geometric Algorithms Work"
261,"International Conference on Availability, Reliability and Security",18127613,5134,"The ARES - The International Conference on Availability, Reliability and Security focuses on rigorous and novel research in the field of dependability, computer and information security. In cooperation with the conference several workshops are held covering a huge variety of security topics. The Conference and Workshop Proceedings are published by IEEE Computer Society Press. In the CORE ranking, ARES is ranked as B. Participants from almost 40 countries attend ARES 2013.
The conference is hosted by universities and research institutions:
2006: Vienna University of Technology, Austria
2007: Vienna University of Technology, Austria, in co-operation with ENISA - The Network and Information Security Agency of the European Union
2008: Polytechnic University of Catalonia, Spain, in co-operation with ENISA
2009: Fukuoka Institute of Technology, Japan
2010: Krakowska Akademia, Poland
2011: Vienna University of Technology, Austria
2012: University of Economics, Prague, Czech Republic
2013: University of Regensburg, Germany


== 2013: University of Regensburg, Germany ==
In 2013 the keynotes of the ARES conference are held by
Elena Ferrari, University of Insubria, Italy
Carl Gunter, University of Illinois, USA
Furthermore, a panel about Threats & Risk Management - Bridging the Gap between Industry needs and Research takes place. The panelists are:
Gary McGraw, Cigital, US
Greg Soukiassian, BC & RS, France
Chris Wills, CARIS Research, UK
Tutorials are held by Gary McGraw, Haya Shulman, Ludwig Fuchs, Stefan Katzenbeisser


== 2012: University of Economics, Prague, Czech Republic ==
In 2012 the keynotes of the ARES conference are held by:
Annie Antón, Georgia Institute of Technology (US)
Chenxi Wang, Vice President, Principal Analyst at Forrester Research (US)
Further, a panel will be moderated by Shari Lawrence Pfleeger with the panelists:
Angela Sasse, University College London (UK)
David Budgen, Durham University (UK)
Kelly Caine, Indiana University (US)


== 2011: Vienna University of Technology, Austria ==
In 2011 the keynotes of the ARES conference will be held by:
Gary McGraw
Shari Pfleeger
Furthermore, Gene Spafford will give an invited talk.


== 2010: Krakowska Akademia, Poland ==
In 2010 the keynotes of the ARES conference were held by:
Gene Spafford (Purdue University)
Ross J. Anderson (Cambridge University)


== 2009: Fukuoka Institute of Technology, Japan ==
In 2009 the keynotes of the ARES conference were held by:
Prof. Elisa Bertino (Purdue University)
Sushil Jajodia (George Mason University Fairfax)
Eiji Okamoto (Tsukuba University)
Additionally an invited talk was held by:
Solange Ghernaouti (University of Lausanne)
The acceptance rate for ARES 2009 was 25% (= 40 full papers)


== 2008: Polytechnic University of Catalonia (UPC) Barcelona, Spain ==
In 2008 the keynotes of the ARES conference were held by:
Prof. Ravi Sandhu, Executive Director, Chief Scientist and Founder, Institute for Cyber Security (ICS) and Lutcher Brown Endowed Chair in Cyber-Security
Prof. Günther Pernul, Department of Information Systems, University of Regensburg
Prof. Vijay Atluri, Management Science and Information Systems Department, Research Director of the Center for Information Management, Integration and Connectivity (CIMIC), Rutgers University
The acceptance rate ARES 2008: 40 full papers of 190 submissions


== 2007 Vienna University of Technology, Austria ==
Since 2007 the ARES conference was held in conjunction with the CISIS conference. In 2007 the keynotes of the ARES conference were held by:
Prof. Reinhard Posch, Chief Information Officer for the Federal Republic of Austria
Prof. Bhavani Thuraisingham, Director of Cyber Security Research Center, University of Texas at Dallas (UTD)


== 2006: Vienna University of Technology, Austria ==
The first ARES conference in 2006 was held in conjunction with the AINA conference. In 2006 the keynotes of the ARES conference were held by:
Dr. Louis Marinos, ENISA Security Competence Department, Risk Management, Greece
Prof. Andrew Steane, Centre for Quantum Computation, University of Oxford, UK
Prof. David Basin, Information Security, Department of Computer Science, ETH Zurich, Switzerland


== External links ==
Current and past ARES Conferences
List of publications of the past ARES Conferences
Current and past CISIS Conferences
ENISA
AINA
DBLP"
262,Simple precedence parser,2012125,5112,"In computer science, a simple precedence parser is a type of bottom-up parser for context-free grammars that can be used only by simple precedence grammars.
The implementation of the parser is quite similar to the generic bottom-up parser. A stack is used to store a viable prefix of a sentential form from a rightmost derivation. Symbols 
  
    
      
        ⋖
      
    
    {\displaystyle \lessdot }
  , 
  
    
      
        
          
            
              =
              ˙
            
          
        
      
    
    {\displaystyle {\dot {=}}}
   and 
  
    
      
        ⋗
      
    
    {\displaystyle \gtrdot }
   are used to identify the pivot, and to know when to Shift or when to Reduce.


== Implementation ==
Compute the Wirth–Weber precedence relationship table.
Start with a stack with only the starting marker $.
Start with the string being parsed (Input) ended with an ending marker $.
While not (Stack equals to $S and Input equals to $) (S = Initial symbol of the grammar)
Search in the table the relationship between Top(stack) and NextToken(Input)
if the relationship is 
  
    
      
        
          
            
              =
              ˙
            
          
        
      
    
    {\displaystyle {\dot {=}}}
   or 
  
    
      
        ⋖
      
    
    {\displaystyle \lessdot }
  
Shift:
Push(Stack, relationship)
Push(Stack, NextToken(Input))
RemoveNextToken(Input)

if the relationship is 
  
    
      
        ⋗
      
    
    {\displaystyle \gtrdot }
  
Reduce:
SearchProductionToReduce(Stack)
RemovePivot(Stack)
Search in the table the relationship between the Non terminal from the production and first symbol in the stack (Starting from top)
Push(Stack, relationship)
Push(Stack, Non terminal)

SearchProductionToReduce (Stack)
search the Pivot in the stack the nearest 
  
    
      
        ⋖
      
    
    {\displaystyle \lessdot }
   from the top
search in the productions of the grammar which one have the same right side than the Pivot


== Example ==
Given the language:

E  --> E + T' | T'
T' --> T
T  --> T * F  | F
F  --> ( E' ) | num
E' --> E

num is a terminal, and the lexer parse any integer as num.
and the Parsing table:

STACK                   PRECEDENCE    INPUT            ACTION

$                            <        2 * ( 1 + 3 )$   SHIFT
$ < 2                        >        * ( 1 + 3 )$     REDUCE (F -> num)
$ < F                        >        * ( 1 + 3 )$     REDUCE (T -> F)
$ < T                        =        * ( 1 + 3 )$     SHIFT
$ < T = *                    <        ( 1 + 3 )$       SHIFT
$ < T = * < (                <        1 + 3 )$         SHIFT
$ < T = * < ( < 1            >        + 3 )$           REDUCE 4 times (F -> num) (T -> F) (T' -> T) (E ->T ') 
$ < T = * < ( < E            =        + 3 )$           SHIFT
$ < T = * < ( < E = +        <        3 )$             SHIFT
$ < T = * < ( < E = + < 3    >        )$               REDUCE 3 times (F -> num) (T -> F) (T' -> T) 
$ < T = * < ( < E = + = T    >        )$               REDUCE 2 times (E -> E + T) (E' -> E)
$ < T = * < ( < E'           =        )$               SHIFT
$ < T = * < ( = E' = )       >        $                REDUCE (F -> ( E' ))
$ < T = * = F                >        $                REDUCE (T -> T * F)
$ < T                        >        $                REDUCE 2 times (T' -> T) (E -> T')
$ < E                        >        $                ACCEPT


== References ==
Alfred V. Aho, Jeffrey D. Ullman (1977). Principles of Compiler Design. 1st Edition. Addison–Wesley.
William A. Barrett, John D. Couch (1979). Compiler construction: Theory and Practice. Science Research Associate.
Jean-Paul Tremblay, P. G. Sorenson (1985). The Theory and Practice of Compiler Writing. McGraw–Hill."
263,Federated Computing Research Conference,21541522,5107,"The Federated Computing Research Conference, FCRC, is an event that brings together several academic conferences, workshops, and plenary talks in the field of computer science. FCRC has been organised in 1993, 1996, 1999, 2003, 2007, 2011 and 2015. The 2015 event was in Portland, Oregon.
In the first FCRC, the main organiser was the Computing Research Association; since then, the Association for Computing Machinery has taken the lead in organising the event.
The Turing Award 1998, 2002, 2006, and 2010 recipients gave plenary talks in FCRC 1999, 2003, 2007, and 2011. Other plenary speakers in FCRC include László Babai, Charles Bennett, Randal Bryant, Bob Colwell, David Culler, Cynthia Dwork, Shafi Goldwasser, Michael J. Flynn, Hector Garcia-Molina, John L. Hennessy, Richard Karp, Randy Katz, Ken Kennedy, James Kurose, Ed Lazowska, Barbara Liskov, Robin Milner, Charles R. (Chuck) Moore, Christos Papadimitriou, Michael Rabin, Scott Shenker, Burton Smith, Guy L. Steele, Jr., Avi Wigderson, Maurice Wilkes, William A. Wulf.


== Locations ==
1993: San Diego, California, USA
1996: Philadelphia, Pennsylvania, USA
1999: Atlanta, Georgia, USA
2003: San Diego, California, USA
2007: San Diego, California, USA
2011: San Jose, California, USA
2015: Portland, Oregon, USA


== Conferences ==
The following table contains conferences that have been part of FCRC at least twice; workshops have not been listed.
Other notable events held in conjunction with FCRC include HOPL III, the History of Programming Languages Conference in 2007.


== References and Notes ==

FCRC 1993 program (a PostScript file, reverse page order).
Information about FCRC 1993 can be found also in the following posts in Usenet news (links to Google Groups):
PPoPP 1993 program.
PPoPP 1993 CFP.
SoCG 1993 CFP.
WOPA 1993 program.

FCRC 1996 web site.
FCRC 1999 web site.
FCRC 2003 web site.
FCRC 2007 web site.
FCRC 2011 web site.
FCRC 1999 on CRA web site.
Erik Demaine's List of Events: FCRC."
264,Szymański's algorithm,29398104,5081,"Szymanski's Mutual Exclusion Algorithm is a mutual exclusion algorithm devised by computer scientist Dr. Boleslaw Szymanski, which has many favorable properties including linear wait, and which extension solved the open problem posted by Leslie Lamport whether there is an algorithm with a constant number of communication bits per process that satisfies every reasonable fairness and failure-tolerance requirement that Lamport conceived of (Lamport's solution used n factorial communication variables vs. Szymanski's 5).


== The algorithm ==
The algorithm is modeled on a waiting room with an entry and exit doorway. Initially the entry door is open and the exit door is closed. All processes which request entry into the critical section at roughly the same time enter the waiting room; the last of them closes the entry door and opens the exit door. The processes then enter the critical section one by one (or in larger groups if the critical section permits this). The last process to leave the critical section closes the exit door and reopens the entry door, so the next batch of processes may enter.
The implementation consists of each process having a flag variable which is written by that process and read by all others (this single-writer property is desirable for efficient cache usage). The status of the entry door is computed by reading the flags of all processes. Pseudo-code is given below:

Note that the order of the ""all"" and ""any"" tests must be uniform.
Despite the intuitive explanation, the algorithm was not easy to prove correct, however due to its favorable properties a proof of correctness was desirable and multiple proofs have been presented.


== References ==


== See also ==
Dekker's algorithm
Eisenberg & McGuire algorithm
Peterson's algorithm
Lamport's bakery algorithm
Semaphores"
265,Iterative refinement,22203945,5074,"Iterative refinement is an iterative method proposed by James H. Wilkinson to improve the accuracy of numerical solutions to systems of linear equations.
When solving a linear system Ax = b, due to the presence of rounding errors, the computed solution x̂ may sometimes deviate from the exact solution x*. Starting with x1 = x̂, iterative refinement computes a sequence {x1,x2,x3,…} which converges to x* when certain assumptions are met.


== Description ==
For m = 1,2,…, the 
  
    
      
        m
      
    
    {\displaystyle m}
  th iteration of iterative refinement consists of three steps:
Compute the residualrm = b − Axm
Solve the systemAdm = rm
Add the correctionxm+1 = xm + dm


== Error analysis ==
As a rule of thumb, iterative refinement for Gaussian elimination produces a solution correct to working precision if double the working precision is used in the computation of r, e.g. by using quad or double extended precision IEEE 754 floating point, and if A is not too ill-conditioned (and the iteration and the rate of convergence are determined by the condition number of A).
More formally, assuming that each solve step is reasonably accurate, i.e., in mathematical terms, for every m, we have
A(I + Fm)dm = rm
where ‖Fm‖∞ < 1, the relative error in the 
  
    
      
        m
      
    
    {\displaystyle m}
  th iterate of iterative refinement satisfies

  
    
      
        
          
            
              ‖
              
                
                  x
                
                
                  m
                
              
              −
              
                
                  x
                
                
                  ∗
                
              
              
                ‖
                
                  ∞
                
              
            
            
              ‖
              
                
                  x
                
                
                  ∗
                
              
              
                ‖
                
                  ∞
                
              
            
          
        
        ≤
        
          
            (
          
        
        σ
        
          κ
          
            ∞
          
        
        (
        
          A
        
        )
        
          ε
          
            1
          
        
        
          
            
              )
            
          
          
            m
          
        
        +
        
          μ
          
            1
          
        
        
          ε
          
            1
          
        
        +
        
          μ
          
            2
          
        
        n
        
          κ
          
            ∞
          
        
        (
        
          A
        
        )
        
          ε
          
            2
          
        
      
    
    {\displaystyle {\frac {\lVert {\boldsymbol {x}}_{m}-{\boldsymbol {x}}^{\ast }\rVert _{\infty }}{\lVert {\boldsymbol {x}}^{\ast }\rVert _{\infty }}}\leq {\bigl (}\sigma \kappa _{\infty }({\boldsymbol {A}})\varepsilon _{1}{\bigr )}^{m}+\mu _{1}\varepsilon _{1}+\mu _{2}n\kappa _{\infty }({\boldsymbol {A}})\varepsilon _{2}}
  
where
‖·‖∞ denotes the ∞-norm of a vector,
κ∞(A) is the ∞-condition number of A,

  
    
      
        n
      
    
    {\displaystyle n}
   is the order of A,
ε1 and ε2 are unit round-offs of floating-point arithmetic operations,
σ, μ1 and μ2 are constants depending on A, ε1 and ε2
if A is “not too badly conditioned”, which in this context means
0 < σκ∞(A)ε1 ≪ 1
and implies that μ1 and μ2 are of order unity.
The distinction of ε1 and ε2 is intended to allow mixed-precision evaluation of rm where intermediate results are computed with unit round-off ε2 before the final result is rounded (or truncated) with unit round-off ε1. All other computations are assumed to be carried out with unit round-off ε1.


== Notes ==


== References ==
Wilkinson, James H. (1963). Rounding Errors in Algebraic Processes. Englewood Cliffs, NJ: Prentice Hall. 
Moler, Cleve B. (April 1967). ""Iterative Refinement in Floating Point"". Journal of the ACM. New York, NY: Association for Computing Machinery. 14 (2): 316–321. doi:10.1145/321386.321394."
266,Afzal Upal,43029598,5052,"Muhammad Afzal Upal is a writer and a cognitive scientist with contributions to cognitive science of religion, machine learning for planning, and agent-based social simulation. His short stories have been published in various magazines including Chowk.com and he has published a book of Urdu poetry titled Kalam-e-Seemab.


== Early life and education ==
Upal went to Talim-ul-Islam School Rabwah and Federal Boys Secondary School Number 1 in Islamabad. He completed his BSc in double Mathematics & Physics from the University of Punjab in 1989 with a second position in Talim-ul-Islam College Rabwah. In 1993, he completed his BSc in Computer Science (with distinction) from the University of Saskatchewan in Saskatoon, Canada. For his masters thesis research, he worked under the supervision of Eric Neufeld. In 1995, he successfully defended his thesis on Monte Carlo Comparison of Non-Hierarchical Unsupervised Classifiers. For his PhD research, he worked under the supervision of Professor Renee Elio at the University of Alberta. In December 1999, he successfully defended his thesis on Learning to Improve the Quality of Plans Produced by Partial-order Planners.


== Leadership ==
He was chair of the First International Workshop on Cognition and Culture, the 14th Annual Conference of the North American Association for Computational, Social, and Organizational Sciences, the AAAI-06 Workshop on Cognitive Modeling and Agent-based Social Simulation,


== Professional career ==
In July 1999, Upal was hired as a tenure-track assistant professor of computer science at Dalhousie University's new Faculty of Computer Science. In 2001, he moved to Information Extraction & Transport (IET) Inc. to work as a senior scientist on various DARPA sponsored projects to develop Bayesian network based decision-aid systems. In July 2003, he joined the University of Toledo's Electrical Engineering & Computer Science Department as a tenure track assistant professor to teach computer science. He is currently the Chair of the Computing & Information Science (CIS) Department at Mercyhurst University in Erie, PA.


== Scientific contributions ==
He has contributed to research areas of Cognition & Culture and Cognitive science of religion through the development of the Context-based model of minimal counterintuiveness. In a 2005 article in the Journal of Cognition and Culture, he proposed a cognitive science of new religious movements. Upal has also pioneered a knowledge-rich agent-based social simulation technique for simulating the development of complex cultural beliefs.


== References =="
267,Sean Kandel,55519034,5030,"Sean Kandel is Trifacta's Chief Technical Officer and Co-founder, along with Joseph M. Hellerstein and Jeffrey Heer. He is known for the development of new tools for data transformation and discovery and is the co-developed of Data Wrangler, an interactive tool for data cleaning and transformation. He previously worked as a data analyst at Citadel Investment Group.


== Education and Research ==
Kandel graduated from Stanford University in 2013 with a Ph.D. in Computer Science. As a Ph.D. student in the Visualization Group at Stanford, he designed and built interactive tools for data analysis, management, and visualization.
He received a Ph.D. from Stanford University in 2013 for his thesis on Interactive systems for data transformation and assessment under primary advisors Jeffrey Heer. While at Stanford, he published multiple research papers and articles with Trifacta co-founders Jeffrey Heer and Joseph Hellerstein on topics of big data analysis, data quality assessment, and visualization for data transformation, as well as other big data research. Kandel’s major research contribution to date has been as co-developer of Data Wrangler, a research initiative between Stanford and the University of California, Berkeley. The project resulted in the company Trifacta eventually selling Data Wrangler as a commercialized product.


== Awards and Recognition ==
In 2017, Kandel was a recipient of an award for Silicon Valley’s young business leaders who are impacting their industries and their communities, Silicon Valley’s 40 Under 40 list. Import.io also included Sean in their list of 40 Data Mavericks under 40 list.
He frequently presents at a variety of big data conferences including Strata World and Hadoop Users Group UK on topics including data lineage, data transformation, machine learning and semi-structured data, big data project success, and other industry subject areas.


== References =="
268,RuleML,1568414,5022,"RuleML is a global initiative, led by a non-profit organization RuleML Inc., that is devoted to advancing research and industry standards design activities in the technical area of rules that are semantic and highly inter-operable. The standards design takes the form primarily of a markup language, also known as RuleML. The research activities include an annual research conference, the RuleML Symposium, also known as RuleML for short. Founded in fall 2000 by Harold Boley, Benjamin Grosof, and Said Tabet, RuleML was originally devoted purely to standards design, but then quickly branched out into the related activities of coordinating research and organizing an annual research conference starting in 2002. The M in RuleML is sometimes interpreted as standing for Markup and Modeling. The markup language was developed to express both forward (bottom-up) and backward (top-down) rules in XML for deduction, rewriting, and further inferential-transformational tasks. It is defined by the Rule Markup Initiative, an open network of individuals and groups from both industry and academia that was formed to develop a canonical Web language for rules using XML markup and transformations from and to other rule standards/systems.
Markup standards and initiatives related to RuleML include:
Rule Interchange Format (RIF): The design and overall purpose of W3C's Rule Interchange Format (RIF) industry standard is based primarily on the RuleML industry standards design. Like RuleML, RIF embraces a multiplicity of potentially useful rule dialects that nevertheless share common characteristics.
RuleML Technical Committee from Oasis-Open: An industry standards effort devoted to legal automation utilizing RuleML.
Semantic Web Rule Language (SWRL): An industry standards design, based primarily on an early version of RuleML, whose development was funded in part by the DARPA Agent Markup Language (DAML) research program.
Semantic Web Services Framework], particularly its Semantic Web Services Language: An industry standards design, based primarily on a medium-mature version of RuleML, whose development was funded in part by the DARPA Agent Markup Language (DAML) research program and the WSMO research effort of the EU.
Mathematical Markup Language (MathML): However, MathML's Content Markup is better suited for defining functions rather than relations or general rules
Predictive Model Markup Language (PMML): With this XML-based language one can define and share various models for data-mining results, including association rules
Attribute Grammars in XML (AG-markup): For AG's semantic rules, there are various possible XML markups that are similar to Horn-rule markup
Extensible Stylesheet Language Transformations (XSLT): This is a restricted term-rewriting system of rules, written in XML, for transforming XML documents into other text documents


== See also ==
RuleML Symposium
Ontology (computer science)
Business rules
Business rules approach
Semantic Web Rule Language
R2ML
Flora-2


== References ==


== External links ==
Official website
AG-markup
Rules Portal"
269,Testbed,1035597,4939,"A testbed (also ""test bed"") is a platform for conducting rigorous, transparent, and replicable testing of scientific theories, computational tools, and new technologies.
The term is used across many disciplines to describe experimental research and new product development platforms and environments. They may vary from hands-on prototype development in manufacturing industries such as automobiles (known as ""mules""), aircraft engines or systems and to intellectual property refinement in such fields as computer software development shielded from the hazards of testing live.


== Software development ==
In software development testbedding is a method of testing a particular module (function, class, or library) in an isolated fashion. It may be used as a proof of concept or when a new module is tested apart from the program/system it will later be added to. A skeleton framework is implemented around the module so that the module behaves as if already part of the larger program.
A typical testbed could include software, hardware, and networking components. In software development, the specified hardware and software environment can be set up as a testbed for the application under test. In this context, a testbed is also known as the test environment.
Testbeds are also pages on the Internet where the public are given the opportunity to test CSS or HTML they have created and want to preview the results, for example:
The Arena web browser was created by the World Wide Web Consortium (W3C) and CERN for testing HTML3, Cascading Style Sheets (CSS), Portable Network Graphics (PNG) and the libwww. Arena was replaced by Amaya to test new web standards.
The Line Mode browser got a new function to interact with the libwww library as a sample and test application.
The libwww was also created to test network protocols which are under development or to experiment with new protocols.


== Aircraft development ==

In aircraft development there are also examples of testbed use like in development of new aircraft engines when these are fitted to a testbed aircraft for flight testing.


== References ==


== See also ==

Sandbox (software development)


== External links ==
PlanetLab Europe, the European portion of the publicly available PlanetLab testbed
CMU's eRulemaking Testbed
US National Science Foundation GENI - Global Environment for Network Innovations Initiative
Helsinki Testbed (meteorology)
Collaborative Adaptive Sensing of the Atmosphere (CASA) IP1 test bed"
270,Bremermann's limit,1824836,4924,"Bremermann's limit, named after Hans-Joachim Bremermann, is the maximum computational speed of a self-contained system in the material universe. It is derived from Einstein's mass-energy equivalency and the Heisenberg uncertainty principle, and is c2/h ≈ 1.36 × 1050 bits per second per kilogram. This value is important when designing cryptographic algorithms, as it can be used to determine the minimum size of encryption keys or hash values required to create an algorithm that could never be cracked by a brute-force search.
For example, a computer with the mass of the entire Earth operating at the Bremermann's limit could perform approximately 1075 mathematical computations per second. If one assumes that a cryptographic key can be tested with only one operation, then a typical 128-bit key could be cracked in under 10−36 seconds. However, a 256-bit key (which is already in use in some systems) would take about two minutes to crack. Using a 512-bit key would increase the cracking time to approaching 1072 years, without increasing the time for encryption by more than a constant factor (depending on the encryption algorithms used).
The limit has been further analysed in later literature as the maximum rate at which a system with energy spread 
  
    
      
        Δ
        E
      
    
    {\displaystyle \Delta E}
   can evolve into an orthogonal and hence distinguishable state to another, 
  
    
      
        Δ
        t
        =
        π
        ℏ
        
          /
        
        2
        Δ
        E
      
    
    {\displaystyle \Delta t=\pi \hbar /2\Delta E}
  . In particular, Margolus and Levitin has shown that a quantum system with average energy E takes at least time 
  
    
      
        Δ
        t
        =
        π
        ℏ
        
          /
        
        2
        E
      
    
    {\displaystyle \Delta t=\pi \hbar /2E}
   to evolve into an orthogonal state. However, it has been shown that access to quantum memory enables computational algorithms that require arbitrarily small amount of energy/time per one elementary computation step.


== See also ==
Margolus–Levitin theorem
Landauer's principle
Bekenstein bound
Kolmogorov complexity
Transcomputational problem
Limits to computation
Ultrafinitism


== References ==


== External links ==
Gorelik, G. (2003). Bremermann's Limit and cGh-physics
Lokshin, A (2017). Arbitrary choice, ‘understanding’ and Gorelik-Bremermann limit. Far East Journal of Mathematical Sciences, V. 102, Issue 1, P. 215–222"
271,Gelato Federation,10727643,4912,"The Gelato Federation (usually just Gelato) was a ""global technical community dedicated to advancing Linux on the Intel Itanium platform through collaboration, education, and leadership."" Formed in 2001, membership included more than seventy academic and research organizations around the world, including several that operated Itanium-based supercomputers on the Top500 list. The organization was active in projects to enhance the Linux kernel for Itanium and GCC for Itanium. The organization took its name from the Italian dessert gelato, paying homage to this by naming sub-projects Gelato Vanilla and Gelato Coconut for varieties of the dessert.


== History ==

In late 2001, representatives from seven organizations met with Hewlett-Packard. The institutions were the Bioinformatics Institute, Singapore; Groupe ESIEE, France; Hewlett-Packard Company; National Center for Supercomputing Applications, USA; Tsinghua University, China; University of Illinois at Urbana-Champaign, USA; University of New South Wales, Australia; and University of Waterloo, Canada. These were the founding members of Gelato.
Representatives from these organizations met twice a year. The first few meetings (in Palo Alto, California 2001 and Paris 2002) were primarily a ""strategy council meeting"" where the by-laws and charter were hammered out.
The Sydney meeting in October 2002 was the first that included a day of technical presentations. These became a regular feature of the meetings, eventually expanded to conferences, and thus the two conferences each year were entirely composed of technical presentations by vendors and members.
The organization apparently ceased operation in 2009.


== Membership ==

The federation grew markedly after its inception. By April 2007, there were more than 70 members and sponsors around the world. Members were institutions, but there were a few individuals who, because of their contribution to IA-64 on Linux or to Gelato, were made Honorary Members. These included Clemens C. J. Roothaan (who contributed to the Itanium math libraries and floating point unit), Brian Lynn (the original HP representative), David Mosberger-Tang (original porter of Linux to IA-64) and Jean-Pol Taffin (ex-general secretary of ESIEE, and very influential in the early days of Gelato).
Institutional members were sponsored by an IA-64 vendor, or came in on their own. Sponsored members typically had specific projects in mind.


== Conferences ==

The Gelato ICE: Itanium Conference & Expo alternated between San Jose, California and somewhere else in the world, often in Southeast Asia or Europe. Gelato conferences were where most of the collaboration and cooperation between members were established, and where Intel revealed some of their future strategy for the Itanium-based platform. The last conference was held in Singapore in October 2007.


== Other activities ==

Apart from the Members' activities, Gelato funded a Central Operations (hosted at the University of Illinois at Urbana-Champaign). Central Operations, in addition to running the twice-a-year meetings, tried to coordinate and manage a number of projects. These included:
Gelato GCC on Itanium Workgroup, a group of members and sponsors of the Gelato Federation and the GCC community interested in improving GCC (the GNU Compiler Collection) on Itanium processors.
Vanilla, a concerted effort to port and tune software for Itanium. In addition to the actual tuned binaries, the tuning process was documented.
Coconut, a system of access to Itanium machines for members.
The Gelato System Grant program, which provided Itanium systems for members.


== Sponsors ==
Gelato was funded by HP, Intel, BP, Itanium Solutions Alliance, and SGI. Gelato Central Operations was housed at the Coordinated Science Lab at the University of Illinois.


== References =="
272,Schema evolution,17364021,4890,"In computer science, schema versioning, and its weaker companion, schema evolution, deal with the need to retain current data and software system functionality in the face of changing database structure. The problem is not limited to the modification of the schema. It, in fact, affects the data stored under the given schema and the queries (and thus the applications) posed on that schema.
In practice the design of a database is expected to be created as a ""one size fits all"" schema capable of accepting all data required by a system, and thus schema evolution is often not considered. This assumption, almost unrealistic in the context of traditional information systems, becomes unacceptable in the context of systems that retain large volumes of historical information or those such as Web Information Systems, that due to the distributed and cooperative nature of their development, are subject of an even stronger pressure toward change (from 39% to over 500% more intense than in traditional settings). Due to this historical heritage the process of schema evolution is nowadays a particularly taxing one. It is, in fact, widely acknowledged that the data management core of an applications is one of the most difficult and critical components to evolve. The key problem is the impact of the schema evolution on queries and applications. As shown in  (which provides an analysis of the MediaWiki evolution) each evolution step might affect up to 70% of the queries operating on the schema, that must be manually reworked consequently.
The problem has been recognized as a pressing one by the database community for more than 12 years. The support for Schema Evolution, is a difficult problem involving complex mapping among schema versions, the tool support has been so far very limited. The recent theoretical advances on mapping composition and mapping invertibility, which represent the core problems underlying the schema evolution remains almost inaccessible to the large public. The issue is particular felt by temporal databases. Researchers in data management community are trying to automate the process of handling schema evolution as efforts towards better data governance and maintenance in big data processing platforms.


== Related works ==
A rich bibliography on Schema Evolution is collected at: http://se-pubs.dbs.uni-leipzig.de/pubs/results/taxonomy%3A100
UCLA university carried out an analysis of the MediaWiki Schema Evolution: Schema Evolution Benchmark
PRISM, a tool to support graceful relational schema evolution: Prism: schema evolution tool
PRIMA, a tool supporting transaction time databases under schema evolution PRIMA: supporting transaction-time DB under schema evolution
Pario is a software development tool that includes fully automated schema evolution
SemLinker is big data integration system that facilitates automatic handling of schema evolution in big data lakes.


== References =="
273,Pachinko allocation,28082011,4839,"In machine learning and natural language processing, the pachinko allocation model (PAM) is a topic model. Topic models are a suite of algorithms to uncover the hidden thematic structure of a collection of documents.  The algorithm improves upon earlier topic models such as latent Dirichlet allocation (LDA) by modeling correlations between topics in addition to the word correlations which constitute topics. PAM provides more flexibility and greater expressive power than latent Dirichlet allocation. While first described and implemented in the context of natural language processing, the algorithm may have applications in other fields such as bioinformatics. The model is named for pachinko machines—a game popular in Japan, in which metal balls bounce down around a complex collection of pins until they land in various bins at the bottom.


== History ==
Pachinko allocation was first described by Wei Li and Andrew McCallum in 2006. The idea was extended with hierarchical Pachinko allocation by Li, McCallum, and David Mimno in 2007. In 2007, McCallum and his colleagues proposed a nonparametric Bayesian prior for PAM based on a variant of the hierarchical Dirichlet process (HDP). The algorithm has been implemented in the MALLET software package published by McCallum's group at the University of Massachusetts Amherst.


== Model ==

PAM connects words in V and topics in T with an arbitrary Directed Acyclic Graph (DAG), where topic nodes occupy the interior levels and the leaves are words.
The probability of generating a whole corpus is the product of the probability for every document:

  
    
      
        P
        (
        
          D
        
        
          |
        
        α
        )
        =
        
          ∏
          
            d
          
        
        P
        (
        d
        
          |
        
        α
        )
      
    
    {\displaystyle P(\mathbf {D} |\alpha )=\prod _{d}P(d|\alpha )}
  


== See also ==
Probabilistic latent semantic indexing (PLSI), an early topic model from Thomas Hofmann in 1999.
Latent Dirichlet allocation, a generalization of PLSI developed by David Blei, Andrew Ng, and Michael Jordan in 2002, allowing documents to have a mixture of topics.
MALLET, an open-source Java library that implements Pachinko allocation.


== References ==


== External links ==
Mixtures of Hierarchical Topics with Pachinko Allocation, a video recording of David Mimno presenting HPAM in 2007."
274,Computational semiotics,505237,4824,"Computational semiotics is an interdisciplinary field that applies, conducts, and draws on research in logic, mathematics, the theory and practice of computation, formal and natural language studies, the cognitive sciences generally, and semiotics proper. A common theme of this work is the adoption of a sign-theoretic perspective on issues of artificial intelligence and knowledge representation. Many of its applications lie in the field of human-computer interaction (HCI) and fundamental devices of recognition.
One part of this field, known as algebraic semiotics, combines aspects of algebraic specification and social semiotics, and has been applied to user interface design and to the representation of mathematical proofs.


== See also ==


== Bibliography ==
Andersen, P.B. (1991). A Theory of Computer Semiotics, Cambridge University Press.
de Souza, C.S., The Semiotic Engineering of Human-Computer Interaction, MIT Press, Cambridge, MA, 2005.
Tanaka-Ishii, K. (2010), ""Semiotics of Programming"", Cambridge University Press.
Hugo, J. (2005), ""The Semiotics of Control Room Situation Awareness"", Fourth International Cyberspace Conference on Ergonomics, Virtual Conference, 15 Sep – 15 Oct 2005. Eprint
Gudwin, R.; Queiroz J. (eds) - Semiotics and Intelligent Systems Development - Idea Group Publishing, Hershey PA, USA (2006), ISBN 1-59904-063-8 (hardcover), 1-59904-064-6 (softcover), 1-59904-065-4 (e-book), 352 ps. Link to publisher
Gudwin, R.; Queiroz, J. - Towards an Introduction to Computational Semiotics - Proceedings of the 2005 IEEE International Conference on Integration of Knowledge Intensive Multi-Agent Systems - KIMAS'05, 18–21 April 2005, Waltham, MA, USA, pp. 393–398.IEEExplore
Mili, A., Desharnais, J., Mili, F., with Frappier, M., Computer Program Construction, Oxford University Press, New York, NY, 1994. — Introduction to Tarskian relation theory and its applications within the relational programming paradigm.
Rieger, Burghard B.: Computing Granular Word Meanings. A fuzzy linguistic approach to Computational Semiotics, in: Wang, Paul P. (ed.): Computing with Words. [Wiley Series on Intelligent Systems 3], New York (John Wiley & Sons) 2001, pp. 147–208.
Rieger, Burghard B.: Computing Fuzzy Semantic Granules from Natural Language Texts. A computational semiotics approach to understanding word meanings, in: Hamza, M.H. (ed.): Artificial Intelligence and Soft Computing, Proceedings of the IASTED International Conference, Anaheim/ Calgary/ Zürich (IASTED/ Acta Press) 1999, pp. 475–479.
Rieger, Burghard B.: A Systems Theoretical View on Computational Semiotics. Modeling text understanding as meaning constitution by SCIPS, in: Proceedings of the Joint IEEE Conference on the Science and Technology of Intelligent Systems (ISIC/CIRA/ISAS-98), Piscataway, NJ (IEEE/Omnipress) 1998, pp. 840–845. IEEExplore


== External links ==
Goguen, J., Algebraic Semiotics
Gudwin, R.R., Computational Semiotics
Gudwin, R.R., List of Publications in Computational Semiotics and other fields
International Computational Semiotics Group
UNICAMP Computational Semiotics Group"
275,Technology transfer in computer science,44409131,4808,"Technology transfer in computer science refers to the transfer of technology developed in computer science or applied computing research, from universities and governments to the private sector. These technologies may be abstract, such as algorithms and data structures, or concrete, such as open source software packages.


== Examples ==

Notable examples of technology transfer in computer science include:


== References =="
276,Fanya Montalvo,33583579,4794,"Fanya S. Montalvo (born in Monterrey, Mexico) Received the Ph.D. in Computer and Information Science  at the University of Massachusetts Amherst in 1976. Her dissertation was entitled Aftereffects, Adaptation, and Plasticity: A Neural Model for Tunable Feature Space. She was advised by Michael Anthony Arbib. Montalvo has been a research scientist at Lawrence Berkeley Labs, HP, MIT, and Digital Equipment Corporation.
Montalvo is a leader in the field of Inconsistency Robustness currently serving on the governing Board of the International Society for Inconsistency Robustness. According to Rosalind Picard, she is involved in considerations within emotional computing.[see: Affective Computing ] She is known for having coined the term ""AI-complete"" to denote an Artificial Intelligence task that is equivalent in difficulty to that of solving the problem of Strong AI.


== Publications ==
Fanya S. Montalvo. pdf. shown at 6th in list (1st retrieval address) ""Diagram understanding:The Intersection of computer vision and graphics"" MIT A.I. Lab Memo 873. November 1983. (retrieved 16:55(GMT)30.10.2011)
Fanya S. Montalvo. ""Consensus versus Competition in Neural Networks: A Comparative Analysis of Three Models"" International Journal of Man-Machine Studies 7(3). 1975.
Fanya S. Montalvo. Diagram Understanding: Associating Symbolic Descriptions with Images IEEE Workshop on Visual Languages. 1986 (retrieved 18:01(GMT)30.10.2011)
Fanya S. Montalvo and Caxton C. Foster. ""An Algorithm for Intercell Communication in a Tesselated Automaton"" published by:IEEE Computer Society (retrieved 18:06(GMT) 30.10.2011)
Fanya S. Montalvo. ""Consensus versus Competition in Neural Networks: A Comparative Analysis of Three Models"" International Journal of Human-Computer Studies, vol. 7, no. 3, 1975 (retrieved 18:09(GMT)30.10.2011)
Fanya S. Montalvo. ""Knowledge visualization: A new framework for interactive graphic interface design"" Applied Intelligence:Volume 1, Number 4, 297-309, doi:10.1007/BF00122019 (retrieved 18:27(GMT) 30.10.2011)
Robert E. Filman, John Lamping, Fanya S. Montalvo. ""Meta-Knowledge and Meta-Reasoning"" IJCAI-83.
Fanya S. Montalvo and Naomi Weisstein. ""An Empirical Method that provides the basis for an organization of relaxation labeling process for vision"" retrieved 18:30(GMT)30.10.2011(shows entire report [3 pages])[University of California]
Fanya S. Montalvo. ""Human Vision Paradox Implicates Relaxation Model"" IJCAI-77
Fanya S. Montalvo. ""The Singularity is Here"" Inconsistency Robustness, Vol. 52 Studies in Logic, College Publications (2015)


== See also ==
AI-complete


== References ==

UMASSCS retrieved 18:04(GMT) 30.10.2011


== External links ==
2011 ACM, Inc. 17:53(UTC) 30.10.2011 (list of publications)"
277,Ε-net (computational geometry),28807941,4747,"An ε-net (pronounced epsilon-net) is any of several related concepts in mathematics, and in particular in computational geometry, where it relates to the approximation of a general set by a collection of simpler subsets. It has a particular meaning in probability theory where it is used to describe the approximation of one probability distribution by another.


== Background ==

Let X be a set and R be a set of subsets of X; such a pair is called a range space or hypergraph, and the elements of R are called ranges or hyperedges. An ε-net of a subset P of X is a subset N of P such that any range r ∈ R with |r ∩ P| ≥ ε|P| intersects N. In other words, any range that intersects at least a proportion ε of the elements of P must also intersect the ε-net N.
For example, suppose X is the set of points in the two-dimensional plane, R is the set of closed filled rectangles (products of closed intervals), and P is the unit square [0, 1] × [0, 1]. Then the set N consisting of the 8 points shown in the adjacent diagram is a 1/4-net of P, because any closed filled rectangle intersecting at least 1/4 of the unit square must intersect one of these points. In fact, any (axis-parallel) square, regardless of size, will have a similar 8-point 1/4-net.
For any range space with finite VC dimension d, regardless of the choice of P, there exists an ε-net of P of size

  
    
      
        O
        
          (
          
            
              
                d
                ε
              
            
            log
            ⁡
            
              
                d
                ε
              
            
          
          )
        
        ;
      
    
    {\displaystyle O\left({\frac {d}{\varepsilon }}\log {\frac {d}{\varepsilon }}\right);}
  
because the size of this set is independent of P, any set P can be described using a set of fixed size.
This facilitates the development of efficient approximation algorithms. For example, suppose we wish to estimate an upper bound on the area of a given region, that falls inside a particular rectangle P. One can estimate this to within an additive factor of ε times the area of P by first finding an ε-net of P, counting the proportion of elements in the ε-net falling inside the region with respect to the rectangle P, and then multiplying by the area of P. The runtime of the algorithm depends only on ε and not P. One straightforward way to compute an ε-net with high probability is to take a sufficient number of random points, where the number of random points also depends only on ε. For example, in the diagram shown, any rectangle in the unit square containing at most three points in the 1/4-net has an area of at most 3/8 + 1/4 = 5/8.
ε-nets also provide approximation algorithms for the NP-complete hitting set and set cover problems.


== Probability theory ==
Let 
  
    
      
        P
      
    
    {\displaystyle P}
   be a probability distribution over some set 
  
    
      
        X
      
    
    {\displaystyle X}
  . An 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
  -net for a class 
  
    
      
        H
        ⊆
        
          2
          
            X
          
        
      
    
    {\displaystyle H\subseteq 2^{X}}
   of subsets of 
  
    
      
        X
      
    
    {\displaystyle X}
   is any subset 
  
    
      
        S
        ⊆
        X
      
    
    {\displaystyle S\subseteq X}
   such that for any 
  
    
      
        h
        ∈
        H
      
    
    {\displaystyle h\in H}
  

  
    
      
        P
        (
        h
        )
        ≥
        ε
        
        ⟹
        
        S
        ∩
        h
        ≠
        ∅
        .
      
    
    {\displaystyle P(h)\geq \varepsilon \quad \Longrightarrow \quad S\cap h\neq \varnothing .}
  
Intuitively 
  
    
      
        S
      
    
    {\displaystyle S}
   approximates the probability distribution.
A stronger notion is 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
  -approximation. An 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
  -approximation for class 
  
    
      
        H
      
    
    {\displaystyle H}
   is a subset 
  
    
      
        S
        ⊆
        X
      
    
    {\displaystyle S\subseteq X}
   such that for any 
  
    
      
        h
        ∈
        H
      
    
    {\displaystyle h\in H}
   it holds

  
    
      
        
          |
          
            P
            (
            h
            )
            −
            
              
                
                  
                    |
                  
                  S
                  ∩
                  h
                  
                    |
                  
                
                
                  
                    |
                  
                  S
                  
                    |
                  
                
              
            
          
          |
        
        <
        ε
        .
      
    
    {\displaystyle \left|P(h)-{\frac {|S\cap h|}{|S|}}\right|<\varepsilon .}
  


== References =="
278,Random indexing,37697003,4729,"Random indexing is a dimensionality reduction method and computational framework for Distributional semantics, based on the insight that very-high-dimensional Vector Space Model implementations are impractical, that models need not grow in dimensionality when new items (e.g. new terminology) is encountered, and that a high-dimensional model can be projected into a space of lower dimensionality without compromising L2 distance metrics if the resulting dimensions are chosen appropriately.
This is the original point of the random projection approach to dimension reduction first formulated as the Johnson–Lindenstrauss lemma, and Locality-sensitive hashing has some of the same starting points. Random indexing, as used in representation of language, originates from the work of Pentti Kanerva on Sparse distributed memory, and can be described as an incremental formulation of a random projection.
It can be also verified that random indexing is a random projection technique for the construction of Euclidean spaces---i.e. L2 normed vector spaces. In Euclidean spaces, random projections are elucidated using the Johnson–Lindenstrauss lemma.
TopSig  extends the Random Indexing model to produce bit vectors for comparison with the Hamming distance similarity function. It is used for improving the performance of information retrieval and document clustering. In a similar line of research, Random Manhattan Integer Indexing is proposed for improving the performance of the methods that employ the Manhattan distance between text units. Many random indexing methods primarily generate similarity from co-occurrence of items in a corpus. Reflexive Random Indexing  generates similarity from co-occurrence and from shared occurrence with other items.


== Weblinks ==
Zadeh Behrang Qasemi, Handschuh Siegfried. (2015) Random indexing explained with high probability, TSD.


== References =="
279,J. H. Wilkinson Prize for Numerical Software,13532851,4709,"The J. H. Wilkinson Prize for Numerical Software is awarded every four years to honor outstanding contributions in the field of numerical software. The award is named to commemorate the outstanding contributions of James H. Wilkinson in the same field. 
The award, consisting of $US3000 and a trophy, is jointly presented every four years by the Argonne National Laboratory, the National Physical Laboratory and the Numerical Algorithms Group.


== Eligibility and selection criteria ==
The winner must be at most 40 years of age as of January 1 of the year of the award. The award is given on the basis of:
Clarity of the software implementation and documentation.
Clarity of the paper accompanying the entry.
Portability, reliability, efficiency and usability of the software implementation.
Depth of analysis of the algorithm and the software.
Importance of application addressed by the software.
Quality of the test software


== Winners ==


=== 1991 ===
The first prize in 1991 was awarded to Linda Petzold for DASSL, a differential algebraic equation solver. This code is available in the public domain.


=== 1995 ===
The 1995 prize was awarded to Chris Bischof and Alan Carle for ADIFOR 2.0, an automatic differentiation tool for Fortran 77 programs. The code is available for educational and non-profit research.


=== 1999 ===
The 1999 prize was awarded to Matteo Frigo and Steven G. Johnson for FFTW, a C library for computing the discrete Fourier transform.


=== 2003 ===
The 2003 prize was awarded to Jonathan Shewchuk for Triangle, a two-dimensional mesh generator and Delaunay Triangulator. It is freely available.


=== 2007 ===
The 2007 prize was awarded to Wolfgang Bangerth, Guido Kanschat, and Ralf Hartmann for deal.II, a software library for computational solution of partial differential equations using adaptive finite elements. It is freely available.


=== 2011 ===
Andreas Waechter (IBM T. J. Watson Research Center) and Carl Laird (Texas A&M University) were awarded the 2011 prize for IPOPT, an object-oriented library for solving large-scale continuous optimization problems. It is freely available.


=== 2015 ===
The 2015 prize was awarded to Patrick Farrell (University of Oxford), Simon Funke (Simula Research Laboratory), David Ham (Imperial College London), and Marie Rognes (Simula Research Laboratory) for the development of dolfin-adjoint, a package which automatically derives and solves adjoint and tangent linear equations from high-level mathematical specifications of finite element discretisations of partial differential equations.


== References ==


== External links ==
Official Website"
280,Pushmeet Kohli,47399711,4707,"Pushmeet Kohli is a computer scientist in the Microsoft Research lab in Cambridge. He also is a member of the psychometrics center in University of Cambridge and is an ACM Distinguished speaker.
The majority of his research is in the field of machine learning and computer vision. However, he has also made contributions in game theory, discrete algorithms and psychometrics. He is the recipient of the BMVA Sullivan Prize. His papers have received awards at CVPR 2015, WWW 2014, ISMAR 2011 and ECCV 2010.


== Notable Works ==
Picture programming language (with Tejas Kulkarni, Vikash Mansinghka and Josh Tenenbaum). CVPR 2015.
3D scene reconstruction and understanding.
Inference in Higher Order Graphical Models.
Community based Crowdsourcing
User Personality Prediction from Browsing behavior
Behavioral experiments on Social networks
Human Pose Estimation using the Kinect
Video Editing (Unwrap Mosaics) - SIGGRAPH 2008


== References =="
281,Attribute-oriented programming,2099600,4704,"Attribute-oriented programming (@OP) is a program-level marking technique. Programmers can mark program elements (e.g. classes and methods) to indicate that they maintain application-specific or domain-specific semantics. For example, some programmers may define a ""logging"" attribute and associate it with a method to indicate the method should implement a logging function, while other programmers may define a ""web service"" attribute and associate it with a class to indicate the class should be implemented as a web service. Attributes separate application's core logic (or business logic) from application-specific or domain-specific semantics (e.g. logging and web service functions). By hiding the implementation details of those semantics from program code, attributes increase the level of programming abstraction and reduce programming complexity, resulting in simpler and more readable programs. The program elements associated with attributes are transformed to more detailed programs by a supporting tool (e.g. preprocessor). For example, a preprocessor may insert a logging program into the methods associated with a ""logging"" attribute.


== Attribute-oriented programming in various languages ==


=== Java ===
With the inclusion of The Metadata Facility for the Java Programming Language (JSR-175) into the J2SE 5.0 release it is possible to utilize attribute-oriented programming right out of the box. XDoclet library makes it possible to use attribute-oriented programming approach in earlier versions of Java.


=== C# ===
The C# language has supported attributes from its very first release. However these attributes are used to give run-time information and are not used by a pre-processor (there isn't one in C#'s reference implementation).


=== UML ===
The Unified Modeling Language (UML) supports a kind of attribute named Stereotypes.


== References ==


== Tools ==
Annotation Processing Tool (apt)
Spoon, an Annotation-Driven Java Program Transformer
XDoclet, a Javadoc-Driven Program Generator


== External links ==
Don Schwarz. Peeking Inside the Box: Attribute-Oriented Programming with Java5.
Sun JSR 175.
Attributes and Reflection - sample chapter from Programming C# book.
Modeling Turnpike Project.
Fraclet: An annotation-based programming model for the Fractal component model.
Attribute Enabled Software Development book"
282,Context-adaptive variable-length coding,9058366,4697,"Context-adaptive variable-length coding (CAVLC) is a form of entropy coding used in H.264/MPEG-4 AVC video encoding. It is an inherently lossless compression technique, like almost all entropy-coders. In H.264/MPEG-4 AVC, it is used to encode residual, zig-zag order, blocks of transform coefficients. It is an alternative to context-based adaptive binary arithmetic coding (CABAC). CAVLC requires considerably less processing to decode than CABAC, although it does not compress the data quite as effectively. CAVLC is supported in all H.264 profiles, unlike CABAC which is not supported in Baseline and Extended profiles.
CAVLC is used to encode residual, zig-zag ordered 4x4 (and 2x2) blocks of transform coefficients. CAVLC is designed to take advantage of several characteristics of quantized 4x4 blocks:
After prediction, transformation and quantization, blocks are typically sparse (containing mostly zeros).
The highest non-zero coefficients after zig-zag scan are often sequences of +/- 1. CAVLC signals the number of high-frequency +/-1 coefficients in a compact way.
The number of non-zero coefficients in neighbouring blocks is correlated. The number of coefficients is encoded using a look-up table; the choice of look-up table depends on the number of non-zero coefficients in neighbouring blocks.
The level (magnitude) of non-zero coefficients tends to be higher at the start of the reordered array (near the DC coefficient) and lower towards the higher frequencies. CAVLC takes advantage of this by adapting the choice of VLC look-up table for the “level” parameter depending on recently coded level magnitudes.


== Coded Elements ==
Parameters that required to be encoded and transmitted include the following table:


== CAVLC Examples ==
In all following examples, we assume that table Num-VLC0 is used to encode coeff_token.

0,3,0,1,-1,-1,0,1,0… TotalCoeffs = 5 (indexed from highest frequency [4] to lowest frequency [0])
TotalZeros = 3
T1s = 3 (in fact there are 4 trailing ones but only 3 can be encoded as a “special case”)
Encoding:
The transmitted bistream for this block is 000010001110010111101101.
Decoding: The output array is “built up” from the decoded values as shown below. Values added to the output array at each stage are underlined.
The decoder has inserted two zeros; however, TotalZeros is equal to 3 and so another 1 zero is inserted before the lowest coefficient, making the final output array: 0, 3, 0, 1, -1, -1, 0, 1


== See also ==
H.264 (entropy coding)
Data compression
Lossless compression


== External links ==
Context adaptive variable length coding tutorial(out of date)"
283,Trimming (computer programming),4496246,4693,"In computer programming, trimming (trim) or stripping (strip) is a string manipulation in which leading and trailing whitespace is removed from a string.
For example, the string (enclosed by apostrophes)

would be changed, after trimming, to


== Variants ==
Left or right trimming
The most popular variants of the trim function strip only the beginning or end of the string. Typically named ltrim and rtrim respectively, or in the case of Python: lstrip and rstrip. C# uses TrimStart and TrimEnd, and Common Lisp string-left-trim and string-right-trim. Pascal and Java do not have these variants built-in, although Object Pascal (Delphi) has TrimLeft and TrimRight functions.
Whitespace character list parameterization
Many trim functions have an optional parameter to specify a list of characters to trim, instead of the default whitespace characters. For example, PHP and Python allow this optional parameter, while Pascal and Java do not. With Common Lisp's string-trim function, the parameter (called character-bag) is required. The C++ Boost library defines space characters according to locale, as well as offering variants with a predicate parameter (a functor) to select which characters are trimmed.
Special empty string return value
An uncommon variant of trim returns a special result if no characters remain after the trim operation. For example, Apache Jakarta's StringUtils has a function called stripToNull which returns null in place of an empty string.
Space normalization
Space normalization is a related string manipulation where in addition to removing surrounding whitespace, any sequence of whitespace characters within the string is replaced with a single space. Space normalization is performed by the function named Trim() in spreadsheet applications (including Excel, Calc, Gnumeric, and Google Docs), and by the normalize-space() function in XSLT and XPath,
In-place trimming
While most algorithms return a new (trimmed) string, some alter the original string in-place. Notably, the Boost library allows either in-place trimming or a trimmed copy to be returned.


== Definition of whitespace ==
The characters which are considered whitespace varies between programming languages and implementations. For example, C traditionally only counts space, tab, line feed, and carriage return characters, while languages which support Unicode typically include all Unicode space characters. Some implementations also include ASCII control codes (non-printing characters) along with whitespace characters.
Java's trim method considers ASCII spaces and control codes as whitespace, contrasting with the Java isWhitespace() method, which recognizes all Unicode space characters.
Delphi's Trim function considers characters U+0000 (NULL) through U+0020 (SPACE) to be whitespace.


=== Non-space blanks ===
The Braille Patterns Unicode block contains U+2800 ⠀ Braille pattern blank (HTML &#10240;), a Braille pattern with no dots raised. The Unicode standard explicitly states that it does not act as a space.
The Non-breaking space U+00A0   Non-breaking space (HTML &#160; ·  &nbsp;) can also be treated as non-space for trimming purposes.


== Usage ==


== References ==


== External links ==
Tcl: string trim
Faster JavaScript Trim - compares various JavaScript trim implementations
php string cut and trimming- php string cut and trimming"
284,Artificial empathy,51023476,4686,"Artificial empathy (AE) is the development of AI systems − such as companion robots − that are able to detect and respond to human emotions. According to scientists, although the technology can be perceived as scary or threatening by many people, it could also have a significant advantage over humans in professions which are traditionally involved in emotional role-playing such as the health care sector. From the care-giver perspective for instance, performing emotional labor above and beyond the requirements of paid labor often results in chronic stress or burnout, and the development of a feeling of being desensitized to patients. However, it is argued that the emotional role-playing between the care-receiver and a robot can actually have a more positive outcome in terms of creating the conditions of less fear and concern for one's own predicament best exemplified by the phrase: ""if it is just a robot taking care of me it cannot be that critical."" Scholars debate the possible outcome of such technology using two different perspectives. Either, the AE could help the socialization of care-givers, or serve as role model for emotional detachment.


== Areas of research ==

There are a variety of philosophical, theoretical, and applicative questions related to AE. For example:
Which conditions would have to be met for a robot to respond competently to a human emotion?
What models of empathy can or should be applied to Social and Assistive Robotics?
Does the interaction of humans with robots have to imitate affective interaction between humans?
Can a robot help science learn about affective development of humans?
Would robots create unforeseen categories of inauthentic relations?
What relations with robots can be considered truly authentic? 


== See also ==
Artificial intelligence § Social intelligence
Artificial human companion
Case-based reasoning
Commonsense reasoning
Emotion recognition
Facial recognition system
Human–robot interaction
Soft computing
Machine learning
Evolutionary computing

Glossary of artificial intelligence
Blade Runner / Do Androids Dream of Electric Sheep?
Pepper (robot)


== References =="
285,Higher Computing,26564860,4682,"Higher Computing, SQA course code C206 12, is a qualification taught in Scottish secondary schools, usually in the fifth or sixth year of study (S5/S6). Candidates usually progress into Higher Computing with a pass at either Intermediate 2 or Standard Grade (credit).
The course consists of three units: two mandatory units and one optional unit selected by the centre. The required units are Computer Systems (DF2X 12) and Software Development (DF2Y 12) and the optional units are Computer Networking (DF30 12), Artificial Intelligence (DF31 12) and Multimedia Technology (DF32 12).


== Assessment ==
Candidates are assessed on both practical and theory work. In October, the SQA issues a coursework task to centres which candidates will attempt between November and March. The task is split into two parts: 1.A programming task and 2.An investigation related to hardware and software studied in the Computer Systems unit.
As with most SQA qualifications, candidates sit for a written exam in May/June. This is split into two sections. Section I is worth 20 marks and contains short response questions, while section II involves more in-depth answers and is worth a total of 70 marks.
Therefore, candidates can receive up to 150 marks. The grade is then worked out as follows:
Note: In some years the criteria for these grades may change to reflect the difficulty of the exam.


== Units ==
All three optional units are assigned 50 marks in the final exam.


=== Computer Systems ===
The Computer Systems unit of the course focuses on how modern day computers work and can be broken down into the following sub-sections:
Data representation
Computer structure
Peripherals
Networking
Computer software
30 marks in the coursework task (50%) are for this unit, while the written exam has approximately 45 marks worth of questions on Computer Systems.


=== Software Development ===
Software Development is about the process of developing commercial software and includes a practical element where students create their own programs using a high level language of the site's choice. The sub-sections are:
Software development process
Software development languages and environments
High level programming language constructs
Standard Algorithms
As with the Computer Systems unit, 30 marks in the coursework task (50%) are for this unit and the exam has around 45 marks worth of Software Development questions.


=== Artificial Intelligence ===
The first optional unit, Artificial Intelligence, teaches students about the history and technology behind AI. The four sub-sections are:
The development of artificial intelligence
Applications and uses of artificial intelligence
Search techniques
Knowledge representation
This unit has similar content to the Expert Systems unit of Higher Information Systems.


=== Computer Networking ===
Computer Networking contains the following sub-units:
Network protocols
Network applications
Network security
Data transmission
This unit has similar content to the Internet unit of Higher Information Systems.


=== Multimedia Technology ===
Multimedia Technology consists of the following:
Development process for multimedia applications
Bit-mapped graphic data
Digitised sound data
Video data
Vector graphics data
Synthesised sound data
Implications of the use of multimedia technology
This unit has similar content to the Applied Multimedia unit of Higher Information Systems.


== References ==


== External links ==
Higher Computing on the official SQA website: http://www.sqa.org.uk/sqa/2472.html
The course arrangements are available in a digital format on the Higher Computing section of the SQA website."
286,Adaptive sampling,37092233,4668,"Adaptive sampling is a technique used in computational molecular biology to efficiently simulate protein folding.


== Background ==
Proteins spend a large portion – nearly 96% in some cases – of their folding time ""waiting"" in various thermodynamic free energy minima. Consequently, a straightforward simulation of this process would spend a great deal of computation to this state, with the transitions between the states – the aspects of protein folding of greater scientific interest – taking place only rarely. Adaptive sampling exploits this property to simulate the protein's phase space in between these states. Using adaptive sampling, molecular simulations that previously would have taken decades can be performed in a matter of weeks.


== Theory ==
If a protein folds through the metastable states A -> B -> C, researchers can calculate the length of the transition time between A and C by simulating the A -> B transition and the B -> C transition. The protein may fold through alternative routes which may overlap in part with the A -> B -> C pathway. Decomposing the problem in this manner is efficient because each step can be simulated in parallel.


== Applications ==
Adaptive sampling is used by the Folding@home distributed computing project in combination with Markov state models.


== Disadvantages ==
While adaptive sampling is useful for short simulations, longer trajectories may be more helpful for certain types of biochemical problems.


== See also ==
Folding@home
Hidden markov model
Computational biology
Molecular biology


== References ==


== External links =="
287,Asymptotic decider,32895131,4661,"In scientific visualization the asymptotic decider is an algorithm developed by Nielson and Hamann in 1991 that creates isosurfaces from a given scalar field. It was proposed as an improvement to the marching cubes algorithm, which can produce some ""bad"" topology, but can also be considered an algorithm in its own right.


== Principle ==
The algorithm first divides the scalar field into uniform cubes. It draws topologically correct contours on the sides (interface) of the cubes. These contours can then be connected to polygons and triangulated. The triangles of all cubes form the isosurfaces and are thus the output of the algorithm. Sometimes there is more than one way to connect adjacent constructs. This algorithm describes a method for resolving these ambiguous configurations in a consistent manner.
Ambiguous cases often occur if diagonally opposing points are found on the same side of the isoline, but on a different side to the other points in the square (for 2D systems) or cube (for 3D systems). In a 2D case this means that there are two possibilities. If we suppose that we mark the corners as positive if their value is greater than that of the isoline, or negative if it is less, then either the positive corners are separated by two isolines, or the positive corners are in the main section of the square and the negative corners are separated by two isolines. The correct situation depends on the value at the asymptote of the isolines. Isolines are hyperbolae which can be described using the following formula:

  
    
      
        f
        (
        α
        ,
        β
        )
        =
        γ
        (
        α
        −
        
          α
          
            0
          
        
        )
        (
        β
        −
        
          β
          
            0
          
        
        )
        +
        δ
      
    
    {\displaystyle f(\alpha ,\beta )=\gamma (\alpha -\alpha _{0})(\beta -\beta _{0})+\delta }
  
where 
  
    
      
        α
      
    
    {\displaystyle \alpha }
   is the normalised distance in the square from the left-hand side, and 
  
    
      
        β
      
    
    {\displaystyle \beta }
   is the normalised distance in the square from the bottom. The values 
  
    
      
        
          α
          
            0
          
        
      
    
    {\displaystyle \alpha _{0}}
   and 
  
    
      
        
          β
          
            0
          
        
      
    
    {\displaystyle \beta _{0}}
   are therefore the coordinates of the asymptotes, and 
  
    
      
        δ
      
    
    {\displaystyle \delta }
   is the value at the position 
  
    
      
        (
        α
        ,
        β
        )
      
    
    {\displaystyle (\alpha ,\beta )}
  . This point ought to belong to the section which contains two corners. Therefore if 
  
    
      
        δ
      
    
    {\displaystyle \delta }
   is greater than the value of the isoline the positive corners are in the main section of the square and the negative corners are separated by two isolines, and if 
  
    
      
        δ
      
    
    {\displaystyle \delta }
   is less than the value of isoline the negative corners are in the main section of the square and the positive corners are separated by two isolines. A similar solution is used the 3D version


== See also ==
Isosurface
Marching cubes

 Computer science portal
 Science portal


== References ==
Notes

Bibliography


== Further reading ==
Charles D. Hansen; Chris R. Johnson (2004). Visualization Handbook. Academic Press. pp. 7–12. ISBN 978-0-12-387582-2. 
A. Lopes; K. Bordlie (2005). ""Interactive approaches to contouring and isosurfaces for geovisualization"". In Jason Dykes; Alan M. MacEachren; M. J. Kraak. Exploring Geovisualization. Elsevier. pp. 352–353. ISBN 978-0-08-044531-1."
288,Alex Graves (computer scientist),50568903,4659,"Alex Graves is a research scientist at DeepMind. He did a BSc in Theoretical Physics at Edinburgh and obtained a PhD in AI under Jürgen Schmidhuber at IDSIA. He was also a postdoc at TU Munich and under Geoffrey Hinton at the University of Toronto.
At IDSIA, he trained long short-term memory neural networks by a novel method called connectionist temporal classification (CTC). This method outperformed traditional speech recognition models in certain applications. In 2009, his CTC-trained LSTM was the first recurrent neural network to win pattern recognition contests, winning several competitions in connected handwriting recognition. This method has become very popular. Google uses CTC-trained LSTM for speech recognition on the smartphone.
Graves is also the creator of neural Turing machines and of the closely related differentiable neural computer.


== References =="
289,International Supercomputing Conference,27614604,4639,"The International Supercomputing Conference is a yearly conference on supercomputing which has been held in Europe since 1986.
Not to be confused with the ACM/IEEE Supercomputing Conference which has been held in the US since 1988.


== History ==
In 1986 Professor Dr. Hans Werner Meuer, director of the computer centre and professor for computer science at the University of Mannheim (Germany) co-founded and organized the ""Mannheim Supercomputer Seminar"" which had 81 participants. This was held yearly and became the annual International Supercomputing Conference and Exhibition (ISC). The conference is attended by speakers, exhibitors, and researchers from all over the world. Since 1993 the conference has been the venue for one of the twice yearly TOP500 announcements where the fastest 500 supercomputers in the world are named. The other annual announcement is in November at Supercomputing (The International Conference for High Performance Computing, Networking, Storage and Analysis) in the USA.
The conference celebrated 25 years with the 30 March 2010 meeting in Hamburg, Germany.


== References ==


== External links ==
International Supercomputing Conference Official Website"
290,Medical Image Understanding and Analysis conference,43042472,4599,"Medical Image Understanding and Analysis (MIUA) is a UK-based meeting for the communication of research related to image analysis and its application to medical imaging and biomedicine. The meetings provide an opportunity to present and discuss research in medical image understanding and analysis; a rapidly growing subject with ever increasing real-world applicability.
MIUA is collaborated with the BMVA. Medical Image understanding is an intrinsically interdisciplinary activity, drawing on expertise from computer science, engineering, physics, clinical practice and fundamental bioscience. Many researchers with an interest in MIUA topics will also belong to wider communities in each of these disciplines, which overlap, but are not congruent with, the MIUA community. See more information 
The 19th Annual Conference in Medical Image Understanding and Analysis (MIUA 2015) was held at the University of Lincoln. MIUA is a UK-based meeting for the communication of research related to image analysis and its application to medical imaging and biomedicine. The meetings provide an opportunity to present and discuss research in medical image understanding and analysis; a rapidly growing subject with ever increasing real-world applicability.


== The list of current and previous MIUAs and Chairs ==
2017: University of Edinburgh. 

Maria del Carmen Valdés Hernández, Victor Gonzalez-Castro

2016: University of Loughborough.

Alastair Gale, Yan Chen

2015: University of Lincoln.

Tryphon Lambrou, Xujiong Ye

2014: City University London.

Constantino Carlos Reyes-Aldasoro, Greg Slabaugh

2013: Birmingham.

Ela Claridge

2012: Swansea.

Xianghua Xie

2011: London (KCL).

William Crum, Graemme Penney

2010: Warwick.

Abhir Bhalearo, Nasir Rajpoot

2009: Kingston.

Jamshid Dehmeshki, Darrel Greenhill, Andreas Hoppe

2008: Dundee.

Stephen McKenna, Jesse Hoey

2007: Aberystwyth.

Fred Labrosse, Reyer Zwiggelaar

2006: Manchester.

S.M. Astley, T.F. Cootes, J. Graham, N. Thacker.

2005: Bristol.

Majid Mirmehdi

2004: London (IC).

Daniel Rueckert

2003: Sheffield.

David Barbar

2002: Portsmouth.

Alex Houston, Reyer Zwiggelaar

2001: Birmingham.

Ela Claridge

2000: London (UCL).
1999: Oxford.

Mike Brady

1998: Leeds.

Liz Berry, DC Hogg, KV Mardia, MA Smith

1997: Oxford.

Alison Noble


== References ==


== See also ==
BMVA Summer School
BMVC"
291,Erdős–Rényi Prize,43630513,4596,"The Erdős–Rényi Prize is awarded by the Network Science Society at the yearly flagship NetSci Conference to a selected young scientist (under 40 years old on the day of the nomination deadline) for their achievements in research activities in the area of network science, broadly construed. While the achievements can be both theoretical and experimental, the prize is aimed at emphasizing outstanding contributions relevant to the interdisciplinary progress of network science. The Prize is named for the mathematicians Paul Erdős and Alfréd Rényi, whose seminal contributions to the study of random graphs represent foundational work in the modern field of network science.
Recipients are awarded the prize at a special ceremony during the annual NetSci International Conference on Network Science. The prize is presented by the chair of the prize committee and the president of the Network Science Society, who present an official citation for the basis of the award. Awardees receive a commemorative plaque noting their award, a small cash prize, and are invited to give a prize lecture at the conference.


== Recipients ==
2012: Roger Guimera, Rovira i Virgili University. Guimera received the 2012 prize for ""outstanding work as a young researcher in Network Science for the technical depth and the interdisciplinary values of his scientific contributions to the analysis of network cartography and community identification.""
2013: Adilson E. Motter, Northwestern University. Motter received the 2013 prize for ""his groundbreaking contributions to the study of synchronization phenomena and the control of cascading failures in complex networks.""
2014: Mason A. Porter, University of Oxford. Porter was awarded the 2014 prize, for his ""fundamental research on the mathematics of networks and his outreach efforts to teach network science to students in schools.""
2015: Chaoming Song, University of Miami. Song was awarded the 2015 prize, recognizing him ""as an outstanding young researcher in Network Science for the breadth and depth of his influential work, ranging from network applications of self-similarity and renormalization group theory, to the in-depth analysis of big data on human mobility.""
2016: Aaron Clauset, University of Colorado Boulder. Clauset was awarded the 2016 prize ""for his contributions to the study of network structure, including Internet mapping, inference of missing links, and community structure, and for his provocative analyses of human conflicts and social stratification.""
2017: Vittoria Colizza,. for ""contributions to fundamental and data-driven network-based modeling of epidemic processes, including seminal studies on metapopulation systems, the impact of air transportation, and the predictability of epidemic outbreak.""


== References ==


== External links ==
Network Science Society"
292,ISCB Africa ASBCB Conference on Bioinformatics,35528199,4591,"The ISCB Africa ASBCB Conference on Bioinformatics is a biennial academic conference on the subjects of bioinformatics and computational biology, organized by the African Society for Bioinformatics and Computational Biology (ASBCB). The conference was first held in 2007 as the ""ASBCB Conference on the Bioinformatics of African Pathogens, Hosts and Vectors"". Since 2009, the conference has been jointly organized with the International Society for Computational Biology (ISCB) and held in different locations within Africa. Although having an evident African focus, the meeting is intended to be a truly international event, encompassing scientists and students from leading institutions in the US, Latin America, Europe and Africa. Holding this event in Africa, ISCB and ASBCB intend to promote local efforts for cooperation and dissemination of leading research techniques to combat major African diseases.


== Format of the Meeting ==
The meeting usually consists of a 3-day conference followed by practical workshops. The main 3-day meeting includes keynote presentations by up to 6 invited speakers from around the world, including Africa. Session Chairs introduce Keynote Speakers with an overview of the session, highlighting the most significant challenges and the current state of the art in the field before the keynote speakers launch their presentations. Highly accomplished researchers, primarily but not exclusively from non-African countries, present during the post conference tutorial workshops.


== Conference Goals ==
To directly impact existing capacity to develop public health interventions in endemic Africa countries by driving collaboration and networks development and training.
To expose and educate young and established scientists to the latest bioinformatics tools and techniques used in researching treatments and cures for African hosts, vectors and disease.


== Scientific publications ==
Since 2009, the ISCB Africa ASBCB Conference has been partnering with the Genes, Infection and Evolution journal to publish top papers presented at the conference.


== List of conferences ==


== References ==


== External links ==
2011 Conference Website
2009 Conference Website"
293,Rajesh K. Gupta,52397627,4582,"Rajesh K. Gupta (born 1961) is a computer scientist and engineer, currently the Qualcomm Professor in Embedded Microsystems at University of California, San Diego. His research concerns design and optimization of Cyber-physical systems (CPS). He is a Principal Investigator in the NSF MetroInsight project  and serves as Associate Director of the Qualcomm Institute (also known as California Institute for Telecommunications and Information Technology). His research contributions include SystemC and SPARK Parallelizing High-level Synthesis. Earlier he led NSF Expeditions on Variability in Microelectronic circuits.
He chaired the Computer Science and Engineering department at UC San Diego until 2016  during a time of extraordinary growth in Computer Science nationwide.
He is a Fellow of the IEEE and holds INRIA International Chair at the French international research institute in Rennes, Bretagne Atlantique. In 2017 he became a Fellow of the Association for Computing Machinery.


== References ==


== External links ==
Rajesh K. Gupta publications indexed by Google Scholar
Gupta's webpage
Revolutionizing how we keep track of time in cyber-physical systems
San Diego Struggles to Keep Its Young Tech Talent
Analysts fear even bigger cyber attacks are coming
Do people want to talk to Bots?
PCs that work while they sleep
Moneta system points to the future of computer storage
Want to date your smartphone?
Gift to transform UCSD Computer Science
How much life is left in Moore's Law?"
294,Alan Smeaton,5763387,4578,"Professor Alan F. Smeaton MRIA is a researcher and academic at Dublin City University. Among his accomplishments are founding TRECVid, the Centre for Digital Video Processing, and being a winner of the University President's Research Award in Science and Engineering in 2002 and the DCU Educational Trust Leadership Award in 2009. He is a founding director of Insight Centre for Data Analytics at Dublin City University (2013-2019). Prior to that he was a Principal Investigator and Deputy Director of CLARITY: Centre for Sensor Web Technologies, (2008-2013) and .
Currently (2013), Prof. Smeaton also serves on the editorial boards for the ACM Journal on Computers and Cultural Heritage, Information Processing and Management and the journal Foundations and Trends in Information Retrieval. Prof. Smeaton was elected a Member of the Royal Irish Academy in May 2013, the highest academic distinction in Ireland.
In 2012 Prof. Smeaton was appointed by Minister Sean Sherlock to the board of the Irish Research Council


== Published works ==
Books by Alan Smeaton include:
Smeaton, Alan F (1987). Using Parsing of Natural Language as Part of Document Retrieval. 
Smeaton, Alan F; Dublin, National Institute for Higher Education (1989*). Information Retrieval and Natural Language Processing.  
Agosti, Maristella; Smeaton, Alan F (1996). Information retrieval and hypertext. ISBN 9780792397106. 
Smeaton, Alan F; Society, British Computer (1990). AI and cognitive science '89: Dublin City University, 14-15 September, 1989. ISBN 9783540196082. 
Representative Papers by Alan Smeaton include:
TRECVID: Benchmarking the Effectiveness of Information Retrieval Tasks on Digital Video, Alan Smeaton and Paul Over, in Proceedings of the Second International Conference on Image and Video Retrieval, CIVR 2003, Springer Lecture Notes in Computer Science, vol. LNCS2728, Springer, London, 2003.
Evaluation Campaigns and TRECVID, Alan Smeaton, Paul Over, and Wessel Kraaij, in Proceedings of the Eight ACM International Workshop on Multimedia Information Retrieval, MIR 2006, ACM, 2006.
Smeaton, A. F. (1992). ""Progress in the Application of Natural Language Processing to Information Retrieval Tasks"". The Computer Journal. 35 (3): 268. doi:10.1093/comjnl/35.3.268. 


== References ==


== External links ==
""Alan Smeaton's Home Pages"". DCU School of Computing. 
""Staff Details: Prof Alan Smeaton"". DCU School of Computing. Archived from the original on 8 July 2013."
295,Software Engineering Body of Knowledge,28878,4567,"The Software Engineering Body of Knowledge (SWEBOK) is an international standard ISO/IEC TR 19759:2005 specifying a guide to the generally accepted Software Engineering Body of Knowledge.
The Guide to the Software Engineering Body of Knowledge (SWEBOK Guide) has been created through cooperation among several professional bodies and members of industry and is published by the IEEE Computer Society (IEEE). The standard can be accessed freely from the IEEE Computer Society. In late 2013, SWEBOK V3 was approved for publication and released. In 2016, the IEEE Computer Society kicked off the SWEBoK Evolution effort to develop future iterations of the body of knowledge.


== SWEBOK Version 3 ==
The published version of SWEBOK V3 has the following 15 knowledge areas (KAs) within the field of software engineering:
Software requirements
Software design
Software construction
Software testing
Software maintenance
Software configuration management
Software engineering management
Software engineering process
Software engineering models and methods
Software quality
Software engineering professional practice
Software engineering economics
Computing foundations
Mathematical foundations
Engineering foundations
It also recognized, but did not define, these related disciplines:
Computer engineering
Systems engineering
Project management
Quality management
General management
Computer science
Mathematics


== 2004 Edition of the SWEBOK ==
The 2004 edition of the SWEBOK guide defined ten knowledge areas (KAs) within the field of software engineering:
Software requirements
Software design
Software construction
Software testing
Software maintenance
Software configuration management
Software engineering management (Engineering management)
Software engineering process
Software engineering tools and methods
Software quality
The SWEBOK also defines disciplines related to software engineering:
Computer engineering
Computer science
Management
Mathematics
Project management
Quality management
Software ergonomics (Cognitive ergonomics)
Systems engineering


== Similar Efforts ==
A similar effort to define a body of knowledge for software engineering is the ""Computing Curriculum Software Engineering (CCSE),"" officially named Software Engineering 2004 (SE2004). The curriculum largely overlaps with the 2004 SWEBOK V2 because the SWEBOK has been used as one of its sources; however, it is more directed towards academia. Whereas the SWEBOK Guide defines the software engineering knowledge that practitioners should have after four years of practice, SE2004 defines the knowledge that an undergraduate software engineering student should possess upon graduation (including knowledge of mathematics, general engineering principles, and other related areas). SWEBOK V3 aims to address these intersections.


== See also ==
Project Management Body of Knowledge (PMBOK)
Enterprise Architecture Body of Knowledge (EABOK)
Business Analysis Body of Knowledge (BABOK)
Automation Body of Knowledge (ABOK)
Data Management Body of Knowledge (DMBOK)
ISO/IEC JTC 1/SC 7


== References ==


== External links ==
Official website"
296,American Federation of Information Processing Societies,28591095,4554,"The American Federation of Information Processing Societies (AFIPS) was an umbrella organization of professional societies established on May 10, 1961 and dissolved in 1990. Its mission was to advance knowledge in the field of information science, and to represent its member societies in international forums.


== History ==
AFIPS grew out of the National Joint Computer Committee (NJCC), an organization formed in 1951, which held two major computer conferences - the Eastern (EJCC) and Western Joint Computer Conferences (WJCC). The three founding societies of AFIPS were the Association for Computing Machinery (ACM), the American Institute of Electrical Engineers (AIEE), and the Institute of Radio Engineers (IRE). AFIPS represented these societies in the International Federation for Information Processing (IFIP), formed a year earlier under the auspices of UNESCO.
In 1962, AFIPS took over sponsorship of the EJCC and WJCC and renamed them the Spring (SJCC) and Fall Joint Computer Conferences (FJCC). In 1973, the two were merged in the National Computer Conference (NCC), which ran annually until it was discontinued in 1987.
AFIPS also sponsored smaller conferences such as the Office Automation Conference, published the Annals of the History of Computing and other magazines, and presented an annual award—the Harry Goode Memorial Award—recognizing outstanding achievement in information processing.
AFIPS was dissolved in 1990. The IEEE Computer Society (IEEE-CS) became the sponsor of the Goode Award, and took over publication of Annals (renamed the IEEE Annals of the History of Computing). The IEEE-CS also joined the ACM to form the Federation on Computing in the United States (FOCUS) in 1991, to take the place of AFIPS as the United States’ representative in IFIP. In 1999, IFIP accepted separate membership for both IEEE-CS and ACM, and FOCUS was dissolved.


== Structure ==
AFIPS was managed by a board of directors, originally called the ""Governing Board."" Each member society had one to three directors on the board depending on the size of the society; each affiliated member had one director. Under this board were various committees including the executive committee, the education committee, the finance committee, and the awards committee. The conferences were managed by a conference board, which set the overall direction and policies of the conferences, coordinated the actions of the Conference Steering Committee and the National Computer Conference Committee, and referred problems to appropriate committees such as the finance and executive committees of AFIPS. The conferences featured technical sessions and exhibits relating to the field of information processing.


== References ==


== External links ==
American Federation of Information Processing Societies (AFIPS) Records, 1960-1990, Charles Babbage Institute, University of Minnesota.
Walter M. Carlson Papers, 1960-1990, Charles Babbage Institute, University of Minnesota. Documents, photographs, and audiovisuals related to AFIPS and its History of Computing Committee.
Claude A. R. Kagan Papers, 1951-1981, Charles Babbage Institute, University of Minnesota. correspondence, conference proceedings, meeting minutes, memoranda, newsletters, and policy manuals on origin and organization of AFIPS.
Oral history interview with Margaret R. Fox, Charles Babbage Institute, University of Minnesota. Among other topics Fox recounts her involvement in the National Joint Computer Committee which led to her work in the American Federation of Information Processing Societies (AFIPS) and describes the role of AFIPS in the International Information Processing Conference in Paris in 1959.
Oral history interview with Willis Ware, Charles Babbage Institute, University of Minnesota. Among many topics, Ware discusses his work for ACM and AFIPS."
297,Ken Kennedy Award,28084344,4553,"The Ken Kennedy Award, established in 2009 by the Association for Computing Machinery and the IEEE Computer Society in memory of Ken Kennedy, is awarded annually and recognizes substantial contributions to programmability and productivity in computing and substantial community service or mentoring contributions. The award includes a $5,000 honorarium and the award recipient will be announced at the ACM - IEEE Supercomputing Conference.


== Ken Kennedy Award Past Recipients ==
2016 William Gropp. ""For highly influential contributions to the programmability of high performance parallel and distributed computers.""
2015 Katherine Yelick. ""For advancing the programmability of HPC systems, strategic national leadership, and mentorship in academia and government labs.""
2013 Jack Dongarra. ""For influential contributions to mathematical software, performance measurement, and parallel programming, and significant leadership and service within the HPC community.” 
2012 Mary Lou Soffa ""For contributions to compiler technology and software engineering, exemplary service to the profession, and life-long dedication to mentoring and improving diversity in computing."" 
2011 Susan L. Graham. “For foundational compilation algorithms and programming tools; research and discipline leadership; and exceptional mentoring.”
2010 David Kuck “For his pioneering contributions to compiler technology and parallel computing, the profound impact of his research on industry, and the widespread and long-lasting influence of his teaching and mentoring.”
2009 Francine Berman. ""For her influential leadership in the design, development and deployment of national-scale cyber infrastructure, her-inspiring work as a teacher and mentor, and her exemplary service to the high performance community.”


== References ==


== Nomination Process ==
IEEE Computer Society Nomination Process


== External links ==
ACM - IEEE CS Ken Kennedy Award"
298,Instance selection,53279262,4524,"Instance selection  (or dataset reduction, or dataset condensation) is an important Data pre-processing step that can be applied in many Machine learning (or Data mining) tasks. Approaches for instance selection can be applied for reducing the original dataset to a manageable volume, leading to a reduction of the computational resources that are necessary for performing the learning process. Algorithms of instance selection can also be applied for removing noisy instances, before applying learning algorithms. This step can improve the accuracy in classification problems.
Algorithm for instance selection should identify a subset of the total available data to achieve the original purpose of the data mining (or machine learning) application as if the whole data had been used. Considering this, the optimal outcome of IS would be the minimum data subset that can accomplish the same task with no performance loss, in comparison with the performance achieved when the task is performed using the whole available data. Therefore, every instance selection strategy should deal with a trade-off between the reduction rate of the dataset and the classification quality.


== Instance selection algorithms ==
The literature provides several different algorithms for instance selection. They can be distinguished from each other according to several different criteria. Considering this, instance selection algorithms can be grouped in two main classes, according to what instances they select: algorithms that preserve the instances at the boundaries of classes and algorithms that preserve the internal instances of the classes. Within the category of algorithms that select instances at the boundaries it is possible to cite DROP3, ICF  and LSBo. On the other hand, within the category of algorithms that select internal instances, it is possible to mention ENN  and LSSm. In general, algorithm such as ENN and LSSm are used for removing harmful (noisy) instances from the dataset. They do not reduce the data as the algorithms that select border instances, but they remove instances at the boundaries that have a negative impact on the data mining task. They can be used by other instance selection algorithms, as a filtering step. For example, the ENN algorithm is used by DROP3 as the first step, and the LSSm algorithm is used by LSBo.
There is also another group os algorithms that adopt different selection criteria. For example, the algorithms LDIS  and CDIS  select the densest instances in a given arbitrary neighborhood. The selected instances can include both, border and internal instances. The LDIS and CDIS algorithms are very simple and select subsets that are very representative of the original dataset. Besides that, since they search by the representative instances in each class separately, they are faster (in terms of time complexity and effective running time) than other algorithms, such as DROP3 and ICF.


== References =="
299,Separation of protection and security,12310990,4522,"In computer sciences the separation of protection and security is a design choice. Wulf et al. identified protection as a mechanism and security as a policy, therefore making the protection-security distinction a particular case of the separation of mechanism and policy principle.


== Overview ==
The adoption of this distinction in a computer architecture, usually means that protection is provided as a fault tolerance mechanism by hardware/firmware and kernel, whereas the operating system and applications implement their security policies. In this design, security policies rely therefore on the protection mechanisms and on additional cryptography techniques.
The major hardware approach for security or protection is the use of hierarchical protection domains. Prominent example of this approach is ring architecture with ""supervisor mode"" and ""user mode""). Such approach adopts a policy already at the lower levels (hardware/firmware/kernel), restricting the rest of the system to rely on it. Therefore, the choice to distinguish between protection and security in the overall architecture design implies rejection of the hierarchical approach in favour of another one, the capability-based addressing.
Examples of models with protection and security separation include: access matrix, UCLA Data Secure Unix, take-grant and filter. Such separation is not found in models like: high-water mark, Bell–LaPadula (original and revisited), information flow, strong dependency and constraints.


== See also ==
Capability-based addressing
Computer security policy


== Notes ==


== References ==
Houdek, M. E., Soltis, F. G., and Hoffman, R. L. 1981. IBM System/38 support for capability-based addressing. In Proceedings of the 8th ACM International Symposium on Computer Architecture. ACM/IEEE, pp. 341–348.
Intel Corporation (2002) The IA-32 Architecture Software Developer’s Manual, Volume 1: Basic Architecture
Carl E. Landwehr Formal Models for Computer Security [1] Volume 13, Issue 3 (September 1981) pp. 247 – 278
Swift, Michael M; Brian N. Bershad, Henry M. Levy, Improving the reliability of commodity operating systems, [2] ACM Transactions on Computer Systems (TOCS), v.23 n.1, p. 77-110, February 2005
Wulf, W.; E. Cohen; W. Corwin; A. Jones; R. Levin; C. Pierson; F. Pollack (June 1974). ""HYDRA: the kernel of a multiprocessor operating system"". Communications of the ACM. 17 (6): 337–345. doi:10.1145/355616.364017. ISSN 0001-0782.  [3]
Feltus, Christophe (2008). ""Preliminary Literature Review of Policy Engineering Methods - Toward Responsibility Concept"". Proceeding of 3rd international conference on information and communication technologies : from theory to applications (ICTTA 08), Damascus, Syria; Preliminary Literature Review of Policy Engineering Methods - Toward Responsibility Concept."
300,Bachelor of Computer Science,2701254,4513,"The Bachelor of Computer Science or Bachelor of Science in Computer Science (abbreviated BCompSc or BCS or BS CS or B.Sc. CS) is a type of bachelor's degree, usually awarded after three or four years of collegiate study in computer science, but possibly awarded in fewer years depending on factors such as an institution's course requirements and academic calendar. In some cases it can be awarded in five years. In general, computer science degree programs emphasize the mathematical and theoretical foundations of computing.
The same core curriculum may, depending on the school, result in other degrees, including:
Bachelor of Arts (BA) in Computer Science
Bachelor of Applied Science (BASc) in Computer Science
Bachelor of Technology in Computer Science and Engineering (B.Tech)
Bachelor of Science in Information Technology
Bachelor of Mathematics in Computer Science
Bachelor of Engineering (BEng or BE) in Computer Science
Bachelor of Computing in Computer Science
Bachelor of Science in Engineering (Computer Science) - BSE (CS)
Bachelor of Computer Security in Computer Science
Bachelor of Science (BSc or BS) in Computer Science (BSc CS or BSCS or BSc (Comp)
In many post-secondary institutions, an Honors Bachelor of Computer Science degree has been introduced as an upgrade to the regular bachelor's program and usually requires at least one additional year of study.


== Typical requirements ==
Because computer science is a wide field, courses required to earn a bachelor of computer science degree vary. A typical list of course requirements includes topics such as:
Computer programming
Programming paradigms
Algorithms
Data structures
Logic & Computation
Computer architecture
Some schools may place more emphasis on mathematics and require additional courses such as:
Linear algebra
Calculus
Probability theory and statistics
Combinatorics and discrete mathematics
Differential calculus and mathematics
Beyond the basic set of computer science courses, students can typically choose additional courses from a variety of different fields, such as:
Theory of computation
Operating systems
Numerical computation
Compilers, compiler design
Real-time computing
Distributed systems
Computer networking
Data communication
Computer graphics
Artificial intelligence
Human-computer interaction
Information theory
Software testing
Information assurance
Some schools allow students to specialize in a certain area of computer science.


== Related degrees ==
Bachelor of Software Engineering
Bachelor of Science in Information Technology
Bachelor of Computing
Bachelor of Information Technology
Bachelor of Computer Information Systems
Bachelor of computer design


== References =="
301,IEEE Undergraduate Teaching Award,23265697,4508,"The IEEE Undergraduate Teaching Award is a Technical Field Award of the IEEE that was established by the IEEE Board of Directors in 1990. It is presented for inspirational teaching of undergraduate students in the fields of interest of the IEEE.
This award may be given to an individual only.
Recipients of this award receive a bronze medal, certificate, and honorarium.


== Recipients ==
The recipients of the IEEE Undergraduate Teaching Award include the following people:
2016: Terri Fiez 
2015: Branislav M. Notaros
2014: Hsi-Tseng Chou
2013: Charles Kenneth Alexander 
2012: Santosh K. Kurinec 
2011: Raghunath Shevgaonkar 
2010: Ned Mohan 
2009: John C. Bean 
2008: Muhammad Harunur Rashid
2007: Clayton R. Paul
2006: John B. Peatman
2005: Yannis Tsividis
2004: Richard C. Jaeger
2003: Mehrdad Ehsani
2002: No Award
2001: No Award
2000: Haniph A. Lachman 
1999: Michael G. Pecht 
1998: J. David Irwin 
1997: Chand R. Viswanathan
1996: Karan L. Watson
1996: David A. Patterson
1995: David G. Meyer
1994: N. Narayana Rao
1993: Ronald G. Hoelzeman
1992: James W. Nilsson


== References =="
302,Chih-Jen Lin,49301001,4497,"Chih-Jen Lin (Chinese: 林智仁; pinyin: Lín Zhìrén) is Distinguished Professor of Computer Science at National Taiwan University, and a leading researcher in machine learning, optimization, and data mining. He is best known for the open source library LIBSVM, an implementation of support vector machines.


== Biography ==
Chih-Jen Lin received his B.Sc. (1993) in Mathematics at National Taiwan University, and M.SE (1996) and Ph.D.(1998) in Operations at University of Michigan.


== Awards and honors ==
ACM Fellow (2015) 
For contributions to the theory and practice of machine learning and data mining.
AAAI Fellow (2014) 
For significant contributions to the field of machine learning, and the development of a widely used SVM software.
IEEE Fellow (2011)
For contributions to support vector machine algorithms and software.


== Selected works ==


=== Software ===
LIBSVM implements the SMO algorithm for kernelized support vector machines. LIBSVM Homepage


=== Articles ===
Chang, Chih-Chung; Lin, Chih-Jen (2011). ""LIBSVM: A library for support vector machines"". ACM Transactions on Intelligent Systems and Technology. 2 (3). 


== References ==


== External links ==
Chih-Jen Lin Google Scholar, h-index is 50."
303,Milind Tambe,34260601,4466,"Milind Tambe is Helen N. and Emmett H. Jones Professor in Engineering and a Professor of Computer Science and Industrial and Systems Engineering at the University of Southern California, Los Angeles. He is a fellow of AAAI (Association for Advancement of Artificial Intelligence) and has received ACM SIGART Autonomous Agents Research Award.
Milind Tambe's research is focused on agents and multi-agent systems and his algorithms have been deployed by USA security agencies such as LAX police division, the Federal Air Marshals Service, the US Coast Guard and the Transportation Security Administration.
In 2013 he became a Fellow of the Association for Computing Machinery.


== Bibliography ==
Security and Game Theory: Algorithms, Deployed Systems, Lessons Learned (1st edition) 2011. Cambridge University Press, ISBN 1-107-09642-1
Keep the Adversary Guessing: Agent Security by Policy Randomization 2008. VDM Verlag Dr. Mueller e.K., ISBN 3-639-01925-3


== References ==


== External links ==
Home page: Milind Tambe"
304,Thread automaton,37721302,4462,"In automata theory, the thread automaton (plural: automata) is an extended type of finite-state automata that recognizes a mildly context-sensitive language class above the tree-adjoining languages.


== Formal definition ==
A thread automaton consists of
a set N of states,
a set Σ of terminal symbols,
a start state AS ∈ N,
a final state AF ∈ N,
a set U of path components,
a partial function δ: N → U⊥, where U⊥ = U ∪ {⊥} for ⊥ ∉ U,
a finite set Θ of transitions.
A path u1...un ∈ U* is a string of path components ui ∈ U; n may be 0, with the empty path denoted by ε. A thread has the form u1...un:A, where u1...un ∈ U* is a path, and A ∈ N is a state. A thread store S is a finite set of threads, viewed as a partial function from U* to N, such that dom(S) is closed by prefix.
A thread automaton configuration is a triple ‹l,p,S›, where l denotes the current position in the input string, p is the active thread, and S is a thread store containing p. The initial configuration is ‹0,ε,{ε:AS}›. The final configuration is ‹n,u,{ε:AS,u:AF}›, where n is the length of the input string and u abbreviates δ(AS). A transition in the set Θ may have one of the following forms, and changes the current automaton configuration in the following way:
SWAP B →a C:   consumes the input symbol a, and changes the state of the active thread:
changes the configuration from   ‹l,p,S∪{p:B}›   to   ‹l+1,p,S∪{p:C}›
SWAP B →ε C:   similar, but consumes no input:
changes   ‹l,p,S∪{p:B}›   to   ‹l,p,S∪{p:C}›
PUSH C:   creates a new subthread, and suspends its parent thread:
changes   ‹l,p,S∪{p:B}›   to   ‹l,pu,S∪{p:B,pu:C}›   where u=δ(B) and pu∉dom(S)
POP [B]C:   ends the active thread, returning control to its parent:
changes   ‹l,pu,S∪{p:B,pu:C}›   to   ‹l,p,S∪{p:C}›   where δ(C)=⊥ and pu∉dom(S)
SPUSH [C] D:   resumes a suspended subthread of the active thread:
changes   ‹l,p,S∪{p:B,pu:C}›   to   ‹l,pu,S∪{p:B,pu:D}›   where u=δ(B)
SPOP [B] D:   resumes the parent of the active thread:
changes   ‹l,pu,S∪{p:B,pu:C}›   to   ‹l,p,S∪{p:D,pu:C}›   where δ(C)=⊥
One may prove that δ(B)=u for POP and SPOP transitions, and δ(C)=⊥ for SPUSH transitions.
An input string is accepted by the automaton if there is a sequence of transitions changing the initial into the final configuration.


== Notes ==


== References =="
305,ICAART,51301084,4459,"The International Conference on Agents and Artificial Intelligence (ICAART) is a meeting point for researchers (among others) with interest in the areas of Agents and Artificial Intelligence. There are 2 tracks in ICAART, one related to Agents and Distributed AI in general and the other one focused in topics related to Intelligent Systems and Computational Intelligence.
The conference program is composed of several different kind of sessions like technical sessions, poster sessions, keynote lectures, tutorials, special sessions, doctoral consortiums, panels and industrial tracks. The papers presented in the conference are made available at the SCITEPRESS digital library, published in the conference proceedings and some of the best papers are invited to a post-publication with Springer.
ICAART's first edition was in 2009 counting with several keynote speakers like Marco Dorigo, Edward H. Shortliffe and Timo Honkela. Since then, the conference had several other invited speakers like Katia Sycara, Nick Jennings, Robert Kowalski, Boi Faltings and Tim Finin. Francesca Rossi is one of the names confirmed for the next edition of this conference.
Since 2012 the conference is held in conjunction with 2 other conferences: the International Conference on Operations Research and Enterprise Systems (ICORES) and the International Conference on Pattern Recognition Applications and Methods (ICPRAM).


== Areas ==


=== Agents ===
Multi-agent systems
Semantic Web
Cognitive robotics
Autonomous Systems
Agent communication languages


=== Artificial intelligence ===
Bayesian networks
Intelligent user interfaces
Pattern recognition
Natural language processing
Machine learning


== Editions ==
ICAART 2016 – Rome, Italy
ICAART 2015 – Lisbon, Portugal
ICAART 2014 – ESEO, Angers, Loire Valley, France
ICAART 2013 – Barcelona, Spain
ICAART 2012 – Vilamoura, Algarve, Portugal
ICAART 2011 – Rome, Italy
ICAART 2010 – Valencia, Spain
ICAART 2009 – Porto, Portugal


== References ==


== External links ==
Official website"
306,XCOPY,1396026,4455,"In computing, XCOPY is a command used on PC DOS, MS-DOS, OS/2, Microsoft Windows, and related operating systems for copying multiple files or entire directory trees from one directory to another and for copying files across a network. XCOPY stands for extended copy, and was created as a more functional file copying utility than the copy command found in these operating systems. XCOPY first appeared in DOS 3.2.


== Example ==
Create a new directory by copying all contents of the existing directory, including any files or subdirectories having the ""hidden"" or ""system"" attributes and empty directories.

If the directory names include blank signs (spaces), the names can be put in quotation marks.

Copy entire drive in to a mapped network drive while ignoring any errors in network restartable mode.

Copy a single file without prompt if it is a file or a directory


== Deprecation ==
While still included in Windows 10, Xcopy has been deprecated in favor of Robocopy, a more powerful copy tool, which is now built into the Microsoft Windows Server and Desktop operating systems.


== Limitations ==
Xcopy fails with an ""insufficient memory"" error when the path plus filename is longer than 254 characters and moving large files without the ""/j"" option (available only after Server 2008R2) can consume all available RAM on a system.


=== No open files ===
Xcopy will not copy open files. Any process may open files for exclusive read access by withholding the FILE_SHARE_READ https://msdn.microsoft.com/en-us/library/aa363858.aspx
The Windows Volume Shadow Copy service is used for such situations, but Xcopy does not use it. Therefore, Xcopy is not useful for backing up live operating system volumes.


== See also ==
List of file copying software
List of MS-DOS commands
Robocopy
XCOPY deployment


== References ==


== External links ==
Switches That You Can Use with Xcopy and Xcopy32 Commands, Windows 95, Windows 98, Windows Me
Xcopy, Microsoft Windows XP
Xcopy, Technet
Microsoft TechNet Xcopy article
VariableGHz article depicting CRC errors and XCOPY as a solution
XCOPY Command in a post build event does not execute
XP_CMDSHELL Does Not Work with XCOPY
See also Microsoft Product Documentation"
307,Experimental software engineering,3750731,4440,"Experimental software engineering is a part of software engineering that focuses on gathering evidence, through measurements and experiments involving software systems (software products, processes, and resources). This data is intended to be used as the basis of theories about the processes involved in software engineering (theory backed by data is a fundamental tenet of the scientific method). A number of research groups primarily use empirical and experimental techniques.
Empirical software engineering is a related concept, sometimes used synonymously with experimental software engineering. Empirical software engineering emphasizes the use of empirical studies of all kinds to accumulate knowledge. Methods used include experiments, case studies, surveys, and using whatever data is available.


== Future of empirical software engineering research ==
In a recent keynote at the premier conference on empirical methods in software engineering (International Symposium on Empirical Software Engineering and Measurement http://www.esem-conferences.org/) Prof. Wohlin recommended ten commitments that the research community should follow to increase the relevance and impact of empirical software engineering research. However, at the same conference Dr. Ali effectively argued that solely following these will not be enough and we need to do more than just show the evidence substantiating the claimed benefits of our interventions but instead what is required for practical relevance and potential impact is the evidence for cost-effectiveness.


=== International Software Engineering Research Network (ISERN) ===
International Software Engineering Research Network (ISERN) is a global community of research groups who are active in experimental software engineering. Its purpose is to advance the practice of and foster university and industry collaborations within experimental software engineering. ISERN holds annual meetings in conjunction with the International Symposium on Empirical Software Engineering and Measurement (ESEM) conference.


== References ==


== Bibliography ==
Victor Basili, Richard W. Selby, David H. Hutchens, ""Experimentation in Software Engineering"", IEEE Transactions on Software Engineering, Vol. SE-12, No.7, July 1986
Barry Boehm, Hans Dieter Rombach, and Marvin V. Zelkowitz (eds.), Foundations of Empirical Software Engineering — The Legacy of Victor R. Basili, Springer-Verlag, 2005, ISBN 3-540-24547-2.
H. Dieter Rombach, Victor R. Basili and Richard W. Selby (eds.), [Experimental Software Engineering Issues: Critical Assessment and Future Directions], Springer-Verlag, 1993, ISBN 3-540-57092-6.
Basili, V.; Rombach, D.; Schneider, K.; Kitchenham, B.; Pfahl, D.; Selby, R. (Eds.),Empirical Software Engineering Issues. Critical Assessment and Future Directions, Springer-Verlag, 2007, ISBN 978-3-540-71300-5."
308,Conference on Automated Deduction,2467166,4430,"The Conference on Automated Deduction (CADE) is the premier academic conference on automated deduction and related fields. The first CADE was organized in 1974 at the Argonne National Laboratory near Chicago. Most CADE meetings have been held in Europe and the United States. However, conferences have been held all over the world. Since 1996, CADE has been held yearly. In 2001, CADE was, for the first time, merged into the International Joint Conference on Automated Reasoning (IJCAR). This has been repeated biannually since 2004.
In 1996, CADE Inc. was formed as a non-profit sub-corporation of the Association for Automated Reasoning to organize the previously individually organized conferences.


== External links ==
CADE web page
AAR web page


== References =="
309,Summer School Marktoberdorf,47690099,4407,"The International Summer School Marktoberdorf is an annual two-week summer school for international computer science and mathematics postgraduate students and other young researchers, held annually since 1970 in Marktoberdorf, near Munich in southern Germany. Students are accommodated in the boarding house of a local high school, Gymnasium Marktoberdorf. Proceedings are published when appropriate.


== Status ==
This is a summer school for theoretical computer science researchers, with some directors/co-directors who are ACM Turing Award winners (the nearest equivalent to the Nobel Prize in computer science).
The summer school is supported as an Advanced Study Institute of the NATO Science for Peace and Security Program. It is administered by the Faculty of Informatics at the Technical University of Munich.


== Directors ==

Past academic directors and co-directors include:

* ACM Turing Award winners.


== References ==


== External links ==
Official website"
310,Uncertain data,19058043,4402,"In computer science, uncertain data is data that contains noise that makes it deviate from the correct, intended or original values. In the age of big data, uncertainty or data veracity is one of the defining characteristics of data. Data is constantly growing in volume, variety, velocity and uncertainty (1/veracity). Uncertain data is found in abundance today on the web, in sensor networks, within enterprises both in their structured and unstructured sources. For example, there may be uncertainty regarding the address of a customer in an enterprise dataset, or the temperature readings captured by a sensor due to aging of the sensor. In 2012 IBM called out managing uncertain data at scale in its global technology outlook report that presents a comprehensive analysis looking three to ten years into the future seeking to identify significant, disruptive technologies that will change the world. In order to make confident business decisions based on real-world data, analyses must necessarily account for many different kinds of uncertainty present in very large amounts of data. Analyses based on uncertain data will have an effect on the quality of subsequent decisions, so the degree and types of inaccuracies in this uncertain data cannot be ignored.
Uncertain data is found in the area of sensor networks; text where noisy text is found in abundance on social media, web and within enterprises where the structured and unstructured data may be old, outdated, or plain incorrect; in modeling where the mathematical model may only be an approximation of the actual process. When representing such data in a database, some indication of the probability of the correctness of the various values also needs to be estimated.
There are three main models of uncertain data in databases. In attribute uncertainty, each uncertain attribute in a tuple is subject to its own independent probability distribution. For example, if readings are taken of temperature and wind speed, each would be described by its own probability distribution, as knowing the reading for one measurement would not provide any information about the other.
In correlated uncertainty, multiple attributes may be described by a joint probability distribution. For example, if readings are taken of the position of an object, and the x- and y-coordinates stored, the probability of different values may depend on the distance from the recorded coordinates. As distance depends on both coordinates, it may be appropriate to use a joint distribution for these coordinates, as they are not independent.
In tuple uncertainty, all the attributes of a tuple are subject to a joint probability distribution. This covers the case of correlated uncertainty, but also includes the case where there is a probability of a tuple not belonging in the relevant relation, which is indicated by all the probabilities not summing to one. For example, assume we have the following tuple from a probabilistic database:
Then, the tuple has 10% chance of not existing in the database.


== References ==

Volk, Habich; Clemens Utzny, Ralf Dittmann, Wolfgang Lehner. ""Error-Aware Density-Based Clustering of Imprecise Measurement Values"". Seventh IEEE International Conference on Data Mining Workshops, 2007. ICDM Workshops 2007. IEEE.  CS1 maint: Multiple names: authors list (link)
Rosentahl, Volk; Martin Hahmann, Dirk Habich, Wolfgang Lehner. ""Clustering Uncertain Data With Possible Worlds"". Proceedings of the 1st Workshop on Management and mining Of Uncertain Data in conjunction with the 25th International Conference on Data Engineering, 2009. IEEE.  CS1 maint: Multiple names: authors list (link)"
311,Hofstadter's law,1865442,4372,"Hofstadter's law is a self-referential time-related adage, coined by Douglas Hofstadter and named after him.

Hofstadter's Law: It always takes longer than you expect, even when you take into account Hofstadter's Law.

Hofstadter's law was a part of Douglas Hofstadter's 1979 book Gödel, Escher, Bach: An Eternal Golden Braid. The ""law"" is a statement regarding the difficulty of accurately estimating the time it will take to complete tasks of substantial complexity. It is often cited by programmers, especially in discussions of techniques to improve productivity, such as The Mythical Man-Month or extreme programming. The recursive nature of the law is a reflection of the widely experienced difficulty of estimating complex tasks despite all best efforts, including knowing that the task is complex.
The law was initially introduced in connection with a discussion of chess-playing computers, where top-level players were continually beating machines, even though the machines outweighed the players in recursive analysis. The intuition was that the players were able to focus on particular positions instead of following every possible line of play to its conclusion. Hofstadter wrote in 1979, ""In the early days of computer chess, people used to estimate that it would be ten years until a computer (or program) was world champion. But after ten years had passed, it seemed that the day a computer would become world champion was still more than ten years away ... This is just one more piece of evidence for the rather recursive Hofstadter's Law:"" (Notably, that day did indeed come, when Deep Blue defeated Garry Kasparov in 1997).


== See also ==
List of eponymous laws
Ninety-ninety rule
Optimism bias
Parkinson's law
Planning fallacy
Reference class forecasting
Student syndrome
Lindy Effect
Valve Time


== References =="
312,Prpl Foundation,46273602,4371,"The prpl Foundation is a non-profit computer industry association started by Imagination Technologies and others to encourage use of the MIPS architecture (and “open to others”), through the promotion of standards and open source solutions, with a particular focus on equipment for data centers, networking, and devices for the Internet of Things.
The Foundation manages projects in specific topic areas via “PEGs” (prpl Engineer Groups), including groups focused on processor emulation (QEMU), carrier-grade networking (prplwrt, based on OpenWRT), and virtualization and security. The organization also collects and disseminates information of interest to its members, including patterns in consumer use of smart devices and security issues. In 2016 the organization released a study, ""The prpl Foundation Smart Home Security Report"". The group also finds and reports security issues in smart devices.
Members of prpl include: Broadcom, Cavium, Ikanos, Imagination Technologies, Ineda Systems, Ingenic Semiconductor, Lantiq, Nevales Networks, PMC, and Qualcomm. The security PEG includes several of the above, as well as CUPP Computing, Elliptic Technologies, Imperas Software, Kernkonzept, and Seltech.


== References ==


== External links ==
Official website"
313,POPLmark challenge,10323007,4358,"In programming language theory, the POPLmark challenge (from ""Principles of Programming Languages benchmark"", formerly Mechanized Metatheory for the Masses!) (Aydemir, 2005) is a set of benchmarks designed to evaluate the state of automated reasoning (or mechanization) in the metatheory of programming languages, and to stimulate discussion and collaboration among a diverse cross section of the formal methods community. Very loosely speaking, the challenge is about measurement of how well programs may be proven to match a specification of how they are intended to behave (and the many complex issues that this involves). The challenge was initially proposed by the members of the PL club at the University of Pennsylvania, in association with collaborators around the world. The Workshop on Mechanized Metatheory is the main meeting of researchers participating in the challenge.
The design of the POPLmark benchmark is guided by features common to reasoning about programming languages. The challenge problems do not require the formalisation of large programming languages, but they do require sophistication in reasoning about:
Binding 
Most programming languages have some form of binding, ranging in complexity from the simple binders of simply typed lambda calculus to complex, potentially infinite binders needed in the treatment of record patterns.
Induction 
Properties such as subject reduction and strong normalisation often require complex induction arguments.
Reuse 
Furthering collaboration being a key aim of the challenge, the solutions are expected to contain reusable components that would allow researchers to share language features and designs without requiring them to start from scratch every time.


== The problems ==
As of 2007, the POPLmark challenge is composed of three parts. Part 1 concerns solely the types of System F<: (System F with subtyping), and has problems such as:
Checking that the type system admits transitivity of subtyping.
Checking the transitivity of subtyping in the presence of records
Part 2 concerns the syntax and semantics of System F<:. It concerns proofs of
Type safety for the pure fragment
Type safety in the presence of pattern matching
Part 3 concerns the usability of the formalisation of System F<:. In particular, the challenge asks for:
Simulating and animating the operational semantics
Extracting useful algorithms from the formalisations
Several solutions have been proposed for parts of the POPLmark challenge, using following tools: Isabelle/HOL, Twelf, Coq, αProlog, ATS, Abella and Matita.


== See also ==
Expression problem
QED manifesto
POPL conference


== References ==
Brian E. Aydemir, Aaron Bohannon, Matthew Fairbairn, J. Nathan Foster, Benjamin C. Pierce, Peter Sewell, Dimitrios Vytiniotis, Geoffrey Washburn, Stephanie C. Weirich, and Stephan A. Zdancewic. Mechanized metatheory for the masses: The POPLmark challenge. In Theorem Proving in Higher Order Logics, 18th International Conference, TPHOLs 2005, volume 3603 of Lecture Notes in Computer Science, pages 50–65. Springer, Berlin/ Heidelberg/ New York, 2005.
Benjamin C. Pierce, Peter Sewell, Stephanie Weirich, Steve Zdancewic, It Is Time to Mechanize Programming Language Metatheory, In Bertrand Meyer, Jim Woodcock (Eds.) Verified Software: Theories, Tools, Experiments, LNCS 4171, Springer Berlin / Heidelberg, 2008, pp. 26–30, ISBN 978-3-540-69147-1


== External links ==
The POPLmark wiki"
314,Faugère's F4 and F5 algorithms,6212640,4351,"In computer algebra, the Faugère F4 algorithm, by Jean-Charles Faugère, computes the Gröbner basis of an ideal of a multivariate polynomial ring. The algorithm uses the same mathematical principles as the Buchberger algorithm, but computes many normal forms in one go by forming a generally sparse matrix and using fast linear algebra to do the reductions in parallel.
The Faugère F5 algorithm first calculates the Gröbner basis of a pair of generator polynomials of the ideal. Then it uses this basis to reduce the size of the initial matrices of generators for the next larger basis:

If Gprev is an already computed Gröbner basis (f2, …, fm) and we want to compute a Gröbner basis of (f1) + Gprev then we will construct matrices whose rows are m f1 such that m is a monomial not divisible by the leading term of an element of Gprev.

This strategy allows the algorithm to apply two new criteria based on what Faugère calls signatures of polynomials. Thanks to these criteria, the algorithm can compute Gröbner bases for a large class of interesting polynomial systems, called regular sequences, without ever simplifying a single polynomial to zero—the most time-consuming operation in algorithms that compute Gröbner bases. It is also very effective for a large number of non-regular sequences.


== Implementations ==
The Faugère F4 algorithm is implemented
in FGb, Faugère's own implementation, which includes interfaces for using it from C/C++ or Maple,
in Maple computer algebra system, as the option method=fgb of function Groebner[gbasis] (this is an older version of FGb, with limitations of the size of the problems that can be solved),
in the Magma computer algebra system,
in the SageMath computer algebra system,
in SymPy Python package.
Study versions of the Faugère F5 algorithm is implemented in
the SINGULAR computer algebra system;
the SageMath computer algebra system.


== Applications ==
The previously intractable ""cyclic 10"" problem was solved by F5, as were a number of systems related to cryptography; for example HFE and C*.


== References ==

Faugère, J.-C. (June 1999). ""A new efficient algorithm for computing Gröbner bases (F4)"" (PDF). Journal of Pure and Applied Algebra. Elsevier Science. 139 (1): 61–88. doi:10.1016/S0022-4049(99)00005-5. ISSN 0022-4049. 
Faugère, J.-C. (July 2002). ""A new efficient algorithm for computing Gröbner bases without reduction to zero (F5)"" (PDF). Proceedings of the 2002 international symposium on Symbolic and algebraic computation (ISSAC). ACM Press: 75–83. doi:10.1145/780506.780516. ISBN 1-58113-484-3. 
Till Stegers Faugère's F5 Algorithm Revisited (alternative link). Diplom-Mathematiker Thesis, advisor Johannes Buchmann, Technische Universität Darmstadt, September 2005 (revised April 27, 2007). Many references, including links to available implementations.


== External links ==
Faugère's home page (includes pdf reprints of additional papers)
An introduction to the F4 algorithm."
315,Don't-care term,7444599,4319,"In digital logic, a don't-care term for a function is an input-sequence (a series of bits) for which the function output does not matter. An input that is known never to occur is a can't-happen term. Both these types of conditions are treated the same way in logic design and may be referred to collectively as don't-care conditions for brevity. The designer of a logic circuit to implement the function need not care about such inputs, but can choose the circuit's output arbitrarily, usually such that the simplest circuit results (minimization). Examples of don't-care terms are the binary values 1010 through 1111 (10 through 15 in decimal) for a function that takes a binary-coded decimal (BCD) value, because a BCD value never takes on such values (so called pseudo-tetrades); in the pictures, the circuit computing the lower left bar of a 7-segment display can be minimized to a b + a c by an appropriate choice of circuit outputs for dcba=1010...1111.
Don't-care terms are important to consider in minimizing logic circuit design, using Karnaugh maps and the Quine–McCluskey algorithm. Don't care optimization can also be used in the development of highly size-optimized assembly or machine code taking advantage of side effects.


== X value ==
""Don't care"" may also refer to an unknown value in a multi-valued logic system, in which case it may also be called an X value. In the Verilog hardware description language such values are denoted by the letter ""X"". In the VHDL hardware description language such values are denoted (in the standard logic package) by the letter ""X"" (forced unknown) or the letter ""W"" (weak unknown).
An X value does not exist in hardware. In simulation, an X value can result from two or more sources driving a signal simultaneously, or the stable output of a flip-flop (electronics) not having been reached. In synthesized hardware, however, the actual value of such a signal will be either 0 or 1, but will not be determinable from the circuit's inputs.


== See also ==
Decision table


== References =="
316,Instrumentation (computer programming),11171531,4318,"In the context of computer programming, instrumentation refers to an ability to monitor or measure the level of a product's performance, to diagnose errors and to write trace information. Programmers implement instrumentation in the form of code instructions that monitor specific components in a system (for example, instructions may output logging information to appear on screen). When an application contains instrumentation code, it can be managed using a management tool. Instrumentation is necessary to review the performance of the application. Instrumentation approaches can be of two types: Source instrumentation and binary instrumentation.


== Output ==
In programming, instrumentation means the ability of an application to incorporate:
Code tracing - receiving informative messages about the execution of an application at run time.
Debugging and (structured) exception handling - tracking down and fixing programming errors in an application under development.
Profiling - a means by which dynamic program behaviors can be measured during a training run with a representative input. This is useful for properties of a program which cannot be analyzed statically with sufficient precision, such as alias analysis.
Performance counters - components that allow the tracking of the performance of the application.
Computer data logging - components that allow the logging and tracking of major events in the execution of the application.


== Limitations ==
Instrumentation is limited by execution coverage. If the program never reaches a particular point of execution, then instrumentation at that point collects no data. For instance, if a word processor application is instrumented, but the user never activates the print feature, then the instrumentation can say nothing about the routines which are used exclusively by the printing feature.
Some types of instrumentation may cause a dramatic increase in execution time. This may limit the application of instrumentation to debugging contexts.


== See also ==
Hooking - range of techniques used to alter or augment the behavior of an operating system, of applications, or of other software components by intercepting function calls or messages or events passed between software components
Instruction set simulator - simulation of all instructions at machine code level to provide instrumentation
Runtime intelligence - technologies, managed services and practices for the collection, integration, analysis, and presentation of application usage levels, patterns and practices
Software performance analysis - techniques to monitor code performance, including instrumentation
Hardware performance counter
Application Response Measurement - standardized instrumentation API for C and Java
Dynamic recompilation - a feature of some emulators and virtual machines, where the system may recompile some part of a program during execution


== References ==

Introduction to Instrumentation and Tracing: Microsoft Developer Network
https://developer.apple.com/library/content/documentation/DeveloperTools/Conceptual/InstrumentsUserGuide/index.html Apple Developer Tools: Introduction to Instruments]
SystemTap provides free software (GPL) infrastructure to simplify the gathering of information about the running Linux system."
317,Starvation (computer science),501591,4301,"In computer science, starvation is a problem encountered in concurrent computing where a process is perpetually denied necessary resources to process its work. Starvation may be caused by errors in a scheduling or mutual exclusion algorithm, but can also be caused by resource leaks, and can be intentionally caused via a denial-of-service attack such as a fork bomb.
The impossibility of starvation in a concurrent algorithm is called starvation-freedom, lockout-freedom or finite bypass, is an instance of liveness, and is one of the two requirements for any mutual exclusion algorithm (the other being correctness). The name ""finite bypass"" means that any process (concurrent part) of the algorithm is bypassed at most a finite number times before being allowed access to the shared resource.


== Scheduling ==
Starvation is usually caused by an overly simplistic scheduling algorithm. For example, if a (poorly designed) multi-tasking system always switches between the first two tasks while a third never gets to run, then the third task is being starved of CPU time. The scheduling algorithm, which is part of the kernel, is supposed to allocate resources equitably; that is, the algorithm should allocate resources so that no process perpetually lacks necessary resources.
Many operating system schedulers employ the concept of process priority. A high priority process A will run before a low priority process B. If the high priority process (process A) blocks and never yields, the low priority process (B) will (in some systems) never be scheduled—it will experience starvation. If there is an even higher priority process X, which is dependent on a result from process B, then process X might never finish, even though it is the most important process in the system. This condition is called a priority inversion. Modern scheduling algorithms normally contain code to guarantee that all processes will receive a minimum amount of each important resource (most often CPU time) in order to prevent any process from being subjected to starvation.
In computer networks, especially wireless networks, scheduling algorithms may suffer from scheduling starvation. An example is maximum throughput scheduling.
Starvation is normally cause by deadlock in that it causes a process to freeze. Two or more processes become deadlocked when each of them is doing nothing while waiting for a resource occupied by another program in the same set. On the other hand, a process is in starvation when it is waiting for a resource that is continuously given to other processes. Starvation-freedom is a stronger guarantee than the absence of deadlock: a mutual exclusion algorithm that must choose to allow one of two processes into a critical section and picks one arbitrarily is deadlock-free, but not starvation-free.
A possible solution to starvation is to use a scheduling algorithm with priority queue that also uses the aging technique. Aging is a technique of gradually increasing the priority of processes that wait in the system for a long time.


== See also ==
Room synchronization


== Notes =="
318,Tony Kent Strix award,504195,4293,"The UKeiG Strix award is an annual award for outstanding contributions to the field of information retrieval and is presented in memory of Dr Tony Kent, a past Fellow of the Institute of Information Scientists (IIS), who died in 1997. Tony Kent made a major contribution to the development of information science and information services both in the UK and internationally, particularly in the field of chemistry. The name 'Strix' was chosen to reflect Tony's interest in ornithology, and as the name of the last and most successful information retrieval packages that he created.
The Award is given in recognition of an outstanding contribution to the field of information retrieval that meets one of the following criteria:
a major and/or sustained contribution to the theoretical or experimental understanding of the information retrieval process;
development of, or significant improvement in, mechanisms, a product or service for the retrieval of information, either generally or in a specialised field;
development of, or significant improvement in, ease of access to an information service;
a sustained contribution over a period of years to the field of information retrieval; for example, by running an information service or by contributing at national or international level to organisations active in the field.
Recipients so far have been: Source: Cilip
1998: Prof Stephen Robertson
1999: Dr Donna Harman
2000: Dr Martin Porter
2001: Prof Peter Willett
2002: Malcolm Jones
2003: Dr Herbert Van de Sompel
2004: Prof C. J. 'Keith' van Rijsbergen
2005: Jack Mills
2006: Stella Dextre Clarke
2007: Mats Lindquist
2008: Prof Kalervo Jarvelin
2009: Carol Ann Peters
2010: Michael Lynch
2011: Prof Alan Smeaton
2012: Doug Cutting and David Hawking
2013: Prof W. Bruce Croft
2014: Dr Susan Dumais
2015: Prof Peter Ingwersen
2016: Prof Maristella Agosti
2017: Prof Maarten de Rijke
Since 2014, the winner of the Tony Kent Strix Award in year _n_ is giving the Tony Kent Strix Annual Lecture in year _n+1_. Annual lectures so far:
2015: Dr Susan Dumais on ""Understanding and Improving Search using Large-Scale Behavioural Data"" 
2016: Prof Peter Ingwersen on ""Context in Interactive IR"" 
2017: Prof Maristella Agosti on ""Behind the Scenes of Research and Innovation"" 
2018: Prof Maarten de Rijke will give the fourth annual lecture in this series.
UKeiG is a special interest group of the Chartered Institute of Library and Information Professionals.


== See also ==
List of prizes


== References ==


== External links ==
The Tony Kent Strix Award, offered by UKeiG
UKeiG
CILIP"
319,Certified Forensic Computer Examiner,15702854,4292,"The Certified Forensic Computer Examiner (CFCE) credential was the first certification demonstrating competency in computer forensics in relation to Windows based computers. The CFCE training and certification is conducted by the International Association of Computer Investigative Specialists (IACIS), a non-profit, all volunteer organization of current and former law enforcement members.


== History ==
IACIS was formed and commenced training in 1990. The predecessor to the CFCE was the DOS Processing Certificate (DPC). The CFCE was introduced in 1998 when the training was expanded to include examination of Windows-based computers. The course materials also cover other operating systems and such as Linux and Mac OS and their associated file systems, however the certificate only states proficiency in Windows.


== Eligibility ==
In order to become a member of IACIS and undertake the CFCE or Certified Electronic Evidence Collection Specialist courses, previously a person must generally be a full-time member - sworn or unsworn - of a law enforcement agency, however this is no longer a requirement. In some of those cases, a contract employee of a law enforcement agency or retired law enforcement officer may be eligible. All IACIS members must sign agreement with the IACIS Code of Ethics.


== Certification process ==
The certification process may be taken internally or externally and is conducted in two phases: Peer Review and Certification.
An internal certification candidate attends a 2-week training course given by IACIS. Two courses are conducted annually. The US based course is conducted in the first half of the calendar year whilst the European-based course is conducted in the second half of the year. Upon successful completion of the course, the member is assigned a (volunteer) coach. The coach guides the student through the Peer Review phase, often by suggesting reading materials or experiments for the student, which is intended to assist the student in fully understanding issues with which the student may be having difficulty. Upon successful completion of the Peer Review phase the candidate is eligible to enter the Certification phase which consists of a practical exam based on a hard drive examination and a final exam.
An external certification candidate does not attend the training, however they have to have the equivalent 72 hours of training that is comparable to the IACIS training.


== Recertification ==
In order for certification to remain current, a member must undertake a proficiency test once per 3-year period after certification as well as complete 60 hours of continuing training in computer forensics or a related field. Additionally, the member must conduct as a minimum an average of 1 forensic examination per year, for a minimum of 3 examinations over the 3-year period. The member must also pay dues ($75 per year) and remain a member in good standing of IACIS.


== Recognition ==
CFCE is one of the most widely recognized non-tool certifications in computer forensics for current and former law enforcement personnel. Some organizations such as the Computer Forensics Laboratory at Miami-Dade Police require their members to complete and maintain this certification.


== References ==


== External links ==
Official website
CFCE
CFCE FAQ"
320,Konrad Zuse Medal,35020080,4288,"Konrad Zuse (German: [ˈkɔnʁat ˈtsuːzə]; 22 June 1910 – 18 December 1995) was a German civil engineer, inventor and computer pioneer. His greatest achievement was the world's first programmable computer; the functional program-controlled Turing-complete Z3 became operational in May 1941. Thanks to this machine and its predecessors, Zuse has often been regarded as the inventor of the modern computer.
Zuse was also noted for the S2 computing machine, considered the first process control computer. He founded one of the earliest computer businesses in 1941, producing the Z4, which became the world's first commercial computer. From 1943 to 1945 he designed the first high-level programming language, Plankalkül. In 1969, Zuse suggested the concept of a computation-based universe in his book Rechnender Raum (Calculating Space).
Much of his early work was financed by his family and commerce, but after 1939 he was given resources by the Nazi German government. Due to World War II, Zuse's work went largely unnoticed in the United Kingdom and the United States. Possibly his first documented influence on a US company was IBM's option on his patents in 1946.
There is a replica of the Z3, as well as the original Z4, in the Deutsches Museum in Munich. The Deutsches Technikmuseum in Berlin has an exhibition devoted to Zuse, displaying twelve of his machines, including a replica of the Z1 and several of Zuse's paintings.


== Pre-World War II work and the Z1 ==

Born in Berlin, Germany, on 22 June 1910, he moved with his family in 1912 to East Prussian Braunsberg (now Braniewo in Poland), where his father was a postal clerk. Zuse attended the Collegium Hosianum in Braunsberg. In 1923, the family moved to Hoyerswerda, where he passed his Abitur in 1928, qualifying him to enter university.
He enrolled in the Technische Hochschule Berlin (now Technical University of Berlin) and explored both engineering and architecture, but found them boring. Zuse then pursued civil engineering, graduating in 1935. For a time, he worked for the Ford Motor Company, using his considerable artistic skills in the design of advertisements. He started work as a design engineer at the Henschel aircraft factory in Schönefeld near Berlin. This required the performance of many routine calculations by hand, which he found mind-numbingly boring, leading him to dream of doing them by machine.
Beginning in 1935 he experimented in the construction of computers in his parents' flat on Wrangelstraße 38, moving with them into their new flat on Methfesselstraße 10, the street leading up the Kreuzberg, Berlin. Working in his parents' apartment in 1936, he produced his first attempt, the Z1, a floating point binary mechanical calculator with limited programmability, reading instructions from a perforated 35 mm film. In 1937, Zuse submitted two patents that anticipated a von Neumann architecture. He finished the Z1 in 1938. The Z1 contained some 30,000 metal parts and never worked well due to insufficient mechanical precision. On 30 January 1944, the Z1 and its original blueprints were destroyed with his parents' flat and many neighbouring buildings by a British air raid in World War II.
Between 1987 and 1989, Zuse recreated the Z1, suffering a heart attack midway through the project. It cost 800,000 DM, (approximately $500,000) and required four individuals (including Zuse) to assemble it. Funding for this retrocomputing project was provided by Siemens and a consortium of five companies.


== The Z2, Z3, and Z4 ==

Zuse completed his work entirely independently of other leading computer scientists and mathematicians of his day. Between 1936 and 1945, he was in near-total intellectual isolation. In 1939, Zuse was called to military service, where he was given the resources to ultimately build the Z2. In September 1940 Zuse presented the Z2, covering several rooms in the parental flat, to experts of the Deutsche Versuchsanstalt für Luftfahrt (DVL; i.e. German Research Institute for Aviation). The Z2 was a revised version of the Z1 using telephone relays.
The DVL granted research subsidies so that in 1941 Zuse started a company, Zuse Apparatebau (Zuse Apparatus Construction), to manufacture his machines, renting a workshop on the opposite side in Methfesselstraße 7 and stretching through the block to Belle-Alliance Straße 29 (renamed and renumbered as Mehringdamm 84 in 1947).
Improving on the basic Z2 machine, he built the Z3 in 1941. On 12 May 1941 Zuse presented the Z3, built in his workshop, to the public. The Z3 was a binary 22-bit floating point calculator featuring programmability with loops but without conditional jumps, with memory and a calculation unit based on telephone relays. The telephone relays used in his machines were largely collected from discarded stock. Despite the absence of conditional jumps, the Z3 was a Turing complete computer. However, Turing-completeness was never considered by Zuse (who had practical applications in mind) and only demonstrated in 1998 (see History of computing hardware).
The Z3, the first fully operational electromechanical computer, was partially financed by German government-supported DVL, which wanted their extensive calculations automated. A request by his co-worker Helmut Schreyer—who had helped Zuse build the Z3 prototype in 1938—for government funding for an electronic successor to the Z3 was denied as ""strategically unimportant"".

In 1937, Schreyer had advised Zuse to use vacuum tubes as switching elements; Zuse at this time considered it a crazy idea (""Schnapsidee"" in his own words). Zuse's workshop on Methfesselstraße 7 (with the Z3) was destroyed in an Allied Air raid in late 1943 and the parental flat with Z1 and Z2 on 30 January the following year, whereas the successor Z4, which Zuse had begun constructing in 1942 in new premises in the Industriehof on Oranienstraße 6, remained intact. On 3 February 1945, aerial bombing caused devastating destruction in the Luisenstadt, the area around Oranienstraße, including neighbouring houses. This event effectively brought Zuse's research and development to a complete halt. The partially finished, relay-based Z4 was packed and moved from Berlin on 14 February, only arriving in Göttingen two weeks later.
Work on the Z4 could not be resumed immediately in the extreme privation of post-war Germany, and it was not until 1949 that he was able to resume work on it. He showed it to the mathematician Eduard Stiefel of the Swiss Federal Institute of Technology Zurich (Eidgenössische Technische Hochschule (ETH) Zürich) who ordered one in 1950. On 8 November 1949, Zuse KG was founded. The Z4 was delivered to ETH Zurich on 12 July 1950, and proved very reliable.


== S1 and S2 ==
In 1940, the German government began funding him through the Aerodynamische Versuchsanstalt (AVA, Aerodynamic Research Institute, forerunner of the DLR), which used his work for the production of glide bombs. Zuse built the S1 and S2 computing machines, which were special purpose devices which computed aerodynamic corrections to the wings of radio-controlled flying bombs. The S2 featured an integrated analog-to-digital converter under program control, making it the first process-controlled computer.
These machines contributed to the Henschel Werke Hs 293 and Hs 294 guided missiles developed by the German military between 1941 and 1945, which were the precursors to the modern cruise missile. The circuit design of the S1 was the predecessor of Zuse's Z11. Zuse believed that these machines had been captured by occupying Soviet troops in 1945.


== Plankalkül ==

While working on his Z4 computer, Zuse realised that programming in machine code was too complicated. He started working on a PhD thesis containing groundbreaking research years ahead of its time, mainly the first high-level programming language, Plankalkül (""Plan Calculus"") and, as an elaborate example program, the first real computer chess engine. After the 1945 Luisenstadt bombing, he flew from Berlin for the rural Allgäu, and, unable to do any hardware development, he continued working on the Plankalkül, eventually publishing some brief excerpts of his thesis in 1948 and 1959; the work in its entirety, however, remained unpublished until 1972. The PhD thesis was submitted at University of Augsburg, but rejected for formal reasons, because Zuse forgot to pay the 400 Mark university enrollment fee. (The rejection did not bother him.) Plankalkül slightly influenced the design of ALGOL 58 but was itself implemented only in 1975 in a dissertation by Joachim Hohmann. Heinz Rutishauser, one of the inventors of ALGOL, wrote: ""The very first attempt to devise an algorithmic language was undertaken in 1948 by K. Zuse. His notation was quite general, but the proposal never attained the consideration it deserved"". Further implementations followed in 1998 and then in 2000 by a team from the Free University of Berlin. Donald Knuth suggested a thought experiment: What might have happened had the bombing not taken place, and had the PhD thesis accordingly been published as planned?


== Personal life ==
Konrad Zuse married Gisela Brandes in January 1945, employing a carriage, himself dressed in tailcoat and top hat and with Gisela in a wedding veil, for Zuse attached importance to a ""noble ceremony"". Their son Horst, the first of five children, was born in November 1945.
While Zuse never became a member of the Nazi Party, he is not known to have expressed any doubts or qualms about working for the Nazi war effort. Much later, he suggested that in modern times, the best scientists and engineers usually have to choose between either doing their work for more or less questionable business and military interests in a Faustian bargain, or not pursuing their line of work at all.
According to the memoirs of the German computer pioneer Heinz Billing from the Max Planck Institute for Physics, published by Genscher, Düsseldorf, there was a meeting between Alan Turing and Konrad Zuse. It took place in Göttingen in 1947. The encounter had the form of a colloquium. Participants were Womersley, Turing, Porter from England and a few German researchers like Zuse, Walther, and Billing. (For more details see Herbert Bruderer, Konrad Zuse und die Schweiz).
After he retired, he focused on his hobby of painting.
Zuse was an atheist.


== Death ==
Zuse died on 18 December 1995 in Hünfeld, Germany (near Fulda) from heart failure.


== Zuse the entrepreneur ==

During World War 2, Zuse founded one of the earliest computer companies: the Zuse-Ingenieurbüro Hopferau. Capital was raised in 1946 through ETH Zurich and an IBM option on Zuse's patents.
Zuse founded another company, Zuse KG in Haunetal-Neukirchen in 1949; in 1957 the company’s head office moved to Bad Hersfeld. The Z4 was finished and delivered to the ETH Zurich, Switzerland in September 1950. At that time, it was the only working computer in continental Europe, and the second computer in the world to be sold, beaten only by the BINAC, which never worked properly after it was delivered. Other computers, all numbered with a leading Z, up to Z43, were built by Zuse and his company. Notable are the Z11, which was sold to the optics industry and to universities, and the Z22, the first computer with a memory based on magnetic storage.
By 1967, the Zuse KG had built a total of 251 computers. Owing to financial problems, the company was then sold to Siemens.


== Calculating Space ==

In 1967, Zuse also suggested that the universe itself is running on a cellular automaton or similar computational structure (digital physics); in 1969, he published the book Rechnender Raum (translated into English as Calculating Space). This idea has attracted a lot of attention, since there is no physical evidence against Zuse's thesis. Edward Fredkin (1980s), Jürgen Schmidhuber (1990s), and others have expanded on it.


== Awards and honours ==
Zuse received several awards for his work:
Werner von Siemens Ring in 1964 (together with Fritz Leonhardt and Walter Schottky)
Harry H. Goode Memorial Award in 1965 (together with George Stibitz)
Wilhelm Exner Medal in 1969.
Bundesverdienstkreuz in 1972 – Great Cross of Merit
Computer History Museum Fellow Award in 1999 ""for his invention of the first program-controlled, electromechanical, digital computer and the first high-level programming language, Plankalkül.""
The Zuse Institute Berlin is named in his honour.
The Konrad Zuse Medal of the Gesellschaft für Informatik, and the Konrad Zuse Medal of the Zentralverband des Deutschen Baugewerbes (Central Association of German Construction), are both named after Zuse.


== Zuse Year 2010 ==
The 100th anniversary of the birth of this computer pioneer was celebrated by exhibitions, lectures and workshops to remember his life and work and to bring attention to the importance of his invention to the digital age. The movie Tron: Legacy, which revolves around a world inside a computer system, features a character named Zuse, presumably in honour of Konrad Zuse. German posts DP AG issued a commemorative stamp at this occasion, June 6, 2010: a Zuse portrait, composed solely by the binary code numbers 1 and 0 in fine print.


== Literature ==
Konrad Zuse: The Computer – My Life, Springer Verlag, ISBN 3-540-56453-5, ISBN 0-387-56453-5
Jürgen Alex, Hermann Flessner, Wilhelm Mons, Horst Zuse: Konrad Zuse: Der Vater des Computers. Parzeller, Fulda 2000, ISBN 3-7900-0317-4
Raul Rojas (Hrsg.): Die Rechenmaschinen von Konrad Zuse. Springer, Berlin 1998, ISBN 3-540-63461-4.
Wilhelm Füßl (Ed.): 100 Jahre Konrad Zuse. Einblicke in den Nachlass, München 2010, ISBN 978-3-940396-14-3.
Jürgen Alex: Wege und Irrwege des Konrad Zuse. In: Spektrum der Wissenschaft (dt. Ausgabe von Scientific American) 1/1997, ISSN 0170-2971.
Hadwig Dorsch: Der erste Computer. Konrad Zuses Z1 – Berlin 1936. Beginn und Entwicklung einer technischen Revolution. Mit Beiträgen von Konrad Zuse und Otto Lührs. Museum für Verkehr und Technik, Berlin 1989.
Clemens Kieser: „Ich bin zu faul zum Rechnen“ – Konrad Zuses Computer Z22 im Zentrum für Kunst und Medientechnologie Karlsruhe. In: Denkmalpflege in Baden-Württemberg, 4/34/2005, Esslingen am Neckar, S. 180–184, ISSN 0342-0027.
Mario G. Losano (ed.), Zuse. L'elaboratore nasce in Europa. Un secolo di calcolo automatico, Etas Libri, Milano 1975, pp. XVIII-184.
Arno Peters: Was ist und wie verwirklicht sich Computer-Sozialismus: Gespräche mit Konrad Zuse. Verlag Neues Leben, Berlin 2000, ISBN 3-355-01510-5.
Paul Janositz: Informatik und Konrad Zuse: Der Pionier des Computerbaus in Europa – Das verkannte Genie aus Adlershof. In: Der Tagesspiegel Nr. 19127, Berlin, 9. März 2006, Beilage Seite B3.
Jürgen Alex: Zum Einfluß elementarer Sätze der mathematischen Logik bei Alfred Tarski auf die drei Computerkonzepte des Konrad Zuse. TU Chemnitz 2006.
Jürgen Alex: Zur Entstehung des Computers – von Alfred Tarski zu Konrad Zuse. VDI-Verlag, Düsseldorf 2007, ISBN 978-3-18-150051-4, ISSN 0082-2361.
Herbert Bruderer: Konrad Zuse und die Schweiz. Wer hat den Computer erfunden? Charles Babbage, Alan Turing und John von Neumann Oldenbourg Verlag, München 2012, XXVI, 224 Seiten, ISBN 978-3-486-71366-4


== See also ==


== References ==


== Sources ==


== External links ==
Konrad Zuse Internet Archive
The Life and Work of Konrad Zuse at the Wayback Machine (archived April 18, 2010) – By Prof. Horst Zuse (K. Zuse's son); an extensive and well-written historical account
O'Connor, John J.; Robertson, Edmund F., ""Konrad Zuse"", MacTutor History of Mathematics archive, University of St Andrews .
Technical University of Berlin
Free University of Berlin
Konrad Zuse and his computers, from Technische Universität Berlin
Konrad Zuse
Konrad Zuse, inventor of first working programmable computer
Zuse's thesis of digital physics and the computable universe
Deutsches Technikmuseum Berlin
Konrad Zuse Museum Hoyerswerda
Konrad Zuse and The Invention of the Computer
Computermuseum Kiel Z11
Computermuseum Kiel Z22
Computermuseum Kiel Z25 at the Wayback Machine (archived July 25, 2011)
Video lecture by Zuse discussing the history of Z1 to 4"
321,Opaque data type,22512142,4284,"In computer science, an opaque data type is a data type whose concrete data structure is not defined in an interface. This enforces information hiding, since its values can only be manipulated by calling subroutines that have access to the missing information. The concrete representation of the type is hidden from its users, and the visible implementation is incomplete. A data type whose representation is visible is called transparent. Opaque data types are frequently used to implement abstract data types.
Typical examples of opaque data types include handles for resources provided by an operating system to application software. For example, the POSIX standard for threads defines an application programming interface based on a number of opaque types that represent threads or synchronization primitives like mutexes or condition variables.
An opaque pointer is a special case of an opaque data type, a datatype that is declared to be a pointer to a record or data structure of some unspecified data type. For example, the standard library that forms part of the specification of the C programming language provides functions for file input and output that return or take values of type ""pointer to FILE"" that represent file streams (see C file input/output), but the concrete implementation of the type FILE is not specified.


== Uses in various languages ==
Some languages, such as C, allow the declaration of opaque records (structs), whose size and fields are hidden from the client. The only thing that the client can do with an object of such a type is to take its memory address, to produce an opaque pointer.
If the information provided by the interface is sufficient to determine the type's size, then clients can declare variables, fields, and arrays of that type, assign their values, and possibly compare them for equality. This is usually the case for opaque pointers.
In some languages, such as Java, the only kind of opaque type provided is the opaque pointer. Indeed, in Java (and several other languages) records are always handled through pointers.
Some languages allow partially opaque types, e.g. a record which has some public fields, known and accessible to all clients, and some hidden fields which are not revealed in the interface. Such types play a fundamental role in object-oriented programming.
The information which is missing in the interface may be declared in its implementation, or in another ""friends-only"" interface. This second option allows the hidden information to be shared by two or more modules.


== See also ==
Abstract data type
Black box
Forward declaration
Information hiding


== References =="
322,Master of IT in Business,54301620,4225,"The Master of Business Administration (MBA or M.B.A.) is a master's degree in business administration (management). The MBA degree originated in the United States in the early 20th century when the country industrialized and companies sought scientific approaches to management. The core courses in an MBA program cover various areas of business such as accounting, applied statistics, business communication, business ethics, business law, finance, managerial economics, management, marketing and operations in a manner most relevant to management analysis and strategy.
Most programs also include elective courses and concentrations for further study in a particular area, for example accounting, finance, and marketing. MBA programs in the United States typically require completing about sixty credits, nearly twice the number of credits typically required for degrees that cover some of the same material such as the Master of Economics, Master of Finance, Master of Accountancy, Master of Science in Marketing and Master of Science in Management.
The MBA is a terminal degree and a professional degree. Accreditation bodies specifically for MBA programs ensure consistency and quality of education. Business schools in many countries offer programs tailored to full-time, part-time, executive (abridged coursework typically occurring on nights or weekends) and distance learning students, many with specialized concentrations.


== History ==
The first school of business in the United States was The Wharton School of the University of Pennsylvania established in 1881 through a donation from Joseph Wharton [4]. In 1900, the Tuck School of Business was founded at Dartmouth College conferring the first advanced degree in business, specifically, a Master of Science in Commerce, the predecessor to the MBA.
The Harvard Graduate School of Business Administration established the first MBA program in 1908, with 15 faculty members, 33 regular students and 47 special students. Its first-year curriculum was based on Frederick Winslow Taylor's scientific management. The number of MBA students at Harvard increased quickly, from 80 in 1908, over 300 in 1920, and 1,070 in 1930. At this time, only American universities offered MBAs. Other countries preferred that people learn business on the job.
Other milestones include:
1930: First management and leadership education program for executives and mid-career experienced managers (the Sloan Fellows Program at the Massachusetts Institute of Technology).
1943: First Executive MBA (EMBA) program for working professionals at the University of Chicago Booth School of Business. Chicago was also the first business school to establish permanent campuses on three continents in Chicago (USA), Barcelona (Europe), and Singapore (Asia). Most business schools today offer a global component to their executive MBA. Since the program was established, the school has moved its campuses and is now based in Chicago, London, and Hong Kong.
1946: First MBA focused on global management at Thunderbird School of Global Management.
1950: First MBA outside of the United States, in Canada (Richard Ivey School of Business at The University of Western Ontario), followed by the University of Pretoria in South Africa in 1951.
1955: First MBA offered at an Asian school at the Institute of Business Administration Karachi at the University of Karachi in Pakistan, in collaboration with the Wharton School of the University of Pennsylvania.
1957: First MBA offered at a European school (INSEAD).
1963: First MBA offered in Korea by Korea University Business School (KUBS).
1986: First MBA program requiring every student to have a laptop computer in the classroom at the Roy E. Crummer Graduate School of Business at Rollins College (Florida). Beginning with the 1992–1993 academic year, Columbia Business School required all incoming students to purchase a laptop computer with standard software, becoming the first business school to do so.
1994: First online executive MBA program at Athabasca University (Canada).
The MBA degree has been adopted by universities worldwide in both developed and developing countries.


== Accreditation ==


=== United States ===
Business school or MBA program accreditation by external agencies provides students and employers with an independent view of the school or program's quality, as well as whether the curriculum meets specific quality standards. The three major accrediting bodies in the United States are:
Association to Advance Collegiate Schools of Business (AACSB),
Accreditation Council for Business Schools and Programs (ACBSP), and
International Assembly for Collegiate Business Education (IACBE).
All of these groups also accredit schools outside the US. The ACBSP and the IACBE are themselves recognized in the United States by the Council for Higher Education Accreditation (CHEA). MBA programs with specializations for students pursuing careers in healthcare management also eligible for accreditation by the Commission on the Accreditation of Healthcare Management Education (CAHME).
US MBA programs may also be accredited at the institutional level. Bodies that accredit institutions as a whole include:
Middle States Association of Colleges and Schools (MSA),
New England Association of Schools and Colleges (NEASC),
Higher Learning Commission(HLC),
Northwest Commission on Colleges and Universities (NWCCU),
Southern Association of Colleges and Schools (SACS), and
Western Association of Schools and Colleges (WASC).


=== Other countries ===
Accreditation agencies outside the United States include the Association of MBAs (AMBA), a UK-based organization that accredits MBA, DBA and MBM programs worldwide, government accreditation bodies such as the All India Council for Technical Education (AICTE), which accredits MBA and Postgraduate Diploma in Management (PGDM) programs across India. Some of the leading bodies in India that certify MBA institutions and their programs are the All India Council for Technical Education (AICTE) and the University Grants Commission (UGC). A distance MBA program needs to be accredited by the Distance Education Council (DEC) in India. The Council on Higher Education (CHE) in South Africa, the European Foundation for Management Development operates the European Quality Improvement System (EQUIS) for mostly European, Australian, New Zealand and Asian schools, the Foundation for International Business Administration Accreditation (FIBAA), and Central and East European Management Development Association (CEEMAN) in Europe.


== Programs ==
Two-year (full-time) MBA programs normally take place over two academic years (i.e. approximately 18 months of term time). For example, in the Northern Hemisphere they often begin in late August/September of year one and continue until May of year two, with a three- to four-month summer break in between years one and two. Students enter with a reasonable amount of prior real-world work experience and take classes during weekdays like other university students. A typical full-time, accelerated, part-time, or modular MBA requires 60 credits (600 class hours) of graduate work.

Accelerated MBA programs are a variation of the two-year programs. They involve a higher course load with more intense class and examination schedules and are usually condensed into one year. They usually have less down time during the program and between semesters. For example, there is no three to four-month summer break, and between semesters there might be seven to ten days off rather than three to five weeks vacation. Accelerated programs typically have a lower cost than full-time two-year programs.
Part-time MBA programs normally hold classes on weekday evenings after normal working hours, or on weekends. Part-time programs normally last three years or more. The students in these programs typically consist of working professionals, who take a light course load for a longer period of time until the graduation requirements are met.
Evening (second shift) MBA programs are full-time programs that normally hold classes on weekday evenings, after normal working hours, or on weekends for a duration of two years. The students in these programs typically consist of working professionals, who can not leave their work to pursue a full-time regular shift MBA. Most second shift programs are offered at universities in India.
Modular MBA programs are similar to part-time programs, although typically employing a lock-step curriculum with classes packaged together in blocks lasting from one to three weeks.
Executive MBA (EMBA) programs developed to meet the educational needs of managers and executives, allowing students to earn an MBA (or another business-related graduate degree) in two years or less while working full-time. Participants come from every type and size of organization – profit, nonprofit, government – representing a variety of industries. EMBA students typically have a higher level of work experience, often 10 years or more, compared to other MBA students. In response to the increasing number of EMBA programs offered, The Executive MBA Council was formed in 1981 to advance executive education.
Full-time executive MBA programs are a new category of full-time 1 year MBA programs aimed at professionals with approx. 5 years or more. They are primarily offered in countries like India where the 2-year MBA program is targeted at fresh graduates with no experience or minimal experience. These full-time executive MBA programs are similar to 1 year MBA programs offered by schools like Insead and IMD.
Distance learning MBA programs hold classes off-campus. These programs can be offered in a number of different formats: correspondence courses by postal mail or email, non-interactive broadcast video, pre-recorded video, live teleconference or videoconference, offline or online computer courses. Many schools offer these programs.
Blended learning programs combine distance learning with face-to-face instruction. These programs typically target working professionals who are unable to attend traditional part-time programs.
MBA dual degree programs combine an MBA with others (such as an MS, MA, or a JD, etc.) to let students cut costs (dual programs usually cost less than pursuing 2 degrees separately), save time on education and to tailor the business education courses to their needs. This is generally achieved by allowing core courses of one program count as electives in the other. Some business schools offer programs in which students can earn both a bachelor's degree in business administration and an MBA in five years.
Mini-MBA is a term used by many non-profit and for-profit institutions to describe a training regimen focused on the fundamentals of business. In the past, Mini-MBA programs have typically been offered as non-credit bearing courses that require less than 100 hours of total learning. However, due to the criticisms of these certificates, many schools have now shifted their programs to offer courses for full credit so that they may be applied towards a complete traditional MBA degree. This is to allow students to verify business related coursework for employment purposes and still allow the option to complete a full-time MBA degree program at a later period, if they elect to do so.
MOOCs have begun disrupting the MBA program in recent years as a result of increasing supply of MBA graduates around the world. 


== Admissions criteria ==
Many programs base their admission decisions on a combination of undergraduate grade point average, academic transcripts, entrance exam scores, a résumé containing significant work experience, essays, letters of recommendation, and personal interviews. Some schools are also interested in extracurricular activities, community service activities or volunteer work, and how the student can improve the school's diversity and contribute to the student body as a whole.
The Graduate Management Admission Test (GMAT) is the most prominently used entrance exam for admissions into MBA programs. The Graduate Record Examination (GRE) is also accepted by almost all MBA programs in order to fulfill any entrance exam requirement they may have. Some schools do not weigh entrance exam scores as heavily as other criteria, and some programs do not require entrance exam scores for admission. In order to achieve a diverse class, business schools also consider the target male-female ratio and local-international student ratios. In rare cases, some MBA degrees do not require students to have an undergraduate degree and will accept significant management experience in lieu of an undergraduate degree. In the UK, for example an HND or even HNC is acceptable in some programs.
Depending on the program, type and duration of work experience can be a critical admissions component for many MBA programs. Many top-tier programs require five or more years of work experience for admission.
MBA admissions consulting services exist to counsel MBA applicants to improve their chances of getting admission to their desired Business Schools. These services range from evaluating a candidate's profile, GMAT preparation, suggesting the schools to which they can apply, writing and editing essay, conducting mock interviews as preparation for MBA admission interviews, as well as post-MBA career counseling.


== Content ==
In general, MBA programs are structured around core courses (an essentially standard curriculum) and elective courses that (may) allow for a subject specialty or concentration. Thus, in the program's first year (or part), students acquire both a working knowledge of management functions and the analytical skills required for these, while in the second year (part), students pursue elective courses, which may count towards a specialization. (Topics in business ethics may be included at the generalist or specialist level.) After the first year, many full-time students seek internships. The degree culminates with coursework in business strategy, the program capstone. A dissertation or major project is usually a degree requirement after the completion of coursework. Many MBA programs end with a comprehensive exit examination; see below.
For Executive MBA programs, the core curriculum is generally similar, but may seek to leverage the strengths associated with the more seasoned and professional profile of the student body, emphasizing leadership, and drawing more from the specific experience of the individual students.
Programs are designed such that students gain exposure to theory and practice alike. Courses therefore include lectures, case studies, and team projects; the mix though, will differ by school and by format. Theory is covered in the classroom setting by academic faculty, and is reinforced through the case method, placing the student in the role of the decision maker. Similar to real world business situations, cases include both constraints and incomplete information. Practical learning (field immersion) often comprises consulting projects with real clients, and is generally undertaken in teams (or ""syndicates""). The practical elements (as well as the case studies) often involve external practitioners—sometimes business executives—supporting the teaching from academic faculty.
As above, courses begin with underlying topics and then progress to more advanced functional topics where these are applied; see aside.
The underlying analytic skills required for management are usually covered initially. The accounting course(s) may treat financial and management accounting separately or in one hybrid course. Financial accounting deals mainly in the preparation of financial statements while management accounting deals mainly in analysis of internal results. Managerial economics is a technical course that mainly focuses on product pricing as influenced by many micro-economic theories and principals, while the aggregate or macro-economics course deals with topics like the banking system, the money supply, and inflation. Operations Research and statistics are sometimes combined as ""Managerial Decision-Making"" or ""Quantitative Decision-Making""; organizational behavior and human resource management may similarly be combined. In many programs, applicants with appropriate background may be exempt from various of the analytical courses.
As regards the functional courses, some programs treat the curricula here in two parts: the first course provides an overview, while the second revisits the subject in depth (perhaps as specializations); alternatively, the first addresses short-term, tactical problems, while the second addresses long-term, strategic problems (e.g., ""Financial Management I"" might cover working capital management, while part II covers capital investment decisions). An Information systems / technology course is increasingly included as a core functional course rather than an elective. Ethics training is often delivered with coursework in corporate social responsibility and corporate governance. Note that courses here, although technical in content are, ultimately, oriented toward corporate management. (For example, the principal finance course may cover the technicalities of financial instrument valuation and capital raising, but is in fact focused on managerial- and corporate finance.) Technically-oriented courses, if offered, will be via a specialization.
Programs may also include (coursework-based) training in the skills needed at senior levels of management: soft skills, such as (general) leadership and negotiation; hard skills, such as spreadsheets and project management; thinking skills such as innovation and creativity. Training in areas such as multiculturalism and corporate social responsibility is similarly included. Company visits (including overseas travel), and guest lectures or seminars with CEOs and management personalities may also be included. These, with the core subjects, provide the graduate with breadth, while the specialty courses provide depth.
For the business strategy component, the degree capstone, the focus is on finding competitive advantage and the long-term positioning and management of the entity as a whole. Here, the key functional areas are thus synthesized or integrated into an overall view and the strategy course depicts how the various sub-disciplines integrate to tell one continuous story with each discipline providing depth of understanding in the others. Corresponding training in business leadership may also be scheduled and participation in a business simulation or game is also a common degree requirement. ""Strategy"" may be offered as a sequence of courses, beginning in the first part (planning) and culminating in the second (execution), or as a single intensive course, offered during the second part. Some programs offer a specialization in ""strategy"", others in management consulting which substantially addresses the same issues.
The MBA dissertation (or thesis in some universities) will, in general, comprise the following in some combination: a discussion of the literature, providing a critical review and structuring of what is known on a given topic, with a view to addressing a specific problem; a case study that goes beyond simple description, containing the analysis of hitherto unpublished material; a test of the application or limitations of some known principle or technique in a particular situation, and / or suggested modifications. As an alternative to the dissertation, some programs instead allow for a major project. Here (part-time) students will address a problem current in their organization; particularly in programs with an action learning orientation, these may be practically oriented. Most MBA programs require additional course work in research methodology, preceding the dissertation or project. Some programs allow that the research component as a whole may be substituted with additional elective coursework.


=== Exit examination ===
Many MBA programs culminate in a comprehensive exit examination. The national standardized exam known as the Major Field Test for MBAs (MFT-MBA) has been administered in the MBA programs of over 300 U.S. universities. The MFT-MBA aims to assess skills, knowledge, and reasoning ability within the domain of standard MBA curriculum. It is administered by Educational Testing Service. Another prominent option for comprehensive exit exams is the Common Professional Component Comprehensive Exam for MBAs (CPC COMP Exam for MBAs) owned by Peregrine Academic Services. Many programs choose to administer their own in-house exam rather than a standardized test.


== Careers ==
An MBA prepares individuals for many types of careers. According to a survey by the Graduate Management Admissions Council, 64% MBA graduates of year 2012 used their MBA to change careers. Some of the more common jobs an MBA prepares one for include:
Business analyst or strategist
Business development analyst, associate, or manager
Director (of a department)
Entrepreneur/founder
Financial analyst
Management consultant
Marketing associate, analyst, or manager
Portfolio manager
Project, product, or program manager
Operations analyst, associate, or manager


== Europe ==


=== History ===
In 1957, INSEAD (French name ""Institut Européen d'Administration des Affaires"", or European Institute of Business Administration) became the first European university offering the MBA degree, followed by EDHEC Business School in 1959 and ICADE in 1960 (who had started offering in 1956 a ""Technical Seminary for Business Administration""), ESADE and IESE Business School (first two-year program in Europe) in 1964, UCD Smurfit Business School and Cranfield School of Management in 1964, Manchester Business School and London Business School in 1965, The University of Dublin (Trinity College), the Rotterdam School of Management in 1966, the Vlerick Business School in 1968 and in 1969 by the HEC School of Management (in French, the École des Hautes Études Commerciales) and the Institut d'Etudes Politiques de Paris. In 1972, Swiss business school IMEDE (now IMD) began offering a full-time MBA program, followed by IE Business School (in Spanish, Instituto de Empresas) in 1973, and AGH University of Science and Technology in Cracow, Poland in 1974. In 1991, IEDC-Bled School of Management became the first school in the ex-socialist block of the Central and Eastern to offer an MBA degree.


=== Bologna Accord ===
In Europe, the recent Bologna Accord established uniformity in three levels of higher education: Bachelor (three or four years), Masters (one or two years, in addition to three or four years for a Bachelor), and Doctorate (an additional three or four years after a Master). Students can acquire professional experience after their initial bachelor's degree at any European institution and later complete their masters in any other European institution via the European Credit Transfer and Accumulation System.


=== Accreditation standards ===
Accreditation standards are not uniform in Europe. Some countries have legal requirements for accreditation (e.g. most German states), in some there is a legal requirement only for universities of a certain type (e.g. Austria), and others have no accreditation law at all. Even where there is no legal requirement, many business schools are accredited by independent bodies voluntarily to ensure quality standards.


=== Austria ===
In Austria, MBA programs of private universities have to be accredited by the Austrian Accreditation Council (Österreichischer Akkreditierungsrat). State-run universities have no accreditation requirements, however, some of them voluntarily undergo accreditation procedures by independent bodies. There are also MBA programs of non-academic business schools, who are entitled by the Austrian government to offer these programs until the end of 2012 (Lehrgang universitären Charakters). Some non-academic institutions cooperate with state-run universities to ensure legality of their degrees.


=== Czech Republic ===
January 1999 saw the first meeting of the Association of the Czech MBA Schools (CAMBAS). The association is housed within the Centre for Doctoral and Managerial Studies of UEP, Prague. All of the founding members of the association to have their MBA programs accredited by partner institutions in the United Kingdom or United States of America.


=== France and French speaking countries ===
In France and in the Francophone countries such as Switzerland, Monaco, Belgium, and Canada, the MBA degree programs at the public accredited schools are similar to those offered in the Anglo-Saxon countries. Most French Business Schools are accredited by the Conférence des Grandes Écoles, which is an association of higher educational establishments outside the mainstream framework of the public education system.


=== Germany ===
Germany was one of the last Western countries to adopt the MBA degree. In 1998, the Hochschulrahmengesetz (Higher Education Framework Act), a German federal law regulating higher education including the types of degrees offered, was modified to permit German universities to offer master's degrees. The traditional German degree in business administration was the Diplom in Betriebswirtschaft (Diplom-Kaufmann) but since 1999, bachelor's and master's degrees have gradually replaced the traditional degrees due to the Bologna process. Today most German business schools offer the MBA. Most German states require that MBA degrees have to be accredited by one of the six agencies officially recognized by the German Akkreditierungsrat (accreditation council), the German counterpart to the American CHEA. The busiest of these six agencies (in respect to MBA degrees) is the Foundation for International Business Administration Accreditation (FIBAA). All universities themselves have to be institutionally accredited by the state (staatlich anerkannt).


=== Italy ===
Italian MBAs programs at public accredited schools are similar to those offered elsewhere in Europe. Italian Business Schools are accredited by EQUIS and by ASFOR.


=== Poland ===
There are several MBA programs offered in Poland. Some of these are run as partnerships with American or Canadian Universities. Others rely on their own faculty and enrich their courses by inviting visiting lecturers. Several MBA programs in Poland are also offered in English.


=== Portugal ===
Several business schools offer highly ranked MBA programs in Portugal. Portuguese MBA programs are increasingly internationally oriented, being taught in English.


=== Spain ===
Spain has a long history in offering MBA programs with three MBA programs frequently being ranked in the Top 25 worldwide by several international rankings. Spanish MBAs are culturally diverse and taught in English.


=== Switzerland ===
There are several schools in Switzerland that offer an MBA as full-time, part-time and executive education programs. Some business schools that offer MBA programs with specializations such as Finance and Healthcare, technology management, and others. As a country with four different national languages (German, French, Italian and Romansh), Switzerland offers most of its programs in English to attract international students to the country.


=== Ukraine ===
Recently MBA programs appeared in Ukraine where there are now about twenty schools of business offering a variety of MBA programs. Three of these are subsidiaries of European schools of business, while the remaining institutions are independent. Ukrainian MBA programs are concentrated mainly on particulars of business and management in Ukraine. For example, 2/3 of all case studies are based on real conditions of Ukrainian companies.


=== United Kingdom ===
The UK-based Association of MBAs (AMBA) was established in 1967 and is an active advocate for MBA degrees. The association's accreditation service is internationally recognised for all MBA, DBA and Masters in Business and Management (MBM) programs. AMBA also offer the only professional membership association for MBA students and graduates. UK MBA programs typically consist of a set number of taught courses plus a dissertation or project.


== Africa ==
The Financial Times in its Executive Education Rankings for 2012 included 5 African business schools.


=== Nigeria ===
Business schools administered as colleges within the traditional universities offer a variety of MBA programs. In addition, a few standalone business schools allied with foreign business schools exist.


=== South Africa ===

In 2004 South Africa's Council on Higher Education (CHE) completed an extensive re-accreditation of MBA degrees offered in the country.


=== Ghana ===
Business schools of the traditional universities run a variety of MBA programs. In addition, foreign accredited institutions offer MBA degrees by distance learning in Ghana.


=== Kenya ===

MBA programs are offered in many public and private universities.
Students choose to specialize in one of the following areas: Accounting, Finance, Entrepreneurship, Insurance and Human Resources. The course takes 4 semesters of about 4 months each.


== Asia-Pacific ==

International MBA programs are acquiring brand value in Asia. For example, while a foreign MBA is still preferred in the Philippines, many students are now studying at one of many ""Global MBA"" English language programs being offered. English-only MBA programs are also offered in Hong Kong, Indonesia, Malaysia, Singapore, South Korea, Taiwan, and Thailand. For international students who want a different experience, many Asian programs offer scholarships and discounted tuition to encourage an international environment in the classroom.
Rankings have been published for Asia Pacific schools by the magazine Asia Inc. which is a regional business magazine with distribution worldwide. The importance of MBA education in China has risen, too.


=== Bangladesh ===
Bangladesh was one of the first countries in Asia to offer MBA degree. There are now more than 50 business schools in Bangladesh offering the MBA, predominantly targeting graduates without any work experience. Most MBAs are two years full-time. There is little use of GMAT. The Business Schools conduct their own admission tests instead. Classes are taught in English.


=== India ===
There are many business schools and colleges in India offering two-year MBA or PGDM programs accredited by AICTE or UGC.
The Indian Institutes of Management are among the world's most selective schools according to Bloomberg magazine. They offer a post graduate degree in management. There are 20 IIMs in total, 12 of which were established after the year 2010.


=== Malaysia ===
Malaysia is one of the pioneer country in South East Asia to offer MBA programs. Both public and private universities offers MBA degrees. Most MBAs are in full-time mode and part-time mode. All MBA degrees are conducted in English.


=== Singapore ===
Singapore is South East Asia's leading financial hub. Its competitive educational system starts from primary schools to universities and eventually post-graduate studies such as EMBA programs.


=== Japan ===
In Japan 2 business schools offer the accredited MBA degree (AACSB, AMBA or EQUIS). The concept of an MBA is still not considered mainstream as traditional companies still perceive that knowledge and learning with respect to business and management can only be effectively gained through experience and not within a classroom. In fact, some companies have been known to place recent MBA recipients in unrelated fields, or try to re-acclimate their Japanese employees who have spent years overseas earning the degree. As a consequence, academic institutions in Japan are attempting to reinvent the perception of the MBA degree, by taking into account the local corporate culture.


=== Pakistan ===
Pakistan first offered an MBA program outside the United States in 1955 in collaboration with the University of Pennsylvania. Now in Pakistan, there are 187 Universities/Institutes which are recognized by the Higher Education Commission of Pakistan, offering MBA programs to students and professionals.


=== Australia ===

In Australia, 42 Australian business schools offer the MBA degree (16 are AACSB, AMBA or EQUIS accredited). Universities differentiate themselves by gaining international accreditation and focusing on national and international rankings. Most MBAs are one to two years full-time. There is little use of GMAT, and instead each educational institution specifies its own requirements, which normally entails several years of management-level work experience as well as proven academic skills.
Graduate Management Association of Australia carries out ratings for Australian MBAs and annually publishes Australian MBA Star Ratings. The Financial Review Boss carries out biennial rankings of Australian MBAs.


=== New Zealand ===
In New Zealand, most universities offer MBA classes, typically through part-time arrangement or evening classes. Only two universities offer full-time programs to international students - University of Otago (Otago MBA) and Auckland University of Technology (AUT). The Otago MBA is the longer established of the two, offering a 240 points program while AUT MBA is a 180-point program.


=== South Korea ===
Korean universities offer full-time and part-time MBA programs that usually consist of a two-year curriculum. The first MBA program was offered in 1963 by Korea University Business School (KUBS). In 2007, the Korean Government established ""BK21,"" a project that supports Korean universities in order to develop their competitiveness in the global MBA market. Korea University Business School topped the evaluation of BK21 professional business graduate schools for six consecutive years. In the meantime, only two universities in Korea ranked in the ""2015 Global Top 100 Executive MBA (EMBA) Rankings"" conducted by UK Financial Times (Korea University Business School and Yonsei University ranked 27th and 45th worldwide, respectively).


== Program rankings ==

Since 1967, publications have ranked MBA programs using various methods. The Gourman Report (1967–1997) did not disclose criteria or ranking methods, and these reports were criticized for reporting statistically impossible data, such as no ties among schools, narrow gaps in scores with no variation in gap widths, and ranks of nonexistent departments. In 1977 The Carter Report ranked MBA programs based on the number of academic articles published by faculty, the Ladd & Lipset Survey ranked business schools based on faculty surveys, and MBA Magazine ranked schools based on votes cast by business school deans.
Today, publications by the Aspen Institute, Business Week, The Economist, Financial Times, Forbes, Quacquarelli Symonds, US News & World Report, University of Texas at Dallas, and the Wall Street Journal make their own rankings of MBA programs. Schools' ranks can vary across publications, as the methodologies for rankings differ among publications:
The Aspen Institute publishes the Beyond Grey Pinstripes rankings which are based on the integration of social and environmental stewardship into university curriculum and faculty research. Rankings are calculated on the amount of sustainability coursework made available to students (20%), amount of student exposure to relevant material (25%), amount of coursework focused on stewardship by for-profit corporations (30%), and relevant faculty research (25%). The 2011 survey and ranking include data from 150 universities.
Business Week's rankings are based on student surveys, a survey of corporate recruiters, and an intellectual capital rating.
The Economist Intelligence Unit, published in The Economist, surveys both business schools (80%) and students and recent graduates (20%). Ranking criteria include GMAT scores, employment and salary statistics, class options, and student body demographics.
Financial Times uses survey responses from alumni who graduated three years prior to the ranking and information from business schools. Salary and employment statistics are weighted heavily.
Forbes considers only the return of investment five years after graduation. MBA alumni are asked about their salary, the tuition fees of their MBA program and other direct costs as well as opportunity costs involved. Based on this data, a final ""5-year gain"" is calculated and determines the MBA ranking position.
Quacquarelli Symonds QS Global 200 Business Schools Report compiles regional rankings of business schools around the world. Ranks are calculated using a two-year moving average of points assigned by employers who hire MBA graduates.
U.S. News & World Report incorporates responses from deans, program directors, and senior faculty about the academic quality of their programs as well as the opinions of hiring professionals. The ranking is calculated through a weighted formula of quality assessment (40%), placement success (35%), and student selectivity (25%).
UT-Dallas Top 100 Business School Research Rankings ranks business schools on the research faculty publish, similar to The Carter Report of the past.
The Wall Street Journal, which stopped ranking full-time MBA programs in 2007, based its rankings on skill and behavioral development that may predict career success, such as social skills, teamwork orientation, ethics, and analytic and problem-solving abilities.
The ranking of MBA programs has been discussed in articles and on academic websites. Critics of ranking methodologies maintain that any published rankings should be viewed with caution for the following reasons:
Rankings exhibit intentional selection bias as they limit the surveyed population to a small number of MBA programs and ignore the majority of schools, many with excellent offerings.
Ranking methods may be subject to personal biases and statistically flawed methodologies (especially methods relying on subjective interviews of hiring managers, students, or faculty).
Rankings use no objective measures of program quality.
The same list of schools appears in each ranking with some variation in ranks, so a school ranked as number 1 in one list may be number 17 in another list.
Rankings tend to concentrate on representing MBA schools themselves, but some schools offer MBA programs of different qualities and yet the ranking will only rely upon information from the full-time program (e.g., a school may use highly reputable faculty to teach a daytime program, but use adjunct faculty in its evening program or have drastically lower admissions criteria for its evening program than for its daytime program).
A high rank in a national publication tends to become a self-fulfilling prophecy.
Some leading business schools including Harvard, INSEAD, Wharton and Sloan provide limited cooperation with certain ranking publications due to their perception that rankings are misused.
One study found that ranking MBA programs by a combination of graduates' starting salaries and average student GMAT score can approximately duplicate the top 20 list of the national publications, and concluded that a truly objective ranking would use objective measures of program quality and be individualized to the needs of each prospective student. National publications have recognized the value of rankings against different criteria, and now offer lists ranked different ways: by salary, GMAT score of students, selectivity, and so forth. While useful, these rankings have yet to meet the critique that rankings are not tailored to individual needs, that they use an incomplete population of schools, may fail to distinguish between the different MBA program types offered by each school, or rely on subjective interviews.


== Criticism ==
The media raised questions about the value and content of business school programs after the financial crisis of 2007–2010. In general, graduates had reportedly tended to go into finance after receiving their degrees. As financial professionals are widely seen as responsible for the global economic meltdown, anecdotal evidence suggests new graduates are choosing different career paths.
Deans at top business schools have acknowledged that media and public perception of the MBA degree shifted as a result of the financial crisis. Articles have been written about public perceptions of the crisis, ranging from schools' acknowledgment of issues with the training students receive to criticisms of the MBA's role in society.


== See also ==

Outline of business management
Bachelor of Business Administration


=== Related graduate business degrees ===
see: Business education#Postgraduate education.
Master of Accountancy (MAcc or MAcy) / Master of Professional Accountancy (MPA, or MPAcc), a postgraduate degree in accounting
Master of Commerce (MCom or MComm), a postgraduate business degree usually focused on a particular area
Master of Economics (M.Econ./M.Ec.)
Master of Enterprise (MEnt), a postgraduate, technology & enterprise-based qualification
Master of Bioscience Enterprise (MBioEnt), a postgraduate degree focused on the commercialization of biotechnology
Master of Finance (MFin), a postgraduate degree in finance
Master of Health Administration (MHA), a postgraduate health administration degree
Master of International Business (MIB), a postgraduate degree focused on International Business
Master of Management (MM), a postgraduate business degree
Master of Science in Management, a postgraduate business degree
Master of Marketing Research (MMR) a postgraduate degree focusing on research in the field of marketing
Master of Nonprofit Organizations (MNO or MNPO), the postgraduate degree for philanthropy and voluntary sector professionals
Master of Public Administration (MPA), a postgraduate public administration degree
Master of Social Science (MSS), a postgraduate degree
Master of Project Management (MSPM or MPM), a postgraduate project management degree
Masters of Management: Co-operatives and Credit Unions, a post-graduate degree for co-operative and credit union managers
Master in Sustainable Business (MSB)
Master of Real Estate (MScRE), a postgraduate degree focusing on real estate.
Master of Information Management (MIM), a postgraduate degree focusing on information management.


==== Executive ====
Executive Master of Science in Business Administration (Executive MScBA), a postgraduate degree focusing advanced-level conceptual foundation in a student's chosen field such as operational excellence in the biotech/pharma industry.


==== Doctoral ====
Doctor of Business Administration (DBA), a doctorate in business administration
Doctor of Management (D.M.)
PhD in Management (PhD), a business doctoral degree
D.Phil in Management (D.Phil), a doctorate in business
Engineering Doctorate (EngD), A professional doctorate involving a management thesis and taught MBA courses in the UK


== Further reading ==
Patterson, Sarah E.; Damaske, Sarah; Sheroff, Christen (June 2017). ""Gender and the MBA: differences in career trajectories, institutional support, and outcomes"". Gender & Society. Sage. 31 (3): 310–332. doi:10.1177/0891243217703630. 


== References ==


== External links =="
323,Agent systems reference model,14273658,4223,"The agent systems reference model (ASRM) is a layered, abstract description for multiagent systems. As such, the reference model
provides a taxonomy of terms, concepts and definitions to compare agent systems;
identifies functional elements that are common in agent systems;
captures data flow and dependencies among the functional elements in agent systems; and
specifies assumptions and requirements regarding the dependencies among these elements.
The ASRM differentiates itself from technical standards, such as Knowledge Interchange Format, Knowledge Query and Manipulation Language, and those of the Foundation for Intelligent Physical Agents in that it defines the required existence of components of a multiagent system; standards prescribe how they are designed.


== Technical approach ==
The ASRM was technically constructed through forensic software analysis of existing agent-based systems. Such fielded systems include JaDE, Cougaar, EMAA, NOMADS, Retsina, A-Globe, among others. In so doing, through empirical evidence, the ASRM motivates its functional breakdown of agent-based systems.


== Description of the ASRM layers ==


== History ==
The ASRM was started in July 2005, with the first draft having been completed in November 2006. Contributors to the document have included Drexel University, Cougaar Software, Global InfoTek (see also: CoABS), Soar Technology (see also: Soar), Penn State University, University of Southern California, University of South Carolina, the Florida Institute for Human and Machine Cognition, University of West Florida, BBN Technologies, Telcordia, Lockheed Martin, General Dynamics and others.


== See also ==
4D-RCS Reference Model Architecture
Agent based model
Artificial Intelligence
Complex systems
Distributed artificial intelligence
Intelligent agent
Multiagent system
Reference model
Software agent


== Further reading ==
Version 1.0a of the ASRM
The Case for a Reference Model for Agent-Based Systems. Pragnesh Jay Modi, William C. Regli and Israel Mayk. In Proceedings of the IEEE Workshop on Distributed Intelligent Systems: Collective Intelligence and Its Applications. June, 2006. Pages 321–325.
Regli, W. C., Mayk, I., Dugan, C. J., Kopena, J. B., Lass, R. N., Modi, P. J., Mongan, W. M., Salvage, J. K., and Sultanik, E. A. ""Development and specification of a reference model for agent-based systems."" IEEE Trans. Sys. Man Cyber Part C 39, 5 (Sep. 2009), 572–596.[1]"
324,Cluster state,14241236,4207,"In quantum information and quantum computing, a cluster state is a type of highly entangled state of multiple qubits. Cluster states are generated in lattices of qubits with Ising type interactions. A cluster C is a connected subset of a d-dimensional lattice, and a cluster state is a pure state of the qubits located on C. They are different from other types of entangled states such as GHZ states or W states in that it is more difficult to eliminate quantum entanglement (via projective measurements) in the case of cluster states. Another way of thinking of cluster states is as a particular instance of graph states, where the underlying graph is a connected subset of a d-dimensional lattice. Cluster states are especially useful in the context of the one-way quantum computer. For a comprehensible introduction to the topic see.
Formally, cluster states 
  
    
      
        
          |
        
        
          ϕ
          
            {
            κ
            }
          
        
        
          ⟩
          
            C
          
        
      
    
    {\displaystyle |\phi _{\{\kappa \}}\rangle _{C}}
   are states which obey the set eigenvalue equations:

  
    
      
        
          K
          
            (
            a
            )
          
        
        
          
            
              |
              
                ϕ
                
                  {
                  κ
                  }
                
              
              ⟩
            
            
              C
            
          
        
        =
        (
        −
        1
        
          )
          
            
              κ
              
                a
              
            
          
        
        
          
            
              |
              
                ϕ
                
                  {
                  κ
                  }
                
              
              ⟩
            
            
              C
            
          
        
      
    
    {\displaystyle K^{(a)}{\left|\phi _{\{\kappa \}}\right\rangle _{C}}=(-1)^{\kappa _{a}}{\left|\phi _{\{\kappa \}}\right\rangle _{C}}}
  
where 
  
    
      
        
          K
          
            (
            a
            )
          
        
      
    
    {\displaystyle K^{(a)}}
   are the correlation operators

  
    
      
        
          K
          
            (
            a
            )
          
        
        =
        
          σ
          
            x
          
          
            (
            a
            )
          
        
        
          ⨂
          
            b
            ∈
            
              N
            
            (
            a
            )
          
        
        
          σ
          
            z
          
          
            (
            b
            )
          
        
      
    
    {\displaystyle K^{(a)}=\sigma _{x}^{(a)}\bigotimes _{b\in \mathrm {N} (a)}\sigma _{z}^{(b)}}
  
with 
  
    
      
        
          σ
          
            x
          
        
      
    
    {\displaystyle \sigma _{x}}
   and 
  
    
      
        
          σ
          
            z
          
        
      
    
    {\displaystyle \sigma _{z}}
   being Pauli matrices, 
  
    
      
        N
        (
        a
        )
      
    
    {\displaystyle N(a)}
   denoting the neighbourhood of 
  
    
      
        a
      
    
    {\displaystyle a}
   and 
  
    
      
        {
        
          κ
          
            a
          
        
        ∈
        {
        0
        ,
        1
        }
        
          |
        
        a
        ∈
        C
        }
      
    
    {\displaystyle \{\kappa _{a}\in \{0,1\}|a\in C\}}
   being a set of binary parameters specifying the particular instance of a cluster state.
Cluster states have been realized experimentally. They have been obtained in photonic experiments using parametric downconversion  . They have been created also in optical lattices of cold atoms .


== See also ==
Bell state
Graph state


== References =="
325,ACM Eugene L. Lawler Award,40418729,4201,"The ACM Eugene L. Lawler Award is awarded every two or three years by the Association for Computing Machinery to an individual or a group of individuals who have made a significant contribution to the use of information technology for humanitarian purposes in a wide range of social domains. It is named after the computer scientist Eugene Lawler. The award includes a financial reward of US $ 5,000.


== Recipients ==


== References =="
326,Boolean hierarchy,36026354,4183,"The boolean hierarchy is the hierarchy of boolean combinations (intersection, union and complementation) of NP sets. Equivalently, the boolean hierarchy can be described as the class of boolean circuits over NP predicates. A collapse of the boolean hierarchy would imply a collapse of the polynomial hierarchy.


== Formal definition ==
BH is defined as follows:
BH1 is NP.
BH2k is the class of languages which are the intersection of a language in BH2k-1 and a language in coNP.
BH2k+1 is the class of languages which are the union of a language in BH2k and a language in NP.
BH is the union of the BHi


== Derived classes ==
DP (Difference Polynomial Time) is BH2.


== Equivalent definitions ==
Defining the conjunction and the disjunction of classes as follows allows for more compact definitions. The conjunction of two classes contains the languages that are the intersection of a language of the first class and a language of the second class. Disjunction is defined in a similar way with the union in place of the intersection.
C ∧ D = { A ∩ B | A ∈ C   B ∈ D }
C ∨ D = { A ∪ B | A ∈ C   B ∈ D }
According to this definition, DP = NP ∧ coNP. The other classes of the Boolean hierarchy can be defined as follows.

  
    
      
        
          
            
              B
              H
            
          
          
            2
            k
          
        
        =
        
          
            c
            o
            N
            P
          
        
        ∧
        
          
            
              B
              H
            
          
          
            2
            k
            −
            1
          
        
      
    
    {\displaystyle {\mathsf {BH}}_{2k}={\mathsf {coNP}}\wedge {\mathsf {BH}}_{2k-1}}
  

  
    
      
        
          
            
              B
              H
            
          
          
            2
            k
            +
            1
          
        
        =
        
          
            N
            P
          
        
        ∨
        
          
            
              B
              H
            
          
          
            2
            k
          
        
      
    
    {\displaystyle {\mathsf {BH}}_{2k+1}={\mathsf {NP}}\vee {\mathsf {BH}}_{2k}}
  
The following equalities can be used as alternative definitions of the classes of the Boolean hierarchy:

  
    
      
        
          
            
              B
              H
            
          
          
            2
            k
          
        
        =
        
          ⋁
          
            i
            =
            1
          
          
            k
          
        
        
          
            D
            P
          
        
      
    
    {\displaystyle {\mathsf {BH}}_{2k}=\bigvee _{i=1}^{k}{\mathsf {DP}}}
  

  
    
      
        
          
            
              B
              H
            
          
          
            2
            k
            +
            1
          
        
        =
        
          
            N
            P
          
        
        ∨
        
          ⋁
          
            i
            =
            1
          
          
            k
          
        
        
          
            D
            P
          
        
      
    
    {\displaystyle {\mathsf {BH}}_{2k+1}={\mathsf {NP}}\vee \bigvee _{i=1}^{k}{\mathsf {DP}}}
  
Alternatively, for every k ≥ 3:

  
    
      
        
          
            
              B
              H
            
          
          
            k
          
        
        =
        
          
            D
            P
          
        
        ∨
        
          
            
              B
              H
            
          
          
            k
            −
            2
          
        
      
    
    {\displaystyle {\mathsf {BH}}_{k}={\mathsf {DP}}\vee {\mathsf {BH}}_{k-2}}
  


== Hardness ==
Hardness for classes of the Boolean hierarchy can be proved by showing a reduction from a number of instances of an arbitrary NP-complete problem A. In particular, given a sequence {x1, ... xm} of instances of A such that xi ∈ A implies xi-1 ∈ A, a reduction is required that produces an instance y such that y ∈ B if and only if the number of xi ∈ A is odd or even:
BH2k-hardness is proved if 
  
    
      
        m
        =
        2
        k
      
    
    {\displaystyle m=2k}
   and the number of xi ∈ A is odd
BH2k+1-hardness is proved if 
  
    
      
        m
        =
        2
        k
        +
        1
      
    
    {\displaystyle m=2k+1}
   and the number of xi ∈ A is even
Such reductions work for every fixed k. If such reductions exist for arbitrary k, the problem is hard for PNP[O(log n)].


== References =="
327,École des technologies numériques appliquées,35709968,4152,"The École des technologies numériques appliquées (ETNA) is a French private university in computer science localized at Ivry-sur-Seine. The school is part of IONIS Education Group. The degrees delivered by the university are recognized by the French state · .
The university is from the same group as EPITA and EPITECH. The partnership ETNA/EPITA/EPITECH allows students to use the equipment to the two other universities. ETNA graduates can proceed to others courses at the IONIS School of Technology and Management or a MBA at the Institut supérieur de gestion. Students of ETNA can prepare major certifications (Microsoft, Cisco Systems, Oracle).


== Curriculum ==
The course is a three-year dual education system and includes 200 hours of lessons and 400 hours of practical cases study per year. Two specializations are available : software development or system and computer network.
Students choose one of these two in the second year.
The main lessons of the first year of study offers the following subjects: management, database, IPv4 network, web technologies, system administration, PHP / web, culture computer, English, mathematics and method. The second year can address the following topics: project, business valuation, English, mathematics, drafting and document analysis, and marketing / business / management. During the last year, the curriculum includes the subjects: project, English, mathematics, drafting and document analysis, marketing / business / management and psychology work.
Since September 2013, the university will start a new program in 2 years after French Baccalauréat which will be entirely free. 250 tickets are available. The innovation is that there are graduate students who will pay for the students in training (250 euros per month during three years).


== External links ==
Official website


== References =="
328,Analytic element method,2289219,4137,"The analytic element method (AEM) is a numerical method used for the solution of partial differential equations. It was initially developed by O.D.L. Strack at the University of Minnesota. It is similar in nature to the boundary element method (BEM), as it does not rely upon discretization of volumes or areas in the modeled system; only internal and external boundaries are discretized. One of the primary distinctions between AEM and BEMs is that the boundary integrals are calculated analytically.
The analytic element method has been applied to problems of groundwater flow governed by a variety of linear partial differential equations including the Laplace, the Poisson equation, the modified Helmholtz equation, the heat equation, and the biharmonic equations.
The basic premise of the analytic element method is that, for linear differential equations, elementary solutions may be superimposed to obtain more complex solutions. A suite of 2D and 3D analytic solutions (""elements"") are available for different governing equations. These elements typically correspond to a discontinuity in the dependent variable or its gradient along a geometric boundary (e.g., point, line, ellipse, circle, sphere, etc.). This discontinuity has a specific functional form (usually a polynomial in 2D) and may be manipulated to satisfy Dirichlet, Neumann, or Robin (mixed) boundary conditions. Each analytic solution is infinite in space and/or time. In addition, each analytic solution contains degrees of freedom (coefficients) that may be calculated to meet prescribed boundary conditions along the element's border. To obtain a global solution (i.e., the correct element coefficients), a system of equations is solved such that the boundary conditions are satisfied along all of the elements (using collocation, least-squares minimization, or a similar approach). Notably, the global solution provides a spatially continuous description of the dependent variable everywhere in the infinite domain, and the governing equation is satisfied everywhere exactly except along the border of the element, where the governing equation is not strictly applicable due to the discontinuity.
The ability to superpose numerous elements in a single solution means that analytical solutions can be realized for arbitrarily complex boundary conditions. That is, models that have complex geometries, straight or curved boundaries, multiple boundaries, transient boundary conditions, multiple aquifer layers, piecewise varying properties and continuously varying properties can be solved. Elements can be implemented using far-field expansions such that model containing many thousands of elements can be solved efficiently to high precision.
A contemporary student of Strack's who is a proponent of the Analytic Element Method (AEM) in groundwater modeling applications is Dr. David Steward of Kansas State University.


== See also ==
Boundary element method


== References ==
Haitjema, H. M. (1995). Analytic element modeling of groundwater flow. San Diego, CA: Academic Press. ISBN 978-0-12-316550-3. 
Strack, O. D. L. (1989). Groundwater Mechanics. Englewood Cliffs, NJ: Prentice Hall. 
Fitts, C. R. (2012). Groundwater Science (2nd ed.). San Diego, CA: Elsevier/Academic Press. ISBN 9780123847058. 


== External links ==
Analytic elements community wiki
Fitts Geolsolutions, AnAqSim (analytic aquifer simulator) and AnAqSimEDU (free) web site"
329,IEEE Richard W. Hamming Medal,12953265,4134,"The IEEE Richard W. Hamming Medal is presented annually to up to three persons, for outstanding achievements in information sciences, information systems and information technology. The recipients receive a gold medal, together with a replica in bronze, a certificate and an honorarium.
The award was established in 1986 by the Institute of Electrical and Electronics Engineers (IEEE) and is sponsored by Qualcomm, Inc. It is named after Richard W. Hamming, whose work has had many implications for computer science and telecommunications. His contributions include the invention of the Hamming code, and error-correcting code.


== Recipients ==
The following people have received the IEEE Richard W. Hamming Medal:


== See also ==
Richard W. Hamming
List of prizes, medals and awards
Prizes named after people


== References =="
330,William C. Carter Award,31691663,4129,"The William C. Carter Award is a technical award presented annually since 1997 for individuals whose graduate dissertation research has made an important contribution to the field of dependable computing. It is named after, and honors, the late William C. Carter, an important figure in the field. The award is sponsored by IEEE Technical Committee on Fault-Tolerant Computing (TC-FTC) and the IFIP Working Group on Dependable Computing and Fault Tolerance (WG 10.4).


== Past recipients ==


== References =="
331,Thomas N. Hibbard,46646966,4122,"Thomas Nathaniel Hibbard (March 14, 1929 – February 11, 2016) was an American mathematician and computer scientist.
Thomas N. Hibbard received the B.S. degree in physics from Pacific University, Forest Grove, OR, in 1951, the M.S. degree in mathematics from the University of Illinois, Urbana, in 1954, and the Ph.D. degree in mathematics from the University of California, Los Angeles, in 1966.
From 1955 to 1958 T. N. Hibbard was a Scientific Programmer at the RAND Corporation, Santa Monica, CA, programming the JOHNNIAC, an early computer built by Rand, and from 1959 to 1965 a member of the research staff of the System Development Corporation, Santa Monica, CA, where he worked with Seymour Ginsburg and Joseph Ullian in automata theory and formal languages. Following a three-year visiting faculty appointment at the Catholic University of Salta, Argentina, he joined the University of Southern California, Los Angeles, as an Assistant Professor of Computer Science in 1970. He conducted research in searching, sorting, and data structures, helping to pioneer the field of analysis of algorithms.[H62][H63] In 1974, he started research with his then faculty colleague Armin B. Cremers, initiating the theory and applications of data spaces.[CH] In February 1976, he joined the staff of the Jet Propulsion Laboratory, Pasadena, CA, working on the Voyager, IRAS and Galileo projects until his retirement from JPL in 1986. At that time, he joined the Information Sciences Institute (ISI), Marina del Rey, CA, and did experimental research on parallel computing until 1989, when he returned to Salta, Argentina, to teach at the National University (UNSA).


== Selected publications ==


== References ==


== External links ==
Publications of Thomas N. Hibbard in the dblp computer science bibliography."
332,Timothy Jurka,50070773,4105,"Timothy Paul Jurka (born September 21, 1988) is a Polish-American computer scientist and political scientist. He is the son of computational biologist Jerzy Jurka.


== Background ==
Jurka is best known for his work on text classification software, including the development of RTextTools and MaxEnt for the R statistical programming language. He currently manages the data teams that build the LinkedIn news feed. Previously, Jurka served as the architect of machine learning algorithms for news recommendations in the Pulse news reading application, which was acquired by LinkedIn in 2013.
Additionally, Jurka has collaborated on numerous projects in political science spanning media framing, civic engagement, and tobacco and immunization policy.


== References =="
333,Hennessy–Milner logic,12809481,4105,"In computer science, Hennessy–Milner logic (HML) is a dynamic logic used to specify properties of a labeled transition system (LTS), a structure similar to an automaton. It was introduced in 1980 by Matthew Hennessy and Robin Milner in their paper ""On observing nondeterminism and concurrency"" (ICALP).
Another variant of the HML involves the use of recursion to extend the expressibility of the logic, and is commonly referred to as 'Hennessy-Milner Logic with recursion'. Recursion is enabled with the use of maximum and minimum fixed points.


== Syntax ==
A formula is defined by the following BNF grammar for Act some set of actions:

  
    
      
        Φ
        ::=
        
          
            tt
          
        
        
        
        
        
          |
        
        
        
        
        
          
            ff
          
        
        
        
        
        
          |
        
        
        
        
        
          Φ
          
            1
          
        
        ∧
        
          Φ
          
            2
          
        
        
        
        
        
          |
        
        
        
        
        
          Φ
          
            1
          
        
        ∨
        
          Φ
          
            2
          
        
        
        
        
        
          |
        
        
        
        
        [
        A
        c
        t
        ]
        Φ
        
        
        
        
          |
        
        
        
        
        ⟨
        A
        c
        t
        ⟩
        Φ
      
    
    {\displaystyle \Phi ::={\textit {tt}}\,\,\,|\,\,\,{\textit {ff}}\,\,\,|\,\,\,\Phi _{1}\land \Phi _{2}\,\,\,|\,\,\,\Phi _{1}\lor \Phi _{2}\,\,\,|\,\,\,[Act]\Phi \,\,\,|\,\,\,\langle Act\rangle \Phi }
  
That is, a formula can be
constant truth 
  
    
      
        
          
            tt
          
        
      
    
    {\displaystyle {\textit {tt}}}
  
always true
constant false 
  
    
      
        
          
            ff
          
        
      
    
    {\displaystyle {\textit {ff}}}
  
always false
formula conjunction
formula disjunction

  
    
      
        
          
            [
            A
            c
            t
            ]
            Φ
          
        
      
    
    {\displaystyle \scriptstyle {[Act]\Phi }}
   formula 
for all Act-derivatives, Φ must hold

  
    
      
        
          
            ⟨
            A
            c
            t
            ⟩
            Φ
          
        
      
    
    {\displaystyle \scriptstyle {\langle Act\rangle \Phi }}
   formula 
for some Act-derivative, Φ must hold


== Formal semantics ==
Let 
  
    
      
        L
        =
        (
        S
        ,
        
          
            A
            c
            t
          
        
        ,
        →
        )
      
    
    {\displaystyle L=(S,{\mathsf {Act}},\rightarrow )}
   be a labeled transition system, and let 
  
    
      
        
          
            H
            M
            L
          
        
      
    
    {\displaystyle {\mathsf {HML}}}
   be the set of HML formulae. The satisfiability relation 
  
    
      
        

        
        ⊨
        

        
        ⊆
        (
        S
        ×
        
          
            H
            M
            L
          
        
        )
      
    
    {\displaystyle {}\models {}\subseteq (S\times {\mathsf {HML}})}
   relates states of the LTS to the formulae they satisfy, and is defined as the smallest relation such that, for all states 
  
    
      
        s
        ∈
        S
      
    
    {\displaystyle s\in S}
   and formulae 
  
    
      
        ϕ
        ,
        
          ϕ
          
            1
          
        
        ,
        
          ϕ
          
            2
          
        
        ∈
        
          
            H
            M
            L
          
        
      
    
    {\displaystyle \phi ,\phi _{1},\phi _{2}\in {\mathsf {HML}}}
  ,

  
    
      
        s
        ⊨
        
          
            tt
          
        
      
    
    {\displaystyle s\models {\textit {tt}}}
  ,
if there exists a state 
  
    
      
        
          s
          ′
        
        ∈
        S
      
    
    {\displaystyle s'\in S}
   such that 
  
    
      
        s
        
          
            →
            
              a
            
          
        
        
          s
          ′
        
      
    
    {\displaystyle s{\xrightarrow {a}}s'}
   and 
  
    
      
        
          s
          ′
        
        ⊨
        ϕ
      
    
    {\displaystyle s'\models \phi }
  , then 
  
    
      
        s
        ⊨
        ⟨
        a
        ⟩
        ϕ
      
    
    {\displaystyle s\models \langle a\rangle \phi }
  ,
if for all 
  
    
      
        
          s
          ′
        
        ∈
        S
      
    
    {\displaystyle s'\in S}
   such that 
  
    
      
        s
        
          
            →
            
              a
            
          
        
        
          s
          ′
        
      
    
    {\displaystyle s{\xrightarrow {a}}s'}
   it holds that 
  
    
      
        
          s
          ′
        
        ⊨
        ϕ
      
    
    {\displaystyle s'\models \phi }
  , then 
  
    
      
        s
        ⊨
        [
        a
        ]
        ϕ
      
    
    {\displaystyle s\models [a]\phi }
  ,
if 
  
    
      
        s
        ⊨
        
          ϕ
          
            1
          
        
      
    
    {\displaystyle s\models \phi _{1}}
  , then 
  
    
      
        s
        ⊨
        
          ϕ
          
            1
          
        
        ∨
        
          ϕ
          
            2
          
        
      
    
    {\displaystyle s\models \phi _{1}\lor \phi _{2}}
  ,
if 
  
    
      
        s
        ⊨
        
          ϕ
          
            2
          
        
      
    
    {\displaystyle s\models \phi _{2}}
  , then 
  
    
      
        s
        ⊨
        
          ϕ
          
            1
          
        
        ∨
        
          ϕ
          
            2
          
        
      
    
    {\displaystyle s\models \phi _{1}\lor \phi _{2}}
  ,
if 
  
    
      
        s
        ⊨
        
          ϕ
          
            1
          
        
      
    
    {\displaystyle s\models \phi _{1}}
   and 
  
    
      
        s
        ⊨
        
          ϕ
          
            2
          
        
      
    
    {\displaystyle s\models \phi _{2}}
  , then 
  
    
      
        s
        ⊨
        
          ϕ
          
            1
          
        
        ∧
        
          ϕ
          
            2
          
        
      
    
    {\displaystyle s\models \phi _{1}\land \phi _{2}}
  .


== See also ==
The modal μ-calculus, which extends HML with fixed point operators
Dynamic logic, a multimodal logic with infinitely many modalities


== References ==


== Sources ==
Colin P. Stirling (2001). Modal and temporal properties of processes. Springer. pp. 32–39. ISBN 978-0-387-98717-0. 
Sören Holmström. 1988. ""Hennessy-Milner Logic with Recursion as a Specification Language, and a Refinement Calculus based on It"". In Proceedings of the BCS-FACS Workshop on Specification and Verification of Concurrent Systems, Charles Rattray (Ed.). Springer-Verlag, London, UK, 294–330."
334,Lori A. Clarke,39687564,4088,"Lori A. Clarke is an American computer scientist noted for her research on software engineering.


== Biography ==
Clarke received a B.A. in Mathematics from the University of Rochester in 1969. She received a Ph.D in Computer Science from the University of Colorado in 1976.
She then joined the Department of Computer Science at the University of Massachusetts Amherst as an assistant professor in 1976. While there she was promoted to associate professor in 1981 and to professor in 1986. In 2011, she became the chair of the School of Computer Science.In 2015 she became an Emeritus Professor.
She was a board member for SIGSOFT from 1985 to 2001, including the chair from 1993 to 1997. She was a board member of CRA from 1999 to 2009. She is also noted for her leadership in broadening participation in computing. She has been a member of the CRA-W Board since 2001 and was the co-chair of CRA-W from 2005 to 2008.


== Awards ==
In the year 1998 she was named an ACM Fellow.
Her other notable awards include:
IEEE Fellow in 2011 for contributions to software testing and verification.
ACM SIGSOFT Outstanding Research Award, 2011
ACM SIGSOFT Distinguished Service Award, 2002


== References ==


== External links ==
University of Massachusetts Amherst: Lori A. Clarke, Department of Computer Science"
335,Hash consing,21898171,4086,"In computer science, particularly in functional programming, hash consing is a technique used to share values that are structurally equal. The term hash consing originates from implementations of Lisp that attempt to reuse cons cells that have been constructed before, avoiding the penalty of memory allocation. Hash consing is most commonly implemented with hash tables storing weak references that may be garbage-collected when the data stored therein contains no references from outside the table. Hash consing has been shown to give dramatic performance improvements—both space and time—for symbolic and dynamic programming algorithms. An interesting property of hash consing is that two structures can be tested for equality in constant time, which in turn can improve efficiency of divide and conquer algorithms when data sets contain overlapping blocks.
In other communities a similar idea is known as the Flyweight pattern. When applied to strings this technique is also known as string interning.


== Examples ==


=== Scheme ===
Simple, not very efficient, but suitable for demonstration of the concept implementation of a memoizer by means of hash table and weak references in Scheme:


== References ==


== Further reading ==
Merkle tree
Ershov, A.P. (1958). ""On programming of arithmetic operations"". Communications of the ACM. 1 (8): 3–6. doi:10.1145/368892.368907. 
Jean Goubault. Implementing Functional Languages with Fast Equality, Sets and Maps: an Exercise in Hash Consing. In Journées Francophones des Langages Applicatifs (JFLA’93), pages 222–238, Annecy, February 1993."
336,International Conference on Parallel and Distributed Systems,26985865,4085,"The International Conference on Parallel and Distributed Systems (ICPADS) is an academic conference sponsored by the IEEE Computer Society that brings together researchers and practitioners from academia and industry around the world to advance the theories, technologies, and applications of parallel and distributed systems.
The ICPADS was an international conference launched in 1992 by the computer science and engineering community in Taiwan in cooperation with the Technical Committee on Parallel Processing (TCPP) and Technical Committee on Distributed Processing (TCDP) of the IEEE Computer Society. Later it was sponsored by computer communities in Taiwan, Japan, and South Korea and held the conference annually in these countries. Since 1998, the ICPADS has been sponsored by the TCPP and TCDP of the IEEE Computer Society. ICPADS 2004 is the first ICPADS held outside the East Asia region. Professor Wen-Tsuen Chen of the National Tsing Hua University, Taiwan served as the founding General Chairman of ICPADS 1992. He had been the Steering Committee Chairman of the ICPADS since 1992 until he was succeeded by Professor Nionel M. Ni of the Hong Kong University of Science and Technology in 2008.


== Topics covered by ICPADS ==
Parallel and Distributed Algorithms and Applications
Multi-core and Multithreaded Architectures
High Performance Computational Biology and Bioinformatics
Power-aware and Green Computing
Resource Management and Scheduling
Peer-to-Peer Computing
Cluster, Grid and Cloud Computing
Web-based Computing and Service-Oriented Architecture
Data Intensive Computing and Data Centre Architecture
Wireless and Mobile Computing
Security and Privacy
Performance Modelling and Evaluation
Systems topics include the following:
Distributed and Parallel Operating Systems
Communication and Networking Systems
Dependable and Trustworthy Systems
Real-Time and Multimedia Systems
Ad Hoc and Sensor Networks
Cyber-Physical Systems
Embedded Systems


== ICPADS locations ==
Main locations (1992–2015, 20 conferences): 17 times in Asia, 2 times in North America, 2 times in Oceania
21. ICPADS 2015: Melbourne, Australia
20. ICPADS 2014: Hsinchu, Taiwan
19. ICPADS 2013: Seoul, South Korea
18. ICPADS 2012: Singapore, Singapore
17. ICPADS 2011: Tainan, Taiwan
16. ICPADS 2010: Shanghai, China
15. ICPADS 2009: Shenzhen, China
14. ICPADS 2008: Melbourne, Australia
13. ICPADS 2007: Hsinchu, Taiwan
12. ICPADS 2006: Minneapolis, Minnesota, USA
11. ICPADS 2005: Fukuoka, Japan
10. ICPADS 2004: Newport Beach, California, USA
9. ICPADS 2002: Zhongli, Taiwan
8. ICPADS 2001: KyongJu, Korea
7. ICPADS 2000: Iwate, Japan
6. ICPADS 1998: Tainan, Taiwan
5. ICPADS 1997: Seoul, Korea
4. ICPADS 1996: Tokyo, Japan
3. ICPADS 1994: Hsinchu, Taiwan
2. ICPADS 1993: Taipei, Taiwan
1. ICPADS 1992: Hsinchu, Taiwan


== See also ==
List of distributed computing conferences
List of computer science conferences"
337,Hydroinformatics,1402463,4074,"Hydroinformatics is a branch of informatics which concentrates on the application of information and communications technologies (ICTs) in addressing the increasingly serious problems of the equitable and efficient use of water for many different purposes. Growing out of the earlier discipline of computational hydraulics, the numerical simulation of water flows and related processes remains a mainstay of hydroinformatics, which encourages a focus not only on the technology but on its application in a social context.
On the technical side, in addition to computational hydraulics, hydroinformatics has a strong interest in the use of techniques originating in the so-called artificial intelligence community, such as artificial neural networks or recently support vector machines and genetic programming. These might be used with large collections of observed data for the purpose of data mining for knowledge discovery, or with data generated from an existing, physically based model in order to generate a computationally efficient emulator of that model for some purpose.
Hydroinformatics recognises the inherently social nature of the problems of water management and of decision making processes, and strives to understand the social processes by which technologies are brought into use. Since the problems of water management are most severe in the majority world, while the resources to obtain and develop technological solutions are concentrated in the hands of the minority, the need to examine these social processes are particularly acute.
Hydroinformatics draws on and integrates hydraulics, hydrology, environmental engineering and many other disciplines. It sees application at all points in the water cycle from atmosphere to ocean, and in artificial interventions in that cycle such as urban drainage and water supply systems. It provides support for decision making at all levels from governance and policy through management to operations.
Hydroinformatics has a growing world-wide community of researchers and practitioners, and postgraduate programmes in Hydroinformatics are offered by many leading institutions. The Journal of Hydroinformatics provides a specific outlet for Hydroinformatics research, and the community gathers to exchange ideas at the biennial conferences. These activities are coordinated by the joint IAHR, IWA, IAHS Hydroinformatics Section.
There is a growing need for professionals and managers to appreciate and work with these new technologies and tools.


== References ==


== External links ==
Hydroinformatics Lab at the University of Iowa - Research and Community Platform.
IHE Delft MSc / PhD in Hydroinformatics.
EuroAquae - European master course of Hydroinformatics and Water Management.
Hydroinformatics MSc at Newcastle University.
The Consortium of Universities for the Advancement of Hydrologic Science, Inc. (CUAHSI) Hydrologic Information System."
338,Igor Jurisica,50137769,4070,"Igor Jurisica is a Professor in the departments of Computer Science and Medical Biophysics at the University of Toronto. He is a Tier I Canada Research Chair in Integrative Cancer Informatics, and an associate editor for BMC Bioinformatics, Proteomes, Cancer Informatics, International Journal of Knowledge Discovery in Bioinformatics, and Interdisciplinary Sciences: Computational Life Sciences. In 2014, 2015 and 2016, he is an ISI Highly Cited Researcher.


== See also ==
Computational biology


== External links ==
List of Jurisica's publications


== References =="
339,Symposium on Logic in Computer Science,6543509,4049,"The ACM–IEEE Symposium on Logic in Computer Science (LICS) is an annual academic conference on the theory and practice of computer science in relation to mathematical logic. Extended versions of selected papers of each year's conference appear in renowned international journals such as Logical Methods in Computer Science and ACM Transactions on Computational Logic.


== History ==
LICS was originally sponsored solely by the IEEE, but as of the 2014 founding of the ACM Special Interest Group on Logic and Computation LICS has become the flagship conference of SIGLOG, under the joint sponsorship of ACM and IEEE.
Since the first installment in 1988, the cover page of the conference proceedings has featured an artwork entitled Irrational Tiling by Logical Quantifiers, by Alvy Ray Smith.
Since 1995, each year the Kleene award is given to the best student paper. In addition, since 2006, the LICS Test-of-Time Award is given annually to one among the twenty-year-old LICS papers that have best met the test of time.


== LICS Awards ==


=== Test-of-Time Award ===
Each year, since 2006, the LICS Test-of-Time Award recognizes those articles from LICS proceedings 20 years earlier, which have become influential.


==== 2014 ====
Martin Hofmann, Thomas Streicher, ""The groupoid model refutes uniqueness of identity proofs""
Dale A. Miller, ""A multiple-conclusion meta-logic""


==== 2013 ====
Leo Bachmair, Harald Ganzinger, Uwe Waldmann, ""Set constraints are the monadic class""
André Joyal, Mogens Nielson, Glynn Winskel, ""Bisimulation and open maps""
Benjamin C. Pierce, Davide Sangiorgi, ""Typing and subtyping for mobile processes""


==== 2012 ====
Thomas Henzinger, Xavier Nicollin, Joseph Sifakis, Sergio Yovine, ""Symbolic model checking for real-time systems""
Jean-Pierre Talpin, Pierre Jouvelot, ""The type and effect discipline""


==== 2011 ====
Patrice Godefroid, Pierre Wolper, ""A partial approach to model checking""
Joshua Hodas, Dale A. Miller, ""Logic programming in a fragment of intuitionistic linear logic""
Dexter Kozen, ""A completeness theorem for Kleene algebras and the algebra of regular events""


==== 2010 ====
Rajeev Alur, Costas Courcoubetis, David L. Dill, ""Model-checking for real-time systems""
Jerry R. Burch, Edmund Clarke, Kenneth L. McMillan, David L. Dill, James Hwang, ""Symbolic model checking: 10^20 states and beyond""
Max Dauchet, Sophie Tison, ""The theory of ground rewrite systems is decidable""
Peter Freyd, ""Recursive types reduced to inductive types""


==== 2009 ====
Eugenio Moggi, ""Computational lambda-calculus and monads""


==== 2008 ====
Martin Abadi, Leslie Lamport, ""The existence of refinement mappings""


==== 2007 ====
Samson Abramsky, ""Domain theory in Logical Form""


=== Kleene award ===
At each conference the Kleene award, in honour of S.C. Kleene, is given for the best student paper.


== See also ==
The list of computer science conferences contains other academic conferences in computer science.


== Notes ==


== External links ==
LICS home page"
340,David Magerman,53826325,4038,"David Mitchell Magerman (born 1968) is an American computer scientist and philanthropist.


== Biography ==


=== Personal life and education ===
Magerman was born to Melvin and Sheila Magerman. His father owned All-City Taxi in Miami, Florida, and his mother was a secretary for a group of accounting firms in Tamarac. Magerman married Debra Magerman (née Kampel), on 8 August 1999. They have four children.
Magerman received his [[Ph.D].] degree from Stanford University in computer science.  He also received his B.S. from the University of Pennsylvania.


=== Business ventures ===
In order to maintain the Jewish community in his neighborhood, Magerman has worked on several business ventures. His first of these was Citron and Rose, an upscale kosher certified meat restaurant, with Michael Solomonov as the head chef. After seeing little to no benefit of an upscale dining experience, Magerman re-branded the establishment as C&R Kitchen, and split with head chef Solomonov. During this time, he opened a a casual dairy restaurant across the street. To better serve the community, C&R Kitchen was closed and replaced with a more upscale place, The Dairy Express, known for its high capacity pizza oven. Currently under construction is C&R Tavern, a mix between a kosher grocery store and a dining establishment.


=== Philanthropy ===
Through founding The Kohelet Foundation, Magerman has donated tens of millions of dollars to local causes. Some of these include Kohelet Yeshiva High School (renamed from Stern Hebrew High School in the gift's honor), Yeshiva University and the Yeshiva Lab School.


=== Controversy ===
In 2017, Magerman publicly opposed the views of his boss, Robert Mercer, concerning politics and race issues in America. Mercer, the co-CEO of Renaissance Technology, suspended Magerman without pay and later made the suspension permanent. In may 2017 it was reported that Magerman was seeking an excess of $150,000 in damages over alleged racist comments and unlawful termination.


== References =="
341,Nuclear computation,56237732,4016,"Nuclear computation is a type of computation which allows threads to either spawn new threads or converge many threads to one. The aim of nuclear computation is to take advantage of threading abilities of modern multi-core processors where the trend is to increase their hardware ability to compute more threads then their earlier generation processors .
Nuclear computation focuses on real time processing for things like multimedia such as processing audio where a real time deadline (the sample rate in Hz) exists. For that reason it should not block and computational processes which alter shared memory must be atomic (excuted in one clock cycle without locking).
Nuclear computation allows a computational thread to use thread fission to turn one thread into many or thread fusion to turn many threads into one.


== Analogy to nuclear reactions ==
As the name ""nuclear computation"" implies, there is an analogy between nuclear reactions and nuclear computation.


=== The nuclear fission analogy ===
In nuclear physics, atoms decay or react where the atom's nucleus splits, producing several atoms. In nuclear computation, a computational thread splits into several processing threads.


=== The nuclear fusion analogy ===
In nuclear physics, atoms may react together to fuse where several atomic nuclii may fuse into one nucleus. In nuclear computation, several computational threads fuse into one processing thread.


=== Component analogy ===


=== Speed ===
Nuclear explosions are fase and lockless. Which suggests some requirements :
lockless
parallel
ordered
light weight
low latency


== Description ==


=== Thread fission ===
Conceptually fission computation can cause a chain reaction, where one thread can signal many threads to start processing and they too may signal other threads to start processing. It is possible to starve the computer, where the computer runs out of resources and halts - either do to a lack of memory, power or disk resources.


=== Thread fusion ===
Fusion computation is a type of threshold triggered computation, where several threads signal the single waiting thread, which begins execution once the required number of thread signals exceed the threshold of the waiting thread.


== Implementation examples ==


== History ==
A previous analogy between nuclear reactions and computation were termed loop fission and fusion which were forms of compiler preprocessing. Loop fission (loop distribution) allowed one computational loop to be broken into separate loops by a compiler at compile time. Loop fusion (loop jamming) allowed many computational loops to be combined into one by the compiler at compiler time. These processes were not directly under the control of the programmer and were decided and controlled by the compiler. In contrast to loop fission and fusion, nuclear computation fission and fusion are directly under the control of the programmer or the program at run time.


== See Also ==
Concurrency pattern


== References =="
342,Ewan Stafford Page,51933946,4006,"Ewan Stafford Page (b. August 1928) is a British academic and computer scientist, and former vice-chancellor of the University of Reading.
Ewan Page was educated at Wyggeston Grammar School for Boys in Leicester and at the University of Cambridge. After a period of National Service, he was a research student in the field of statistics at the University of Cambridge between 1951 and 1954, at a time when the EDSAC computer was new. In 1957, he was appointed as director of the Durham University's Computing Laboratory, located at King's College, Newcastle. When King's College became Newcastle University, the Computing Laboratory became part of that University, and eventually Ewan Page was appointed a pro-vice-chancellor. In 1976, when the then incumbent died unexpectedly, he served as acting vice-chancellor of Newcastle University.
In 1979, Ewan Page was appointed vice-chancellor of the University of Reading, a position he held until 1993. He served as president of the British Computer Society in 1984/5.


== Books ==
Page, Ewan Stafford; Wilson, Leslie Blackett (1978). Information, Representation and Manipulation in a Computer (second ed.). New York, NY, USA: Cambridge University Press. ISBN 0-521-29357-X. 
Page, Ewan Stafford; Wilson, Leslie Blackett (1979). An introduction to computational combinatorics. Cambridge Computer Science Texts. 9. New York, NY, USA: Cambridge University Press. ISBN 0-521-29492-4. 
Page, Ewan Stafford; Wilson, Leslie Blackett (1983). Information Representation and Manipulation Using Pascal. Cambridge Computer Science Texts. 15. New York, NY, USA: Cambridge University Press. ISBN 0-521-24954-6. 


== References =="
343,Stefano Soatto,53773102,4002,"Stefano Soatto is Professor of Computer Science at the University of California, Los Angeles (UCLA), in Los Angeles, CA, where he is also Professor of Electrical Engineering and Founding Director of the UCLA Vision Lab. He was named Fellow of the Institute of Electrical and Electronics Engineers (IEEE) in 2013 for contributions to dynamic visual processes. He received the David Marr Prize in Computer Vision in 1999. 


== Academic biography ==
Soatto obtained his D. Eng. in Electrical Engineering Cum Laude from the University of Padova in 1992, was an EAP Fellow at the University of California at Berkeley in 1990-91, and received his Ph.D. in Control and Dynamical Systems from the California institute of Technology in 1996 with dissertation “A Geometric Approach to Dynamic Vision”. In 1996-97 he was a Postdoctoral Scholar at Harvard University, and subsequently held positions as Assistant and Associate Professor of Electrical Engineering and Biomedical Engineering at Washington University, St. Louis, and of Mathematics and Information Science at the University of Udine, Italy. He has been at UCLA since 2000.


== Research ==
Soatto’s research focuses on Computer Vision, Machine Learning and Robotics. He co-developed optimal algorithms for Structure From Motion (SFM, or Visual SLAM, simultaneous localization and mapping, in Robotics; Best Paper Award at CVPR 1998), characterized its ambiguities (David Marr Prize at ICCV 1999), also characterized the identifiability and observability of visual-inertial sensor fusion (Best Paper Award at ICRA 2015). His research focus is the development of representations, that are functions of the data that capture their informative content and discard irrelevant variability in the data (a generalized form of ‘noise’ or ‘clutter’). Soatto’s lab first to demonstrate real-time SFM and Augmented Reality (AR) on commodity hardware in live demos at CVPR 2000, ICCV 2001, and ECCV 2002. He also co-led the UCLA-Golem Team in the second DARPA Grand Challenge for autonomous vehicles, with Emilio Frazzoli (co-founder of NuTonomy), and Amnon Shashua (co-founder of Mobileye).


== References ==


== External links ==
Stefano Soatto professional home page"
344,Treiber Stack,49807686,3974,"The Treiber Stack Algorithm is a scalable lock-free stack utlizing the fine-grained concurrency primitive Compare-and-swap It is believed that R. Kent Treiber was the first to publish it in his 1986 article ""Systems Programming: Coping with Parallelism"" 


== Basic Principle ==
The basic principle for the algorithm is to only add something new to the stack once you know the item you are trying to add is the only thing that has been added since you began the operation. This is done by using compare-and-swap. With stacks when adding a new item, you take the top of the stack and put it after your new item. You then compare the second item of this newly constructed head element (old head) to the current one. If the two match, then you can swap old head to the new one, if not then it means another thread has added another item to the stack in which case you must try again.
When popping an item from the stack, before returning the item you must check another thread has not added another item since the operation began.


== Correctness ==
In some languages -- particularly, those without garbage collection -- the Treiber stack can be at risk for the ABA problem. When a process is about to remove an element from the stack (just before the compare and set in the pop routine below) another process can change the stack such that the head is the same, but the second element is different. The compare and swap will set the head of the stack to the old second element in the stack mixing up the complete data structure. However, the Java version on this page is not subject to this problem, because of the stronger guarantees offered by the Java runtime (it is impossible for a newly-created, unaliased object reference to be reference-equal to any other reachable object.)
Testing for failures such as ABA can be exceedingly difficult, because the problematic sequence of events is very rare. Model checking is an excellent way to uncover such problems. See for instance exercise 7.3.3 in ""Modeling and analysis of communicating Systems"".


== Java Example ==
Below is an implementation of the Treiber Stack in Java, based on the one provided by Java Concurrency in Practice 


== References =="
345,Philosophy of computer science,22458313,3971,"The philosophy of computer science is concerned with the philosophical questions that arise with the study of computer science, which is understood to mean not just programming but the whole study of concepts and methods that assist in the development and maintenance of computer systems. There is still no common understanding of the content, aim, focus, or topic of the philosophy of computer science, despite some attempts to develop a philosophy of computer science like the philosophy of physics or the philosophy of mathematics.
The philosophy of computer science as such deals with the meta-activity that is associated with the development of the concepts and methodologies that implement and analyze the computational systems.


== See also ==
Computer-assisted proof: Philosophical objections
Philosophy of artificial intelligence
Philosophy of information
Philosophy of mathematics
Philosophy of science
Philosophy of technology


== References ==


== Further reading ==
Scott Aaronson. ""Why Philosophers Should Care About Computational Complexity"". In Computability: Gödel, Turing, Church, and beyond.
Timothy Colburn. Philosophy and Computer Science. Explorations in Philosophy. M.E. Sharpe, 1999. ISBN 1-56324-991-X.
A.K. Dewdney. New Turing Omnibus: 66 Excursions in Computer Science
Luciano Floridi (editor). The Blackwell Guide to the Philosophy of Computing and Information, 2004.
Luciano Floridi (editor). Philosophy of Computing and Information: 5 Questions. Automatic Press, 2008.
Luciano Floridi. Philosophy and Computing: An Introduction, Routledge, 1999.
Christian Jongeneel. The informatical worldview, an inquiry into the methodology of computer science.
Jan van Leeuwen. ""Towards a philosophy of the information and computing sciences"", NIAS Newsletter 42, 2009.
Moschovakis, Y. (2001). What is an algorithm? In Enquist, B. and Schmid, W., editors, Mathematics unlimited — 2001 and beyond, pages 919–936. Springer.
Alexander Ollongren, Jaap van den Herik. Filosofie van de informatica. London and New York: Routledge, 1999. ISBN 0-415-19749-X
Tedre, Matti (2014), The Science of Computing: Shaping a Discipline  Taylor and Francis.
Ray Turner and Ammon H. Eden. ""The Philosophy of Computer Science"". Stanford Encyclopedia of Philosophy.
Matti Tedre (2011). Computing as a Science: A Survey of Competing Viewpoints. Minds & Machines 21, 3, 361–387.


== External links ==
The International Association for Computing and Philosophy
Philosophy of Computing and Information at PhilPapers
A draft version of Philosophy of Computer Science by William J. Rapaport"
346,Single version of the truth,8658994,3919,"In computerized business management, single version of the truth (SVOT), is a technical concept describing the data warehousing ideal of having either a single centralised database, or at least a distributed synchronised database, which stores all of an organisation's data in a consistent and non-redundant form. This contrasts with the related concept of single source of truth (SSOT), which refers to a data storage principle to always source a particular piece of information from one place.


== Applied to message sequencing ==
In some systems and in the context of message processing systems (often realtime systems), this term also refers to the goal of establishing a single agreed sequence of messages within a database formed by a particular but arbitrary sequencing of records. The key concept is that data combined in a certain sequence is a ""truth"" which may be analyzed and processed giving particular results, and that although the sequence is arbitrary (and thus another correct but equally arbitrary sequencing would ultimately provide different results in any analysis), it is desirable to agree that the sequence enshrined in the ""single version of the truth"" is the version that will be considered ""the truth"", and that any conclusions drawn from analysis of the database are valid and unarguable, and (in a technical context) the database may be duplicated to a backup environment to ensure a persistent record is kept of the ""single version of the truth"".
The key point is when the database is created using an external data source (such as a sequence of trading messages from a stock exchange) an arbitrary selection is made of one possibility from two or more equally valid representations of the input data, but henceforth the decision sets ""in stone"" one and only one version of the truth.


== As applied to message sequencing ==
Critics of SVOT as applied to message sequencing argue that this concept is not scalable. As the world moves towards systems spread over many processing nodes, the effort involved in negotiating a single agreed-upon sequence becomes prohibitive.
But as pointed out by Owen Rubel at his API World talk 'The New API Pattern', the SVOT is always going to be the service layer in a distributed architecture where Input/Output meet; This also is where the actual endpoint binding belongs to allow for modularization and better abstraction of the I/O data across the architecture to avoid the architectural cross cutting concern.


== See also ==
Closed world assumption
Open world assumption


== References ==


== Bibliography ==
King, Julia (2003-12-22). ""Business Intelligence: One Version of the Truth"". ComputerWorld. 
Lamport, Leslie (July 1978). ""Time, Clocks, and the Ordering of Events in a Distributed System"" (PDF). Communications of the ACM. 21 (7): 558–565. doi:10.1145/359545.359563. 
Inmon, Bill (2004-09-09). ""The Single Version Of The Truth"". Business Intelligence Network. Powell Media. 
http://www.industryweek.com/EventDetail.aspx?EventID=179
Chisholm, Malcolm (December 2006), ""There is No Single Version of the Truth"", Information Management Magazine, retrieved 2010-03-15 


== External links =="
347,Microsoft Award,7813116,3913,"The Royal Society and Académies des sciences Microsoft Award was an annual award given by the Royal Society and the Académie des sciences to scientists working in Europe who had made a major contribution to the advancement of science through the use of computational methods. It was sponsored by Microsoft Research.
The award was open to any research scientist who had made a significant contribution at the intersection of computing and the sciences covering Biological Sciences, Physical Sciences, Mathematics and Engineering. The prize recognized the importance of interdisciplinary research at the interface of science and computing for advancing scientific boundaries, as well as the importance of investing in European scientists to give Europe a competitive science base. The recipient was selected by a Committee comprising members of the Académie des sciences and Fellows of the Royal Society. The prize consisted of a trophy and monetary amount of €250,000, of which €7,500 is prize money and the rest earmarked for further research.
The first award was made in 2006 and the last in 2009. It has now been replaced by the Royal Society Milner Award.


== List of winners ==


== Microsoft Award today ==
Today Microsoft is giving awards for top-performing partners in various countries.


== References ==


== External links ==
Official website"
348,Victor Vianu,31256904,3911,"Victor Vianu is a computer scientist, a professor of computer science and engineering at the University of California, San Diego. He served as editor-in-chief of the Journal of the ACM from 2009 to 2015.
Vianu did his graduate studies at the University of Southern California, earning his Ph.D. in 1983 under the supervision of Seymour Ginsburg; he joined the UCSD faculty in 1984.
Vianu's book Foundations of Databases (with Serge Abiteboul and Richard Hull, Addison-Wesley, 1995) is a standard graduate textbook in database theory. In finite model theory and computational complexity theory, the Abiteboul–Vianu theorem (also published with Abiteboul, at the 1991 Symposium on Theory of Computing) states that polynomial time equals PSPACE if and only if fixed point logic equals partial fixed point logic. At the 2010 Symposium on Principles of Database Systems, Vianu and his co-authors Dan Suciu and Tova Milo won the Alberto O. Mendelzon Test-of-Time Award for their work ten years prior on type checking for XML transformation languages. Vianu and his co-author Luc Segoufin won a second Alberto O. Mendelzon Test-of-Time award in 2015, for their 2005 article ""Views and Queries: Determinacy and Rewriting.""
In 2006, Vianu was elected as a Fellow of the ACM for his ""contributions to database management systems"".
In 2013, he was elected Fellow of the AAAS (American Association for the Advancement of Science). He was elected to Academia Europaea in 2014.
In his first paper recorded by DBLP (presented at MFCS, 1977), Vianu acknowledges Solomon Marcus for guidance.


== References ==


== External links ==
Home page at UCSD
Victor Vianu at the Mathematics Genealogy Project"
349,Devavrat Shah,40180792,3906,"Devavrat Shah is a professor in the Electrical Engineering and Computer Science department at MIT. He is director of Statistics and Data Science Center at MIT. He received a B.Tech. degree in Computer Science from IIT Bombay in 1999 and a Ph.D. in Computer Science from Stanford University in 2004, where his thesis was completed under the supervision of Balaji Prabhakar.


== Research ==
Shah's research focuses on the theory of large complex networks which includes network algorithms, stochastic networks, network information theory and large scale statistical inference. His work has had significant impact both in the development of theoretical tools and in its practical application. This is highlighted by the ""Best Paper"" awards he has received from top publication venues such as ACM SIGMETRICS, IEEE INFOCOM and NIPS. Additionally, his work has been recognized by the INFORMS Applied Probability Society via the Erlang Prize, given for outstanding contributions to applied probability by a researcher not more than 9 years from their PhD and the ACM SIGMETRICS Rising Star award, given for outstanding contributions to computer/communication performance evaluation by a research not more than 7 years from their PhD. He is a young distinguished alumni of his alma mater IIT Bombay.


== Awards ==
Shah has received many awards, including
Erlang Prize from Applied Probability Society of INFORMS 2010
ACM SIGMETRICS/Performance best student paper award 2009 (supervised)
ACM SIGMETRICS Rising Star Award 2008
Neural Information Processing System (NIPS) outstanding paper award 2008 (supervised)
ACM SIGMETRICS/Performance best paper award 2006
NSF CAREER Award 2006
George B. Dantzig best dissertation award from INFORMS 2005
IEEE INFOCOM best paper award 2004
President of India Gold Medal at Indian Institute of Technology-Bombay 1999


== Industry ==
Shah is a co-founded Celect, Inc. in 2013.


== References =="
350,Program transformation,2799283,3892,"A program transformation is any operation that takes a computer program and generates another program. In many cases the transformed program is required to be semantically equivalent to the original, relative to a particular formal semantics and in fewer cases the transformations result in programs that semantically differ from the original in predictable ways.
While the transformations can be performed manually, it is often more practical to use a program transformation system that applies specifications of the required transformations. Program transformations may be specified as automated procedures that modify compiler data structures (e.g. abstract syntax trees) representing the program text, or may be specified more conveniently using patterns representing parameterized source code text fragments.
A practical requirement for source code transformation systems is that they be able to effectively process programs written in a programming language. This usually requires integration of a full front-end for the programming language of interest, including source code parsing, building internal program representations of code structures, the meaning of program symbols, useful static analyses, and regeneration of valid source code from transformed program representations. The problem of building and integrating adequate front ends for conventional languages (Java, C++, PHP, ...) may be of equal difficulty as building the program transformation system itself because of the complexity of such languages. To be widely useful, a transformation system must be able to handle many target programming languages, and must provide some means of specifying such front ends.
A generalisation of semantic equivalence is the notion of program refinement: one program is a refinement of another if it terminates on all the initial states for which the original program terminates, and for each such state it is guaranteed to terminate in a possible final state for the original program. In other words, a refinement of a program is more defined and more deterministic than the original program. If two programs are refinements of each other, then the programs are equivalent.


== See also ==
List of program transformation systems
Metaprogramming
Program synthesis
Source-to-source compiler
Source code generation
Transformation language
Dynamic recompilation


== References ==


== External links ==
The Program transformation Wiki
Papers on program transformation theory and practice
Transformation Technology Bibliography
DMS Software Reengineering Toolkit: A Program Transformation System for DSLs and modern (C++, Java, ...) and legacy (COBOL, RPG) computer languages
Spoon: A library to analyze, transform, rewrite, and transpile Java source code. It parses source files to build a well-designed AST with powerful analysis and transformation API."
351,Suffix automaton,44050791,3875,"In computer science, a suffix automaton or directed acyclic word graph is a finite automaton that recognizes the set of suffixes of a given string. (Note that the phrase 'directed acyclic word graph' more commonly refers to a Deterministic acyclic finite state automaton) It can be thought of as a compressed form of the suffix tree, a data structure that efficiently represents the suffixes of the string. For example, a suffix automaton for the string ""suffix"" can be queried for other strings; it will report ""true"" for any of the strings ""suffix"", ""uffix"", ""ffix"", ""fix"", ""ix"" and ""x"", and ""false"" for any other string.
The suffix automaton of a set of strings U has at most 2Q − 2 states, where Q is the number of nodes of a prefix-tree representing the strings in U.
Suffix automata have applications in approximate string matching.


== See also ==
GADDAG
Suffix array


== References ==


== Additional reading =="
352,Code Louisville,46307473,3858,"Code Louisville is a public–private partnership program in Louisville, Kentucky with the aim of fostering software developers to bolster technological innovation in the region. It received national attention in April 2015 when President Barack Obama visited the region to announce TechHire and promote the value of the federal government working with local governments.


== Purpose ==
Code Louisville is a public–private partnership program in Louisville that began in November 2013. Metro Louisville Department of Economic Growth and Innovation, Greater Louisville Inc., EnterpriseCorp, the Louisville Free Public Library, and KentuckianaWorks are partnered with local employers partner who hire graduates of the program. It is a free 12-week online coding course open to those with a library card. It aims to foster software developers in the area, so as to bolster a technological innovation in the region. The program works in collaboration with Treehouse, including a prerequisite course.


== Effectiveness ==
The program has been lauded for its success compared to similar programs in other cities. In April 2015, President Obama visited Louisville to praise the program and to use it as an example of the federal TechHire initiative that provides grants to similar programs. In 2015, it was announced that Code Louisville would attempt to create a program to teach others cities how to run similar programs.


== References =="
353,Relationship extraction,11547598,3857,"A relationship extraction task requires the detection and classification of semantic relationship mentions within a set of artifacts, typically from text or XML documents. The task is very similar to that of information extraction (IE), but IE additionally requires the removal of repeated relations (disambiguation) and generally refers to the extraction of many different relationships.


== Applications ==
Application domains where relationship extraction is useful include gene-disease relationships, protein-protein interaction etc.
Never-Ending Language Learning is a semantic machine learning system developed by a research team at Carnegie Mellon University that extracts relationships from the open web.


== Approaches ==
One approach to this problem involves the use of domain ontologies. Another approach involves visual detection of meaningful relationships in parametric values of objects listed on a data table that shift positions as the table is permuted automatically as controlled by the software user. The poor coverage, rarity and development cost related to structured resources such as semantic lexicons (e.g. WordNet, UMLS) and domain ontologies (e.g. the Gene Ontology) has given rise to new approaches based on broad, dynamic background knowledge on the Web. For instance, the ARCHILES technique uses only Wikipedia and search engine page count for acquiring coarse-grained relations to construct lightweight ontologies.
The relationships can be represented using a variety of formalisms/languages. One such representation language for data on the Web is RDF.


== See also ==
Text analytics
Semantic analytics
Semantic role labeling
Information extraction
Business Intelligence 2.0


== References =="
354,Phonetic algorithm,36712,3856,"A phonetic algorithm is an algorithm for indexing of words by their pronunciation. Most phonetic algorithms were developed for use with the English language; consequently, applying the rules to words in other languages might not give a meaningful result.
They are necessarily complex algorithms with many rules and exceptions, because English spelling and pronunciation is complicated by historical changes in pronunciation and words borrowed from many languages.
Among the best-known phonetic algorithms are:
Soundex, which was developed to encode surnames for use in censuses. Soundex codes are four-character strings composed of a single letter followed by three numbers.
Daitch–Mokotoff Soundex, which is a refinement of Soundex designed to better match surnames of Slavic and Germanic origin. Daitch–Mokotoff Soundex codes are strings composed of six numeric digits.
Cologne phonetics: This is similar to Soundex, but more suitable for German words.
Metaphone, Double Metaphone, and Metaphone 3 which are suitable for use with most English words, not just names. Metaphone algorithms are the basis for many popular spell checkers.
New York State Identification and Intelligence System (NYSIIS), which maps similar phonemes to the same letter. The result is a string that can be pronounced by the reader without decoding.
Match Rating Approach developed by Western Airlines in 1977 - this algorithm has an encoding and range comparison technique.
Caverphone, created to assist in data matching between late 19th century and early 20th century electoral rolls, optimized for accents present in parts of New Zealand.


== Common uses ==
Spell checkers can often contain phonetic algorithms. The Metaphone algorithm, for example, can take an incorrectly spelled word and create a code. The code is then looked up in directory for words with the same or similar Metaphone. Words that have the same or similar Metaphone become possible alternative spellings.
Search functionality will often use phonetic algorithms to find results that don't match exactly the term(s) used in the search. Searching for names can be difficult as there are often multiple alternative spellings for names. An example is the name Claire. It has two alternatives, Clare/Clair, which are both pronounced the same. Searching for one spelling wouldn't show results for the two others. Using Soundex all three variations produce the same Soundex code, C460. By searching names based on the Soundex code all three variations will be returned.


== See also ==
Approximate string matching
Hamming distance
Levenshtein distance
Damerau–Levenshtein distance


== References ==

 This article incorporates public domain material from the NIST document: Black, Paul E. ""phonetic coding"". Dictionary of Algorithms and Data Structures. 


== External links ==
Algorithm for converting words to phonemes and back.
StringMetric project a Scala library of phonetic algorithms.
clj-fuzzy project a Clojure library of phonetic algorithms.
SoundexBR library of phonetic algorithm implemented in R.
Talisman a JavaScript library collecting various phonetic algorithms that one can try online."
355,Consortium for Computing Sciences in Colleges,48080791,3853,"The Consortium for Computing Sciences in Colleges (CCSC) is a nonprofit organization divided into ten regions that roughly match geographical areas in the United States. The purpose of the consortium is to: promote, support and improve computing curricula in colleges and universities; encompass regional constituencies devoted to this purpose; and promote a national liaison among local, regional and national organizations devoted to this purpose. Predominantly these colleges and universities are oriented toward teaching, rather than research.


== Regions ==
CCSC regions include:
Central Plains
Eastern
Midsouth
Midwest
Northeastern
Northwestern
Rocky Mountain
South Central
Southeastern
Southwestern


== Conferences ==
Conferences are typically held annually by region, and include presentation of peer-reviewed papers, as well as student papers, posters, and programming contests, workshops, and special sessions for innovative assignments and approaches in the area of computer science technology and education.  


== Journal ==
CCSC publishes the Journal of Computing Sciences in Colleges, containing the proceedings of each annual regional conference. The journal is distributed to approximately 600 faculty members from 350 colleges and universities.


== See also ==
Association for Computing Machinery (ACM)
ACM Special Interest Group on Computer Science Education (SIGCSE)
Computer Science
Google for Education
National Center for Women & Information Technology
Upsilon Pi Epsilon


== References ==


== External links ==
Official website"
356,Data drilling,2605930,3845,"Data drilling (also drilldown) refers to any of various operations and transformations on tabular, relational, and multidimensional data. The term has widespread use in various contexts, but is primarily associated with specialized software designed specifically for data analysis.


== Common data drilling operations ==
There are certain operations that are common to applications that allow data drilling. Among them are:
Query operations:
tabular query
pivot query


=== Tabular query ===
Tabular query operations consist of standard operations on data tables.
Among these operations are:
search
sort
filter (by value)
filter (by extended function or condition)
transform (e.g., by adding or removing columns)
Consider the following example:
Fred and Wilma table (Fig 001):

   gender  , fname    , lname        , home
   male    , fred     , chopin       , Poland
   male    , fred     , flintstone   , bedrock
   male    , fred     , durst        , usa
   female  , wilma    , flintstone   , bedrock
   female  , wilma    , rudolph      , usa
   female  , wilma    , webb         , usa
   male    , fred     , johnson      , usa

The preceding is an example of a simple flat file table formatted as comma-separated values. The table includes first name, last name, gender and home country for various people named fred or wilma. Although the example is formatted this way, it is important to emphasize that tabular query operations (as well as all data drilling operations) can be applied to any conceivable data type, regardless of the underlying formatting. The only requirement is that the data be readable by the software application in use.


=== Pivot query ===
A pivot query allows multiple representations of data according to different dimensions. This query type is similar to tabular query, except it also allows data to be represented in summary format, according to a flexible user-selected hierarchy. This class of data drilling operation is formally (and loosely) known by different names, including crosstab query, pivot table, data pilot, selective hierarchy, intertwingularity and others.
To illustrate the basics of pivot query operations, consider the Fred and Wilma table (Fig 001). A quick scan of the data reveals that the table has redundant information. This redundancy could be consolidated using an outline or a tree structure or in some other way. Moreover, once consolidated, the data could have many different alternate layouts.
Using a simple text outline as output, the following alternate layouts are all possible with a pivot query:
Summarize by gender (Fig 001):

   female
       flintstone, wilma
       rudolph, wilma
       webb, wilma
   male
       chopin, fred
       flintstone, fred
       durst, fred
       johnson, fred
   
   (Dimensions = gender; Tabular fields = lname, fname;)

Summarize by home, lname (Fig 001):

   bedrock
       flintstone
           fred
           wilma
   Poland
       chopin
           fred
   usa
       ...
   
   (Dimensions = home, lname; Tabular fields = fname;)


==== Uses ====
Pivot query operations are useful for summarizing a corpus of data in multiple ways, thereby illustrating different representations of the same basic information. Although this type of operation appears prominently in spreadsheets and desktop database software, its flexibility is arguably under-utilized. There are many applications that allow only a 'fixed' hierarchy for representing data, and this represents a substantial limitation."
357,Mesh generation,3050716,3840,"Mesh generation is the practice of generating a polygonal or polyhedral mesh that approximates a geometric domain. The term ""grid generation"" is often used interchangeably. Typical uses are for rendering to a computer screen or for physical simulation such as finite element analysis or computational fluid dynamics. The input model form can vary greatly but common sources are CAD, NURBS, B-rep, STL or a point cloud. The field is highly interdisciplinary, with contributions found in mathematics, computer science, and engineering.
Three-dimensional meshes created for finite element analysis need to consist of tetrahedra, pyramids, prisms or hexahedra. Those used for the finite volume method can consist of arbitrary polyhedra. Those used for finite difference methods usually need to consist of piecewise structured arrays of hexahedra known as multi-block structured meshes. A mesh is otherwise a discretization of a domain existing in one, two or three dimensions.


== See also ==
Meshfree methods
Principles of grid generation
Types of mesh
Grid classification
Delaunay triangulation
Fortune's algorithm
Polygon mesh
Regular grid
Ruppert's algorithm
Tessellation
Unstructured grid
Stretched grid method
Parallel mesh generation


== References ==
Edelsbrunner, Herbert (2001), Geometry and Topology for Mesh Generation, Cambridge University Press, ISBN 978-0-521-79309-4 .
Frey, Pascal Jean; George, Paul-Louis (2000), Mesh Generation: Application to Finite Elements, Hermes Science, ISBN 978-1-903398-00-5 .
P. Smith and S. S. Sritharan (1988), ""Theory of Harmonic Grid Generation"" (PDF), Complex Variables, 10: 359–369., doi:10.1080/17476938808814314 
S. S. Sritharan (1992), ""Theory of Harmonic Grid Generation-II"", Applicable Analysis, 44 (1): 127–149., doi:10.1080/00036819208840072 
Thompson, J. F.; Warsi, Z. U. A.; Mastin, C. W. (1985), Numerical Grid Generation: Foundations and Applications, North-Holland, Elsevier .
CGAL The Computational Geometry Algorithms Library


== External links ==
Bubble Mesh: Automated Triangular Meshing of Non-Manifold Geometry by Sphere Packing [1].
Mesh generation in CGAL, the Computational Geometry Algorithms Library:
2D Conforming Triangulations and Meshes
3D Mesh Generation

Mesh generators lists:
Free/open source mesh generators
Public domain and commercial mesh generators"
358,Basel Computational Biology Conference,44399285,3824,"The Basel Computational Biology Conference (stylized as [BC]2) is a scientific meeting on the subjects of bioinformatics and computational biology. It covers a wide spectrum of disciplines, including bioinformatics, computational biology, genomics, computational structural biology, and systems biology. The conference is organized biannually by the SIB Swiss Institute of Bioinformatics in Basel, Switzerland. The next edition will take place 12–15 September 2017 at the Congress Center Basel.


== List of previous conferences ==
2015 [BC]2 Basel Computational Biology Conference.
2013 ""Genetic Variation + Human Health""
2012 ""ECCB'12, 11th European Conference on Computational Biology"" in association with the 10th Basel Computational Biology conference.
2011 ""Multiscale Modeling""
2010 ""Regulation & Control in Biological Systems""
2009 ""Molecular Evolution""
2008 ""Computational Structural Biology""
2007 ""From Euler to Computational Biology"" in association with USGEB
2006 ""Comparative Genomics""
2005 ""Biological Systems In Silico""
2004 ""From Information to Simulation""
2003 ""Life Sciences Meet IT""


== References =="
359,Milan Vojnovic,40240350,3821,"Milan Vojnovic is a professor of data science with the Department of Statistics at the London School of Economics, where he is also director of the MSc Data Science Programme. Prior to this, he worked as a researcher with Microsoft Research from 2004 to 2016.
He received his Ph.D. degree in Technical Sciences from École Polytechnique Fédérale de Lausanne in 2003, and both M.Sc. and B.Sc. degrees in Electrical Engineering from the University of Split, Croatia, in 1995 and 1998, respectively. He undertook an internship with the Mathematical Research Centre at Bell Labs in 2001. From 2005 to 2014, he was a visiting professor at the University of Split, Croatia. From 2014 to 2016, he was an affiliated lecturer at the Statistical Laboratory, University of Cambridge.


== Research ==
His research interests include data science, machine learning, artificial intelligence, game theory, multi-agent systems and information networks. He has made contributions to the theory and the design of computation platforms for processing large-scale data.
He received several prizes for his work. In 2010, he was awarded the ACM SIGMETRICS Rising Star Researcher Award, and in 2005, the ERCIM Cor Baayen Award. He received the IEEE IWQoS 2007 Best Student Paper Award (with Shao Liu and Dinan Gunawardena), the IEEE INFOCOM 2005 Best Paper Award (with Jean-Yves Le Boudec), the ACM SIGMETRICS 2005 Best Paper Award (with Laurent Massoulie) and the ITC 2001 Best Student Paper Award (with Jean-Yves Le Boudec).
Vojnovic authored the book Contest Theory: Incentive Mechanisms and Ranking Methods.


== References =="
360,Gordon Bell Prize,5454605,3816,"The Gordon Bell Prize is an award presented by the Association for Computing Machinery each year in conjunction with the SC Conference series (formerly known as the Supercomputing Conference). The prize recognizes outstanding achievement in high-performance computing applications. The main purpose is to track the progress over time of parallel computing, by acknowledging and rewarding innovation in applying high-performance computing to applications in science, engineering, and large-scale data analytics. The prize was established in 1987. A cash award of $10,000 (since 2011) accompanies the recognition, funded by Gordon Bell, a pioneer in high-performance and parallel computing.
The Prizes were preceded by a nominal prize ($100) established by Alan Karp, a numerical analyst (then of IBM) who challenged claims of MIMD performance improvements proposed in the Letters to the Editor section of the Communications of the ACM. Karp went on to be one of the first Gordon Bell Prize judges.
Individuals or teams may apply for the award by submitting a technical paper describing their work through the SC conference submissions process. Finalists present their work at that year's conference, and their submissions are included in the conference proceedings.


== Prize criteria ==
The ACM Gordon Bell Prize is primarily intended to recognize performance achievements that demonstrate:
evidence of important algorithmic and/or implementation innovations
clear improvement over the previous state-of-the-art
solutions that don’t depend on one-of-a-kind architectures (systems that can only be used to address a narrow range of problems, or that can’t be replicated by others)
performance measurements that have been characterized in terms of scalability (strong as well as weak scaling), time to solution, efficiency (in using bottleneck resources, such as memory size or bandwidth, communications bandwidth, I/O), and/or peak performance
achievements that are generalizable, in the sense that other people can learn and benefit from the innovations
In earlier years, multiple prizes were sometimes awarded to reflect different types of achievements. According to current policies, the Prize can be awarded in one or more of the following categories, depending on the entries received in a given year:
Peak Performance: If the entry demonstrates outstanding performance in terms of floating point operations per second on an important science/engineering problem; the efficiency of the application in using bottleneck resources (such as memory size or bandwidth) is also taken into consideration.
Special Achievement in Scalability, Special Achievement in Time to Solution: If the entry demonstrates exceptional Scalability, in terms of both strong and weak scaling, and/or total time to solve an important science/engineering problem.


== References ==


== External links ==

Gordon Bell Prize - Award Winners: List By Year
Gordon Bell Prize description from SC13
ACM Gordon Bell Prize Winners 2006-present
Earlier Prize Winners 1987–1999
Gordon Bell Prize official page on ACM Website
The SC (formerly ""Supercomputing"") Conference Series"
361,Real computation,397247,3802,"In computability theory, the theory of real computation deals with hypothetical computing machines using infinite-precision real numbers. They are given this name because they operate on the set of real numbers. Within this theory, it is possible to prove interesting statements such as ""The complement of the Mandelbrot set is only partially decidable.""
These hypothetical computing machines can be viewed as idealised analog computers which operate on real numbers, whereas digital computers are limited to computable numbers. They may be further subdivided into differential and algebraic models (digital computers, in this context, should be thought of as topological, at least insofar as their operation on computable reals is concerned). Depending on the model chosen, this may enable real computers to solve problems that are inextricable on digital computers (For example, Hava Siegelmann's neural nets can have noncomputable real weights, making them able to compute nonrecursive languages.) or vice versa. (Claude Shannon's idealized analog computer can only solve algebraic differential equations, while a digital computer can solve some transcendental equations as well. However this comparison is not entirely fair since in Claude Shannon's idealized analog computer computations are immediately done; i.e. computation is done in real time. Shannon's model can be adapted to cope with this problem.)
A canonical model of computation over the reals is Blum–Shub–Smale machine (BSS).
If real computation were physically realizable, one could use it to solve NP-complete problems, and even #P-complete problems, in polynomial time. Unlimited precision real numbers in the physical universe are prohibited by the holographic principle and the Bekenstein bound.


== See also ==
Hypercomputation, for other such powerful machines.


== References ==


== Further reading ==
Lenore Blum, Felipe Cucker, Michael Shub, and Stephen Smale. Complexity and Real Computation. ISBN 0-387-98281-7. CS1 maint: Multiple names: authors list (link)
Campagnolo, Manuel Lameiras (July 2001). Computational complexity of real valued recursive functions and analog circuits. Universidade Técnica de Lisboa, Instituto Superior Técnico. 
Natschläger, Thomas, Wolfgang Maass, Henry Markram. The ""Liquid Computer"" A Novel Strategy for Real-Time Computing on Time Series (PDF). CS1 maint: Multiple names: authors list (link)
Siegelmann, Hava. Neural Networks and Analog Computation: Beyond the Turing Limit. ISBN 0-8176-3949-7. 
Siegelmann, Hava & Sontag, Eduardo D. On The Computational Power Of Neural Nets."
362,Synaptic weight,14405160,3796,"In neuroscience and computer science, synaptic weight refers to the strength or amplitude of a connection between two nodes, corresponding in biology to the amount of influence the firing of one neuron has on another. The term is typically used in artificial and biological neural network research.


== Computation ==
In a computational neural network, a vector or set of inputs 
  
    
      
        
          
            x
          
        
      
    
    {\displaystyle {\textbf {x}}}
   and outputs 
  
    
      
        
          
            y
          
        
      
    
    {\displaystyle {\textbf {y}}}
  , or pre- and post-synaptic neurons respectively, are interconnected with synaptic weights represented by the matrix 
  
    
      
        w
      
    
    {\displaystyle w}
  , where for a linear neuron

  
    
      
        
          y
          
            j
          
        
        =
        
          ∑
          
            i
          
        
        
          w
          
            i
            j
          
        
        
          x
          
            i
          
        
         
         
        
          
            or
          
        
         
         
        
          
            y
          
        
        =
        w
        
          
            x
          
        
      
    
    {\displaystyle y_{j}=\sum _{i}w_{ij}x_{i}~~{\textrm {or}}~~{\textbf {y}}=w{\textbf {x}}}
  .
The synaptic weight is changed by using a learning rule, the most basic of which is Hebb's rule, which is usually stated in biological terms as

Neurons that fire together, wire together.

Computationally, this means that if a large signal from one of the input neurons results in a large signal from one of the output neurons, then the synaptic weight between those two neurons will increase. The rule is unstable, however, and is typically modified using such variations as Oja's rule, radial basis functions or the backpropagation algorithm.


== Biology ==
For biological networks, the effect of synaptic weights is not as simple as for linear neurons or Hebbian learning. However, biophysical models such as BCM theory have seen some success in mathematically describing these networks.
In the mammalian central nervous system, signal transmission is carried out by interconnected networks of nerve cells, or neurons. For the basic pyramidal neuron, the input signal is carried by the axon, which releases neurotransmitter chemicals into the synapse which is picked up by the dendrites of the next neuron, which can then generate an action potential which is analogous to the output signal in the computational case.
The synaptic weight in this process is determined by several variable factors:
How well the input signal propagates through the axon (see myelination),
The amount of neurotransmitter released into the synapse and the amount that can be absorbed in the following cell (determined by the number of AMPA and NMDA receptors on the cell membrane and the amount of intracellular calcium and other ions),
The number of such connections made by the axon to the dendrites,
How well the signal propagates and integrates in the postsynaptic cell.
The changes in synaptic weight that occur is known as synaptic plasticity, and the process behind long-term changes (long-term potentiation and depression) is still poorly understood. Hebb's original learning rule was originally applied to biological systems, but has had to undergo many modifications as a number of theoretical and experimental problems came to light.


== References ==


== See also ==
Neural network
Synaptic plasticity
Hebbian theory"
363,Generic sensor format,38253347,3778,"The generic sensor format (GSF) is a file format used for storing bathymetry data, such as that gathered by a multibeam echosounder. The format was created by Scott Ferguson and Daniel A. Chayes.
The file format specifications and C source code for a library to read and write GSF files are available from Leidos, who maintain both the format specification and the source code. The GSF library source code is published under the GNU Lesser General Public License, version 2.1.


== Usage ==
The following software packages support GSF:
Teledyne CARIS HIPS and SIPS
EIVA NaviEdit
Fledermaus
MB-System
QINSy
BeamworX AutoClean
WASSP
Qimera
HYPACK
ISS-2000
SABER


== References ==


== External links ==
Official website"
364,Moshe Tennenholtz,53750964,3772,"Moshe Tennenholtz is an Israeli computer scientist and professor with the faculty of Industrial Engineering and Management at the Technion, where he holds the Sondheimer Technion Academic Chair.


== Biography ==
Moshe Tennenholtz received his B.Sc. in Mathematics from Tel-Aviv University in 1986, and his M.Sc. and Ph.D. in 1987 and 1991) respectively from the Department of Applied Mathematics and Computer Science in the Weizmann Institute. From 1991 to 1993 he worked in the Robotics Laboratory at Stanford University, after which he joined the faculty at the Technion. He returned to Stanford briefly as a visiting professor from 1999 to 2002 before returning to the Technion. In 2008 he started working at Microsoft Research and in 2011 he founded the basic research group at the Microsoft Israel R&D center. He has served as editor-in-chief of the Journal of Artificial Intelligence Research, associate editor of Games and Economic Behavior, the international journal of Autonomous Agents and Multi-Agent Systems, served on the editorial board of the Journal of Machine Learning Research, and served on the editorial board of AI Magazine. He is an AAAI Fellow and a fellow of the Society for the Advancement of Economic Theory. He served as program chair of the ACM Electronic Commerce conference and of the TARK conference. He is a winner of the Allen Newell award and of the John McCarthy award for pioneering cobtributions to the interplay between artificial intelligence and game theory. He also received the ACM SIGART Autonomous Agents Research Award for 2012.


== References =="
365,Distributed Event-Based Systems,38184699,3761,"The International Conference on Distributed Event-Based Systems is a conference in computer science.


== History ==
The DEBS event began as a series of five workshops run annually from 2002 to 2006. These DEBS workshops were co-located variously with International Conference on Distributed Computing Systems (IEEE ICDCS), ACM SIGMOD Conference/PODS and International Conference on Software Engineering (ACM ICSE).
The inaugural DEBS conference was held in 2007, in Toronto, Canada, and has been held annually since.


== Conference structure ==
DEBS events follow the structure of many computer science conferences, runs a sequential track program, and includes tracks for:
Research papers
Industry submissions
Tutorials
Demonstrations and posters
and a doctoral workshop.
A recent, novel feature of the conference is the ""Grand Challenges"" track, which aims to provide a datasets and exercises by which academic and industrial teams may compete to demonstrate the strengths of their solutions.


== Location history ==
2017: Barcelona, Spain
2016: Irvine, California, United States
2015: Oslo, Norway
2014: Mumbai, India
2013: Arlington, Texas, United States
2012: Berlin, Germany
2011: New York City, New York, United States
2010: Cambridge, United Kingdom
2009: Nashville, Tennessee, United States
2008: Rome, Italy
2007: Toronto, Ontario, Canada


== DEBS Workshops ==
2006: Lisbon, Portugal
2005: Columbus, Ohio, United States
2004: Edinburgh, Scotland
2003: San Diego, California, USA
2002: Vienna, Austria


== See also ==
List of computer science conferences


== References ==


== External links ==
http://debs.org/
DEBS 2017- June 19-23, 2017, Barcelona, Spain
DEBS 2016- June 20–24, 2016, Irvine, CA, USA
DEBS 2015- June 29-July 3, 2015, Oslo, Norway
DEBS 2014 - May 26–29, 2014, Mumbai, India
DEBS 2013 - June 29-July 3, 2013, Arlington, Texas, USA
DEBS 2012 - July 16–20, 2012, Freie Universitaet Berlin, Berlin, Germany
DEBS 2011 - July 11–14, 2011, New York, U.S.
DEBS 2010 - July 12–15, 2010, Cambridge, United Kingdom
DEBS 2009 - July 6–9, 2009, Vanderbilt University Campus, Nashville, TN, USA
DEBS 2008 - July 2–4, 2008, Rome, Italy
DEBS 2007 - June 20–22, Toronto, Canada"
366,Nevanlinna Prize,21459,3758,"The Rolf Nevanlinna Prize (named in honor of Rolf Nevanlinna) is awarded once every 4 years at the International Congress of Mathematicians, for outstanding contributions in Mathematical Aspects of Information Sciences including:
All mathematical aspects of computer science, including computational complexity theory, logic of programming languages, analysis of algorithms, cryptography, computer vision, pattern recognition, information processing and modelling of intelligence.
Scientific computing and numerical analysis. Computational aspects of optimization and control theory. Computer algebra.
The prize was established in 1981 by the Executive Committee of the International Mathematical Union IMU and named to honour the Finnish mathematician Rolf Nevanlinna who had died a year earlier. The award consists of a gold medal and cash prize. Like the Fields Medal the prize is targeted at younger mathematicians, and only those younger than 40 on January 1 of the award year are eligible.
The medal features a profile of Nevanlinna, the text ""Rolf Nevanlinna Prize"", and very small characters ""RH 83"" on its obverse. RH refers to Raimo Heino, the medal's designer, and 83 to the year of first minting. On the reverse, two figures related to the University of Helsinki, the prize sponsor, are engraved. The rim bears the name of the prizewinner.


== Laureates ==


== See also ==
Turing Award
Gödel Prize
Abel Prize
Fields Medal
Gauss Prize
Chern Medal
Schock Prize
Wolf Prize
List of science and technology awards


== Notes ==


== External links ==
Rolf Nevanlinna Prizes - Official site"
367,Bus contention,376507,3752,"Bus contention, in computer design, is an undesirable state of the bus in which more than one device on the bus attempts to place values on the bus at the same time. Most bus architectures require their devices to follow an arbitration protocol carefully designed to make the likelihood of contention negligible. However, when devices on the bus have logic errors, manufacturing defects, or are driven beyond their design speeds, arbitration may break down and contention may result. Contention may also arise on systems which have a programmable memory mapping when illegal values are written to the registers controlling the mapping.
Contention can lead to erroneous operation, and, in unusual cases, permanent damage to the hardware—such as burning out a MOSFET, or fusing of the bus wiring.
Bus contention is sometimes countered by buffering the output of memory-mapped devices. However, it has been noted that high impedance from one device will still interfere with the bus values of other devices. Currently, no standard solution exists for data-bus contention between memory devices, such as EEPROM and SRAM.
Most small-scale computer systems are carefully designed to avoid bus contention on the system bus. They use a single device, called bus arbiter, that controls which device is allowed to drive the bus at each instant, so bus contention never happens in normal operation.
The standard solution to bus contention between memory devices, such as EEPROM and SRAM, is the three-state bus with a bus arbiter.
Some networks, such as token ring, are also designed to avoid bus contention, so bus contention never happens in normal operation.
Most networks are designed with hardware robust enough to tolerate occasional bus contention on the network. CAN bus, ALOHAnet, Ethernet, etc., all experience occasional bus contention in normal operation, but use some protocol (such as Multiple Access with Collision Avoidance, carrier-sense multiple access with collision detection, or automatic repeat request) to minimize the times that contention occurs, and to re-send data that was corrupted in a packet collision.
Bus contention is the kind of telecommunication contention that occurs when all communicating devices communicate directly with each other through a single shared channel (or through a switch), and contrasted with ""network contention"" that occurs when communicating devices communicate indirectly with each other, through point-to-point connections through routers or bridges.


== References =="
368,X + Y sorting,43934137,3748,"In computer science, X + Y sorting is the problem of sorting pairs of numbers by their sum. Given two finite sets X and Y, the problem is to order all pairs (x, y) in the Cartesian product X × Y by the key x + y. The problem is attributed to Elwyn Berlekamp.
This problem can be solved using a straightforward comparison sort on the Cartesian product, taking time O(nm log(nm)) for sets of sizes n and m. When it is assumed that m = n, the complexity is O(n2 log n2) = O(n2 log n), which is also the best known bound on the problem, but whether X + Y sorting can be done strictly faster than sorting n⋅m arbitrary numbers is an open problem. The number of required comparisons is certainly lower than for ordinary comparison sorting: Fredman showed, in 1976, that X + Y sorting can be done using only O(n2) comparisons, though he did not show an algorithm. The first actual algorithm that achieves this number of comparisons and O(n2 log n) total complexity was only published sixteen years later.
On a RAM machine with word size w and integer inputs 0 ≤ {x, y} < n = 2w, the problem can be solved in O(n log n) operations by means of the fast Fourier transform.
Skiena recounts a practical application in transit fare minimisation, an instance of the shortest path problem: given fares x and y for trips from departure A to some intermediate destination B and from B to final destination C, determine the least expensive combined trip from A to C.


== See also ==
3SUM
Integer sorting


== References =="
369,Order statistic tree,36548922,3747,"In computer science, an order statistic tree is a variant of the binary search tree (or more generally, a B-tree) that supports two additional operations beyond insertion, lookup and deletion:
Select(i) — find the i'th smallest element stored in the tree
Rank(x) – find the rank of element x in the tree, i.e. its index in the sorted list of elements of the tree
Both operations can be performed in O(log n) worst case time when a self-balancing tree is used as the base data structure.


== Augmented search tree implementation ==
To turn a regular search tree into an order statistic tree, the nodes of the tree need to store one additional value, which is the size of the subtree rooted at that node (i.e., the number of nodes below it). All operations that modify the tree must adjust this information to preserve the invariant that

size[x] = size[left[x]] + size[right[x]] + 1

where size[nil] = 0 by definition. Select can then be implemented as

function Select(t, i)
    // Returns the i'th element (zero-indexed) of the elements in t
    l ← size[left[t]]
    if i = l
        return key[t]
    else if i < l
        return Select(left[t], i)
    else
        return Select(right[t], i - (l + 1))

Rank can be implemented as

function Rank(T, x)
    // Returns the position of x (one-indexed) in the linear sorted list of elements of the tree T
    r ← size[left[x]] + 1
    y ← x
    while y ≠ T.root
         if y = right[y.p]
              r ← r + size[left[y.p]] + 1
         y ← y.p
    return r

Order-statistic trees can be further amended with bookkeeping information to maintain balance (e.g., tree height can be added to get an order statistic AVL tree, or a color bit to get a red-black order statistic tree). Alternatively, the size field can be used in conjunction with a weight-balancing scheme at no additional storage cost.


== Other implementations ==
Another way to implement an order statistic tree is an implicit data structure derived from the min-max heap.


== References ==


== External links ==
Order statistic tree on PineWiki, Yale University.
The Python package blist uses order statistic B-trees to implement lists with fast insertion at arbitrary positions."
370,Graph kernel,39419087,3736,"In structure mining, a domain of learning on structured data objects in machine learning, a graph kernel is a kernel function that computes an inner product on graphs. Graph kernels can be intuitively understood as functions measuring the similarity of pairs of graphs. They allow kernelized learning algorithms such as support vector machines to work directly on graphs, without having to do feature extraction to transform them to fixed-length, real-valued feature vectors. They find applications in bioinformatics, in chemoinformatics (as a type of molecule kernels), and in social network analysis.
Concepts of graph kernels have been around since the 1999, when D. Haussler introduced convolutional kernels on discrete structures. The term graph kernels was more officially coined in 2002 by R. I. Kondor and John Lafferty as kernels on graphs, i.e. similarity functions between the nodes of a single graph, with the World Wide Web hyperlink graph as a suggested application. Vishwanathan et al. instead defined kernels between graphs. Lately in 2018, the works of Ghosh et. al  described the rich history of graph kernels and its evolution through a span of two decades.
An example of a kernel between graphs is the random walk kernel, which conceptually performs random walks on two graphs simultaneously, then counts the number of paths that were produced by both walks. This is equivalent to doing random walks on the direct product of the pair of graphs, and from this, a kernel can be derived that can be efficiently computed.


== References ==


== See also ==
Tree kernel, as special case of non-cyclic graphs
Molecule mining, as special case of small multi-label graphs"
371,Web@cademie,52321029,3711,"Web@cademie is a private, nonprofit and tuition-free computer programming school created and funded by French IONIS Education Group with several partners including Epitech and Zup de Co association. The school was first opened in Paris in 2010.
Headquartered in Le Kremlin-Bicêtre, the school has branches in Lyon and Strasbourg.
Web@cademie delivers a two-year program dedicated to people with no degree and no background (without the French Baccalaureate but with a strong motivation in computer science). This is to help dropout students to have a job in a competitive industry.
The school is supported by French government and has received the award fr:Grande École du Numérique (IT university). The school is a non-profit organization and is entirely free. Major company such as Microsoft give financial support.


== References ==


== External links ==
Official website (in English) (in French)"
372,Information Processing Society of Japan,40804819,3707,"The Information Processing Society of Japan (""IPSJ"") is a Japanese learned society for computing. Founded in 1960, it is headquartered in Tokyo, Japan. IPSJ publishes a magazine and several professional journals mainly in Japanese, and sponsors conferences and workshops, also mainly conducted in Japanese. It has nearly 20,000 members. IPSJ is a full member of the International Federation for Information Processing. The current president is Masaru Kitsuregawa, appointed in 2013.


== Publications ==
IPSJ publishes one magazine, several journals, and several peer-reviewed transactions. Most of these publications primarily carry articles and peer-reviewed papers in Japanese, but accept some articles in English, especially for transactions special issues.
Joho Shori magazine
Journal of Information Processing
Journal of Digital Practice
Transactions on:Programming (PRO)
Database (TOD)
Consumer Device & System (CDS)
Bioinformatics (TBIO) (English only)
Computer Vision and Applications (CVA) (English only)
Mathematical Modeling and its Applications (TOM)
Advanced Computing Systems (ACS)
Digital Contents (DCON)
System Design LSI Methodology (T-SDLM) (English only)

IPSJ Online Transactions (open access republishing of English-language papers previously published in primarily-Japanese transactions)


== Fellows ==
Every year since 1999, IPSJ has inducted a new group of Japanese Fellows. It has no foreign or international fellows and most, if not all, fellows are Japanese.


== Online Computer Museum ==
IPSJ maintains an excellent online Computer Museum of computers developed in Japan, featuring equipment ranging from old mechanical calculators to modern supercomputers, in both English and Japanese.


== References ==


== External links ==
Official website
IPSJ Computer Museum"
373,Conference on Embedded Networked Sensor Systems,5963255,3693,"SenSys, the ACM Conference on Embedded Networked Sensor Systems, is an annual academic conference in the area of embedded networked sensors.


== About SenSys ==
ACM SenSys is a selective, single-track forum for the presentation of research results on systems issues in the area of embedded networked sensors. The conference provides a venue to address the research challenges facing the design, deployment, use, and fundamental limits of these systems. Sensor networks require contributions from many fields, from wireless communication and networking, embedded systems and hardware, distributed systems, data management, and applications. SenSys welcomes cross-disciplinary work.


== Ranking ==
Although there is no official ranking of academic conferences on wireless sensor networks, SenSys is widely regarded by researchers as one of the two (along with IPSN) most prestigious conferences focusing on sensor network research. SenSys focuses more on system issues while IPSN on algorithmic and theoretical considerations. The acceptance rate for 2017 was 17.2% (26 out of 151 papers accepted for publication).


== SenSys Events ==
SenSys started in year 2003 and following is a list of SenSys events from 2003 to 2017:
SenSys 2017, Delft, The Netherlands, November 5–8, 2017
SenSys 2016, Stanford, CA, USA, November 14–16, 2016
SenSys 2015, Seoul, South Korea, November 1–4, 2015
SenSys 2014, Memphis, Tennessee, USA, November 3–6, 2014
SenSys 2013, Rome, Italy, November 11–14, 2013
SenSys 2012, Toronto, Canada, November 6–9, 2012
SenSys 2011, Seattle, WA, USA, November 1–4, 2011
SenSys 2010, Zurich, Switzerland, November 3–5, 2010
SenSys 2009, Berkeley, California, USA, November 4–6, 2009
SenSys 2008, Raleigh, North Carolina, USA, November 5–7, 2008
SenSys 2007, Sydney, Australia, November 6–9, 2007
SenSys 2006, Boulder, Colorado, USA, November 1–3, 2006
SenSys 2005, San Diego, CA, USA, November 2–4, 2005
SenSys 2004, Baltimore, MD, USA, November 3–5, 2004
SenSys 2003, Los Angeles, California, USA, November 5–7, 2003


== Sponsors ==
SenSys is sponsored by the following ACM Special Interest Groups:
SIGCOMM
SIGMOBILE
SIGARCH
SIGOPS
SIGMETRICS
SIGBED


== See also ==
Wireless sensor network


== External links ==
SenSys
SenSys Bibliography (from DBLP)
SenSys Proceedings (from ACM website)"
374,J.W. Graham Medal,30324173,3683,"The J.W. Graham Medal in Computing and Innovation is an award given annually by the University of Waterloo and the University of Waterloo Faculty of Mathematics to ""recognize the leadership and many innovative contributions made to the University of Waterloo, and to the Canadian computer industry.""  Recipients of this award receive a gold medal and certificate. Recipients are graduates of the University of Waterloo Faculty of Mathematics from business, education, or government.
The medal was established in 1994 to recognize Canadian computer industry veteran James Wesley Graham. Generally known as Wes Graham, he was born in Copper Cliff, Ontario on January 17, 1932. He enrolled in the University of Toronto in 1950, and graduated with a BA in Mathematics and Physics in 1954, and an MA in Mathematics in 1955. He worked as a systems engineer for IBM in Canada, and then joined the faculty of the University of Waterloo in 1959. A team of his students developed the WATFOR series of compilers starting in 1965. He formed a computer science research group, known as the ""Computer Systems Group,"" to distribute and maintain the software, and was also responsible for several spin-off organizations, including Watcom in 1981. He was made a member of the Order of Canada in April 1999. He died later that year on August 23, 1999. In 2001 his papers formed the start of the J. Wesley Graham History of Computer Science Research Collection at the University of Waterloo library.


== J.W. Graham Medal recipients ==
Following people received the J.W. Graham Medal:
1995 - Ian McPhee
1996 - William Reeves
1997 - James G. Mitchell
1998 - Dan Dodge
1999 - Kim Davidson
2000 - Paul Van Oorschot
2001 - Terry Stepien
2002 - Peter Savich
2003 - F. David Boswell
2004 - David P. Yach
2005 - Garth A. Gibson
2006 - Deanne Farrar
2007 - Ricardo Baeza-Yates
2008 - Eric Veach
2009 - Craig Eisler
2010 - Steven Woods
2011 - Zack Urlocker
2012 - Stephen M. Watt
2013 - Jay Steele
2014 - Jeromy Carriere
2015 - Tom Duff
2016 - Tas Tsonis
2017 - Vicki Iverson


== See also ==
List of science and technology awards
Prizes named after people


== References =="
375,List of combinatorial computational geometry topics,2920840,3676,"List of combinatorial computational geometry topics enumerates the topics of computational geometry that states problems in terms of geometric objects as discrete entities and hence the methods of their solution are mostly theories and algorithms of combinatorial character.
See List of numerical computational geometry topics for another flavor of computational geometry that deals with geometric objects as continuous entities and applies methods and algorithms of nature characteristic to numerical analysis.


== Construction/representation ==
Boolean operations on polygons
Convex hull
Hyperplane arrangement
Polygon decomposition
Polygon triangulation
Minimal convex decomposition
Minimal convex cover problem (NP-hard)
Minimal rectangular decomposition

Tessellation problems

Shape dissection problems
Straight skeleton
Stabbing line problem
Triangulation
Delaunay triangulation
Point set triangulation
Polygon triangulation

Voronoi diagram


== Extremal shapes ==
Minimum bounding box (Smallest enclosing box, Smallest bounding box)
2-D case: Smallest bounding rectangle (Smallest enclosing rectangle)
There are two common variants of this problem.
In many areas of computer graphics, the bounding box (often abbreviated to bbox) is understood to be the smallest box delimited by sides parallel to coordinate axes which encloses the objects in question.
In other applications, such as packaging, the problem is to find the smallest box the object (or objects) may fit in (""packaged""). Here the box may assume an arbitrary orientation with respect to the ""packaged"" objects.

Smallest bounding sphere (Smallest enclosing sphere)
2-D case: Smallest bounding circle

Largest empty rectangle (Maximum empty rectangle)
Largest empty sphere
2-D case: Maximum empty circle (largest empty circle)


== Interaction/search ==
Collision detection
Line segment intersection
Point location
Point in polygon

Polygon intersection
Range searching
Orthogonal range searching
Simplex range searching

Ray casting (not to be confused with ray tracing of computer graphics)


=== Proximity problems ===
Closest pair of points
Closest point problem
Diameter of a point set
Delaunay triangulation
Voronoi diagram


=== Visibility ===
Visibility (geometry)
Art gallery problem (The museum problem)
Visibility graph
Watchman route problem
Computer graphics applications:
Hidden surface determination
Hidden line removal

Ray casting (not to be confused with ray tracing of computer graphics)


== Other ==
Happy ending problem
Ham sandwich problem
shape assembly problems
shape matching problems
Klee's measure problem
Problems on isothetic polygons and isothetic polyhedra
Orthogonal convex hull

Path planning
Paths among obstacles
Shortest path in a polygon

Polygon containment
Robust geometric computation addresses two main issues: fixed-precision representation of real numbers in computers and possible geometrical degeneracy (mathematics) of input data"
376,Quaject,51401190,3669,"In computer science, a quaject is an object-like data structure containing both data and code (or pointers to code), exposed as an interface in the form of callentries, and can accept a list of callentries to other quajects for callbacks and callouts. They were developed by Alexia Massalin in 1989 for the Synthesis kernel, and named for the Qua! Machine, a unique hardware platform built by Massalin. The origin of the term 'qua' is unclear; Massalin claims humorously that it is a sound made by koalas.
The main purpose of quajects is to provide an abstraction to manage self-modifying code, by allowing runtime code optimizing on a per-object basis. While the original Synthesis kernel required quajects to be written in hand-developed assembly language, this was done to avoid developing a complex compiler; Massalin noted that just-in-time compilation (JIT) for a high-level programming language that permits runtime code generation, as in Lisp or Smalltalk, can also apply this approach, though she also asserted that the complexity of such a compiler was likely to be prohibitive.
Quajects differ from more conventional objects in two key ways: first, they always use a form of the dependency injection pattern to manage both interfaces to other quajects, and continuations out of the quaject; the list of callentry references for this is part of quaject creation, and may be updated during the quaject's lifetime. Second, and more critically, a given quaject's set of methods can be unique to the specific quaject; methods for a type or class of quajects are stored as one or more templates, rather than as fixed code. While shared methods can be accessed through a common table of pointers, individual quajects can also have methods that are generated specifically to tailor the performance for that quaject's behavior.


== References =="
377,One-hot,2054813,3656,"In digital circuits, one-hot is a group of bits among which the legal combinations of values are only those with a single high (1) bit and all the others low (0). A similar implementation in which all bits are '1' except one '0' is sometimes called one-cold.


== Applications ==
One-hot encoding is often used for indicating the state of a state machine. When using binary or Gray code, a decoder is needed to determine the state. A one-hot state machine, however, does not need a decoder as the state machine is in the nth state if and only if the nth bit is high.
A ring counter with 15 sequentially-ordered states is an example of a state machine. A 'one-hot' implementation would have 15 flip flops chained in series with the Q output of each flip flop connected to the D input of the next and the D input of the first flip flop connected to the Q output of the 15th flip flop. The first flip flop in the chain represents the first state, the second represents the second state, and so on to the 15th flip flop which represents the last state. Upon reset of the state machine all of the flip flops are reset to '0' except the first in the chain which is set to '1'. The next clock edge arriving at the flip flops advances the one 'hot' bit to the second flip flop. The 'hot' bit advances in this way until the 15th state, after which the state machine returns to the first state.
An address decoder converts from binary or gray code to one-hot representation. A priority encoder converts from one-hot representation to binary or gray code.
In natural language processing, a one-hot vector is a 1 × N matrix (vector) used to distinguish each word in a vocabulary from every other word in the vocabulary. The vector consists of 0s in all cells with the exception of a single 1 in a cell used uniquely to identify the word.


== Differences from other encoding methods ==


=== Advantages ===
Determining the state has a low and constant cost of accessing one flip-flop
Changing the state has the constant cost of accessing two flip-flops
Easy to design and modify
Easy to detect illegal states
Takes advantage of an FPGA's abundant flip-flops
Using a one-hot implementation typically allows a state machine to run at a faster clock rate than any other encoding of that state machine.


=== Disadvantages ===
Requires more flip-flops than other encodings, making it impractical for PAL devices
Many of the states are illegal


== See also ==
Bi-quinary coded decimal
Unary numeral system
Uniqueness quantification
XOR gate
Binary decoder
Serial decimal


== References =="
378,Model of computation,1773278,3655,"In computer science, and more specifically in computability theory and computational complexity theory, a model of computation is a model which describes how a set of outputs are computed given a set of inputs. This model describes how units of computations, memories, and communications are organized. The computational complexity of an algorithm can be measured given a model of computation. Using a model allows studying the performance of algorithms independently of the variations that are specific to particular implementations and specific technology.


== Models ==
Models of computation can be classified in three categories: sequential models, functional models, and concurrent models.
Sequential models include:
Finite state machines
Pushdown automata
Turing Machine
Functional models include:
Lambda calculus
Recursive functions
Combinatory logic
Cellular automaton
Abstract rewriting systems
Concurrent models include:
Kahn process networks
Petri nets
Synchronous Dataflow
Interaction nets


== Uses ==
In the field of runtime analysis of algorithms, it is common to specify a computational model in terms of primitive operations allowed which have unit cost, or simply unit-cost operations. A commonly used example is the random access machine, which has unit cost for read and write access to all of its memory cells. In this respect, it differs from the above-mentioned Turing machine model.
In model-driven engineering, the model of computation explains how the behaviour of the whole system is the result of the behaviour of each of its components.
A key point which is often overlooked is that published lower bounds for problems are often given for a model of computation that is more restricted than the set of operations that one could use in practice and therefore there may be algorithms that are faster than what would naïvely be thought possible.


== Categories ==
There are many models of computation, differing in the set of admissible operations and their computations cost. They fall into the following broad categories: abstract machine and models equivalent to it (e.g. lambda calculus is equivalent to the Turing machine), used in proofs of computability and upper bounds on computational complexity of algorithms, and decision tree models, used in proofs of lower bounds on computational complexity of algorithmic problems.


== See also ==
Stack machine (0-operand machine)
Accumulator machine (1-operand machine)
Register machine (2,3,... operand machine)
Random access machine
Cell-probe model


== References ==


== Further reading ==
Fernández, Maribel (2009). Models of Computation: An Introduction to Computability Theory. Undergraduate Topics in Computer Science. Springer. ISBN 978-1-84882-433-1. 
Savage, John E. (1998). Models Of Computation: Exploring the Power of Computing."
379,International Conference on Distributed Computing Systems,5843179,3648,"The International Conference on Distributed Computing Systems (ICDCS) is the oldest conference in the field of distributed computing systems in the world. It was launched by the IEEE Computer Society Technical Committee on Distributed Processing (TCDP) in October 1979, and is sponsored by such committee. It was started as an 18-month conference until 1983 and became an annual conference since 1984. The ICDCS has a long history of significant achievements and worldwide visibility, and has recently celebrated its 37th year.


== Location history ==
2018: Vienna, Austria
2017: Atlanta, GA, United States
2016: Nara, Japan
2015: Columbus, Ohio, United States
2014: Madrid, Spain
2013: Philadelphia, Pennsylvania, United States
2012: Macau, China
2011: Minneapolis, Minnesota, United States
2010: Genoa, Italy
2009: Montreal, Quebec, Canada
2008: Beijing, China
2007: Toronto, Ontario, Canada
2006: Lisbon, Portugal
2005: Columbus, Ohio, United States
2004: Keio University, Japan
2003: Providence, RI, United States
2002: Vienna, Austria
2001: Phoenix, AZ, United States
2000: Taipei, Taiwan
1999: Austin, TX, United States
1998: Amsterdam, The Netherlands
1997: Baltimore, MD, United States
1996: Hong Kong
1995: Vancouver, Canada
1994: Poznań, Poland
1993: Pittsburgh, PA, United States
1992: Yokohama, Japan
1991: Arlington, TX, United States
1990: Paris, France
1989: Newport Beach, CA, United States
1988: San Jose, CA, United States
1987: Berlin, Germany
1986: Cambridge, MA, United States
1985: Denver, CO, United States
1984: San Francisco, CA, United States
1983: Hollywood, FL, United States
1981: Versailles, France
1979: Huntsville, AL, United States


== See also ==
List of distributed computing conferences


== External links ==
ICDCS 2018 - July 2–July 5, 2018, Vienna, Austria
ICDCS 2007 - June 25–June 29, 2007, Toronto, Canada.
ICDCS 2006 - July 4–July 7, 2006, Lisbon, Portugal.
ICDCS 2005 - July 6–July 10, 2005, Columbus, Ohio, United States.
ICDCS 2004 - March 23–March 26, 2004, Keio University, Japan.
ICDCS 2003 - May 19–May 22, 2003, Providence, RI, United States.
ICDCS 2002 - July 2–July 5, 2002, Vienna, Austria.
ICDCS 2001 - April 16–April 19, 2001, Phoenix, AZ, United States."
380,Leon J. Osterweil,39687867,3640,"Leon Joel Osterweil is an American Computer Scientist noted for his research on software engineering.


== Biography ==
Osterweil received a B. A. in Mathematics from Princeton University in 1965. He received a M.A. in Mathematics in 1970 and a Ph.D in Mathematics in 1971 from the University of Maryland.
He then joined the Department of Computer Science at the University of Colorado Boulder as an assistant professor in 1971. While there he was promoted to associate professor in 1977 and to professor in 1982, he was chair of the department from 1981 to 1986. In 1988, he became a professor at the University of California at Irvine and he was department chair from 1989 to 1992. In 1993, he became a professor of Computer Science at the University of Massachusetts Amherst.


== Awards ==
In the year 1998, he was named an ACM Fellow.
His other notable awards include:
ACM SIGSOFT Outstanding Research Award, 2003
ICSE's Most Influential Paper Award, 1997
ACM SIGSOFT Influential Educator Award, 2010


== References ==


== External links ==
University of Massachusetts Amherst: Leon J. Osterweil, Department of Computer Science"
381,International Conference on Functional Programming,4280030,3637,"The ACM SIGPLAN International Conference on Functional Programming (ICFP) is an annual academic conference in the field of computer science sponsored by the ACM SIGPLAN, in association with IFIP Working Group 2.8 (Functional Programming). The conference focuses on functional programming and related areas of programming languages, logic, compilers and software development.
The ICFP was first held in 1996, replacing two biennial conferences: Functional Programming and Computer Architecture (FPCA) and LISP and Functional Programming (LFP). The conference location alternates between Europe (odd-numbered years) and North America (even-numbered years). The conference usually lasts 3 days, surrounded by co-located workshops devoted to particular functional languages or application areas.
The ICFP has also held an open annual programming contest since 1998, called the ICFP Programming Contest.


== History ==
2012: 17th ACM SIGPLAN International Conference on Functional Programming in Copenhagen, Denmark (General Chair: Peter Thiemann, University of Freiburg; Program Chair: Robby Findler, Northwestern University)


== Affiliated events ==
Commercial Users of Functional Programming (CUFP)
Erlang Workshop
Haskell Symposium
Functional and Declarative Programming in Education (FDPE)
Functional Programming Developer Tracks (DEFUN)
MEchanized Reasoning about Languages with varIable biNding (MERLIN)
Workshop on Approaches and Applications of Inductive Programming
Workshop on Curry and Functional Logic Programming
Workshop on Generic Programming (WGP)
Workshop on Mechanizing Metatheory (WMM)
Workshop on ML
Workshop on Scheme and Functional Programming
Programming Languages meets Program Verification (PLPV) — 2007 only, now affiliated with POPL


== See also ==
Related conferences
FLOPS: International Symposium on Functional and Logic Programming
IFL: International Symposia on Implementation and Application of Functional Languages
ISMM: International Symposium on Memory Management
MPC: International Conference on Mathematics of Program Construction
PLDI: Programming Language Design and Implementation
POPL: Principles of Programming Languages
PPDP: International Conference on Principles and Practice of Declarative Programming
TFP: Symposium on Trends in Functional Programming
TLCA: International Conference on Typed Lambda Calculi and Applications
TLDI: International Workshop on Types in Language Design and Implementation
SAS: International Static Analysis Symposium
Related journals
Journal of Functional Programming
Journal of Functional and Logic Programming
Higher-Order and Symbolic Computation
ACM Transactions on Programming Languages and Systems


== External links ==
ICFP main site
ICFP 2008 conference
ICFP 2007 conference
ICFP 2006 conference
ICFP Programming Contest
Functional Programming conference"
382,European Society for Fuzzy Logic and Technology,28232105,3635,"The European Society for Fuzzy Logic and Technology (EUSFLAT) is a scientific association with the aims to disseminate and promote fuzzy logic and related subjects (sometimes comprised under the collective terms soft computing or computational intelligence) and to provide a platform for exchange between scientists and engineers working in these fields. The society is both open for academic and industrial members.


== History ==
EUSFLAT was founded in 1998 in Spain as the successor of the National Spanish Fuzzy Logic Society, ESTYLF, with the aim to open the society for members from other European countries. Since then, the society managed to attract a large share of members from outside Spain, and even beyond Europe, with the Spanish members still being the largest group inside EUSFLAT. For these historical reasons, the society is officially registered in Spain.


== Conferences ==
Starting with 1999, EUSFLAT has been organizing its biannual conferences in odd years. Previous meetings:
Palma de Mallorca, Balearic Islands, Spain, September 22–25, 1999 (jointly with National Spanish conference, ESTYLF)
Leicester, United Kingdom, September 5–7, 2001
Zittau, Germany, September 10–12, 2003
Barcelona, Catalonia, Spain, September 7–9, 2005 (jointly with 11th Rencontres Francophones sur la Logique Floue et ses Applications)
Ostrava, Czech Republic, September 11–14, 2007
Lisbon, Portugal, July 20–24, 2009 (jointly with 13th World Congress of the International Fuzzy Systems Association)
Aix-les-Bains, France, July 18–22, 2011 (jointly with Les Rencontres Francophones sur la Logique Floue et ses Applications)
Milan, Italy, September 11–13, 2013
Gijón, Spain, June, 30–3 July 2015


== Publications ==
EUSFLAT publishes the proceedings of its conferences in an open access manner.
Until 2010, Mathware & Soft Computing was the official journal of EUSFLAT. On July 1, 2010, the International Journal of Computational Intelligence Systems (Atlantis Press, ISSN 1875-6891 (print) / ISSN 1875-6883 (on-line)) became the official journal of EUSFLAT.
EUSFLAT publishes an electronic newsletter with three issues a year.


== Presidents ==
EUSFLAT is led by the President, who is elected for a two-year period, and cannot serve for more than two consecutive periods.
Francesc Esteva (1998–2011)
Luis Magdalena (2001–2005)
Ulrich Bodenhofer (2005–2009)
Javier Montero (2009–2013)
Gabriella Pasi (2013–present)


== References ==


== External links ==
The EUSFLAT website"
383,Dichotomic search,3021223,3612,"In computer science, a dichotomic search is a search algorithm that operates by selecting between two distinct alternatives (dichotomies) at each step. It is a specific type of divide and conquer algorithm. A well-known example is binary search.
Abstractly, a dichotomic search can be viewed as following edges of an implicit binary tree structure until it reaches a leaf (a goal or final state). This creates a theoretical tradeoff between the number of possible states and the running time: given k comparisons, the algorithm can only reach O(2k) possible states and/or possible goals.
Some dichotomic searches only have results at the leaves of the tree, such as the Huffman tree used in Huffman compression, or the implicit classification tree used in Twenty Questions. Other dichotomic searches also have results in at least some internal nodes of the tree, such as a dichotomic search table for Morse code. There is thus some looseness in the definition. Though there may indeed be only two paths from any node, there are thus three possibilities at each step: choose one onwards path or the other, or stop at this node.

Dichotomic searches are often used in repair manuals, sometimes graphically illustrated with a flowchart similar to a fault tree.


== References ==
Dictionary of Algorithms and Data Structures: Dichotomic search"
384,Bachelor of Computer Application,20610484,3610,"Bachelor of Computer Applications is a three-year undergraduate degree course in the field of computer applications/computer science.After BCA the students can do further studies as MCA master in computer application. It is a common degree for CS/IT in Indian universities and is an alternative to the engineering counterpart, BE/BTech in Computer Science/IT which takes 4 years. It is a technical degree that prepares students for a career in the field of computer applications and software development.


== Universities of India offering this course ==
Amrita Vishwa Vidyapeetham
Sambalpur University
Chandigarh University
Osmania University
Nizam College
Bangalore University
Barkatullah University
Devi Ahilya Vishwavidyalaya
Gujarat University
Guru Ghasidas University
Guru Gobind Singh Indraprastha University
Guwahati University
Cotton College State University
Dibrugarh University
University of Rajasthan
Jai Narain Vyas University
Jain University
Jaipur National University
Sambalpur University
Manonmaniam Sundaranar University
Maharshi Dayanand University
Makhanlal Chaturvedi Rashtriya Patrakarita evam Sanchar Vishwavidyalaya
Jiwaji University
Rashtrasant Tukadoji Maharaj Nagpur University
University of Calicut
University of Madras
University of Hyderabad
University of Mysore
University of Pune
TeamLease Skills University
The Glocal University
Tripura University
Veer Narmad South Gujarat University
Lucknow University
Dr. C. V. Raman University
Bharathiar University
Dr. Ram Manohar Lohia Avadh University
Dr. Babasaheb Ambedkar Technological University
DCRUST University, Sonipat
Magadh University
Tumkur University
Institute of Management Studies,Noida
Kanpur University


== Universities of Nepal offering BCA course ==
Purbanchal University
Pokhara University
Tribhuwan University


== Open Universities of India offering this course ==
Indira Gandhi National Open University
Lovely Professional University
Sikkim Manipal University, Distance Education
Symbiosis Center for Distance Learning
TeamLease Skills University


== References =="
385,DevMountain,52190521,3609,"DevMountain is a private coding bootcamp school that offers in-person and online courses ranging from 6 to 26 weeks in a variety of subjects including web development, mobile programming, user experience design, software quality assurance, and salesforce development. The school was founded in Provo, Utah by Cahlan Sharp, Tyler Richards, and Colt Henrie in 2013. DevMountain provides free housing to its full-time students for the length of the program to help them accommodate long hours of work.
As of August 2017, DevMountain has four campus locations in Salt Lake City, Utah, Provo, Utah, Dallas, Texas, and Phoenix, Arizona.
In April 2016, the software engineering school, DevMountain was acquired by Capella Education Company for $20 million.


== See also ==
Web development
Capella Education Company
Coding bootcamp


== References ==


== External links ==
Official website"
386,IBM WebFountain,3103995,3604,"WebFountain is an Internet analytical engine implemented by IBM for the study of unstructured data on the World Wide Web. IBM describes WebFountain as:

. . . a set of research technologies that collect, store and analyze massive amounts of unstructured and semi-structured text. It is built on an open, extensible platform that enables the discovery of trends, patterns and relationships from data.

The project represents one of the first comprehensive attempts to catalog and interpret the unstructured data of the Web in a continuous fashion. To this end its supporting researchers at IBM have investigated new systems for the precise retrieval of subsets of the information on the Web, real-time trend analysis, and meta-level analysis of the available information of the Web.
Factiva, an information retrieval company owned by Dow Jones and Reuters, licensed WebFountain in September 2003, and has been building software which utilizes the WebFountain engine to gauge corporate reputation. Factiva reportedly offers yearly subscriptions to the service for $200,000. Factiva has since decided to explore other technologies, and has severed its relationship with WebFountain.
WebFountain is developed at IBM's Almaden research campus in the Bay Area of California.
IBM has developed software, called UIMA for Unstructured Information Management Architecture, that can be used for analysis of unstructured information. It can perhaps help perform trend analysis across documents, determine the theme and gist of documents, allow fuzzy searches on unstructured documents.


== References ==


== External links ==
IBM Almaden Research Center WebFountain overview
WebFountain on John Battelle's Searchblog
Zdnet article ""Drinking from the Fire Hydrant""
Cnet article at Archive.is (archived 2012-07-21) IBM sets out to make sense of the Web, February 5, 2004
IBM Joins Corporate Monitoring Space with Release of Public Image Monitoring Solution, Search Engine Watch, November 9, 2005"
387,British Colloquium for Theoretical Computer Science,7628891,3597,"The British Colloquium for Theoretical Computer Science (BCTCS) is an organisation that hosts an annual event for UK-based researchers in theoretical computer science. A central aspect of BCTCS is the training of PhD students.
The purpose of BCTCS is:
to offer a regular forum in which UK-based researchers in all aspects of theoretical computer science can meet, present research findings, and discuss recent developments in the field;
to foster an environment within which PhD students undertaking research in theoretical computer science may gain experience in presenting their work in a formal arena, broaden their outlook on the subject, and benefit from contact with established researchers in the community; and
to provide a platform by which the interests and future well-being of British theoretical computer science may be advanced.
The scope of BCTCS includes all aspects of theoretical computer science, including algorithms, complexity, semantics, formal methods, concurrency, types, languages and logics. An emphasis on breadth, together with the inherently mathematical nature of theoretical computer science, means that BCTCS always actively solicits both computer scientists and mathematicians as participants, and offers an environment within which the two communities can meet and exchange ideas.
BCTCS is primarily for the benefit of UK-based researchers. However, to help promote British theoretical computer science in the wider community, BCTCS is also advertised at the international level; participants from outside of the UK are welcome to attend the annual meeting, and the programme of invited talks regularly includes high-profile researchers from outside of the UK.
The first BCTCS meeting was organised in 1985 by John V. Tucker at the University of Leeds.
The BCTCS operates under the direction of an Organising Committee, with an Executive consisting of a President, Secretary and Treasurer. The current President is Faron Moller.


== Past officers of the BCTCS ==


=== Past presidents ===
John V. Tucker (1985–1992)
Alan Gibbons (1992–1998)
Iain Stewart (1998–1999)
Paul Dunne (1999–2001)
Chris Tofts (2001–2004)
Faron Moller (2004–)


=== Past secretaries ===
Mark Jerrum (1989–1992)
Paul Dunne (1992–1999)
Julian Bradfield (1999–2005)
Graham Hutton (2005–2011)
David Manlove (2011-)


=== Past treasurers ===
David Rydeheard (1989–1996)
Chris Tofts (1996–2001)
Faron Moller (2001–2004)
Stephan Reiff-Marganiec (2004–)


=== Past (recent) postgraduate representatives ===
Temesghen Kahsai Azene (2007–2008)
Haris Aziz (2008–2009)
Julian Gutierrez (2009–2010)
Radhakrishnan Delhibabu (2010–2011)
Laurence E. Day (2011-)


== See also ==
Formal Aspects of Computing Science, a British Computer Society Specialist Group.


== External links ==
The British Colloquium for Theoretical Computer Science website"
388,Software metering,8632359,3597,"Software metering refers to several areas:
Tracking and maintaining software licenses. One needs to make sure that only the allowed number of licenses are in use, and at the same time, that there are enough licenses for everyone using it. This can include monitoring of concurrent usage of software for real-time enforcement of license limits. Such license monitoring usually includes when a license needs to be updated due to version changes or when upgrades or even rebates are possible.
Real-time monitoring of all (or selected) applications running on the computers within the organization in order to detect unregistered or unlicensed software and prevent its execution, or limit its execution to within certain hours. The systems administrator can configure the software metering agent on each computer in the organization, for example, to prohibit the execution of games before 5 p.m.
Fixed planning to allocate software usage to computers according to the policies a company specifies and to maintain a record of usage and attempted usage. A company can check out and check in licenses for mobile users, and can also keep a record of all licenses in use. This is often used when limited license counts are available to avoid violating strict license controls.
A method of software licensing where the licensed software automatically records how many times or for how long one or more functions in the software are used, and the user pays fees based on this actual usage (also known as 'pay-per-use')
Software metering provides another benefit to companies by enabling them to set and maintain a consistent set of standards across all levels of a company.
One of the main functions of most software metering programs is to keep track of the software usage statistics in an organization. This assists the IT departments in keeping track of licensed software, which is often from multiple software vendors. Desktop or Network based software metering packages can provide an ""inventory"" of software, give details of all the software installed in the network with the total number of copies with the usage details of each software, and even track metrics of software use such as how often it is used by a particular department, the peak times it is being utilized, and what add-ons are being utilized with it. The possible savings on the cost of renewing the licenses of rarely used programs can be well worth the cost of the software.


== Short Explanation of Active / Passive Software Metering ==
Active software metering occurs when a user is specifically denied use of a metered application. Passive software metering occurs when application use is simply recorded and no control is asserted over maintaining a maximum concurrent usage level.


== References ==


== See also ==
License manager
Product activation
Software license
Systems management
System administration
Key server (software licensing)
License Statistics"
389,Nerode Prize,39039164,3594,"The EATCS--IPEC Nerode Prize is a theoretical computer science prize awarded for outstanding research in the area of multivariate algorithmics. It is awarded by the European Association for Theoretical Computer Science and the International Symposium on Parameterized and Exact Computation. The prize was offered for the first time in 2013.


== Winners ==
The prize winners so far have been:
2013: Chris Calabro, Russell Impagliazzo, Valentine Kabanets, Ramamohan Paturi, and Francis Zane, for their research formulating the exponential time hypothesis and using it to determine the exact parameterized complexity of several important variants of the Boolean satisfiability problem.
2014: Hans L. Bodlaender, Rodney G. Downey, Michael R. Fellows, Danny Hermelin, Lance Fortnow, and Rahul Santhanam, for their work on kernelization, proving that several problems with fixed-parameter tractable algorithms do not have polynomial-size kernels unless the polynomial hierarchy collapses.
2015: Erik Demaine, Fedor V. Fomin, Mohammad Hajiaghayi, and Dimitrios Thilikos, for their research on bidimensionality, defining a broad framework for the design of fixed-parameter-tractable algorithms for domination and covering problems on graphs.
2016: Andreas Björklund for his paper Determinant Sums for Undirected Hamiltonicity, showing that methods based on algebraic graph theory lead to a significantly improved algorithm for finding Hamiltonian cycles 
2017: Fedor V. Fomin, Fabrizio Grandoni, and Dieter Kratsch, for developing the ""measure and conquer"" method for the analysis of backtracking algorithms.


== References =="
390,Jump point search,42146944,3590,"In computer science, Jump Point Search (JPS) is an optimization to the A* search algorithm for uniform-cost grids. It reduces symmetries in the search procedure by means of graph pruning, eliminating certain nodes in the grid based on assumptions that can be made about the current node's neighbors, as long as certain conditions relating to the grid are satisfied. As a result, the algorithm can consider long ""jumps"" along straight (horizontal, vertical and diagonal) lines in the grid, rather than the small steps from one grid position to the next that ordinary A* considers.
Jump point search preserves A*'s optimality, while potentially reducing its running time by an order of magnitude.


== History ==
Harabor and Grastien's original publication provides algorithms for neighbour pruning and identifying successors. The original algorithm for neighbour pruning allowed corner-cutting to occur, which meant the algorithm could only be used for moving agents with zero width; limiting its application to either real-life agents (e.g. robotics) or simulations (e.g. many games).
The authors presented modified pruning rules for applications where corner-cutting is not allowed the following year. This paper also presents an algorithm for pre-processing a grid in order to minimise online search times.
A number of further optimisations were published by the authors in 2014. These optimizations include exploring columns or rows of nodes instead of individual nodes, pre-computing ""jumps"" on the grid, and stronger pruning rules.


== Future work ==
Although jump point search is limited to uniform cost grids and homogeneously sized agents, the authors are placing future research into applying JPS with existing grid-based speed-up techniques such as hierarchical grids.


== References =="
391,Apple Worm,18569641,3571,"The Apple Worm is a computer program written for the Apple computer, and especially for the 6502 microprocessor, which performs dynamic self-relocation. The source code of the Apple Worm is the first program printed in its entirety in Scientific American. The Apple Worm was designed and developed by James R. Hauser and William R. Buckley.
Because the Apple Worm performs dynamic self-relocation within the one main memory of one computer, it does not constitute a computer virus, an apt if somewhat inaccurate description. Though the analogous behavior of copying code between memories is exactly the act performed by a computer virus, the virus has other characters not present in the worm. Such programs do not necessarily cause collateral damage to the computing systems upon which their instructions execute; there is no reliance upon a vector to ensure subsequent execution. This extends to the computer virus; it need not be destructive in order to effect its communication between computational environments.


== Programs ==
A typical computer program manipulates data which is external to the corporeal representation of the computer program. In programmer-ese, this means the code and data spaces are kept separate. Programs which manipulate data which is internal to its corporeal representation, such as that held in the code space, are self-relational; in part at least, its function is to maintain its function. In this sense, a dynamic self-relocator is a self-referential system, as defined by Douglas R. Hofstadter.


== Other examples ==
The instruction set of the PDP-11 computer includes an instruction for moving data, which when constructed in a particular form causes itself to be moved from higher addresses to lower addresses; the form includes an automatic decrement of the instruction pointer register. Hence, when this instruction includes autodecrement of the instruction pointer, it behaves as a dynamic self-relocator.
A more current example of a self-relocating program is an adaptation of the Apple Worm for the Intel 80x86 microprocessor and its derivatives, such as the Pentium, and corresponding AMD microprocessors.


== External links ==
The Apple Worm source code
Video of executing Apple Worm program


== References =="
392,ACM Prize in Computing,53853782,3556,"The ACM Prize in Computing was established by the Association for Computing Machinery to recognize individuals for early to mid-career fundamental innovative contribution in computings. The award carries a prize of $250,000. Financial support is provided by an endowment from Infosys.
The ACM Prize in Computing was previously known as the ACM-Infosys Foundation Award in the Computing Sciences for award years 2007 through 2015. In 2016 it was announced that ACM Prize in Computing recipients are invited to participate in the Heidelberg Laureate Forum (HLF) alongside recipients of the Turing Award, Abel Prize, Fields Medal, and Nevanlinna Prize.


== Recipients ==


== References =="
393,Void safety,23829996,3547,"Void safety (also known as null safety) is a guarantee within an object-oriented programming language that no object references will have null or void values.
In object-oriented languages, access to objects is achieved through references (or, equivalently, pointers). A typical call is of the form:

x.f(a, ...)

where f denotes an operation and x denotes a reference to some object. At execution time, however, a reference can be void (or null). In such cases, the call above will be a void call, leading to a run-time exception, often resulting in abnormal termination of the program.
Void safety is a static (compile-time) guarantee that no void calls will ever arise.


== History ==
In a 2009 talk, Tony Hoare traced the invention of the null pointer to his design of the Algol W language and called it a ""mistake"":

I call it my billion-dollar mistake. It was the invention of the null reference in 1965. At that time, I was designing the first comprehensive type system for references in an object oriented language (ALGOL W). My goal was to ensure that all use of references should be absolutely safe, with checking performed automatically by the compiler. But I couldn't resist the temptation to put in a null reference, simply because it was so easy to implement. This has led to innumerable errors, vulnerabilities, and system crashes, which have probably caused a billion dollars of pain and damage in the last forty years.

Bertrand Meyer introduced the term ""void safety"".


== In programming languages ==
An early attempt to guarantee void safety was the design of the Self programming language.
The Spec# language, a research language from Microsoft Research, has a notion of ""non-nullable type"" addressing void safety.
The Eiffel language is void-safe according to its ISO-ECMA standard; the void-safety mechanism is implemented in EiffelStudio starting with version 6.1 and using a modern syntax starting with version 6.4.
The Kotlin language, a JVM language, uses null-safe types by default.


== See also ==
Nullable type
Option type
Safe navigation operator


== References =="
394,Ukkonen's algorithm,4067031,3545,"In computer science, Ukkonen's algorithm is a linear-time, online algorithm for constructing suffix trees, proposed by Esko Ukkonen in 1995.
The algorithm begins with an implicit suffix tree containing the first character of the string. Then it steps through the string adding successive characters until the tree is complete. This order addition of characters gives Ukkonen's algorithm its ""on-line"" property. The original algorithm presented by Peter Weiner proceeded backward from the last character to the first one from the shortest to the longest suffix. A simpler algorithm was found by Edward M. McCreight, going from the longest to the shortest suffix.
The naive implementation for generating a suffix tree going forward requires O(n2) or even O(n3) time complexity in big O notation, where n is the length of the string. By exploiting a number of algorithmic techniques, Ukkonen reduced this to O(n) (linear) time, for constant-size alphabets, and O(n log n) in general, matching the runtime performance of the earlier two algorithms.


== References ==


== External links ==
Detailed explanation in plain English
Fast String Searching With Suffix Trees Mark Nelson's tutorial. Has an implementation example written with C++.
Implementation in C with detailed explanation
Lecture slides by Guy Blelloch
Ukkonen's homepage
Text-Indexing project (Ukkonen's linear-time construction of suffix trees)
Implementation in C Part 1 Part 2 Part 3 Part 4 Part 5 Part 6"
395,IEEE Computer Science and Engineering Undergraduate Teaching Award,52918006,3539,"The IEEE Computer Science & Undergraduate Teaching Award is a Technical Field Award of the IEEE that was established by the IEEE Computer Society in 1999. It is presented for outstanding contributions to undergraduate computer science education through teaching and service.
The award nomination requires a minimum of 3 endorsements.
Recipients of this award receive a certificate, and honorarium.


== Recipients ==
The recipients of the IEEE Computer Science & Engineering Undergraduate Teaching Award include the following people:
2017: Sven Koenig
2016: Mark Sherriff
2015: Henry C.B. Chan
2014: Elizabeth Gerber
2013: Robert J. Fornaro
2012: Mark Guzdial
2011: Benjamin Hescott
2010: No Award
2009: Judy Robertson
2008: Elizabeth L. Burd
2007: Darrin M. Hanna
2006: No Award
2005: No Award
2004: No Award
2003: Sally A. Fincher 
2002: Alan Clements
2001: Steven S. Skiena, and David G. Meyer
2000: No Award
1999: Joseph L. Zachary, and Bruce W. Weide and Timothy J. Long


== References =="
396,Sum of radicals,22231261,3536,"In computational complexity theory, there is an open problem of whether some information about a sum of radicals may be computed in polynomial time depending on the input size, i.e., in the number of bits necessary to represent this sum. It is of importance for many problems in computational geometry, since the computation of the Euclidean distance between two points in the general case involves the computation of a square root, and therefore the perimeter of a polygon or the length of a polygonal chain takes the form of a sum of radicals.
The sum of radicals is defined as a finite linear combination of radicals:

  
    
      
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          k
          
            i
          
        
        
          
            
              x
              
                i
              
            
            
              
                r
                
                  i
                
              
            
          
        
        ,
      
    
    {\displaystyle \sum _{i=1}^{n}k_{i}{\sqrt[{r_{i}}]{x_{i}}},}
  
where 
  
    
      
        n
        ,
        
          r
          
            i
          
        
      
    
    {\displaystyle n,r_{i}}
   are natural numbers and 
  
    
      
        
          k
          
            i
          
        
        ,
        
          x
          
            i
          
        
      
    
    {\displaystyle k_{i},x_{i}}
   are real numbers.
Most theoretical research in computational geometry of combinatorial character assumes the computational model of infinite precision real RAM, i.e., an abstract computer in which real numbers and operations on them are performed with infinite precision and the input size of a real number and the cost of an elementary operation are constants. However, there is research in computational complexity, especially in computer algebra, where the input size of a number is the number of bits necessary for its representation.
In particular, of interest in computational geometry is the problem of determining the sign of the sum of radicals. For instance, the length of a polygonal path in which all vertices have integer coordinates may be expressed using the Pythagorean theorem as a sum of integer square roots, so in order to determine whether one path is longer or shorter than another in a Euclidean shortest path problem, it is necessary to determine the sign of an expression in which the first path's length is subtracted from the second; this expression is a sum of radicals.
In a similar way, the sum of radicals problem is inherent in the problem of minimum-weight triangulation in the Euclidean metric.
In 1991, Blömer proposed a polynomial time Monte Carlo algorithm for determining whether a sum of radicals is zero, or more generally whether it represents a rational number. While Blömer's result does not resolve the computational complexity of finding the sign of the sum of radicals, it does imply that if the latter problem is in class NP, then it is also in co-NP.


== See also ==
Nested radicals
Abel–Ruffini theorem


== References =="
397,"European Summer School in Logic, Language and Information",22910171,3534,"The European Summer School in Logic, Language and Information (ESSLLI) is an annual academic conference organized by the European Association for Logic, Language and Information. The focus of study is the ""interface between linguistics, logic and computation, with special emphasis on human linguistic and cognitive ability"". The conference is held over two weeks of the European Summer, and offers about 50 courses at introductory and advanced levels. It attracts around 500 participants from all over the world.


== Venues ==


== See also ==
Dynamic semantics
Generalized quantifier
Type theory


== References ==


== Bibliography ==
Program for ESSLLI 2015: Barcelona
Program for ESSLLI 2014: Tübingen
Program for ESSLLI 2013: Düsseldorf
Program for ESSLLI 2012: Opole
Program for ESSLLI 2011: Ljubljana
Program for ESSLLI 2010: Copenhagen
Program for ESSLLI 2009: Bordeaux
Program for ESSLLI 2008: Hamburg
Program for ESSLLI 2007: Dublin
Program for ESSLLI 2006: Málaga
Program for ESSLLI 2005: Edinburgh


== External links ==
Association for Logic, Language and Information — official home page"
398,Prize for Innovation in Distributed Computing,41989958,3515,"The Prize for Innovation in Distributed Computing (also called SIROCCO award) is an award presented annually at the conference International Colloquium on Structural Information and Communication Complexity (SIROCCO) to a living individual (or individuals) who have made a major contribution to understanding ""the relationships between information and efficiency in decentralized computing"", which is main area of interest for this conference. The award recognizes innovation, in particular, it recognizes inventors of new ideas that were unorthodox and outside the mainstream at the time of their introduction. There are two restrictions for being eligible for this award: (1) The original contribution must have appeared in a publication at least five years before the year of the award, (2) One of the articles related to this contribution and authored by this candidate must have appeared in the proceedings of SIROCCO.
The award was presented for the first time in 2009.


== Winners ==


== References ==


== External links ==
Call for Nomination for the Prize (2013)"
399,CHI Academy,16602977,3507,"The CHI Academy is a group of researchers honored by SIGCHI, the Special Interest Group in Computer–Human Interaction of the Association for Computing Machinery. Each year, 5–8 new members are elected for having made a significant, cumulative contributions to the development of the field of human–computer interaction and have influenced the research of others.


== Inductees by year ==


== External links ==
SIGCHI Awards"
400,Computational semantics,6820847,3503,"Computational semantics is the study of how to automate the process of constructing and reasoning with meaning representations of natural language expressions. It consequently plays an important role in natural language processing and computational linguistics.
Some traditional topics of interest are: construction of meaning representations, semantic underspecification, anaphora resolution, presupposition projection, and quantifier scope resolution. Methods employed usually draw from formal semantics or statistical semantics. Computational semantics has points of contact with the areas of lexical semantics (word sense disambiguation and semantic role labeling), discourse semantics, knowledge representation and automated reasoning (in particular, automated theorem proving). Since 1999 there has been an ACL special interest group on computational semantics, SIGSEM.


== See also ==
Discourse representation theory
Minimal recursion semantics
Natural language understanding
Semantic compression
Semantic Web
SemEval
WordNet


== Further reading ==
Blackburn, P., and Bos, J. (2005), Representation and Inference for Natural Language : A First Course in Computational Semantics, CSLI Publications. ISBN 1-57586-496-7.
Bunt, H., and Muskens, R. (1999), Computing Meaning, Volume 1, Kluwer Publishing, Dordrecht. ISBN 1-4020-0290-4.
Bunt, H., Muskens, R., and Thijsse, E. (2001), Computing Meaning, Volume 2, Kluwer Publishing, Dordrecht. ISBN 1-4020-0175-4.
Copestake, A., Flickinger, D. P., Sag, I. A., & Pollard, C. (2005). Minimal Recursion Semantics. An introduction. In Research on Language and Computation. 3:281–332.
Eijck, J. van, and C. Unger (2010): Computational Semantics with Functional Programming. Cambridge University Press. ISBN 978-0-521-75760-7
Wilks, Y., and Charniak, E. (1976), Computational Semantics: An Introduction to Artificial Intelligence and Natural Language Understanding, North-Holland, Amsterdam. ISBN 0-444-11110-7.


== References ==


== External links ==
Special Interest Group on Computational Semantics (SIGSEM) of the Association for Computational Linguistics (ACL)
IWCS - International Workshop on Computational Semantics (endorsed by SIGSEM)
ICoS - Inference in Computational Semantics (endorsed by SIGSEM)
Wolfram Group - Semantic Representation of Pure Mathematics"
401,Kernel debugger,12576291,3492,"A kernel debugger is a debugger present in some operating system kernels to ease debugging and kernel development by the kernel developers. A kernel debugger might be a stub implementing low-level operations, with a full-blown debugger such as gdb, running on another machine, sending commands to the stub over a serial line or a network connection, or it might provide a command line that can be used directly on the machine being debugged.
Operating systems and operating system kernels that contain a kernel debugger:
The Windows NT family includes a kernel debugger named KD, which can act as a local debugger with limited capabilities (reading and writing kernel memory, but not setting breakpoints) and can attach to a remote machine over a serial line, IEEE 1394 connection, USB 2.0 or USB 3.0 connection. The WinDbg GUI debugger can also be used to debug kernels on local and remote machines.
BeOS
DragonFly BSD
Linux kernel; No kernel debugger was included in the mainline Linux tree prior to version 2.6.26-rc1 because Linus Torvalds didn't want a kernel debugger in the kernel.KDB (local)
KGDB (remote)
MDB (local/remote)

NetBSD (DDB for local, KGDB for remote)
macOS, Darwin which runs the XNU kernel using the Mach component


== References =="
402,ACM SIGUCCS Hall of Fame Award,44420375,3469,"The Association for Computing Machinery Special Interest Group on University and College Computing Services Hall of Fame Award was established by the Association for Computing Machinery to recognize individuals whose specific contributions have had a positive impact on the organization and therefore on the professional careers of the members and their institutions.


== Recipients ==


== See also ==
See Qualifications and Nominations page, at the ACM SIGUCCS Web Page.
Hall of Fame Web Page at ACM/SIGUCCS


== References =="
403,Confusion network,55750276,3456,"A confusion network is a natural language processing method that combines outputs from multiple machine translation systems. The defining characteristic of confusion networks is that they allow multiple ambiguous inputs, deferring committal translation decisions until later stages of processing. This approach is used in the open source machine translation software Moses and the proprietary translation API in IBM Bluemix Watson.


== References =="
404,Craig Silverstein,45671685,3454,"Craig Silverstein (born 1972 or 1973) was the first person employed by Larry Page and Sergey Brin at Google, having studied for a PhD alongside them at Stanford University. He graduated from Harvard and was admitted to Phi Beta Kappa.


== Biography ==
His PhD supervisor was Rajeev Motwani. He served as Google’s director of technology. He resigned from the company in February 2012, to work at the Khan Academy.
He and his wife, Mary Obelnicki, are signers of The Giving Pledge. He is of Jewish descent.


== References =="
405,IEEE Circuits and Systems Society,10848603,3446,"is a society of the IEEE. It is also known by the acronym IEEE CAS. In the hierarchy of IEEE, the Circuits and Systems Society is one of close to 40 technical societies organized under the IEEE's Technical Activities Board.
From the IEEE CAS web site, the field of interest of the society is defined to be
""The theory, analysis, design (computer aided design), and practical implementation of circuits, and the application of circuit theoretic techniques to systems and to signal processing. The coverage of this field includes the spectrum of activities from, and including, basic scientific theory to industrial applications.""


== History ==
The first meeting of the IRE Professional Group on Circuit Theory was on March 20 of 1951. After the IRE and the AIEE merged, the IRE Professional Group on Circuit Theory became the IEEE Professional Technical Group on Circuit Theory on March 25 of 1963. In 1966 the group changed its name to the Group on Circuit Theory, and in 1973 became the IEEE Circuits and Systems Society.


== Activities ==
The Society organizes many conferences every year and operates local chapters around the world. It coordinates the operation of several councils, task forces, and technical committees.


=== Publications ===
The Circuits and Systems Society oversees the publication of eleven periodical magazines and scholarly journals:
IEEE Transactions on Circuits & Systems I: Regular Papers
IEEE Transactions on Circuits & Systems II: Express Briefs
IEEE Transactions on Circuits and Systems for Video Technology
IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems
IEEE Transactions on Biomedical Circuits and Systems
IEEE Transactions on Very Large Scale Integration Systems
IEEE Transactions on Multimedia
IEEE Transactions on Mobile Computing
Circuits and Devices Magazine
IEEE Design & Test of Computers Magazine
Circuits and Systems Magazine


=== Conferences ===
The Society organizes, sponsors, and co-sponsors many conferences every year (64 conferences in 2006). A list of them can be retrieved by the link: [1].


== See also ==
International Symposium on Circuits and Systems—Biomedical Circuit and Systems Conference [2] -- Design Automation Conference


== References ==


== External links ==
The Circuit and System Society's Website"
406,AsciiMath,12578002,3422,"AsciiMath is a client-side mathematical markup language for displaying mathematical expressions in web browsers.
Using the JavaScript script ASCIIMathML.js, AsciiMath notation is converted to MathML at the time the page is loaded by the browser, natively in Mozilla Firefox, Safari, and via a plug-in in IE7. The simplified markup language supports a subset of the LaTeX language instructions, as well as a less verbose syntax (which, for example, replaces ""\times"" with ""xx"" to produce the ""×"" symbol). The resulting MathML mathematics can be styled by applying CSS to class ""mstyle"".
The script ASCIIMathML.js is freely available under the MIT License. The latest version also includes support for SVG graphics, natively in Mozilla Firefox and via a plug-in in IE7.
Per May 2009 there is a new version available. This new version still contains the original ASCIIMathML and LaTeXMathML as developed by Peter Jipsen, but the ASCIIsvg part has been extended with linear-logarithmic, logarithmic-linear, logarithmic-logarithmic, polar graphs and pie charts, normal and stacked bar charts, different functions like integration and differentiation and a series of event trapping functions, buttons and sliders, in order to create interactive lecture material and exams online in web pages.
ASCIIMathML.js has been integrated into MathJax, starting with MathJax v2.0.


== Example ==
The well-known quadratic formula

  
    
      
        x
        =
        
          
            
              −
              b
              ±
              
                
                  
                    b
                    
                      2
                    
                  
                  −
                  4
                  a
                  c
                
              
            
            
              2
              a
            
          
        
      
    
    {\displaystyle x={\frac {-b\pm {\sqrt {b^{2}-4ac}}}{2a}}}
  
looks in AsciiMath like this

x=(-b +- sqrt(b^2 – 4ac))/(2a)


== See also ==
Google Chart API


== References ==


== External links ==
Official website
ASCIIMathML on GitHub
ASCIIMathPHP on GitHub"
407,Touch-type Read and Spell,54992495,3419,"Touch-type Read and Spell is a computer program that uses the Orton-Gillingham Method to teach phonics and typing. It is a multi-sensory approach. Keyboarding lessons present words on the screen, play them aloud and provide visual cues of the intended hand movements. The program is multi-step and focuses on accuracy over speed. This makes it appropriate for students with dyslexia and other specific learning differences, ADHD, dyspraxia, adults who struggle with literacy skills and aphasic individuals recovering from a stroke. TTRS has been featured in the Guardian and Forbes and the course and its approach are commonly discussed in books concerning special needs classroom instruction and dyslexia.


== References =="
408,Conference on Information and Knowledge Management,32045322,3407,"The ACM Conference on Information and Knowledge Management (CIKM, pronounced /ˈsikəm/) is an annual computer science research conference dedicated to information management (IM) and knowledge management (KM). Since the first event in 1992, the conference has evolved into one of the major forums for research on database management, information retrieval, and knowledge management. The conference is noted for its interdisciplinarity, as it brings together communities that otherwise often publish at separate venues. Recent editions have attracted well beyond 500 participants. In addition to the main research program, the conference also features a number of workshops, tutorials, and industry presentations.
For many years, the conference was held in the USA. Since 2005, venues in other countries have been selected as well. Locations include:
1992: Baltimore, Maryland, USA
1993: Washington, D.C., USA
1994: Gaithersburg, Maryland, USA
1995: Baltimore, Maryland, USA
1996: Rockville, Maryland, USA
1997: Las Vegas, Nevada, USA
1998: Bethesda, Maryland, USA
1999: Kansas City, Missouri, USA
2000: Washington, D.C., USA
2001: Atlanta, Georgia, USA
2002: McLean, Virginia, USA
2003: New Orleans, Louisiana, USA
2004: Washington, D.C., USA
2005: Bremen, Germany
2006: Arlington, Virginia, USA
2007: Lisbon, Portugal 
2008: Napa Valley, California, USA 
2009: Hong Kong, China 
2010: Toronto, Ontario, Canada 
2011: Glasgow, Scotland, UK 
2016: Indianapolis, USA 
2017: Singapore, Singapore 
2018: Turin, Italy 


== See also ==
SIGIR Conference


== References ==


== External links ==
Official website"
409,Tara Hernandez,52104955,3400,"Tara Hernandez is a professional software developer, and veteran open source contributor. As shown in the documentary Code Rush, she was the manager of Netscape Navigator development at Netscape Communications Corporations, and worked on the preparation of the original Mozilla code for public release, which led to the development of the Firefox browser. She has also worked as a Release Team Manager at Blue Martini software, Senior Infrastructure Engineer and Team Lead at Pixar Animation Studios, Senior Engineering Manager at Lithium Technologies, and currently works as the Senior Director of Systems and Build Engineering at Linden Lab.


== Career ==
Tara began her career as part of the Release Engineering team at Borland, working mainly on development kits for C++ and Delphi. After moving to Netscape, she was ""the first build engineer hired for the Client Engineering team"" for Netscape Navigator, and was later promoted to managing the teams working on Netcape across all platforms. During this time she was also involved in the development of projects like bug tracker Bugzilla, CVS repository browser Bonsai, and pioneering continuous integration server Tinderbox,. After these projects were taken over by the Mozilla project, she served as a Project Owner for Bugzilla (2000-2002) and Bonsai (1999-2006), and is credited with ""keeping Bugzilla development going strong after Terry [Weissman] left mozilla.org"".


== Conference speaking ==
Tara has spoken at a number of technology conferences, including Taking Flight: Career Progression for Women in Tech (2016), and Women Transforming Technology (2016).


== References =="
410,Author profiling,55632076,3398,"Author profiling is a method of analyzing a given number of texts to try to uncover various characteristics of the author (e.g. age and gender) based on stylistic- and content-based features.


== Overview ==
Automatic Authorship Identification (AAI) has existed for almost 120 years. Thomas Corwin Mendenhall was the first to examine works of Francis Bacon, William Shakespeare, and Christopher Marlowe aiming to detect quantitative stylistic differences using word length. Since then, things have changed rapidly due to the development of technology.
There are three major fields in AAI: authorship attribution, author identification, and author profiling. In the first two, the goal is to recognize the author from a set of authors, while in author profiling, the goal is to find specific characteristics of the author, based on stylistic- or content-based features.
The author profiling task is a yet unsolved problem, due to its difficulty. It has been studied by many researchers and, while some show great progress and good results, it still has many unexplored areas and room for improvement. Through the organizational efforts of PAN, many teams around the globe try every year to find the characteristics of authors.
Characteristics vary between approaches, but age and gender are usually among them. Other personality traits have included the zodiac and the occupation of the author.


== References =="
411,Master of Science in Information Technology,22192523,3371,"A Master of Science in Information Technology (abbreviated M.Sc. IT, MSc IT or MSIT) is a type of postgraduate academic master's degree usually offered in a University's College of Business and in the recent years in integrated Information Science & Technology colleges. The MSIT degree is designed for those managing information technology, especially the information systems development process. The MSIT degree is functionally equivalent to a Master of Information Systems Management, which is one of several specialized master's degree programs recognized by the Association to Advance Collegiate Schools of Business (AACSB). MCA is a three year professional master's degree in the field of computer applications, awarded in India. The MCA program is designed for students with variety of undergraduate backgrounds, such as commerce and science, focusing in the field of IT.
A joint committee of Association for Information Systems (AIS) and Association for Computing Machinery (ACM) members develop a model curriculum for the Master of Science in Information Systems (MSIT). The most recent version of the MSIS Model Curriculum was published in 2006.
The course of study is concentrated around the Information Systems discipline. The core courses are (typically) Systems analysis, Systems design, Data Communications, Database design, Project management and Security. The degree typically includes coursework in both computer science and business skills, but the core curriculum might depend on the school and result in other degrees and specializations, including:
Master of Science (Information Technology) M.Sc.(I.T)
Master in Information Science (MIS)
Master of Science in Information and Communication Technologies (MS-ICT)
Master of Science in Information Systems Management (MISM)
Master of Science in Information Technology (MSIT)
Master of Computer Science (MCS)
Master of Science in Information Systems (MSIS)
Master of Science in Management of Information Technology (M.S. in MIT)
Master of Information Technology (M.I.T.)
Master of IT (M. IT or MIT) in Denmark
Master of Computer Applications (MCA)
Candidatus/candidata informationis technologiæ (Cand. it.) in Denmark


== See also ==

ABET - Accreditation Board for Engineering and Technology (United States)
List of master's degrees
Bachelor of Computer Information Systems
Bachelor of Science in Information Technology


== References =="
412,Arithmetic underflow,1254615,3371,"The term arithmetic underflow (or ""floating point underflow"", or just ""underflow"") is a condition in a computer program where the result of a calculation is a number of smaller absolute value than the computer can actually represent in memory on its CPU.
Arithmetic underflow can occur when the true result of a floating point operation is smaller in magnitude (that is, closer to zero) than the smallest value representable as a normal floating point number in the target datatype. Underflow can in part be regarded as negative overflow of the exponent of the floating point value. For example, if the exponent part can represent values from −128 to 127, then a result with a value less than −128 may cause underflow.


== Underflow gap ==
The interval between −fminN and fminN, where fminN is the smallest positive normal floating point value, is called the underflow gap. This is because the size of this interval is many orders of magnitude larger than the distance between adjacent normal floating point values just outside the gap. For instance, if the floating point datatype can represent 20 binary digits, the underflow gap is 221 times larger than the absolute distance between adjacent floating point values just outside the gap.
In older designs, the underflow gap had just one usable value, zero. When an underflow occurred, the true result was replaced by zero (either directly by the hardware, or by system software handling the primary underflow condition). This replacement is called flush to zero.
The 1984 edition of IEEE 754 introduced subnormal numbers. The subnormal numbers (including zero) fill the underflow gap with values where the absolute distance between adjacent values is the same as for adjacent values just outside the underflow gap. This enables gradual underflow where a nearest subnormal value is used, just as a nearest normal value is used when possible. Even when using gradual underflow, the nearest value may be zero.


== Handling of underflow ==
The occurrence of an underflow may set a ('sticky') status bit, raise an exception, at the hardware level generate an interrupt, or may cause some combination of these effects.
As specified in IEEE 754, the underflow condition is only signaled if there is also a loss of precision. Typically this is determined as the final result being inexact. However if the user is trapping on underflow, this may happen regardless of consideration for loss of precision. The default handling in IEEE 754 for underflow (as well as other exceptions) is to record as a floating point status that underflow has occurred. This is specified for the application programming level, but often also interpreted as how to handle it at the hardware level.


== See also ==
IEEE 754
−0 (number)
Subnormal numbers
Normal number (computing)
Integer overflow
Logarithmic number system"
413,Blair MacIntyre,36023273,3365,"Blair MacIntyre is a Professor and Director of the Augmented Environments Lab at Georgia Institute of Technology working in the field of augmented reality.


== Career ==
After completing his doctorate at Columbia University in 1998, MacIntyre moved to the Georgia Institute of Technology where he founded and was appointed director of the newly-formed Augmented Environments Lab. In 2010 MacIntire was named as the director of the Qualcomm Augmented Reality Game Studio. Currently, MacIntyre is on leave from Georgia Tech while working as a Principal Research Scientist in Mozilla’s Emerging Technologies team.
As the director of the KHARMA project, MacIntyre developed the Argon augmented reality browser, which was released for the iPhone in 2011.


== Selected publications ==
Feiner, S., MacIntyre, B., and Seligmann, D. ""Knowledge-based augmented reality"". Communications of the ACM, 36(7), July 1993, 52-62.


== References ==


== External links ==
Blair MacIntyre, Georgia Institute of Technology"
414,Tagged architecture,14695473,3362,"In computer science, a tagged architecture is a particular type of computer architecture where every word of memory constitutes a tagged union, being divided into a number of bits of data, and a tag section that describes the type of the data: how it is to be interpreted, and, if it is a reference, the type of the object that it points to.
Two notable series of American tagged architectures were the Lisp machines, which had tagged pointer support at the hardware and opcode level, and the Burroughs large systems, which had a data-driven tagged and descriptor-based architecture. Another ""exemplary"" instance was the architecture of the Rice Computer. Both the Burroughs and Lisp machine were examples of high-level language computer architectures, where the tagging was used to support types from a high-level language at the hardware level.
In addition to this, the original Xerox Smalltalk implementation used the least-significant bit of each 16-bit word as a tag bit: if it was clear then the hardware would accept it as an aligned memory address while if it was set it was treated as a (shifted) 15-bit integer. Current Intel documentation mentions that the lower bits of a memory address might be similarly used by some interpreter-based systems.
In the Soviet Union, the Elbrus series of supercomputers pioneered the use of tagged architectures in 1973.


== References =="
415,Divergence (computer science),20271419,3357,"In computer science, a computation is said to diverge if it does not terminate or terminates in an (unobservable) exceptional state. Otherwise it is said to converge. In domains where computations are expected to be infinite, such as process calculi, a computation is said to diverge if it fails to be productive (always produces an action within a finite amount of time.)


== Definitions ==
Various subfields of computer science use varying, but mathematically precise, definitions of what it means for a computation to converge or diverge.


=== Rewriting ===
In abstract rewriting, an abstract rewriting system is called convergent if it is both confluent and terminating.
The notation t ↓ n means that t reduces to normal form n in zero or more reductions, t↓ means t reduces to some normal form in zero or more reductions, and t↑ means t does not reduce to a normal form; the latter is impossible in a terminating rewriting system.
In the lambda calculus an expression is divergent if it has no normal form.


=== Denotational semantics ===
In denotational semantics an object function f : A → B can be modelled as a mathematical function f : A ∪ {⊥} → B ∪ {⊥} where ⊥ (bottom) indicates that the object function or its argument diverges.


=== Concurrency theory ===
In the calculus of communicating sequential processes, divergence is a drastic situation where a process performs an endless series of hidden actions. For example, consider the following process, defined by CSP notation:

  
    
      
        C
        l
        o
        c
        k
        =
        t
        i
        c
        k
        →
        C
        l
        o
        c
        k
      
    
    {\displaystyle Clock=tick\rightarrow Clock}
  
The traces of this process are defined as:

  
    
      
        traces
        ⁡
        (
        C
        l
        o
        c
        k
        )
        =
        {
        ⟨
        ⟩
        ,
        ⟨
        t
        i
        c
        k
        ⟩
        ,
        ⟨
        t
        i
        c
        k
        ,
        t
        i
        c
        k
        ⟩
        ,
        ⋯
        }
        =
        {
        t
        i
        c
        k
        
          }
          
            ∗
          
        
      
    
    {\displaystyle \operatorname {traces} (Clock)=\{\langle \rangle ,\langle tick\rangle ,\langle tick,tick\rangle ,\cdots \}=\{tick\}^{*}}
  
Now, consider the following process, which conceals the tick event of the Clock process:

  
    
      
        P
        =
        C
        l
        o
        c
        k
        ∖
        t
        i
        c
        k
      
    
    {\displaystyle P=Clock\backslash tick}
  
By definition, P is called a divergent process.


== See also ==
Infinite loop


== Notes ==


== References ==
Baader, Franz; Nipkow, Tobias (1998). Term Rewriting and All That. Cambridge University Press. 
Pierce, Benjamin C. (2002). Types and Programming Languages. MIT Press. 
J. M. R. Martin and S. A. Jassim (1997). ""How to Design Deadlock-Free Networks Using CSP and Verification Tools: A Tutorial Introduction"" in Proceedings of the WoTUG-20."
416,International Conference on Pattern Recognition and Image Analysis,42091016,3347,"PRIA, the International Conference on Pattern Recognition and Image Analysis, is a biennial scientific international conference organized by the Russian Academy of Sciences and Springer Science+Business Media. It is officially sponsored by the International Association for Pattern Recognition. The conference areas are pattern recognition, image analysis, computer vision, and artificial intelligence. The conference usually includes the meeting of IAPR Technical Committee 16 ""Algebraic and Discrete Mathematical Techniques in Pattern Recognition and Image Analysis"".
The conference was first held in 1991 in Minsk. Other conferences on pattern recognition are the International Conference on Computer Vision and Conference on Computer Vision and Pattern Recognition.


== See also ==
International Conference on Pattern Recognition in Bioinformatics
International Association for Pattern Recognition


== References ==


== External links ==
Official website"
417,Timer coalescing,39657963,3342,"Timer coalescing is a computer system energy-saving technique that reduces central processing unit (CPU) power consumption by reducing the precision of software timers to allow the synchronization of process wake-ups, minimizing the number of times the CPU is forced to perform the relatively power-costly operation of entering and exiting idle states.
The Linux kernel gained support for deferrable timers in 2.6.22, and controllable ""timer slack"" for threads in 2.6.28 allowing timer coalescing.
Timer coalescing has been a feature of Microsoft Windows from Windows 7 onward.
Apple's XNU kernel based OS X gained support as of OS X Mavericks.


== See also ==


== References =="
418,Interconnect bottleneck,4791387,3338,"The interconnect bottleneck refers to limits on integrated circuit (IC) performance due to connections between components instead of their internal speed. In 2006 it was predicted to be a ""looming crisis"" by 2010.
Improved performance of computer systems has been achieved, in large part, by downscaling the IC minimum feature size. This allows the basic IC building block, the transistor, to operate at a higher frequency, performing more computations per second. However, downscaling of the minimum feature size also results in tighter packing of the wires on a microprocessor, which increases parasitic capacitance and signal propagation delay. Consequently, the delay due to the communication between the parts of a chip becomes comparable to the computation delay itself. This phenomenon, known as an “interconnect bottleneck”, is becoming a major problem in high-performance computer systems.
This interconnect bottleneck can be solved by utilizing optical interconnects to replace the long metallic interconnects. Such hybrid optical/electronic interconnects promises better performance even with larger designs. Optics has widespread use in long-distance communications; still it has not yet been widely used in chip-to-chip or on-chip interconnections because they (in centimeter or micrometer range) are not yet industry-manufacturable owing to costlier technology and lack of fully mature technologies. As optical interconnections move from computer network applications to chip level interconnections, new requirements for high connection density and alignment reliability have become as critical for the effective utilization of these links. There are still many materials, fabrication, and packaging challenges in integrating optic and electronic technologies.


== See also ==
Optical Network on Chip
Optical interconnect
Parallel optical interface
Optical communication
Optical fiber cable
Thunderbolt (interface)
Von Neumann architecture
Photonics
Bus (computing)
Random access memory#Memory wall


== References =="
419,Möller–Trumbore intersection algorithm,38898390,3328,"The Möller–Trumbore ray-triangle intersection algorithm, named after its inventors Tomas Möller and Ben Trumbore, is a fast method for calculating the intersection of a ray and a triangle in three dimensions without needing precomputation of the plane equation of the plane containing the triangle. Among other uses, it can be used in computer graphics to implement ray tracing computations involving triangle meshes.


== C++ Implementation ==
The following is an implementation of the algorithm in C++:


== See also ==
Badouel intersection algorithm
MATLAB version of this algorithm (highly vectorized)
Baldwin-Weber ray-triangle intersection algorithm
Schlick–Subrenat algorithm for ray-quadrilateral intersection


== Links ==
Fast Minimum Storage Ray-Triangle Intersection
Optimizations on the basic algorithm by Möller & Trumbore, code from journal of graphics tools


== References =="
420,Brushed metal (interface),11699905,3327,"Brushed metal is a discontinued graphical user interface design used in Apple Computer's Mac OS X operating system for Macintosh computers. The first of Apple's applications to sport this look was the QuickTime Player released as part of QuickTime 4.0 in 1999.
Apple's Human Interface Guidelines state that the brushed metal interface should be used for programs that mimic the operation of, or interface with, common devices. Older versions (before 5.0) of iTunes and the Panther and Tiger Calculator both use brushed metal because they mimic real-world devices, while iSync features the theme because it interfaces with PDAs.
Besides the metal appearance, brushed metal has a few functional differences from other types of Aqua. Brushed metal windows can be moved by clicking any part of the window background which is not occupied by a control; Aqua windows can only be moved by clicking within the title bar.
Continuing the growth of Apple-sponsored, non-Aqua themes, Apple also introduced a Pro theme that is used in its high-end video, music and image production and editing software. At the same time, with the release of Mac OS X v10.4 and new iLife applications, Brushed metal was being slowly replaced with a darker Aqua theme, often dubbed Polished Metal. Mac OS X v10.5 fully replaced brushed metal with the new darkened Aqua theme, finally restoring a consistent look and feel to Mac OS X.
Brushed metal was criticised by user interface purists, who pointed out that Apple frequently violates its own rules for the use of the brushed metal look. The most notable of the supposed violations are Safari, Apple's web browser, and the Macintosh Finder itself. With the release of Mac OS X v10.3, Apple further expanded its list of acceptable uses for brushed metal to include windows that navigate lists of information. This expanded definition includes the Finder, but Safari's use of brushed metal remains a mystery. Opponents of the theory point out that even Apple's guidelines for the use of brushed metal are inconsistent, and that the motivation behind the alternative appearance's existence is unclear.


== References =="
421,Randomization function,3578575,3325,"In computer science, a randomization function or randomizing function is an algorithm or procedure that implements a randomly chosen function between two specific sets, suitable for use in a randomized algorithm.
Randomizing functions are related to random number generators and hash functions, but have somewhat different requirements and uses, and often need specific algorithms.


== Uses ==
Randomizing functions are used to turn algorithms that have good expected performance for random inputs, into algorithms that have the same performance for any input.
For example, consider a sorting algorithm like quicksort, which has small expected running time when the input items are presented in random order, but is very slow when they are presented in certain unfavorable orders. A randomizing function from the integers 1 to n to the integers 1 to n can be used to rerrange the n input items in ""random"" order, before calling that algorithm. This modified (randomized) algorithm will have small expected running time, whatever the input order.


== Requirements ==


=== Randomness ===
In theory, randomization functions are assumed to be truly random, and yield an unpredictably different function every time the algorithm is executed. The randomization technique would not work if, at every execution of the algorithm, the randomization function always performed the same mapping, or a mapping entirely determined by some externally observable parameter (such as the program's startup time). With such a ""pseudo-randomization"" function, one could in principle construct a sequence of calls such that the function would always yield a ""bad"" case for the underlying deterministic algorithm. For that sequence of calls, the average cost would be closer to the worst-case cost, rather than the average cost for random inputs.
In practice, however, the main concern is that some ""bad"" cases for the deterministic algorithm may occur in practice much more often than it would be predicted by chance. For example, in a naive variant of quicksort, the worst case is when the input items are already sorted — which is a very common occurrence in many applications. For such algorithms, even a fixed pseudo-random permutation may be good enough. Even though the resulting ""pseudo-randomized"" algorithm would still have as many ""bad"" cases as the original, they will be certain peculiar orders that would be quite unlikely to arise in real applications. So, in practice one often uses randomization functions that are derived from pseudo-random number generators, preferably seeded with external ""random"" data such as the program's startup time.


=== Uniformity ===
The uniformity requirements for a randomizing function are usually much weaker than those of hash functions and pseudo-random generators. The minimum requirement is that it maps any input of the deterministic algorithm into a ""good"" input with a sufficiently high probability. (However, analysis is usually simpler if the randomizing function implements each possible mapping with uniform probability.)


== References =="
422,Service provider interface,2257691,3315,"Service Provider Interface (SPI) is an API intended to be implemented or extended by a third party. It can be used to enable framework extension and replaceable components.


== Details ==
From Java documentation:

A service is a well-known set of interfaces and (usually abstract) classes. A service provider is a specific implementation of a service. The classes in a provider typically implement the interfaces and subclass the classes defined in the service itself. Service providers can be installed in an implementation of the Java platform in the form of extensions, that is, jar files placed into any of the usual extension directories. Providers can also be made available by adding them to the application's class path or by some other platform-specific means.

The concept can be extended to other platforms using the corresponding tools. In the Java Runtime Environment, SPIs are used in:
Java Database Connectivity
Java Cryptography Extension
Java Naming and Directory Interface
Java API for XML Processing
Java Business Integration
Java Sound
Java Image I/O
Java File Systems


== See also ==
Plug-in (computing)
Java (programming language)
Java (software platform)


== References ==


== External links ==
Replaceable Components and the Service Provider Interface (.pdf at The Software Engineering Institute CMU)
Official Java API documentation: java.util.ServiceLoader class and java.util.spi package"
423,Lagrangian–Eulerian advection,32850307,3313,"In scientific visualization, Lagrangian–Eulerian advection is a technique mainly used for the visualization of unsteady flows. The computer graphics generated by the technique can help scientists visualize changes in velocity fields. This technique uses a hybrid Lagrangian and Eulerian specification of the flow field. It is a special case of a line integral convolution.
The method consists of using nearest-neighbour interpolation followed by an error correction mechanism. The Lagrangian specification is used during the integration to update the particle positions. The property of interest is advected in the Eulerian frame of reference. It was originally designed by Bruno Jobard and others for steady flows but was extended to unsteady flows.
The main idea is to create a white noise texture of the desired resolution, which is used as a base, on top of which the vector field can be applied. That means for every particle looking backward in the vector field to find out the new value for the cell it is contained in. Then looking forward – to calculate the new position of the particle in the cell.
In its application, the Lagrangian–Eulerian method can be accelerated using the GPUs used in common chipsets present in Nvidia and ATI Radeon graphics cards.


== See also ==
Lagrangian analysis
Lagrangian drifter


== References =="
424,Kinetic smallest enclosing disk,35872975,3313,"A kinetic smallest enclosing disk data structure is a kinetic data structure that maintains the smallest enclosing disk of a set of moving points.


== 2D ==
In 2 dimensions, the best known kinetic smallest enclosing disk data structure uses the farthest point delaunay triangulation of the point set to maintain the smallest enclosing disk. The farthest-point Delaunay triangulation is the dual of the farthest-point Voronoi diagram. It is known that if the farthest-point delaunay triangulation of a point set contains an acute triangle, the circumcircle of this triangle is the smallest enclosing disk. Otherwise, the smallest enclosing disk has the diameter of the point set as its diameter. Thus, by maintaining the kinetic diameter of the point set, the farthest-point delaunay triangulation, and whether or not the farthest-point delaunay triangulation has an acute triangle, the smallest enclosing disk can be maintained. This data structure is responsive and compact, but not local or efficient:
Responsiveness: This data structure requires 
  
    
      
        O
        (
        
          log
          
            2
          
        
        ⁡
        n
        )
      
    
    {\displaystyle O(\log ^{2}n)}
   time to process each certificate failure, and thus is responsive.
Locality: A point can be involved in 
  
    
      
        Θ
        (
        n
        )
      
    
    {\displaystyle \Theta (n)}
   certificates. Therefore, this data structure is not local.
Compactness: This data structure requires O(n) certificates total, and thus is compact.
Efficiency: This data structure has 
  
    
      
        O
        (
        
          n
          
            3
            +
            ϵ
          
        
        )
      
    
    {\displaystyle O(n^{3+\epsilon })}
   events total.(for all 
  
    
      
        ϵ
        >
        0
      
    
    {\displaystyle \epsilon >0}
   The best known lower bound on the number of changes to the smallest enclosing disk is 
  
    
      
        Ω
        (
        
          n
          
            2
          
        
        )
      
    
    {\displaystyle \Omega (n^{2})}
  . Thus the efficiency of this data structure, the ratio of total events to external events, is 
  
    
      
        O
        (
        
          n
          
            1
            +
            ϵ
          
        
        )
      
    
    {\displaystyle O(n^{1+\epsilon })}
  .
The existence of kinetic data structure that has 
  
    
      
        o
        (
        
          n
          
            3
            +
            ϵ
          
        
        )
      
    
    {\displaystyle o(n^{3+\epsilon })}
   events is an open problem.


== Approximate 2D ==
The smallest enclosing disk of a set of n moving points can be ε-approximated by a kinetic data structure that processes 
  
    
      
        O
        (
        1
        
          /
        
        
          ϵ
          
            5
            
              /
            
            2
          
        
        )
      
    
    {\displaystyle O(1/\epsilon ^{5/2})}
   events and requires 
  
    
      
        O
        (
        (
        n
        
          /
        
        
          
            ϵ
          
        
        )
        log
        ⁡
        n
        )
      
    
    {\displaystyle O((n/{\sqrt {\epsilon }})\log n)}
   time total.


== Higher dimensions ==
In dimensions higher than 2, efficiently maintaining the smallest enclosing sphere of a set of moving points is an open problem.


== References =="
425,FOSSIL,142583,3306,"A fossil (from Classical Latin fossilis; literally, ""obtained by digging"") is any preserved remains, impression, or trace of any once-living thing from a past geological age. Examples include bones, shells, exoskeletons, stone imprints of animals or microbes, hair, petrified wood, oil, coal, and DNA remnants. The totality of fossils is known as the fossil record.
Paleontology is the study of fossils: their age, method of formation, and evolutionary significance. Specimens are usually considered to be fossils if they are over 10,000 years old. The oldest fossils are from around 3.48 billion years old to 4.1 billion years old. The observation in the 19th century that certain fossils were associated with certain rock strata led to the recognition of a geological timescale and the relative ages of different fossils. The development of radiometric dating techniques in the early 20th century allowed scientists to quantitatively measure the absolute ages of rocks and the fossils they host.
There are many processes that lead to fossilization, including permineralization, casts and molds, authigenic mineralization, replacement and recrystallization, adpression, carbonization, and bioimmuration.

Fossils vary in size from one micrometer bacteria  to dinosaurs and trees, many meters long and weighing many tons. A fossil normally preserves only a portion of the deceased organism, usually that portion that was partially mineralized during life, such as the bones and teeth of vertebrates, or the chitinous or calcareous exoskeletons of invertebrates. Fossils may also consist of the marks left behind by the organism while it was alive, such as animal tracks or feces (coprolites). These types of fossil are called trace fossils or ichnofossils, as opposed to body fossils. Some fossils are biochemical and are called chemofossils or biosignatures.


== Fossilization processes ==
The process of fossilization varies according to tissue type and external conditions.


=== Permineralization ===

Permineralization is a process of fossilization that occurs when an organism is buried. The empty spaces within an organism (spaces filled with liquid or gas during life) become filled with mineral-rich groundwater. Minerals precipitate from the groundwater, occupying the empty spaces. This process can occur in very small spaces, such as within the cell wall of a plant cell. Small scale permineralization can produce very detailed fossils. For permineralization to occur, the organism must become covered by sediment soon after death or soon after the initial decay process. The degree to which the remains are decayed when covered determines the later details of the fossil. Some fossils consist only of skeletal remains or teeth; other fossils contain traces of skin, feathers or even soft tissues. This is a form of diagenesis.


=== Casts and molds ===

In some cases the original remains of the organism completely dissolve or are otherwise destroyed. The remaining organism-shaped hole in the rock is called an external mold. If this hole is later filled with other minerals, it is a cast. An endocast or internal mold is formed when sediments or minerals fill the internal cavity of an organism, such as the inside of a bivalve or snail or the hollow of a skull.


=== Authigenic mineralization ===
This is a special form of cast and mold formation. If the chemistry is right, the organism (or fragment of organism) can act as a nucleus for the precipitation of minerals such as siderite, resulting in a nodule forming around it. If this happens rapidly before significant decay to the organic tissue, very fine three-dimensional morphological detail can be preserved. Nodules from the Carboniferous Mazon Creek fossil beds of Illinois, USA, are among the best documented examples of such mineralization.


=== Replacement and recrystallization ===

Replacement occurs when the shell, bone or other tissue is replaced with another mineral. In some cases mineral replacement of the original shell occurs so gradually and at such fine scales that microstructural features are preserved despite the total loss of original material. A shell is said to be recrystallized when the original skeletal compounds are still present but in a different crystal form, as from aragonite to calcite.


=== Adpression (compression-impression) ===
Compression fossils, such as those of fossil ferns, are the result of chemical reduction of the complex organic molecules composing the organism's tissues. In this case the fossil consists of original material, albeit in a geochemically altered state. This chemical change is an expression of diagenesis. Often what remains is a carbonaceous film known as a phytoleim, in which case the fossil is known as a compression. Often, however, the phytoleim is lost and all that remains is an impression of the organism in the rock—an impression fossil. In many cases, however, compressions and impressions occur together. For instance, when the rock is broken open, the phytoleim will often be attached to one part (compression), whereas the counterpart will just be an impression. For this reason, one term covers the two modes of preservation: adpression.


==== Soft tissue, cell and molecular preservation ====
Because of their antiquity, an unexpected exception to the alteration of an organism's tissues by chemical reduction of the complex organic molecules during fossilization has been the discovery of soft tissue in dinosaur fossils, including blood vessels, and the isolation of proteins and evidence for DNA fragments. In 2014, Mary Schweitzer and her colleagues reported the presence of iron particles (goethite-aFeO(OH)) associated with soft tissues recovered from dinosaur fossils. Based on various experiments that studied the interaction of iron in haemoglobin with blood vessel tissue they proposed that solution hypoxia coupled with iron chelation enhances the stability and preservation of soft tissue and provides the basis for an explanation for the unforeseen preservation of fossil soft tissues. However, a slightly older study based on eight taxa ranging in time from the Devonian to the Jurassic found that reasonably well-preserved fibrils that probably represent collagen were preserved in all these fossils, and that the quality of preservation depended mostly on the arrangement of the collagen fibers, with tight packing favoring good preservation. There seemed to be no correlation between geological age and quality of preservation, within that timeframe.


=== Carbonization ===
Carbonaceous films are thin coatings which consist predominantly of the chemical element carbon. The soft tissues of organisms are made largely of organic carbon compounds and during diagenesis under reducing conditions only a thin film of carbon residue is left which forms a silhouette of the original organism.


=== Bioimmuration ===

Bioimmuration occurs when a skeletal organism overgrows or otherwise subsumes another organism, preserving the latter, or an impression of it, within the skeleton. Usually it is a sessile skeletal organism, such as a bryozoan or an oyster, which grows along a substrate, covering other sessile sclerobionts. Sometimes the bioimmured organism is soft-bodied and is then preserved in negative relief as a kind of external mold. There are also cases where an organism settles on top of a living skeletal organism that grows upwards, preserving the settler in its skeleton. Bioimmuration is known in the fossil record from the Ordovician to the Recent.


== Dating ==


=== Estimating dates ===

Paleontology seeks to map out how life evolved across geologic time. A substantial hurdle is the difficulty of working out fossil ages. Beds that preserve fossils typically lack the radioactive elements needed for radiometric dating. This technique is our only means of giving rocks greater than about 50 million years old an absolute age, and can be accurate to within 0.5% or better. Although radiometric dating requires careful laboratory work, its basic principle is simple: the rates at which various radioactive elements decay are known, and so the ratio of the radioactive element to its decay products shows how long ago the radioactive element was incorporated into the rock. Radioactive elements are common only in rocks with a volcanic origin, and so the only fossil-bearing rocks that can be dated radiometrically are volcanic ash layers, which may provide termini for the intervening sediments.


==== Stratigraphy ====
Consequently, palaeontologists rely on stratgraphy to date fossils. Stratigraphy is the science of deciphering the ""layer-cake"" that is the sedimentary record. Rocks normally form relatively horizontal layers, with each layer younger than the one underneath it. If a fossil is found between two layers whose ages are known, the fossil's age is claimed to lie between the two known ages. Because rock sequences are not continuous, but may be broken up by faults or periods of erosion, it is very difficult to match up rock beds that are not directly adjacent. However, fossils of species that survived for a relatively short time can be used to match isolated rocks: this technique is called biostratigraphy. For instance, the conodont Eoplacognathus pseudoplanus has a short range in the Middle Ordovician period. If rocks of unknown age have traces of E. pseudoplanus, they have a mid-Ordovician age. Such index fossils must be distinctive, be globally distributed and occupy a short time range to be useful. Misleading results are produced if the index fossils are incorrectly dated. Stratigraphy and biostratigraphy can in general provide only relative dating (A was before B), which is often sufficient for studying evolution. However, this is difficult for some time periods, because of the problems involved in matching rocks of the same age across continents. Family-tree relationships also help to narrow down the date when lineages first appeared. For instance, if fossils of B or C date to X million years ago and the calculated ""family tree"" says A was an ancestor of B and C, then A must have evolved earlier.
It is also possible to estimate how long ago two living clades diverged, in other words approximately how long ago their last common ancestor must have lived, by assuming that DNA mutations accumulate at a constant rate. These ""molecular clocks"", however, are fallible, and provide only approximate timing: for example, they are not sufficiently precise and reliable for estimating when the groups that feature in the Cambrian explosion first evolved, and estimates produced by different techniques may vary by a factor of two.


=== Limitations ===

Organisms are only rarely preserved as fossils in the best of circumstances, and only a fraction of such fossils have been discovered. This is illustrated by the fact that the number of species known through the fossil record is less than 5% of the number of known living species, suggesting that the number of species known through fossils must be far less than 1% of all the species that have ever lived. Because of the specialized and rare circumstances required for a biological structure to fossilize, only a small percentage of life-forms can be expected to be represented in discoveries, and each discovery represents only a snapshot of the process of evolution. The transition itself can only be illustrated and corroborated by transitional fossils, which will never demonstrate an exact half-way point.
The fossil record is strongly biased toward organisms with hard-parts, leaving most groups of soft-bodied organisms with little to no role. It is replete with the mollusks, the vertebrates, the echinoderms, the brachiopods and some groups of arthropods.


== Sites ==


=== Lagerstätten ===

Fossil sites with exceptional preservation—sometimes including preserved soft tissues—are known as Lagerstätten - German for ""storage places"". These formations may have resulted from carcass burial in an anoxic environment with minimal bacteria, thus slowing decomposition. Lagerstätten span geological time from the Cambrian period to the present. Worldwide, some of the best examples of near-perfect fossilization are the Cambrian Maotianshan shales and Burgess Shale, the Devonian Hunsrück Slates, the Jurassic Solnhofen limestone, and the Carboniferous Mazon Creek localities.


=== Stromatolites ===

Stromatolites are layered accretionary structures formed in shallow water by the trapping, binding and cementation of sedimentary grains by biofilms of microorganisms, especially cyanobacteria. Stromatolites provide some of the most ancient fossil records of life on Earth, dating back more than 3.5 billion years ago.
Stromatolites were much more abundant in Precambrian times. While older, Archean fossil remains are presumed to be colonies of cyanobacteria, younger (that is, Proterozoic) fossils may be primordial forms of the eukaryote chlorophytes (that is, green algae). One genus of stromatolite very common in the geologic record is Collenia. The earliest stromatolite of confirmed microbial origin dates to 2.724 billion years ago.
A 2009 discovery provides strong evidence of microbial stromatolites extending as far back as 3.45 billion years ago.
Stromatolites are a major constituent of the fossil record for life's first 3.5 billion years, peaking about 1.25 billion years ago. They subsequently declined in abundance and diversity, which by the start of the Cambrian had fallen to 20% of their peak. The most widely supported explanation is that stromatolite builders fell victims to grazing creatures (the Cambrian substrate revolution), implying that sufficiently complex organisms were common over 1 billion years ago.
The connection between grazer and stromatolite abundance is well documented in the younger Ordovician evolutionary radiation; stromatolite abundance also increased after the end-Ordovician and end-Permian extinctions decimated marine animals, falling back to earlier levels as marine animals recovered. Fluctuations in metazoan population and diversity may not have been the only factor in the reduction in stromatolite abundance. Factors such as the chemistry of the environment may have been responsible for changes.
While prokaryotic cyanobacteria themselves reproduce asexually through cell division, they were instrumental in priming the environment for the evolutionary development of more complex eukaryotic organisms. Cyanobacteria (as well as extremophile Gammaproteobacteria) are thought to be largely responsible for increasing the amount of oxygen in the primeval earth's atmosphere through their continuing photosynthesis. Cyanobacteria use water, carbon dioxide and sunlight to create their food. A layer of mucus often forms over mats of cyanobacterial cells. In modern microbial mats, debris from the surrounding habitat can become trapped within the mucus, which can be cemented by the calcium carbonate to grow thin laminations of limestone. These laminations can accrete over time, resulting in the banded pattern common to stromatolites. The domal morphology of biological stromatolites is the result of the vertical growth necessary for the continued infiltration of sunlight to the organisms for photosynthesis. Layered spherical growth structures termed oncolites are similar to stromatolites and are also known from the fossil record. Thrombolites are poorly laminated or non-laminated clotted structures formed by cyanobacteria common in the fossil record and in modern sediments.
The Zebra River Canyon area of the Kubis platform in the deeply dissected Zaris Mountains of south western Namibia provides an extremely well exposed example of the thrombolite-stromatolite-metazoan reefs that developed during the Proterozoic period, the stromatolites here being better developed in updip locations under conditions of higher current velocities and greater sediment influx.


== Types ==


=== Index ===

Index fossils (also known as guide fossils, indicator fossils or zone fossils) are fossils used to define and identify geologic periods (or faunal stages). They work on the premise that, although different sediments may look different depending on the conditions under which they were deposited, they may include the remains of the same species of fossil. The shorter the species' time range, the more precisely different sediments can be correlated, and so rapidly evolving species' fossils are particularly valuable. The best index fossils are common, easy to identify at species level and have a broad distribution—otherwise the likelihood of finding and recognizing one in the two sediments is poor.


=== Trace ===

Trace fossils consist mainly of tracks and burrows, but also include coprolites (fossil feces) and marks left by feeding. Trace fossils are particularly significant because they represent a data source that is not limited to animals with easily fossilized hard parts, and they reflect animal behaviours. Many traces date from significantly earlier than the body fossils of animals that are thought to have been capable of making them. Whilst exact assignment of trace fossils to their makers is generally impossible, traces may for example provide the earliest physical evidence of the appearance of moderately complex animals (comparable to earthworms).
Coprolites are classified as trace fossils as opposed to body fossils, as they give evidence for the animal's behaviour (in this case, diet) rather than morphology. They were first described by William Buckland in 1829. Prior to this they were known as ""fossil fir cones"" and ""bezoar stones."" They serve a valuable purpose in paleontology because they provide direct evidence of the predation and diet of extinct organisms. Coprolites may range in size from a few millimetres to over 60 centimetres.


=== Transitional ===

A transitional fossil is any fossilized remains of a life form that exhibits traits common to both an ancestral group and its derived descendant group. This is especially important where the descendant group is sharply differentiated by gross anatomy and mode of living from the ancestral group. Because of the incompleteness of the fossil record, there is usually no way to know exactly how close a transitional fossil is to the point of divergence. These fossils serve as a reminder that taxonomic divisions are human constructs that have been imposed in hindsight on a continuum of variation.


=== Microfossils ===

Microfossil is a descriptive term applied to fossilized plants and animals whose size is just at or below the level at which the fossil can be analyzed by the naked eye. A commonly applied cutoff point between ""micro"" and ""macro"" fossils is 1 mm. Microfossils may either be complete (or near-complete) organisms in themselves (such as the marine plankters foraminifera and coccolithophores) or component parts (such as small teeth or spores) of larger animals or plants. Microfossils are of critical importance as a reservoir of paleoclimate information, and are also commonly used by biostratigraphers to assist in the correlation of rock units.


=== Resin ===

Fossil resin (colloquially called amber) is a natural polymer found in many types of strata throughout the world, even the Arctic. The oldest fossil resin dates to the Triassic, though most dates to the Cenozoic. The excretion of the resin by certain plants is thought to be an evolutionary adaptation for protection from insects and to seal wounds. Fossil resin often contains other fossils called inclusions that were captured by the sticky resin. These include bacteria, fungi, other plants, and animals. Animal inclusions are usually small invertebrates, predominantly arthropods such as insects and spiders, and only extremely rarely a vertebrate such as a small lizard. Preservation of inclusions can be exquisite, including small fragments of DNA.


=== Derived ===

A derived, reworked or remanié fossil is a fossil found in rock that accumulated significantly later than when the fossilized animal or plant died. Reworked fossils are created by erosion exhuming (freeing) fossils from the rock formation in which they were originally deposited and their redeposition in an younger sedimentary deposit.


=== Wood ===

Fossil wood is wood that is preserved in the fossil record. Wood is usually the part of a plant that is best preserved (and most easily found). Fossil wood may or may not be petrified. The fossil wood may be the only part of the plant that has been preserved: therefore such wood may get a special kind of botanical name. This will usually include ""xylon"" and a term indicating its presumed affinity, such as Araucarioxylon (wood of Araucaria or some related genus), Palmoxylon (wood of an indeterminate palm), or Castanoxylon (wood of an indeterminate chinkapin).


=== Subfossil ===

The term subfossil can be used to refer to remains, such as bones, nests, or defecations, whose fossilization process is not complete, either because the length of time since the animal involved was living is too short (less than 10,000 years) or because the conditions in which the remains were buried were not optimal for fossilization. Subfossils are often found in caves or other shelters where they can be preserved for thousands of years. The main importance of subfossil vs. fossil remains is that the former contain organic material, which can be used for radiocarbon dating or extraction and sequencing of DNA, protein, or other biomolecules. Additionally, isotope ratios can provide much information about the ecological conditions under which extinct animals lived. Subfossils are useful for studying the evolutionary history of an environment and can be important to studies in paleoclimatology.
Subfossils are often found in depositionary environments, such as lake sediments, oceanic sediments, and soils. Once deposited, physical and chemical weathering can alter the state of preservation.


=== Chemical fossils ===

Chemical fossils, or chemofossils, are chemicals found in rocks and fossil fuels (petroleum, coal, and natural gas) that provide an organic signature for ancient life. Molecular fossils and isotope ratios represent two types of chemical fossils. The oldest traces of life on Earth are fossils of this type, including carbon isotope anomalies found in zircons that imply the existence of life as early as 4.1 billion years ago.


== Astrobiology ==
It has been suggested that biominerals could be important indicators of extraterrestrial life and thus could play an important role in the search for past or present life on the planet Mars. Furthermore, organic components (biosignatures) that are often associated with biominerals are believed to play crucial roles in both pre-biotic and biotic reactions.
On 24 January 2014, NASA reported that current studies by the Curiosity and Opportunity rovers on Mars will now be searching for evidence of ancient life, including a biosphere based on autotrophic, chemotrophic and/or chemolithoautotrophic microorganisms, as well as ancient water, including fluvio-lacustrine environments (plains related to ancient rivers or lakes) that may have been habitable. The search for evidence of habitability, taphonomy (related to fossils), and organic carbon on the planet Mars is now a primary NASA objective.


== Pseudofossils ==

Pseudofossils are visual patterns in rocks that are produced by geologic processes rather than biologic processes. They can easily be mistaken for real fossils. Some pseudofossils, such as dendrites, are formed by naturally occurring fissures in the rock that get filled up by percolating minerals. Other types of pseudofossils are kidney ore (round shapes in iron ore) and moss agates, which look like moss or plant leaves. Concretions, spherical or ovoid-shaped nodules found in some sedimentary strata, were once thought to be dinosaur eggs, and are often mistaken for fossils as well.


== History of the study of fossils ==

Gathering fossils dates at least to the beginning of recorded history. The fossils themselves are referred to as the fossil record. The fossil record was one of the early sources of data underlying the study of evolution and continues to be relevant to the history of life on Earth. Paleontologists examine the fossil record to understand the process of evolution and the way particular species have evolved.


=== Before Darwin ===
Many early explanations relied on folktales or mythologies. In China the fossil bones of ancient mammals including Homo erectus were often mistaken for ""dragon bones"" and used as medicine and aphrodisiacs. In the West fossilized sea creatures on mountainsides were seen as proof of the biblical deluge.
In 1027, the Persian Avicenna explained fossils' stoniness in The Book of Healing:

If what is said concerning the petrifaction of animals and plants is true, the cause of this (phenomenon) is a powerful mineralizing and petrifying virtue which arises in certain stony spots, or emanates suddenly from the earth during earthquake and subsidences, and petrifies whatever comes into contact with it. As a matter of fact, the petrifaction of the bodies of plants and animals is not more extraordinary than the transformation of waters.

Greek scholar Aristotle realized that fossil seashells from rocks were similar to those found on the beach, indicating the fossils were once living animals. Aristotle previously explained it in terms of vaporous exhalations, which Avicenna modified into the theory of petrifying fluids (succus lapidificatus), later elaborated by Albert of Saxony in the 14th century and accepted in some form by most naturalists by the 16th century.
More scientific views of fossils emerged during the Renaissance. Leonardo da Vinci concurred with Aristotle's view that fossils were the remains of ancient life. For example, da Vinci noticed discrepancies with the biblical flood narrative as an explanation for fossil origins:

If the Deluge had carried the shells for distances of three and four hundred miles from the sea it would have carried them mixed with various other natural objects all heaped up together; but even at such distances from the sea we see the oysters all together and also the shellfish and the cuttlefish and all the other shells which congregate together, found all together dead; and the solitary shells are found apart from one another as we see them every day on the sea-shores.

And we find oysters together in very large families, among which some may be seen with their shells still joined together, indicating that they were left there by the sea and that they were still living when the strait of Gibraltar was cut through. In the mountains of Parma and Piacenza multitudes of shells and corals with holes may be seen still sticking to the rocks....""

Robert Hooke (1635-1703) included micrographs of fossils in his Micrographia and was among the first to observe fossil forams. His observations on fossils, which he stated to be the petrified remains of creatures some of which no longer existed, were published posthumously in 1705.
William Smith (1769–1839), an English canal engineer, observed that rocks of different ages (based on the law of superposition) preserved different assemblages of fossils, and that these assemblages succeeded one another in a regular and determinable order. He observed that rocks from distant locations could be correlated based on the fossils they contained. He termed this the principle of faunal succession. This principle became one of Darwin's chief pieces of evidence that biological evolution was real.
Georges Cuvier came to believe that most if not all the animal fossils he examined were remains of extinct species. This led Cuvier to become an active proponent of the geological school of thought called catastrophism. Near the end of his 1796 paper on living and fossil elephants he said:
All of these facts, consistent among themselves, and not opposed by any report, seem to me to prove the existence of a world previous to ours, destroyed by some kind of catastrophe.


=== Linnaeus and Darwin ===
Early naturalists well understood the similarities and differences of living species leading Linnaeus to develop a hierarchical classification system still in use today. Darwin and his contemporaries first linked the hierarchical structure of the tree of life with the then very sparse fossil record. Darwin eloquently described a process of descent with modification, or evolution, whereby organisms either adapt to natural and changing environmental pressures, or they perish.
When Darwin wrote On the Origin of Species by Means of Natural Selection, or the Preservation of Favoured Races in the Struggle for Life, the oldest animal fossils were those from the Cambrian Period, now known to be about 540 million years old. He worried about the absence of older fossils because of the implications on the validity of his theories, but he expressed hope that such fossils would be found, noting that: ""only a small portion of the world is known with accuracy."" Darwin also pondered the sudden appearance of many groups (i.e. phyla) in the oldest known Cambrian fossiliferous strata.


=== After Darwin ===
Since Darwin's time, the fossil record has been extended to between 2.3 and 3.5 billion years. Most of these Precambrian fossils are microscopic bacteria or microfossils. However, macroscopic fossils are now known from the late Proterozoic. The Ediacara biota (also called Vendian biota) dating from 575 million years ago collectively constitutes a richly diverse assembly of early multicellular eukaryotes.
The fossil record and faunal succession form the basis of the science of biostratigraphy or determining the age of rocks based on embedded fossils. For the first 150 years of geology, biostratigraphy and superposition were the only means for determining the relative age of rocks. The geologic time scale was developed based on the relative ages of rock strata as determined by the early paleontologists and stratigraphers.
Since the early years of the twentieth century, absolute dating methods, such as radiometric dating (including potassium/argon, argon/argon, uranium series, and, for very recent fossils, radiocarbon dating) have been used to verify the relative ages obtained by fossils and to provide absolute ages for many fossils. Radiometric dating has shown that the earliest known stromatolites are over 3.4 billion years old.


=== Modern era ===

Paleontology has joined with evolutionary biology to share the interdisciplinary task of outlining the tree of life, which inevitably leads backwards in time to Precambrian microscopic life when cell structure and functions evolved. Earth's deep time in the Proterozoic and deeper still in the Archean is only ""recounted by microscopic fossils and subtle chemical signals."" Molecular biologists, using phylogenetics, can compare protein amino acid or nucleotide sequence homology (i.e., similarity) to evaluate taxonomy and evolutionary distances among organisms, with limited statistical confidence. The study of fossils, on the other hand, can more specifically pinpoint when and in what organism a mutation first appeared. Phylogenetics and paleontology work together in the clarification of science's still dim view of the appearance of life and its evolution.

Niles Eldredge's study of the Phacops trilobite genus supported the hypothesis that modifications to the arrangement of the trilobite's eye lenses proceeded by fits and starts over millions of years during the Devonian. Eldredge's interpretation of the Phacops fossil record was that the aftermaths of the lens changes, but not the rapidly occurring evolutionary process, were fossilized. This and other data led Stephen Jay Gould and Niles Eldredge to publish their seminal paper on punctuated equilibrium in 1971.
Synchrotron X-ray tomographic analysis of early Cambrian bilaterian embryonic microfossils yielded new insights of metazoan evolution at its earliest stages. The tomography technique provides previously unattainable three-dimensional resolution at the limits of fossilization. Fossils of two enigmatic bilaterians, the worm-like Markuelia and a putative, primitive protostome, Pseudooides, provide a peek at germ layer embryonic development. These 543-million-year-old embryos support the emergence of some aspects of arthropod development earlier than previously thought in the late Proterozoic. The preserved embryos from China and Siberia underwent rapid diagenetic phosphatization resulting in exquisite preservation, including cell structures. This research is a notable example of how knowledge encoded by the fossil record continues to contribute otherwise unattainable information on the emergence and development of life on Earth. For example, the research suggests Markuelia has closest affinity to priapulid worms, and is adjacent to the evolutionary branching of Priapulida, Nematoda and Arthropoda.


== Trading and collecting ==

Fossil trading is the practice of buying and selling fossils. This is many times done illegally with artifacts stolen from research sites, costing many important scientific specimens each year. The problem is quite pronounced in China, where many specimens have been stolen.
Fossil collecting (some times, in a non-scientific sense, fossil hunting) is the collection of fossils for scientific study, hobby, or profit. Fossil collecting, as practiced by amateurs, is the predecessor of modern paleontology and many still collect fossils and study fossils as amateurs. Professionals and amateurs alike collect fossils for their scientific value.


== Gallery ==


== See also ==


== References ==


== Further reading ==
It's extremely hard to become a fossil, by Olivia Judson, The New York Times
Bones Are Not the Only Fossils, by Olivia Judson, The New York Times


== External links ==
Fossils on In Our Time at the BBC.
The Virtual Fossil Museum throughout Time and Evolution
Paleoportal, geology and fossils of the United States
The Fossil Record, a complete listing of the families, orders, class and phyla found in the fossil record
Paleontology at Curlie (based on DMOZ)
 Ernest Ingersoll (1920). ""Fossils"". Encyclopedia Americana. 
 ""Fossil"". New International Encyclopedia. 1905."
426,White Camel award,29474944,3290,"The ""White Camel"" award is given to important contributors to the Perl Programming Language community.
The awards were initiated by Perl Mongers and O'Reilly & Associates at The Perl Conference in 1999. Today, The Perl Foundation acknowledges these exceptional individuals annually.


== By Year ==
1999: Tom Christiansen, Kevin Lenzo, Adam Turoff
2000: Elaine Ashton, Chris Nandor, Nathan Torkington
2001: David H. Adler, Ask Bjørn Hansen, YAPC::Europe team
2002: Graham Barr, Tim Maher, Tim Vroom
2003: Jarkko Hietaniemi, Andreas Koenig, Robert Spier
2004: Dave Cross, brian d foy, Jon Orwant
2005: Stas Bekman, Eric Cholet, Andy Lester
2006: Jay Hannah, Josh McAdams, Randal Schwartz
2007: Allison Randal, Tim O'Reilly, Norbert E. Grüner
2008: Tatsuhiko Miyagawa, Jacinta Richardson, Gabor Szabo
2009: Tim Bunce, Philippe Bruhat, Michael Schwern
2010: José Castro (cog), Paul Fenwick, Barbie
2011: Lep Lapworth, Daisuke Maki, Andrew Shitov
2012: Renee Baecker, Breno G. de Oliveira, Jim Keenan
2013: Thiago Rondon, Wendy and Liz, Fred Moyer
2014: Amalia Pomian, VM Brasseur, Neil Bowers
2015: Chris Prather, Sawyer X, Steffen Müller
2016: David Golden, Karen Pauley, and Thomas Klausner


== See also ==
The Perl programming language
YAPC, Yet Another Perl Conference
The Perl Foundation
O'Reilly and Associates


== References ==


== External links ==
White camel information on perl.org
The Second Annual YAPC
News of the 2011 Award
News of the 2012 Awards"
427,Nenad Medvidović,53708393,3286,"Nenad Medvidović is a Professor of Computer Science and Informatics at the University of Southern California, Los Angeles. He is a fellow of the IEEE and an ACM Distinguished Scientist. He is chair of ACM SIGSOFT and co-author of Software Architecture: Foundations, Theory, and Practice (2009). In 2008, he received the Most Influential Paper Award for a paper titled ""Architecture-Based Runtime Software Evolution"" published in the ACM/IEEE International Conference on Software Engineering 1998. In 2017, he received an IEEE International Conference on Software Architecture Best Paper Award for his paper titled ""Continuous Analysis of Collaborative Design"".


== Bibliography ==
Software Architecture: Foundations, Theory, and Practice 2009. Wiley, ISBN 978-0-470-16774-8


== References ==


== External links ==
Website"
428,European Conference on Information Retrieval,10328235,3282,"The European Conference on Information Retrieval (ECIR) is the main European research conference for the presentation of new results in the field of information retrieval (IR). It is organized by the Information Retrieval Specialist Group of the British Computer Society (BCS-IRSG).
The event started its life as the Annual Colloquium on Information Retrieval Research in 1978 and was held in the UK each year until 1998 when it was hosted in Grenoble, France. Since then the venue has alternated between the United Kingdom and continental Europe. To mark the metamorphosis from a small informal colloquium to a major event in the IR research calendar, the BCS-IRSG later renamed the event to European Conference on Information Retrieval. In recent years, ECIR has continued to grow and has become the major European forum for the discussion of research in the field of Information Retrieval.
Some of the topics dealt with include:
IR models, techniques, and algorithms
IR applications
IR system architectures
Test and evaluation methods for IR
Natural Language Processing for IR
Distributed IR
Multimedia and cross-media IR


== Time and Location ==
Traditionally, the ECIR is held in Spring, near the Easter weekend. A list of locations and planned venues are presented below.
Aberdeen, 2017 [1]
Padova, Italy, 2016 [2]
Vienna, Austria, 2015 [3]
Amsterdam, Netherlands, 2014 [4]
Moscow, Russia, 2013 [5]
Barcelona, Spain, 2012 [6]
Dublin, Ireland, 2011 [7]
Milton Keynes, 2010 [8]
Toulouse, 2009 [9]
Glasgow, 2008 [10]
Rome, 2007 [11]
London, 2006 [12]
Santiago, 2005 [13]
Sunderland, 2004 [14]
Pisa, 2003 [15]
Glasgow, 2002 [16]*
Darmstadt, 2001* (organized by GMD)
Cambridge, 2000* (organized by Microsoft Research)
Glasgow, 1999*
Grenoble, 1998*
Aberdeen, 1997*
Manchester, 1996*
Crewe, 1995* (organized by Manchester Metropolitan University)
Drymen, Scotland, 1994* (organized by Strathclyde University)
Glasgow, 1993* (organized by Strathclyde University)
Lancaster, 1992*
Lancaster, 1991*
Huddersfield, 1990*
Huddersfield, 1989*
Huddersfield, 1988*
Glasgow, 1987*
Glasgow, 1986*
Bradford, 1985*
Bradford, 1984*
Sheffield, 1983*
Sheffield, 1982*
Birmingham, 1981*
Leeds, 1980*
Leeds, 1979*

*as the Annual Colloquium on Information Retrieval Research


== External links ==
Official page at the website of the British Computer Society"
429,Path cover,2243546,3280,"Given a directed graph G = (V, E), a path cover is a set of directed paths such that every vertex v ∈ V belongs to at least one path. Note that a path cover may include paths of length 0 (a single vertex).
A path cover may also refer to a vertex-disjoint path cover, i.e., a set of paths such that every vertex v ∈ V belongs to exactly one path.


== Properties ==
A theorem by Gallai and Milgram shows that the number of paths in a smallest path cover cannot be larger than the number of vertices in the largest independent set. In particular, for any graph G, there is a path cover P and an independent set I such that I contains exactly one vertex from each path in P. Dilworth's theorem follows as a corollary of this result.


== Computational complexity ==
Given a directed graph G, the minimum path cover problem consists of finding a path cover for G having the least number of paths.
A minimum path cover consists of one path if and only if there is a Hamiltonian path in G. The Hamiltonian path problem is NP-complete, and hence the minimum path cover problem is NP-hard. However, if the graph is acyclic, the problem is in complexity class P and can therefore be solved in polynomial time by transforming it in a matching problem.


== Applications ==
The applications of minimum path covers include software testing. For example, if the graph G represents all possible execution sequences of a computer program, then a path cover is a set of test runs that covers each program statement at least once.


== See also ==
Covering (disambiguation)#Mathematics


== Notes ==


== References ==
Bang-Jensen, Jørgen; Gutin, Gregory (2006), Digraphs: Theory, Algorithms and Applications (1st ed.), Springer .
Diestel, Reinhard (2005), Graph Theory (3rd ed.), Springer .
Franzblau, D. S.; Raychaudhuri, A. (2002), ""Optimal Hamiltonian completions and path covers for trees, and a reduction to maximum flow"", ANZIAM Journal, 44 (2): 193–204, doi:10.1017/S1446181100013894 .
Ntafos, S. C.; Hakimi, S. Louis. (1979), ""On path cover problems in digraphs and applications to program testing"", IEEE Transactions on Software Engineering, 5 (5): 520–529, doi:10.1109/TSE.1979.234213 ."
431,Kleene award,24272293,3277,"The Kleene Award  is awarded at the annual IEEE Symposium on Logic in Computer Science (LICS) to the author(s) of the best student paper(s). A paper qualifies as a student paper if each author is a student at the date of the submission. Also eligible are authors who have graduated only recently, provided the submitted paper is based on work carried out when he or she still was a student. The award decision is made by the Program Committee.
The award is named after Stephen Cole Kleene, who did pioneering work in the field of logic as related to computer science.


== Past recipients ==
Past recipients of the Kleene award are tabulated below.


== See also ==
Machtey Award


== Notes ==


== External links ==
Kleene award"
432,Vertex enumeration problem,19558316,3264,"In mathematics, the vertex enumeration problem for a polytope, a polyhedral cell complex, a hyperplane arrangement, or some other object of discrete geometry, is the problem of determination of the object's vertices given some formal representation of the object. A classical example is the problem of enumeration of the vertices of a convex polytope specified by a set of linear inequalities:

  
    
      
        A
        x
        ≤
        b
      
    
    {\displaystyle Ax\leq b}
  
where A is an m×n matrix, x is an n×1 column vector of variables, and b is an m×1 column vector of constants.


== Computational complexity ==
The computational complexity of the problem is a subject of research in computer science. For unbounded polyhedra, the problem is known to be NP-hard, more precisely, there is no algorithm that runs in polynomial time in the combined input-output size, unless P=NP .
A 1992 article by David Avis and Komei Fukuda presents an algorithm which finds the v vertices of a polytope defined by a nondegenerate system of n inequalities in d dimensions (or, dually, the v facets of the convex hull of n points in d dimensions, where each facet contains exactly d given points) in time O(ndv) and space O(nd). The v vertices in a simple arrangement of n hyperplanes in d dimensions can be found in O(n2dv) time and O(nd) space complexity. The Avis–Fukuda algorithm adapted the criss-cross algorithm for oriented matroids.


== Notes ==


== References ==
Avis, David; Fukuda, Komei (December 1992). ""A pivoting algorithm for convex hulls and vertex enumeration of arrangements and polyhedra"". Discrete and Computational Geometry. 8 (1): 295–313. doi:10.1007/BF02293050. MR 1174359."
433,Implicit parallelism,3453888,3264,"In computer science, implicit parallelism is a characteristic of a programming language that allows a compiler or interpreter to automatically exploit the parallelism inherent to the computations expressed by some of the language's constructs. A pure implicitly parallel language does not need special directives, operators or functions to enable parallel execution, as opposed to explicit parallelism.
Programming languages with implicit parallelism include Axum, BMDFM, HPF, Id, LabVIEW, MATLAB M-code, NESL, SaC, SISAL, ZPL, and pH.


== Example ==
If a particular problem involves performing the same operation on a group of numbers (such as taking the sine or logarithm of each in turn), a language that provides implicit parallelism might allow the programmer to write the instruction thus:

The compiler or interpreter can calculate the sine of each element independently, spreading the effort across multiple processors if available.


== Advantages ==
A programmer that writes implicitly parallel code does not need to worry about task division or process communication, focusing instead on the problem that his or her program is intended to solve. Implicit parallelism generally facilitates the design of parallel programs and therefore results in a substantial improvement of programmer productivity.
Many of the constructs necessary to support this also add simplicity or clarity even in the absence of actual parallelism. The example above, of List comprehension in the sin() function, is a useful feature in of itself. By using implicit parallelism, languages effectively have to provide such useful constructs to users simply to support required functionality (a language without a decent for() loop, for example, is one few programmers will use).


== Disadvantages ==
Languages with implicit parallelism reduce the control that the programmer has over the parallel execution of the program, resulting sometimes in less-than-optimal parallel efficiency. The makers of the Oz programming language also note that their early experiments with implicit parallelism showed that implicit parallelism made debugging difficult and object models unnecessarily awkward.
A larger issue is that every program has some parallel and some serial logic. Binary I/O, for example, requires support for such serial operations as Write() and Seek(). If implicit parallelism is desired, this creates a new requirement for constructs and keywords to support code that cannot be threaded or distributed.


== Notes =="
434,EXperimental Computing Facility,2364464,3257,"Founded in 1986, the eXperimental Computing Facility (XCF) is an undergraduate computing-interest organization at University of California, Berkeley. The ""Experimental"" description was given in contrast to the Open Computing Facility and the Computer Science Undergraduate Association, which support most of the general-interest computing desires of the campus. As such, the XCF stands as a focus for a small group of computer-scientists uniquely interested in computer science.
Members of the organization have been involved in projects such as NNTP, GTK+, GIMP, Gnutella, and Viola. Members of the XCF were instrumental in defending against the Morris Internet worm.


== Notable alumni ==
Notable alumni of the organization include: Jonathan Blow, Gene Kan, Spencer Kimball, Peter Mattis, Pei-Yuan Wei, and Phil Lapsley.


== References ==


== External links ==
Official website
""Student Organization Website"". 
""Older XCF website"". eXperimental Computing Facility, University of California at Berkeley. Archived from the original on 27 October 2010. Retrieved 29 March 2015."
435,Unix domain socket,4769697,3254,"A Unix domain socket or IPC socket (inter-process communication socket) is a data communications endpoint for exchanging data between processes executing on the same host operating system. Like named pipes, Unix domain sockets support transmission of a reliable stream of bytes (SOCK_STREAM, compare to TCP). In addition, they support ordered and reliable transmission of datagrams (SOCK_SEQPACKET, compare to SCTP), or unordered and unreliable transmission of datagrams (SOCK_DGRAM, compare to UDP). The Unix domain socket facility is a standard component of POSIX operating systems.
The API for Unix domain sockets is similar to that of an Internet socket, but rather than using an underlying network protocol, all communication occurs entirely within the operating system kernel. Unix domain sockets use the file system as their address name space. Processes reference Unix domain sockets as file system inodes, so two processes can communicate by opening the same socket.
In addition to sending data, processes may send file descriptors across a Unix domain socket connection using the sendmsg() and recvmsg() system calls. This allows the sending processes to grant the receiving process access to a file descriptor for which the receiving process otherwise does not have access. This can be used to implement a rudimentary form of capability-based security. For example, this allows the Clam AntiVirus scanner to run as an unprivileged daemon on Linux and BSD, yet still read any file sent to the daemon's Unix domain socket.


== See also ==
Raw socket
Datagram socket
Stream socket
Network socket
Berkeley sockets
Pipeline


== References ==


== External links ==
socketpair – System Interfaces Reference, The Single UNIX Specification, Issue 7 from The Open Group
sendmsg – System Interfaces Reference, The Single UNIX Specification, Issue 7 from The Open Group
recvmsg – System Interfaces Reference, The Single UNIX Specification, Issue 7 from The Open Group
cmsg(3) – Linux Programmer's Manual – Library Functions
ucspi-unix, UNIX-domain socket client-server command-line tools
Unix sockets vs Internet sockets
Unix Sockets - Beej's Guide to Unix IPC"
436,Literal (computer programming),1908624,3253,"In computer science, a literal is a notation for representing a fixed value in source code. Almost all programming languages have notations for atomic values such as integers, floating-point numbers, and strings, and usually for booleans and characters; some also have notations for elements of enumerated types and compound values such as arrays, records, and objects. An anonymous function is a literal for the function type.
In contrast to literals, variables or constants are symbols that can take on one of a class of fixed values, the constant being constrained not to change. Literals are often used to initialize variables, for example, in the following, 1 is an integer literal and the three letter string in ""cat"" is a string literal:

In lexical analysis, literals of a given type are generally a token type, with a grammar rule, like ""a string of digits"" for an integer literal. Some literals are specific keywords, like true for the boolean literal ""true"".
In some object-oriented languages (like ECMAScript), objects can also be represented by literals. Methods of this object can be specified in the object literal using function literals. The brace notation below, which is also used for array literals, is typical for object literals:


== Literals of objects ==
In ECMAScript (as well as its derivatives JavaScript and ActionScript), an object with methods can be written using the object literal like this:

These object literals are similar to anonymous classes in other languages like Java. The difference being that ECMAScript doesn't have classes because it's a prototype-based language.
The JSON data interchange format is based on a subset of the JavaScript object literal syntax, with some additional restrictions (among them requiring all keys to be quoted, and disallowing functions and everything else except data literals). Because of this, almost every valid JSON document (except for some subtleties with escaping) is also valid JavaScript code, a fact exploited in the JSONP technique.


== See also ==
Character literal
Function literal
Here document – a file literal or stream literal
Integer literal
String literal"
437,Dvd+rw-tools,8368665,3245,"dvd+rw-tools (also known as growisofs, its main part) is a collection of open source DVD and Blu-ray Disc tools for Linux, OpenBSD, NetBSD, FreeBSD, Solaris, HP-UX, IRIX, Windows and OS X. dvd+rw-tools does not operate CD media. 
The package itself requires another program which is used to create ISO 9660 images on the fly. This is provided by mkisofs (from the cdrtools package) or genisoimage (from the cdrkit package) or a symbolic link xorrisofs to xorriso (from the libisoburn or GNU xorriso package).  Released under the GNU General Public License, dvd+rw-tools is free software.


== Programs ==
growisofs burns data to DVD or BD media.
dvd+rw-mediainfo reports about drive and medium status.
dvd+rw-format formats some types of media, blanks DVD-RW media.
dvd-ram-control controls Defect Management and write protection of DVD-RAM media.
dvd+rw-booktype sets the Book Type property of DVD media.


== growisofs ==
growisofs is a SCSI/MMC driver in userspace for burning optical media, like cdrecord or libburn. Its original purpose is to coordinate burning with a run of mkisofs, so that ISO 9660 multi-session becomes possible on DVD+RW media. But soon it supported all kinds of DVD media and later also BD (Blu-ray) media. Further it can burn preformatted data images onto the media, not needing any ISO 9660 formatter program for this task.
growisofs employs a Ring Buffer in userspace to smoothen the data transmission to the drive. The fill level of this buffer is reported during a burn run as ""RBU"", whereas the fill level of the drive's built-in buffer is reported as ""UBU"".


== References ==


== External links ==
Official website"
438,Moral Machine,53210461,3242,"Moral Machine is an online platform, developed by Iyad Rahwan's Scalable Cooperation group at the Massachusetts Institute of Technology, that generates moral dilemmas and collects information on the decisions that people make between two destructive outcomes. The presented scenarios are often variations of the trolley problem, and the information collected would be used for further research regarding the decisions that machine intelligence must make in the future. For example, as artificial intelligence plays an increasingly significant role in autonomous driving technology, research projects like Moral Machine help to find solutions for challenging life-and-death decisions that will face self-driving vehicles.


== References ==


== External links ==
Official website"
439,Marlon Dumas,40566490,3238,"Marlon Gerardo Dumas Menjivar (born 22 August 1975) is a Honduran computer scientist, and Professor of Software Engineering at the University of Tartu in Estonia, known for his contributions in the field of Business Process Management.
Born in Honduras, Dumas received his PhD in Computer Science at the University of Grenoble in France in 2000. From 2000 to 2009 he was Lecturer at the Queensland University of Technology in Brisbane, Australia. In 2007 he was appointed Professor of Software Engineering at the University of Tartu in Estonia.
In 2004 and 2007 Dumas was awarded the Queensland Government Fellowship to ""undertake research on service-oriented software architectures in collaboration with SAP AG.""
Dumas is married with an Estonian, whom he has a daughter with.


== Publications ==
Dumas authored and co-authored numerous publications in the field of computer science. Books:
Marlon Dumas, Wil M.P. van der Aalst, Arthur H.M. ter Hofstede (Editors), Process-Aware Information Systems: Bridging People and Software Through Process Technology, John Wiley & Sons, September 2005.
Marlon Dumas, Marcello La Rosa, Jan Mendling, Hajo A. Reijers. Fundamentals of Business Process Management, Springer, February 2013
Articles, a selection:
Zeng, L., Benatallah, B., Dumas, M., Kalagnanam, J., & Sheng, Q. Z. (2003, May). Quality driven web services composition. In Proceedings of the 12th international conference on World Wide Web (pp. 411–421). ACM.
Zeng, Liangzhao, et al. ""QoS-aware middleware for web services composition."" Software Engineering, IEEE Transactions on 30.5 (2004): 311-327.


== References ==


== External links ==
Marlon Dumas at University of Tartu"
440,Bebugging,12086708,3228,"Bebugging (or fault seeding or error seeding) is a popular software engineering technique used in the 1970s to measure test coverage. Known bugs are randomly added to a program source code and the programmer is tasked to find them. The percentage of the known bugs not found gives an indication of the real bugs that remain.
The term ""bebugging"" was first mentioned in The Psychology of Computer Programming (1970), where Gerald M. Weinberg described the use of the method as a way of training, motivating, and evaluating programmers, not as a measure of faults remaining in a program. The approach was borrowed from the SAGE system, where it was used to keep operators watching radar screens alert. Here's a quote from the original use of the term:

Overconfidence by the programmer could be attacked by a system that introduced random errors into the program under test. The location and nature of these errors would be recorded inside the system but concealed from the programmer. The rate at which he found and removed these known errors could be used to estimate the rate at which he is removing unknown errors. A similar technique is used routinely by surveillance systems in which an operator is expected to spend eight hours at a stretch looking at a radar screen for very rare events—such as the passing of an unidentified aircraft. Tests of performance showed that it was necessary to introduce some nonzero rate of occurrence of artificial events in order to keep the operator in a satisfactory state of arousal. Moreover, since these events were under control of the system, it was able to estimate the current and overall performance of each operator.
Although we cannot introduce program bugs which simulate real bugs as well as we can simulate real aircraft on a radar screen, such a technique could certainly be employed both to train and evaluate programmers in program testing. Even if the errors had to be introduced manually by someone else in the project, it would seem worthwhile to try out such a “bebugging” system. It would give the programmer greatly increased motivation, because he now would know:
There are errors in his program.
He did not put them there.

An early application of bebugging was Harlan Mills's fault seeding approach  which was later refined by stratified fault-seeding. These techniques worked by adding a number of known faults to a software system for the purpose of monitoring the rate of detection and removal. This assumed that it is possible to estimate the number of remaining faults in a software system still to be detected by a particular test methodology.
Bebugging is a type of fault injection.


== See also ==
Fault injection
Mutation testing


== References =="
441,Computation offloading,43315156,3225,"In computer science, computation offloading refers to the transfer of resource intensive computational tasks to an external platform, such as a cluster, grid, or a cloud. Offloading may be necessary due to hardware limitations of a devices, such as limited computational power, storage, and energy. The resource intensive tasks may be for searching, virus scanning, image processing, artificial intelligence, computational decision making etc.
The term is mainly used in context of mobile cloud computing, edge computing, and fog computing.


== References =="
442,Guarded suspension,164871,3225,"In concurrent programming, guarded suspension is a software design pattern for managing operations that require both a lock to be acquired and a precondition to be satisfied before the operation can be executed. The guarded suspension pattern is typically applied to method calls in object-oriented programs, and involves suspending the method call, and the calling thread, until the precondition (acting as a guard) is satisfied.


== UsageEdit ==
Because it is blocking, the guarded suspension pattern is generally only used when the developer knows that a method call will be suspended for a finite and reasonable period of time. If a method call is suspended for too long, then the overall program will slow down or stop, waiting for the precondition to be satisfied. If the developer knows that the method call suspension will be indefinite or for an unacceptably long period, then the balking pattern may be preferred.


== ImplementationEdit ==
In Java, the Object class provides the wait() and notify() methods to assist with guarded suspension. In the implementation below, originally found in Kuchana (2004), if there is no precondition satisfied for the method call to be successful, then the method will wait until it finally enters a valid state.

An example of an actual implementation would be a queue object with a get method that has a guard to detect when there are no items in the queue. Once the ""put"" method notifies the other methods (for example, a get() method), then the get() method can exit its guarded state and proceed with a call. Once the queue is empty, then the get() method will enter a guarded state once again.


== See alsoEdit ==
Read write lock pattern
Balking pattern is an alternative pattern for dealing with a precondition
Guarded commands includes a similar language construct


== NotesEdit ==


== ReferencesEdit ==
Kuchana, Partha (2004). ""Software Architecture Design Patterns in Java"". Boca Raton, Florida: Auerbach Publications. ."
443,Generalized star height problem,669942,3222,"The generalized star-height problem in formal language theory is the open question whether all regular languages can be expressed using generalized regular expressions with a limited nesting depth of Kleene stars. Here, generalized regular expressions are defined like regular expressions, but they have a built-in complement operator. For a regular language, its generalized star height is defined as the minimum nesting depth of Kleene stars needed in order to describe the language by means of a generalized regular expression, hence the name of the problem.
More specifically, it is an open question whether a nesting depth of more than 1 is required, and if so, whether there is an algorithm to determine the minimum required star height.
Regular languages of star-height 0 are also known as star-free languages. The theorem of Schützenberger provides an algebraic characterization of star-free languages by means of aperiodic syntactic monoids. In particular star-free languages are a proper decidable subclass of regular languages.


== See also ==
Eggan's theorem and Generalized star height sections of the Star height article
Star height problem


== References ==

Janusz A. Brzozowski (1980). ""Open problems about regular languages"". In Ronald V. Book. Formal Language Theory: Perspectives and Open Problems. Academic Press. pp. 23–47. 
Wolfgang Thomas (1981). ""Remark on the star-height-problem"". Theoretical Computer Science. 13 (2): 231–237. doi:10.1016/0304-3975(81)90041-4. MR 0594062. 
Jean-Eric Pin; Howard Straubing; Denis Thérien (1992). ""Some results on the generalized star-height problem"" (PDF). Information and Computation. 101 (2): 219–250. doi:10.1016/0890-5401(92)90063-L. 
Sakarovitch, Jacques (2009). Elements of automata theory. Translated from the French by Reuben Thomas. Cambridge: Cambridge University Press. ISBN 978-0-521-84425-3. Zbl 1188.68177. 
Marcel-Paul Schützenberger (1965). ""On finite monoids having only trivial subgroups"". Information and Control. 8 (2): 190–194. doi:10.1016/S0019-9958(65)90108-7. Zbl 0131.02001. 


== External links ==
Jean-Eric Pin: The star-height problem"
444,Raphael Rom,42351907,3219,"Raphael ""Raphi"" Rom is an Israeli computer scientist working at Technion – Israel Institute of Technology.
Rom earned his Ph.D. in 1975 from the University of Utah, under the supervision of Thomas Stockham. He is known for his contribution to the development of the Catmull–Rom spline, and for his research on computer networks.


== Selected publications ==
Catmull, E.; Rom, R. (1974), ""A class of local interpolating splines"", in Barnhill, R. E.; Riesenfeld, R. F., Computer Aided Geometric Design, New York: Academic Press, pp. 317–326 .
Rom, R.; Sidi, M. (1990), Multiple Access Protocols: Performance and Analysis, Springer .
Orda, A.; Rom, R. (1990), ""Shortest-path and minimum-delay algorithms in networks with time-dependent edge-length"", Journal of the ACM, 37 (3): 607–625, doi:10.1145/79147.214078, MR 1072271 .
Orda, A..; Rom, R.; Shimkin, N. (1993), ""Competitive routing in multiuser communication networks"", IEEE/ACM Transactions on Networking, 1 (5): 510–521, doi:10.1109/90.251910 .
Azar, Yossi; Naor, Joseph; Rom, Raphael (1995), ""The competitiveness of on-line assignments"", Journal of Algorithms, 18 (2): 221–237, doi:10.1006/jagm.1995.1008, MR 1317665 .
Cidon, I.; Rom, R.; Shavitt, Y. (1999), ""Analysis of multi-path routing"", IEEE/ACM Transactions on Networking, 7 (6): 885–896, doi:10.1109/90.811453 .


== References ==


== External links ==
Home page"
445,Symbolic Systems,1823103,3212,"Symbolic Systems is an academic program at Stanford University with an interdisciplinary concentration on ""the science of the mind"" including courses in computer science, linguistics, philosophy, and psychology. The program is analogous to cognitive science departments at other schools.
The program offers both bachelor's and master's degrees. The undergraduate major is split into concentrations such as artificial intelligence, human-computer interaction, decision making and rationality, and neuroscience. Student advisors and the student-led Symbolic Systems Society provide support and organize events for the major. Each year, the program hosts a ""Distinguished Speaker"", which in the past have included Daniel Dennett, Steven Pinker, Edward Snowden, and Marvin Minsky.


== Notable graduates ==
Chris Cox, Chief Product Officer at Facebook
Matt Flannery
Scott Forstall
Reid Hoffman
Mike Krieger
Yul Kwon
Marissa Mayer
Yuko Munakata
Srinija Srinivasan
Tina Chen
Barney Pell
Tony Tulathimutte


== References ==


== External links ==
Stanford University's Symbolic Systems Homepage
Arizona State University's Symbolic Systems Certificate"
446,Compressed pattern matching,22095269,3212,"In computer science, compressed pattern matching (abbreviated as CPM) is the process of searching for patterns in compressed data with little or no decompression. Searching in a compressed string is faster than searching an uncompressed string and requires less space.


== Compressed matching problem ==
If the compressed file uses a variable width encoding it could be present a problem: for example, let “100” be the codeword for a and let “110100” be the codeword for b. If we are looking for an occurrence of a in the text we could obtain as result also an occurrence that is within the codeword of b: we call this event false match. So we have to verify if the occurrence detected is effectively aligned on a codeword boundary. However we could always decode the entire text and then apply a classic string matching algorithm, but this usually requires more space and time and often is not possible, for example if the compressed file is hosted online. This problem of verifying the match returned by the compressed pattern matching algorithm is a true or a false match together with the impossibility of decoding an entire text is called the compressed matching problem. Many strategies exist for finding the boundaries of codewords and avoiding full decompression of the text, for example:
List of the indices of first bit of each codeword, where we can apply a binary search;
List of the indices of first bit of each codeword with differential coding, so we can take less space within the file;
Mask of bit, where bit 1 marks the starting bit of each codeword;
Subdivision in blocks, for a partial and aimed decompression.


== References ==

Shmuel T. Klein and Dana Shapira PATTERN MATCHING IN HUFFMAN ENCODED TEXTS (2003)
Marek Karpinski, Wojciech Rytter and Ayumi Shinohara. AN EFFICIENT PATTERN-MATCHING ALGORITHM FOR STRINGS WITH SHORT DESCRIPTIONS. Nordic Journal of Computing 4(2): pp.172-168 (1997).


== External links ==
""Almost optimal fully LZW-compressed pattern matching"". CiteSeerX 10.1.1.44.5521 . 
A Dictionary-based Compressed Pattern Matching Algorithm (PDF), archived from the original (PDF) on March 13, 2003 
""A unifying framework for compressed pattern matching"". CiteSeerX 10.1.1.50.1745 . 
""Speeding Up String Pattern Matching by Text Compression: The Dawn of a New Era"" (PDF). 
""Shift-and approach to pattern matching in LZW compressed text"". CiteSeerX 10.1.1.15.4609 . 
""LZW Algorithm"" (PDF)."
447,"International Conference on User Modeling, Adaptation, and Personalization",56417628,3200,"
== Overview ==
The International Conference on User Modeling, Adaptation, and Personalization (UMAP) is the oldest international conference for researchers and practitioners working on various kinds of user-adaptive computer systems such as Adaptive hypermedia system, Recommender system, Adaptive website, Adaptive learning, Personalized learning and Intelligent tutoring system, Personalized search system, etc. All these kinds of systems adapt to their individual users, or to groups of users (i.e., Personalization). To achieve this goal they and collect and represent information about users or groups (i.e., User modeling). UMAP conferences series has been historically organized under the auspices of User Modeling Inc., a professional organization of User Modeling researchers. . Starting from 2016, UMAP conference series is also affiliated with Association for Computing Machinery where it is supported by ACM SIGWEB and ACM SIGCHI.


== History ==
UMAP is the successor to the biennial conference series on User modeling and Adaptive hypermedia. International Conference on User Modeling (abbreviated as UM) started in 1986 as First International Workshop on User Modeling at Maria Laach, Germany and was first officially called as a conference at its 4th installation, Fourth International Conference on User Modeling in Hyannis, Massachusetts, USA. The last conference in the original series, UMAP 2007, has been held in Corfu, Greece. International Conference on Adaptive Hypermedia and Adaptive Web-based Systems (abbreviated as AH) started in 2000 in Trento, Italy. The last conference in this original series, AH 2008, has been held in 2008 in Hannover, Germany. For several years between 2000 and 2008 UM and AH series ran in coordination as biannual events in alternate years. In 2009, the conference series merged into a single annual series under the title International Conference on User Modeling, Adaptation, and Personalization. The first conference in the joint UMAP series (UMAP 2009) was held in Trento, Italy, the birthplace of AH series.


== Past Conferences ==
The full list of conferences in the series could be found on UM Inc. pages..


== References =="
448,Computer Science and Engineering,524425,3188,"Computer Science and Engineering (CSE) is an interdisciplinary academic program at some universities that combines aspects of both computer science and computer engineering programs. It is a subfield of electronics engineering. However, it covers only the digital aspects of electronics engineering, with added courses like computer architecture, processor design and parallel computing. It focuses more on hardware-software integration, considering the machine as a system.
Computer science programs typically centers primarily around theory and software, with only some hardware; upper division courses tend to allow a lot of freedom to specialize in software and theory related areas (e.g. algorithms, artificial intelligence, cryptography/security, graphics/visualization, numerical and symbolic computing, operating systems/distributed processing, software engineering).
Computer engineering programs tend to resemble computer science at the lower division with similar introductory programming and math courses, but diverges from computer science at the upper division with heavy electrical engineering requirements (e.g. digital and analog circuits, integrated circuit design, VLSI design and control systems. Despite the overlap with computer science at the lower division level, computer engineering skews much more heavily toward the hardware/electronics side that it has more in common with electrical engineering.
Computer Science and Engineering integrates all of the above and is intended to develop a solid understanding of the entire machine (computer hardware and software). The higher unit count required to complete the program often means that a CSE student will need to spend an extra year in university.
Although Computer Science and Engineering is the common designation for the combined program, some universities (such as Berkeley and MIT) deviate by calling their program Electrical Engineering and Computer Science (EECS). Furthermore, there are some universities (such as UCI and UC Merced) that named their department EECS and the program housed within CSE.


== References ==


== See also ==
Computer Science
Computer Engineering
Electrical Engineering
Software Engineering"
449,Compile farm,622986,3180,"A compile farm is a server farm, a collection of one or more servers, which has been set up to compile computer programs remotely for various reasons. Uses of a compile farm include:
Cross-platform development: When writing software that runs on multiple processor architectures and operating systems, it can be infeasible for each developer to have their own machine for each architecture — for example, one platform might have an expensive or obscure type of CPU. In this scenario, a compile farm is useful as a tool for developers to build and test their software on a shared server running the target OS and CPU. Compile farms may be preferable to cross-compilation as cross compilers are often complicated to configure, and in some cases compilation is only possible on the target, making cross-compilation impossible.
Cross-platform continuous integration testing: under this scenario, each server has a different processor architecture or runs a different operating system; scripts automatically build the latest version of a source tree from a version control repository. One of the difficulties of cross-platform development is that a programmer may unintentionally introduce an error that causes the software to stop functioning on a different CPU/OS platform from the one they are using. By using a cross-platform compile farm, such errors can be identified and fixed.
Distributed compilation: Building software packages typically requires operations that can be run in parallel (for example, compiling individual source code files). By using a compile farm, these operations can be run in parallel on separate machines. An example of a program which can be used to do this is distcc.
One example of a compile farm was the service provided by SourceForge until 2006. The SourceForge compile farm was composed of twelve machines of various computer architectures running a variety of operating systems, and was intended to allow developers to test and use their programs on a variety of platforms before releasing them to the public. After a power spike destroyed several of the machines it became non-operational some time in 2006, and was officially discontinued on February 8, 2007.
Other examples are:
GCC Compile Farm https://gcc.gnu.org/wiki/CompileFarm
OpenSUSE Build Service
FreeBSD reports service which lets package maintainers test their own changes on a variety of versions and architectures.
Launchpad Build Farm https://launchpad.net/builders
Mozilla has a build farm, but it is not public https://wiki.mozilla.org/ReleaseEngineering
Debian has a build farm https://buildd.debian.org/


== References =="
450,Hand coding,3420503,3175,"In computing, hand coding means editing the underlying representation of a document or a computer program, when tools that allow working on a higher level representation also exist. Typically this means editing the source code, or the textual representation of a document or program, instead of using a WYSIWYG editor that always displays an approximation of the final product. It may also mean translating the whole or parts of the source code into machine language manually instead of using a compiler or an automatic translator.
Most commonly, it refers to directly writing HTML documents for the web (rather than in a specialized editor), or to writing a program or portion of a program in assembly language (more rarely raw machine code) rather than in a higher level language. It can also include other markup languages, such as wikitext.


== Purpose ==
The reasons to use hand coding include the ability to:
Use features or refinements not supported by the graphical editor or compiler
Control the semantics of a document beyond that allowed by the graphical editor
Produce more elegant source code to help maintenance and integration
Produce better performing machine code than that produced by the compiler (see optimization)
Avoid having to pay for expensive WYSIWYG Editors. Note that there are some open-source editors available on the web, however.
Develop an understanding of the methods underlying a common level of abstraction. For example, although it has become rare in real-life scenarios, computer science students may be required to write a program in an assembly language to get a notion of processor registers and other basal elements of computer architecture.
Escape abstractions and templated code. Hand coding allows more refined control of code, which may improve efficiency, or add functionality that is otherwise unavailable.
Hand coding may require more expertise and time than using automatic tools.


== Hand Code ==
Hand code is source code which does not have tools that can edit it at a more abstract level. Hand code must, by definition, be edited and maintained entirely by hand. Some code can be edited either using an editor/IDE or by hand, but hand code is differentiated from derived code in that it requires human involvement to create and maintain it over time. Projects may include both hand code and derivative code.
Ironically, the automatic tools responsible for creating derivative code are themselves usually made up entirely, or at least in part, of hand code.


== References =="
451,Priority inheritance,621132,3167,"In real-time computing, priority inheritance is a method for eliminating unbounded priority inversion. Using this programming method, a process scheduling algorithm increases the priority of a process (A) to the maximum priority of any other process waiting for any resource on which A has a resource lock (if it is higher than the original priority of A).
The basic idea of the priority inheritance protocol is that when a job blocks one or more high-priority jobs, it ignores its original priority assignment and executes its critical section at an elevated priority level. After executing its critical section and releasing its locks, the process returns to its original priority level.


== Example ==
Consider three jobs:
Suppose H is blocked by L for some shared resource. The priority inheritance protocol requires that L executes its critical section at H's (high) priority. As a result, M will be unable to preempt L and will be blocked. That is, the higher-priority job M must wait for the critical section of the lower priority job L to be executed, because L has inherited H's priority. When L exits its critical section, it regains its original (low) priority and awakens H (which was blocked by L). H, having high priority, preempts L and runs to completion. This enables M and L to resume in succession and run to completion.


== See also ==
Priority ceiling protocol


== References ==
Lui Sha; Ragunathan Rajkumar & John P. Lehoczky (September 1990). ""Priority Inheritance Protocols: An Approach to Real-Time Synchronization"" (PDF). IEEE Transactions on Computers. 39 (9): 1175–1185. doi:10.1109/12.57058. 


== External links ==
""Priority Inheritance: The Real Story"" by Doug Locke
""Against Priority Inheritance"" by Victor Yodaiken
""Implementing Concurrency Control With Priority Inheritance in Real-Time CORBA"" by Steven Wohlever, Victor Fay Wolfe and Russell Johnston
""Priority Inheritance Spin Locks for Multiprocessor Real-Time Systems"" by Cai-Dong Wang, Hiroaki Takada and Ken Sakamura
""Hardware Support for Priority Inheritance"" by Bilge E. S. Akgul, Vincent J. Mooney, Henrik Thane and Pramote Kuacharoen"
452,User exit,9934699,3157,"A user exit is a subroutine invoked by a software package for a predefined event in the execution of the package. Clients of the package can substitute their own subroutines in place of the default ones provided by the package vendor to provide customized functionality.
A typical use is replacing the user exits provided by a sort/merge package, whereby the user program provides its own subroutines for comparing records. The procedures provided by the user take the place of the default routines (usually stubs that do nothing but return to their caller) provided by the package vendor.
Procedures provided as user exits are typically compiled into a static library and linked directly with the package to produce an executable program. Another approach employs dynamic libraries to accomplish the same thing. Finally, user exits may be external programs executed at specific points in the execution of a host package.
If the user site specific code is substituted for the software vendor provided default exit it must interface to the software package using the defined parameters as documented for the default exit. User exits are important because while they enable site specific customization they isolate such installation specific customization to defined and supported points enabling the site to upgrade to follow-on releases of the software package without adverse impact to preexisting customized functionality. Some references to IBM user exit manuals are given below. Other vendors such as SAP, Oracle, IFS, HP, Macro4, Compuware, CA all employ user exits in some of their software products.
Historically, this term is commonly used in IBM mainframe vernacular.
Title: z/OS V1R10 DFSMS Installation Exits Document Number: SC26-7396-11
Title: z/OS V1R10.0 JES2 Installation Exits Document Number: SA22-7534-10
Title: z/OS V1R10.0 MVS Installation Exits Document Number: SA22-7593-14


== Applications ==
Some applications that provide user exits:
Apache Subversion allows ""hooks"", which are ""scripts that run when an action is performed""
IBM CICS
IBM CMVC user exits in the form of Kornshell scripts
IBM JES 2 and 3
IBM MVS, SMS, z/OS and dozens of sub-components such as RACF, SMF, etc.
IBM sort/merge package
Oracle CC&B
SAP R3


== See also ==
Callback
Linking
Hook


== References =="
453,Cellular multiprocessing,2765516,3146,"Cellular multiprocessing is a multiprocessing computing architecture designed initially for Intel central processing units from Unisys, a worldwide information technology consulting services and solutions company.
It consists of the partitioning of processors into separate computing environments running different operating systems. Providing up to 32 processors that are crossbar connected to 64GB of memory and 96 PCI cards, a CMP system provides mainframe-like architecture using Intel CPUs. CMP supports Windows NT and Windows 2000 Server, AIX, Novell NetWare and UnixWare and can be run as one large SMP system or multiple systems with variant operating systems.
There is a concept of creating CPU Partitions in CMPs, e.g. one can create a full partition of 32 processors, Or one can create two partitions of 16 processors each, these two partitions will be visible to the OS installed as two machines. Similarly for 32 processors it is possible to create 32 partitions at max each having a single CPU. Unisys' CMP is the only server architecture to take full advantage of Microsoft's Windows 2000 Datacenter Server operating system's support for 32 processors.
In case of LINUX/UNIX OS the CMP technology is proven to be very best, whereas in case of Windows 2003 Servers installations, there are certain limits for partitions having number of CPUs, like for a windows 2003 installation the maximum CPU in a partition can only be 4, if more CPUs are assigned severe performance degrade are observed. Even on 8 CPU partition the performance is compariable to the performance of a 2 processors partition.
A CMP subpod contains four x86 or Itanium CPUs, which connect through a third-level memory cache to the crossbar. Each crossbar supports two subpods, two direct I/O bridges (DIBs) and can connect to four memory storage units (MSUs).
Unisys is also providing CMP server technology to Compaq, Dell, Hewlett-Packard and ICL, under OEM agreements.


== See also ==
Asymmetric Multi-Processing
Symmetric Multi-Processing


== References =="
454,Processing Instruction,28408442,3131,"A Processing Instruction (PI) is an SGML and XML node type, which may occur anywhere in the document, intended to carry instructions to the application.
Processing instructions are exposed in the Document Object Model as Node.PROCESSING_INSTRUCTION_NODE, and they can be used in XPath and XQuery with the 'processing-instruction()' command.


== Syntax ==
An SGML processing instruction is enclosed within <? and >.
An XML processing instruction is enclosed within <? and ?>, and contains a target and optionally some content, which is the node value, that cannot contain the sequence ?>.

The XML Declaration at the beginning of an XML document (shown below) is not a processing instruction, however its similar syntax has often resulted in it being referred to as a processing instruction.


== Examples ==
The most common use of a processing instruction is to request the XML document be rendered using a stylesheet using the 'xml-stylesheet' target, which was standardized in 1999. It can be used for both XSLT and CSS stylesheets.

The DocBook XSLT stylesheets understand a number of processing instructions to override the default behaviour.
A draft specification for Robots exclusion standard rules inside XML documents uses processing instructions.


== References ==


== External links ==
XML specification section: Processing Instructions
XSLT FAQ: Processing Instructions, Dave Pawson
xslt:processing-instruction, Mozilla"
455,Black Data Processing Associates,8238464,3120,"Black Data Processing Associates (BDPA) is a non-profit organization that serves the professional well-being of its stakeholders. BDPA provides resources that support the professional growth and technical development of minority individuals in the information technology industry. Through education and leadership, BDPA promotes innovation, business skills, and professional development. The organization has over 50 chapters throughout the United States. BDPA National headquarters is located in Largo, Maryland.


== History ==
BDPA was founded in 1975 by Earl A. Pace, Jr. and David Wimberly after the two met in Philadelphia to discuss their concerns about ethnic minorities in the data processing field. The founders cited a lack of minorities in middle and upper management, low recruitment and poor preparation of minorities for these positions, and an overall lack of career mobility.
The founders built an organization of 35 members, hosted presentations to improve data processing skills and launched a job opportunities announcement service. This nucleus has grown to over 50 chapters throughout the United States and thousands of members. The organization is a catalyst for professional growth and technical development for those in the IT industry.


== BDPA High School Computer Competition ==
The National High School Computer Competition (HSCC) was founded in 1986. The competition started as a two-team event between Washington, DC and Atlanta, GA, and now has over 20 teams from chapters throughout the nation.


== References ==


== External links ==
BDPA
BDPA Education and Technology Foundations (BETF)"
456,Escuela Superior Latinoamericana de Informática,2784213,3119,"The Escuela Superior Latinoamericana de Informática (Spanish for ""Latin American Superior School of Informatics"", ESLAI) was an Argentine undergraduate school of computer science established in 1986. Classes were held in a former countryhouse at the Pereyra Iraola Park in Buenos Aires Province, at approximately 40 km from Buenos Aires.
The school had Argentine mathematician Manuel Sadosky among its main founders. In spite of its short life, it had a considerable impact on informatics teaching and research in Argentina and South America. ESLAI courses were attended by students from several Spanish-speaking countries in South America such as Argentina, Uruguay, Paraguay, Bolivia, Peru, Ecuador, Colombia and Venezuela. All students had a full scholarship and the admission process was passed by about 15% of applicants.
The school established cooperation programs with a number of foreign universities in the Americas as well as in Europe. Those agreements sponsored important visitors to the school, such as Alberto O. Mendelzon, Jean-Raymond Abrial, Ugo Montanari, Carlo Ghezzi and Giorgio Ausiello, and enabled its students to attend graduate school at foreign universities.
The school had a remarkable European influence and was oriented towards theoretical aspects of computer science, such as typed lambda calculus, formal verification, and Martin-Löf's intuitionistic type theory.
Unfortunately, the school was never able to develop a relationship with local companies, which in an emergent economy like Argentina's is essential to be involved with more practical problems. Without financial support, ESLAI had to close down in September 1990 during the presidency of Carlos Menem.


== References ==
Arias, María Fernanda (January–May 2009). ""Política Informática y Educación: el caso de la Escuela Superior Latinoamericana de Informática (ESLAI)"" [Informatics Policy and Education: the case of the Latin American Superior School of Informatics (ESLAI)] (PDF) (in Spanish). Monterrey, Mexico: Instituto Tecnológico y de Estudios Superiores de Monterrey. Retrieved 21 June 2013. 
Baum, Gabriel; Martínez López, Pablo E.; Arroyo, Marcelo; Smaldone, Javier; Báez, Mariana; Dalponte, Mara; Yankelevich, Daniel (10 December 2003). ""Newsletter - Número 8"" [Newsletter - Number 8] (PDF) (in Spanish). Buenos Aires, Argentina: Sociedad Argentina de Informática. Retrieved 21 June 2013."
457,Line segment intersection,3194098,3109,"In computational geometry, the line segment intersection problem supplies a list of line segments in the Euclidean plane and asks whether any two of them intersect (cross).
Simple algorithms examine each pair of segments. However, if a large number of possibly intersecting segments are to be checked, this becomes increasingly inefficient since most pairs of segments are not close to one another in a typical input sequence. The most common, and more efficient, way to solve this problem for a high number of segments is to use a sweep line algorithm, where we imagine a line sliding across the line segments and we track which line segments it intersects at each point in time using a dynamic data structure based on binary search trees. The Shamos–Hoey algorithm applies this principle to solve the line segment intersection detection problem, as stated above, of determining whether or not a set of line segments has an intersection; the Bentley–Ottmann algorithm works by the same principle to list all intersections in logarithmic time per intersection.


== See also ==
Line-line intersection


== References ==


=== Inline citations ===


=== General references ===
Mark de Berg; Marc van Kreveld; Mark Overmars; and Otfried Schwarzkopf (2000). Computational Geometry (2nd ed.). Springer. ISBN 3-540-65620-0.  Chapter 2: Line Segment Intersection, pp. 19–44.
Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Introduction to Algorithms, Second Edition. MIT Press and McGraw-Hill, 1990. ISBN 0-262-03293-7. Section 33.2: Determining whether any pair of segments intersects, pp. 934–947.
J. L. Bentley and T. Ottmann., Algorithms for reporting and counting geometric intersections, IEEE Trans. Comput. C28 (1979), 643–647.


== External links ==
Intersections of Lines and Planes Algorithms and sample code by Dan Sunday
Robert Pless. Lecture 4 notes. Washington University in St. Louis, CS 506: Computational Geometry.
Line segment intersection in CGAL, the Computational Geometry Algorithms Library
""Line Segment Intersection"" lecture notes by Jeff Erickson.
Line-Line Intersection Method With C Code Sample Darel Rex Finley"
458,Robert Tomasulo,1938893,3107,"Robert Marco Tomasulo (October 31, 1934 – April 3, 2008) was a computer scientist, and the inventor of the Tomasulo Algorithm. Tomasulo was the recipient of the 1997 Eckert–Mauchly Award ""[f]or the ingenious Tomasulo's Algorithm, which enabled out-of-order execution processors to be implemented.""
Robert Tomasulo attended Regis High School in New York City. He graduated from Manhattan College and then earned an engineering degree from Syracuse University. In 1956 he joined IBM research. After nearly a decade gaining broad experience in a variety of technical and leadership roles, he transitioned to mainframe development, including the IBM System/360 Model 91 and its successors. Following his 25-year career with IBM, Bob worked on an incubator project at Storage Technology Corporation to develop the first CMOS-based mainframe system; co-founded NetFrame, a mid-80s startup to develop one of the earliest microprocessor-based server systems; and worked as a consultant on processor architecture and microarchitecture for Amdahl Consulting.
On January 30, 2008, Tomasulo spoke at the University of Michigan College of Engineering about his career and the history and development of out-of-order execution.


== Notes ==


== External links ==
Lecture, 2008
Description, computer.org"
459,Longuet-Higgins Prize,25395428,3102,"Hugh Christopher Longuet-Higgins  (April 11, 1923 – March 27, 2004) was both a theoretical chemist and a cognitive scientist.


== Education and early life ==
Longuet-Higgins was born on 11 April 1923 in Lenham, Kent, England. His father was Henry H. L. Longuet-Higgins and his mother was Albinia Cecil Bazeley. He was educated at The Pilgrims' School, Winchester, and Winchester College. In 1941, he won a scholarship to Balliol College, Oxford. He read chemistry, but also took Part I of a degree in Music. He was a Balliol organ scholar. As an undergraduate he proposed the correct structure of the chemical compound diborane (B2H6), which was then unknown because it turned out to be different from structures in contemporary chemical valence theory. This was published with his tutor, R. P. Bell. He completed a Doctor of Philosophy degree in 1947 at the University of Oxford under the supervision of Charles Coulson.


== Career and research ==
After his PhD, he did postdoctoral research at the University of Chicago and the University of Manchester. In 1952, he was appointed Professor of Theoretical Physics at King's College London, and in 1954 was appointed John Humphrey Plummer Professor of Theoretical Chemistry at the University of Cambridge, and a Fellow of Corpus Christi College, Cambridge. He became interested in the brain and the new field of artificial intelligence. As a consequence, in 1967, he made a major change in his career by moving to the University of Edinburgh to co-found the Department of Machine intelligence and perception, with Richard Gregory and Donald Michie.
He later moved to the experimental psychology department at Sussex University, Brighton, England. In 1981 he introduced the essential matrix to the computer vision community in a paper which also included the eight-point algorithm for the estimation of this matrix. He retired in 1988. At the time of his death, in 2004, he was Professor Emeritus at the University of Sussex. His work on developing computational models of music understanding was recognized in the nineties by the award of an Honorary Doctorate of Music by the University of Sheffield.
An example of Longuet-Higgins's writings, introducing the field of music cognition:

Longuet-Higgins et al (1994): —
You're browsing, let us imagine, in a music shop, and come across a box of faded pianola rolls. One of them bears an illegible title, and you unroll the first foot or two, to see if you can recognize the work from the pattern of holes in the paper. Are there four beats in the bar, or only three? Does the piece begin on the tonic, or some other note? Eventually you decide that the only way of finding out is to buy the roll, take it home, and play it on the pianola. Within seconds your ears have told you what your eyes were quite unable to make out—that you are now the proud possessor of a piano arrangement of ""Colonel Bogey"".


=== Honours and awards ===
Christopher Longuet-Higgins was elected a Fellow of the Royal Society in 1958, a Foreign Associate of the US Academy of Sciences in 1968 a Fellow of the Royal Society of Edinburgh (FRSE) in 1969, and a Fellow of the Royal Society of Arts (FRSA) in 1970. He was a Fellow of the International Academy of Quantum Molecular Science. He had honorary doctorates from the universities of Bristol, Essex, Sheffield, Sussex and York. Among his notable prizes were the Jasper Ridley prize in music from Balliol College, Oxford, the Harrison memorial prize from the Chemical Society, and the Naylor prize from the London Mathematical Society. He was a governor of the BBC from 1979 to 1984.
In 2005 the Longuet-Higgins Prize for ""Fundamental Contributions in Computer Vision that Have Withstood the Test of Time"" was created in his honor. The prize is awarded every year at the IEEE Computer Vision and Pattern Recognition Conference for up to two distinguished papers published at that same conference ten years earlier.


== Personal life ==
His younger brother is Michael S. Longuet-Higgins. Longuet-Higgins died on 27 March 2004, aged 80. Although he respected many of the features of the Church of England, he was an atheist.


== References =="
460,David P. Robbins Prize,48246244,3098,"The David P. Robbins Prize for papers reporting novel research in algebra, combinatorics, or discrete mathematics is awarded both by the American Mathematical Society (AMS) and by the Mathematical Association of America (MAA). The AMS award recognizes papers with a significant experimental component on a topic which is broadly accessible which provide a simple statement of the problem and clear exposition of the work. Papers eligible for the MAA award are judged on quality of research, clarity of exposition, and accessibility to undergraduates. Both awards consist of $5000 and are awarded once every three years. They are named in the honor of David P. Robbins and were established in 2005 by the members of his family.


== Winners (AMS Robbins Prize) ==
2016 : Manuel Kauers, Christoph Koutschan and Doron Zeilberger for their paper ""Proof of George Andrews's and David Robbins's q-TSPP conjecture”, Proceedings of the National Academy of Sciences (PNAS) 108(6) pp. 2196–2199.
2013 : Alexander Razborov for his paper ""On the minimal density of triangles in graphs”, Combinatorics, Probability and Computing 17(4):603–618, 2008.
2010 : Ileana Streinu for her paper ""Pseudo-triangulations, rigidity and motion planning”, Discrete & Computational Geometry 34(4):587–635, 2005.
2007 : Samuel P. Ferguson and Thomas C. Hales for their paper ""A proof of the Kepler conjecture,"" Annals of Mathematics, 162:1065–1185, 2005.


== Winners (MAA Robbins Prize) ==
2017 :Robert Hough(mathematician) for his paper ""Solution of the minimum modulus problem for covering systems"", Annals of Mathematics, 181: 361-382, 2015.
2014 : Frederick V. Henle and James M. Henle for their paper ""Squaring the plane”, The American Mathematical Monthly, 115:3–12, 2008.
2011 : Mike Paterson, Yuval Peres, Mikkel Thorup, Peter Winkler, and Uri Zwick for their papers ""Overhang”, The American Mathematical Monthly, 116:19–44, 2009, and ""Maximum Overhang”, The American Mathematical Monthly, 116:763–787 2009.
2008 : Neil Sloane for ""The on-line encyclopedia of integer sequences”, Notices of the American Mathematical Society, 50:912–915, 2003.


== References ==


== External links ==
AMS website
MAA website"
461,SQL window function,13568911,3088,"In the SQL database query language, window functions allow access to data in the records right before and after the current record. A window function defines a frame or window of rows with a given length around the current row, and performs a calculation across the set of data in the window.

      NAME |
------------
      Aaron| <-- Preceding (unbounded)
     Andrew|
     Amelia|
      James|
       Jill|
     Johnny| <-- 1st preceding row
    Michael| <-- Current row
       Nick| <-- 1st following row
    Ophelia|
       Zach| <-- Following (unbounded)

In the above table, the next query extracts for each row the values of a window with one preceding and one following row:

The result query contains the following values:

|     PREV |     NAME |     NEXT |
|----------|----------|----------|
|    (null)|     Aaron|    Andrew|
|     Aaron|    Andrew|    Amelia|
|    Andrew|    Amelia|     James|
|    Amelia|     James|      Jill|
|     James|      Jill|    Johnny|
|      Jill|    Johnny|   Michael|
|    Johnny|   Michael|      Nick|
|   Michael|      Nick|   Ophelia|
|      Nick|   Ophelia|      Zach|
|   Ophelia|      Zach|    (null)|


== References =="
462,Multiprocessor scheduling,3302845,3079,"In computer science, multiprocessor scheduling is an NP-hard optimization problem. The problem statement is: ""Given a set J of jobs where job ji has length li and a number of processors m, what is the minimum possible time required to schedule all jobs in J on m processors such that none overlap?"" The applications of this problem are numerous, but are, as suggested by the name of the problem, most strongly associated with the scheduling of computational tasks in a multiprocessor environment.
Multiprocessor schedulers have to schedule tasks which may or may not be dependent upon one another. For example, take the case of reading user credentials from console, then use it to authenticate, then if authentication is successful display some data on the console. Clearly one task is dependent upon another. This is a clear case of where some kind of ordering exists between the tasks. In fact it is clear that it can be modelled with partial ordering. Then, by definition, the set of tasks constitute a lattice structure.
The general multiprocessor scheduling problem is a generalization of the optimization version of the number partitioning problem, which considers the case of partitioning a set of numbers (jobs) into two equal sets (processors).


== Algorithms ==
A simple, often-used algorithm is the LPT algorithm (Longest Processing Time) which sorts the jobs by its processing time and then assigns them to the machine with the earliest end time so far. This algorithm achieves an upper bound of 4/3 - 1/(3m) OPT.


== See also ==
Job shop scheduling, a similar problem for scheduling jobs on machines. Some variants of multiprocessor scheduling and job shop scheduling are equivalent problems.


== References ==

A compendium of NP optimization problems. Editors: Pierluigi Crescenzi, and Viggo Kann [1]"
463,Native Girls Code,53383534,3071,"Native Girls Code (NGC) is a Seattle-based program that focuses on providing computer coding skills with grounding in traditional Indigenous knowledge for Native American girls aged 12–18 through workshops, coaching, teaching and role modeling. It is organized by the non-profit organization Na'ah Illahee Fund (Mother Earth in the Chinook language), in partnership with University of Washington Information School Digital Youth Lab and the Washington NASA Space Consortium, as a way to support and perpetuate traditional knowledge, build leadership of women and encourage greater participation of Native American students in STEM fields.
The program was designed specifically to give Native girls from tribes throughout the United States a place to develop a strong foundation in Native culture, Native science, and build the skills needed to use modern computer technologies, resulting in the creation of websites, online games and virtual worlds. Leaders hope Native Girls Code will enrich both the girls and their communities.


== Awards and grants ==
In 2016 NGC was awarded a grant through the City of Seattle's Technology Matching Fund, aimed at increasing digital equity among underrepresented Seattle citizens. Google has been a major funder of the program and Facebook has donated laptops and filming equipment to NGC.


== See also ==
Girls Who Code
Black Girls Code
I Look Like an Engineer


== References =="
464,International Conference on Language Resources and Evaluation,14947222,3068,"The International Conference on Language Resources and Evaluation is an international conference organised by the European Language Resources Association every other year (on even years) with the support of institutions and organisations involved in Natural language processing. The series of LREC conferences was launched in Granada in 1998.


== History of conferences ==
Forthcoming conference:
LREC 2018 Miyazaki (Japan)
Past conferences:
2016 Portorož (Slovenia)
2014 Reykjavík (Iceland)
2012 Istanbul (Turkey)
2010 Valletta (Malta)
2008 Marrakech (Morocco)
2006 Genoa (Italy)
2004 Lisbon (Portugal)
2002 Las Palmas (Spain)
2000 Athens (Greece)
1998 Granada (Spain)
The survey of the LREC conferences over the period 1998-2013 has been presented during the 2014 conference  in Reykjavik as a closing session. It appears that the number of papers and signatures is increasing over time. The average number of authors per paper is highier too. The percentage of new authors is between 68% and 78%. The distribution between male (65%) and female (35%) authors is stable over time. The most frequent technical term is ""annotation"", then comes ""part-of-speech"".


== The LRE Map ==
The LRE Map was introduced at LREC 2010 and is now a regular feature of the LREC submission process for both the conference papers and the workshop papers. At the submission stage, the authors are asked to provide some basic information about all the resources (in a broad sense, i.e. including tools, standards and evaluation packages), either used or created, described in their papers. All these descriptors are then gathered in a global matrix called the LRE Map. This feature has been extended to several other conferences.


== References ==


== External links ==
Conference website
European Language Resources Association web site"
465,Algebraic specification,4018181,3068,"Algebraic specification, is a software engineering technique for formally specifying system behavior. Algebraic specification seeks to systematically develop more efficient programs by:
formally defining types of data, and mathematical operations on those data types
abstracting implementation details, such as the size of representations (in memory) and the efficiency of obtaining outcome of computations
formalizing the computations and operations on data types
allowing for automation by formally restricting operations to this limited set of behaviors and data types.
An algebraic specification achieves these goals by defining one or more data types, and specifying a collection of functions that operate on those data types. These functions can be divided into two classes:
constructor functions: functions that create or initialize the data elements, or construct complex elements from simpler ones
additional functions: functions that operate on the data types, and are defined in terms of the constructor functions.


== Example ==
Consider a formal algebraic specification for the boolean data type.
One possible algebraic specification may provide two constructor functions for the data-element: a true constructor and a false constructor. Thus, a boolean data element could be declared, constructed, and initialized to a value. In this scenario, all other connective elements, such as XOR and AND, would be additional functions. Thus, a data element could be instantiated with either ""true"" or ""false"" value, and additional functions could be used to perform any operation on the data element.
Alternatively, the entire system of boolean data types could be specified using a different set of constructor functions: a false constructor and a not constructor. In that case, an additional function could be defined to yield the value ""true.""
The algebraic specification therefore describes all possible states of the data element, and all possible transitions between states.


== See also ==
Common Algebraic Specification Language
Donald Sannella
Formal specification


== Notes =="
466,Berlekamp–Zassenhaus algorithm,25744542,3063,"In mathematics, in particular in computational algebra, the Berlekamp–Zassenhaus algorithm is an algorithm for factoring polynomials over the integers, named after Elwyn Berlekamp and Hans Zassenhaus. As a consequence of Gauss's lemma, this amounts to solving the problem also over the rationals.
The algorithm starts by finding factorizations over suitable finite fields using Hensel's lemma to lift the solution from modulo a prime p to a convenient power of p. After this the right factors are found as a subset of these. The worst case of this algorithm is exponential in the number of factors.
Van Hoeij (2002) improved this algorithm by using the LLL algorithm, substantially reducing the time needed to choose the right subsets of mod p factors.


== References ==
Berlekamp, E. R. (1967), ""Factoring polynomials over finite fields"" (PDF), Bell System Technical Journal, 46: 1853–1859, doi:10.1002/j.1538-7305.1967.tb03174.x, MR 0219231 .
Berlekamp, E. R. (1970), ""Factoring polynomials over large finite fields"", Mathematics of Computation, 24: 713–735, doi:10.2307/2004849, JSTOR 2004849, MR 0276200 .
Cantor, David G.; Zassenhaus, Hans (1981), ""A new algorithm for factoring polynomials over finite fields"", Mathematics of Computation, 36 (154): 587–592, doi:10.2307/2007663, JSTOR 2007663, MR 0606517 .
Geddes, K. O.; Czapor, S. R.; Labahn, G. (1992), Algorithms for computer algebra, Boston, MA: Kluwer Academic Publishers, doi:10.1007/b102438, ISBN 0-7923-9259-0, MR 1256483 .
Van Hoeij, Mark (2002), ""Factoring polynomials and the knapsack problem"", Journal of Number Theory, 95 (2): 167–189, doi:10.1016/S0022-314X(01)92763-5, MR 1924096 .
Zassenhaus, Hans (1969), ""On Hensel factorization. I"", Journal of Number Theory, 1: 291–311, doi:10.1016/0022-314X(69)90047-X, MR 0242793 .


== External links ==
Domazet, Haris. ""Berlekamp-Zassenhaus Algorithm"". MathWorld. 


== See also ==
Berlekamp's algorithm"
467,CHREST,12632281,3059,"Joe Chrest is an American academic and actor. He has had roles in numerous films and television shows including 21 Jump Street, 22 Jump Street, Oldboy, Lee Daniels' The Butler, and as Ted Wheeler in Stranger Things.


== Early life ==
Chrest was born and raised in St. Albans, West Virginia, where he attended St. Albans High School.


== Personal life ==
Chrest is also a professor at LSU School of Music.


== References ==


== External links ==
Joe Chrest on IMDB"
468,Extended Semantic Web Conference,39184738,3056,"The Extended Semantic Web Conference (abbreviated as ESWC), formerly known as the European Semantic Web Conference, is a yearly international academic conference on the topic of the Semantic Web. The event began in 2004 as the European Semantic Web Symposium. The goal of the event is ""to bring together researchers and practitioners dealing with different aspects of semantics on the Web"".
Topics covered at the conference include linked data, machine learning, natural language processing and information retrieval, ontologies, reasoning, semantic data management, services, processes, and cloud computing, social Web and Web science, in-use and industrial, digital libraries and cultural heritage, and e-government.


== List of conferences ==
Past and future ESWC conferences include:


== References ==


== External links ==
ESWC website"
469,Single user mode,8077602,3042,"Single user mode is a mode in which a multiuser computer operating system boots into a single superuser. It is mainly used for maintenance of multi-user environments such as network servers. Some tasks may require exclusive access to shared resources, for example running fsck on a network share. This mode can also be used for security purposes - network services are not run, eliminating the possibility of outside interference. On some systems a lost superuser password can be changed by switching to single user mode, but not asking for the password in such circumstances is viewed as a security vulnerability.


== Unix family ==
Unix-like operating systems provide single user mode functionality either through the System V-style runlevels, BSD-style boot-loader options, or other boot-time options.
The runlevel is usually changed using the init command, runlevel 1 or S will boot into single user mode.
Boot-loader options can be changed during startup before the execution of the kernel. In FreeBSD and DragonFly BSD it can be changed before rebooting the system with the command nextboot -o ""-s"" -k kernel, and its bootloader offers the option on bootup to start in single user mode. In Solaris the command reboot -- -s will cause a reboot into single user mode.
macOS users can accomplish this by holding down ⌘ S after powering the system. The user may be required to enter a password set in the firmware. In OS X El Capitan and later releases of macOS, the mode can be reversed to single user mode with the command sudo launchctl reboot userspace -s in Terminal, and the system can be fully rebooted in single-user mode with the command sudo launchctl reboot system -s. Single User Mode is different from a Safe Mode boot in that the system goes directly to the console instead of starting up the core elements of macOS (items in /System/Library/, ignoring /Library/, ~/Library/, et al.). From there users are encouraged by a prompt to run fsck or other command line utilities as needed (or installed).


== Microsoft Windows ==
Microsoft Windows provides Recovery Console, Last Known Good Configuration, Safe Mode and recently Windows Recovery Environment as standard recovery means. Also, bootable BartPE-based third-party recovery discs are available.
Recovery Console and recovery discs are different from single user modes in other operating systems because they are independent of the maintained operating system. This works more like chrooting into other environment with other kernel in Linux.


== References ==
""What is a runlevel?"". Retrieved November 17, 2010."
470,Hardware reset,39658646,3033,"A hardware reset or hard reset of a computer system is a hardware operation that re-initializes the core hardware components of the system, thus ending all current software operations in the system. This is typically, but not always, followed by booting of the system into firmware that re-initializes the rest of the system, and restarts the operating system.
Hardware resets are an essential part of the power-on process, but may also be triggered without power cycling the system by direct user intervention via a physical reset button, watchdog timers, or by software intervention that, as its last action, activates the hardware reset line.
Holding down the power button is a common way to force shutdown a computer. On Macs, pressing Control-Command-Power quits macOS and restarts the computer.


== Hardware reset in 80x86 IBM PC ==
The 8086 microprocessors provide RESET pin that is used to do the hardware reset. When a HIGH is applied to the pin, the CPU immediately stops, and sets the major registers to these values:
The CPU uses the values of CS and IP registers to find the location of the next instruction to execute. Location of next instruction is calculated using this simple equation:
Location of next instruction = (CS<<4) + (IP)
This implies that after the hardware reset, the CPU will start execution at the physical address 0xFFFF0. In IBM PC compatible computers, This address maps to BIOS ROM. The memory word at 0xFFFF0 usually contains a JMP instruction that redirects the CPU to execute the initialization code of BIOS. This JMP instruction is absolutely the first instruction executed after the reset.


=== Hardware reset in later x86 CPUs ===
Later x86 processors reset the CS and IP registers similarly, refer to Reset vector.


== See also ==
Power-on reset
Power-on self test
Reset vector
Reboot (computing)


== References ==

2. https://www.hardreset99.com Reset any electronic device."
471,Semantic URL attack,3044870,3029,"In a semantic URL attack, a client manually adjusts the parameters of its request by maintaining the URL's syntax but altering its semantic meaning. This attack is primarily used against CGI driven websites.
A similar attack involving web browser cookies is commonly referred to as cookie poisoning.


== Example ==
Consider a web-based e-mail application where users can reset their password by answering the security question correctly, and allows the users to send the password to the e-mail address of their choosing. After they answer the security question correctly, the web page will arrive to the following web form where the users can enter their alternative e-mail address:

The receiving page, resetpassword.php, has all the information it needs to send the password to the new e-mail. The hidden variable username contains the value user001, which is the username of the e-mail account.
Because this web form is using the GET data method, when the user submits alternative@emailexample.com as the e-mail address where the user wants the password to be sent to, the user then arrives at the following URL:
http://semanticurlattackexample.com/resetpassword.php?username=user001&altemail=alternative%40emailexample.com
This URL appears in the location bar of the browser, so the user can identify the username and the e-mail address through the URL parameters. The user may decide to steal other people's (user002) e-mail address by visiting the following URL as an experiment:
http://semanticurlattackexample.com/resetpassword.php?username=user002&altemail=alternative%40emailexample.com
If the resetpassword.php accepts these values, it is vulnerable to a semantic URL attack. The new password of the user002 e-mail address will be generated and sent to alternative@emailexmaple.com which causes user002's e-mail account to be stolen.
One method of avoiding semantic URL attacks is by using session variables. However, session variables can be vulnerable to other types of attacks such as session hijacking and cross-site scripting.


== References ==


== See also ==
Query string"
472,MOSEK,31045557,3028,"MOSEK is a software package for the solution of linear, mixed-integer linear, quadratic, mixed-integer quadratic, quadratically constraint, conic and convex nonlinear mathematical optimization problems. The emphasis in MOSEK is on solving large scale sparse problems. Particularly the interior-point optimizer for linear, conic quadratic (a.k.a. Second-order cone programming) and semi-definite (aka. semidefinite programming) problems is very efficient. A special feature of the MOSEK interior-point optimizer is that it is based on the so-called homogeneous model which implies MOSEK can reliably detect a primal and/or dual infeasible status as documented in several published papers.
In addition to the interior-point optimizer MOSEK includes:
Primal and dual simplex optimizer for linear problems.
A primal network simplex optimizer for problems with special network structure.
Mixed-integer optimizer for linear, quadratic and conic quadratic problems.
MOSEK provides interfaces to the C, C#, Java and Python languages. Most major modeling systems are made compatible for MOSEK, examples are: AMPL, and GAMS. MOSEK can also be used from popular tools such as MATLAB, R (an outdated version of package Rmosek is available from the CRAN server, the up-to-date version is provided by Mosek ApS), CVX, and YALMIP.


== References =="
473,International Conference on Intelligent Tutoring Systems,56416028,3027,"The International Conference on Intelligent Tutoring Systems (ITS) is the oldest conference series in the field of intelligent educational systems. It was established in 1988 by Claude Frasson. The first several events were held every four years and, while engaging a broad international audience, were always located in Montreal, Canada. The growth of the field encouraged the transformation into a biannual internationally-organized event, which was originally held every other year, opposite the International Conference on Artificial Intelligence in Education. Since 1992, the Proceedings of the conference are being published by Springer in the popular Lecture Notes in Computer Science series.
The conference was hosted in Montreal by Claude Frasson and Gilles Gauthier in 1988, 1992, 1996, and 2000; in San Antonio (US) by Carol Redfield and Valerie Shute in 1998; in Biarritz (France) and San Sebastian (Spain) by Guy Gouardères and Stefano Cerri in 2002; in Maceio (Brazil) by Rosa Maria Vicari and Fábio Paraguaçu in 2004; in Jhongli (Taiwan) by Tak-Wai Chan in 2006. The conference was back to Montreal in 2008 (for its 20th anniversary) and hosted by Roger Nkambou and Susanne Lajoie. ITS 2010 was held in Pittsburgh (US), hosted by Jack Mostow, Judy Kay, and Vincent Aleven. ITS 2012 was held in Chania (Crete), hosted by George Papadourakis, Stefano Cerri and William Clancey. ITS 2014 was held in Honolulu (Hawaii, US), hosted by Martha Crosby, Stefan Trausan-Matu, and Kristy Elizabeth Boyer. In 2018 the ITS 2018 conference is once again back to Montreal, hosted by Roger Nkambou, for the 30th anniversary of the series.


== References ==


== External links ==
The 11th International Conference on Intelligent Tutoring Systems – Co-adaptation in Learning – Chania (2012)
The 10th International Conference on Intelligent Tutoring Systems – Bridges to Learning – Pittsburgh (2010)
The 9th International Conference on Intelligent Tutoring Systems - Intelligent Tutoring Systems: Past and Future – Montreal (2008)
The 8th International Conference on Intelligent Tutoring Systems (2006)"
474,Piece table,35516996,3025,"In computing, a piece table is a data structure typically used to represent a series of edits on a (potentially) read-only text document. An initial reference (or 'span') to the whole of the original file is created, with subsequent inserts and deletes being created as combinations of one, two, or three references to sections of either the original document or of the spans associated with earlier inserts.
Typically the text of the original document is held in one immutable block, and the text of each subsequent insert is stored in new immutable blocks. Because even deleted text is still included in the piece table, this makes multi-level or unlimited undo easier to implement with a piece table than with alternative data structures such as a gap buffer.
J Strother Moore invented the piece table.
Several text editors use an in-RAM piece table internally, including the highly influential Bravo, Abiword, Atom and Visual Studio Code.
The ""fast save"" feature in some versions of Microsoft Word uses a piece table for the on-disk file format.
The on-disk representation of text files in the Oberon System uses a piece chain technique that allows pieces of one document to point to text stored in some other document, similar to transclusion. 


== See also ==
Git


== References =="
475,Storage violation,5732673,3024,"In computing a storage violation is a hardware or software fault that occurs when a task attempts to access an area of computer storage which it is not permitted to access.


== Types of storage violation ==
Storage violation can, for instance, consist of reading from, writing to, or freeing storage not owned by the task. A common type of storage violation is known as a stack buffer overflow where a program attempts to exceed the limits set for its call stack. It can also refer to attempted modification of memory ""owned"" by another thread where there is incomplete (or no) memory protection.


== Avoidance of storage violations ==
Storage violations can occur in transaction systems such as CICS in circumstances where it is possible to write to storage not owned by the transaction; such violations can be reduced by enabling features such as storage protection and transaction isolation.


== Detection of storage violations ==
Storage violations can be difficult to detect as a program can often run for a period of time after the violation before it crashes. For example, a pointer to a freed area of memory can be retained and later reused causing an error. As a result, efforts focus on detecting violations as they occur, rather than later when the problem is observed.
In systems such as CICS, storage violations are sometimes detected (by the CICS kernel) by the use of ""signatures"", which can be tested to see if they have been overlaid.
An alternative runtime library may be used to better detect storage violations, at the cost of additional overhead. Some programming languages use software bounds checking to prevent these occurrences.
Some program debugging software will also detect violations during testing.


== Common causes ==
A runaway subscript leading to illegal use of reference modification during run time.
Linkage layout mismatch between called and the calling elements.
Use of previously freed (and sometimes already re-allocated) memory.


=== Examples of software detecting storage violations ===
Intertest originally from Online Software International, later Computer Associates


== See also ==
Segmentation fault


== References ==

IBM. ""CICS Transaction Server for z/OS, Version 3 Release 2 Information Center"". IBM. Retrieved 2008-10-20. 
CICS problem determination Guide


== External links ==
https://plus.google.com/u/1/collection/wUwasB Marketing material for other product detecting storage violations"
476,Computational transportation science,26910524,3020,"Computational Transportation Science (CTS) is an emerging discipline that combines computer science and engineering with the modeling, planning, and economic aspects of transport. The discipline studies how to improve the safety, mobility, and sustainability of the transport system by taking advantage of information technologies and ubiquitous computing. A list of subjects encompassed by CTS can be found at include.
Computational Transportation Science is an emerging discipline going beyond vehicular technology, addressing pedestrian systems on hand-held devices but also issues such as transport data mining (or movement analysis), as well as data management aspects. CTS allows for an increasing flexibility of the system as local and autonomous negotiations between transport peers, partners and supporting infrastructure are allowed. Thus, CTS provides means to study localized computing, self-organization, cooperation and simulation of transport systems.
Several academic conferences on CTS have been held up to date:
The Fourth ACM SIGSPATIAL International Workshop on Computational Transportation Science
The Third ACM SIGSPATIAL International Workshop on Computational Transportation Science
Dagstuhl Seminar 10121 on Computational Transportation Science
The Second International Workshop on Computational Transportation Science
The First International Workshop on Computational Transportation Science
There is also an IGERT PHD program on Computational Transportation Science at the University of Illinois at Chicago.


== References ==


== External links ==
Computational Transportation Science"
477,Brace matching,16194435,3019,"Brace matching, also known as bracket matching or parentheses matching, is a syntax highlighting feature of certain text editors and integrated development environments that highlights matching sets of braces (square brackets, curly brackets, or parentheses) in languages such as Java and C++ that use them. The purpose is to help the programmer navigate through the code and also spot any improper matching, which would cause the program to not compile or malfunction. If a closing brace is left out, for instance, the compiler will not know that the end of a block of code has been reached. Brace matching is particularly useful when many nested if statements, program loops, etc. are involved.


== Implementations ==
Vim's % command does brace matching, and NetBeans has brace matching built-in. Brace matching can also be a tool for code navigation. In Visual Studio C++ 6.0, brace matching behavior was set to ignore braces found in comments. In VSC 7.0, its behavior was changed to compute commented braces. IntelliJ IDEA's Ruby on Rails plugin also enables braces matching. It has been proposed that Perl 5 be modified to facilitate braces matching. The Microsoft Excel 2003 formula bar has parentheses matching. Its implementation shows all the pairs of parentheses as different colors, so it is possible to easily analyze them all at once.


== Example ==
In this example, the user has just typed the closing curly brace '}' defining a code block, and that brace and its corresponding opening brace are both highlighted.

for (int i = 0; i < 10; i++)
{
    System.out.println(i);
}│


== References =="
478,Journal of Computational Geometry,34408527,3013,"The Journal of Computational Geometry is an open access mathematics journal that was established in 2010. It covers research in all aspects of computational geometry. All its papers are published free of charge to both authors and readers, and are made freely available through a Creative Commons Attribution license. The current editors-in-chief are Kenneth L. Clarkson and Günter Rote.
Along with its regularly contributed papers, the journal has since 2014 invited selected papers from the annual Symposium on Computational Geometry to a special issue.


== Abstracting and indexing ==
The Journal of Computational Geometry is abstracted and indexed in MathSciNet, Zentralblatt Math, and the Emerging Sources Citation Index.


== References ==


== External links ==
Official website"
479,Blahut–Arimoto algorithm,35573062,3000,"The Blahut–Arimoto algorithm, is often used to refer to a class of algorithms for computing numerically either the information theoretic capacity of a channel, or the rate-distortion function of a source. They are iterative algorithms that eventually converge to the optimal solution of the convex optimization problem that is associated with these information theoretic concepts.
For the case of channel capacity, the algorithm was independently invented by Suguru Arimoto and Richard Blahut. In the case of lossy compression, the corresponding algorithm was invented by Richard Blahut. The algorithm is most applicable to the case of arbitrary finite alphabet sources. Much work has been done to extend it to more general problem instances.


== Algorithm ==
Suppose we have a source 
  
    
      
        X
      
    
    {\displaystyle X}
   with probability 
  
    
      
        p
        (
        x
        )
      
    
    {\displaystyle p(x)}
   of any given symbol. We wish to find an encoding 
  
    
      
        p
        (
        
          
            
              x
              ^
            
          
        
        
          |
        
        x
        )
      
    
    {\displaystyle p({\hat {x}}|x)}
   that generates a compressed signal 
  
    
      
        
          
            
              X
              ^
            
          
        
      
    
    {\displaystyle {\hat {X}}}
   from the original signal while minimizing the expected distortion 
  
    
      
        ⟨
        d
        (
        x
        ,
        
          
            
              x
              ^
            
          
        
        )
        ⟩
      
    
    {\displaystyle \langle d(x,{\hat {x}})\rangle }
  , where the expectation is taken over the joint probability of 
  
    
      
        X
      
    
    {\displaystyle X}
   and 
  
    
      
        
          
            
              X
              ^
            
          
        
      
    
    {\displaystyle {\hat {X}}}
  . We can find an encoding that minimizes the rate-distortion functional locally by repeating the following iteration until convergence:

  
    
      
        
          p
          
            t
            +
            1
          
        
        (
        
          
            
              x
              ^
            
          
        
        )
        =
        
          ∑
          
            x
          
        
        p
        (
        x
        )
        
          p
          
            t
          
        
        (
        
          
            
              x
              ^
            
          
        
        
          |
        
        x
        )
      
    
    {\displaystyle p_{t+1}({\hat {x}})=\sum _{x}p(x)p_{t}({\hat {x}}|x)}
  

  
    
      
        
          p
          
            t
            +
            1
          
        
        (
        
          
            
              x
              ^
            
          
        
        
          |
        
        x
        )
        =
        
          
            
              
                p
                
                  t
                
              
              (
              
                
                  
                    x
                    ^
                  
                
              
              )
              exp
              ⁡
              (
              −
              β
              d
              (
              x
              ,
              
                
                  
                    x
                    ^
                  
                
              
              )
              )
            
            
              
                ∑
                
                  
                    
                      x
                      ^
                    
                  
                
              
              
                p
                
                  t
                
              
              (
              
                
                  
                    x
                    ^
                  
                
              
              )
              exp
              ⁡
              (
              −
              β
              d
              (
              x
              ,
              
                
                  
                    x
                    ^
                  
                
              
              )
              )
            
          
        
      
    
    {\displaystyle p_{t+1}({\hat {x}}|x)={\frac {p_{t}({\hat {x}})\exp(-\beta d(x,{\hat {x}}))}{\sum _{\hat {x}}p_{t}({\hat {x}})\exp(-\beta d(x,{\hat {x}}))}}}
  
where 
  
    
      
        β
      
    
    {\displaystyle \beta }
   is a parameter related to the slope in the rate-distortion curve that we are targeting and thus is related to how much we favor compression versus distortion (higher 
  
    
      
        β
      
    
    {\displaystyle \beta }
   means less compression).


== References =="
480,Gerard Salton Award,1981660,2999,"The Gerard Salton Award is presented by the Association for Computing Machinery (ACM) Special Interest Group on Information Retrieval (SIGIR) every three years to an individual who has made ""significant, sustained and continuing contributions to research in information retrieval"". SIGIR also co-sponsors (with SIGWEB) the Vannevar Bush Award, for the best paper at the Joint Conference on Digital Libraries.


== Chronological honorees and lectures ==
Source: SIGIR
1983 - Gerard Salton, Cornell University : ""About the future of automatic information retrieval.""
1988 - Karen Spärck Jones, University of Cambridge : ""A look back and a look forward.""
1991 - Cyril Cleverdon, Cranfield Institute of Technology : ""The significance of the Cranfield tests on index languages.""
1994 - William S. Cooper, University of California, Berkeley : ""The formalism of probability theory in IR: a foundation or an encumbrance?""
1997 - Tefko Saracevic, Rutgers University : ""Users lost (summary): reflections on the past, future, and limits of information science.""
2000 - Stephen E. Robertson, City University London : ""On theoretical argument in information retrieval.""For ... ""Thirty years of significant, sustained and continuing contributions to research in information retrieval. Of special importance are the theoretical and empirical contributions to the development, refinement, and evaluation of probabilistic models of information retrieval.""
2003 - W. Bruce Croft, University of Massachusetts Amherst : ""Information retrieval and computer science: an evolving relationship.""For ... ""More than twenty years of significant, sustained and continuing contributions to research in information retrieval. His contributions to the theoretical development and practical use of Bayesian inference networks and language modelling for retrieval, and to their evaluation through extensive experiment and application, are particularly important. The Center for Intelligent Information Retrieval which he founded illustrates the strong synergies between fundamental research and its application to a wide range of practical information management problems.""
2006 - C. J. van Rijsbergen, University of Glasgow : ""Quantum haystacks.""
2009 - Susan Dumais, Microsoft Research : ""An Interdisciplinary Perspective on Information Retrieval.""
2012 - Norbert Fuhr, University of Duisburg-Essen: ""Information Retrieval as Engineering Science.""
2015 - Nicholas J. Belkin, Rutgers University : “People, Interacting with Information”


== External links ==
ACM SIGIR homepage
ACM SIGIR awards"
481,Irish Computer Society,23002197,2977,"Irish Computer Society was founded in 1967 as the professional representing information and communication technology professionals in Ireland.
Its objective is to promote professional information and computer skills. The ICS is a member of the Council of European Professional Informatics Societies (CEPIS). In 1997 the ICS founded European Computer Driving Licence (Ireland).
The Irish Computer Society host seminars, workshops and conferences on current topics of interest and also conduct surveys of trends in the industry. National events include the National Data Protection Conference, the Public Sector IT Conference, the Leaders Conference, and in 2014 they ran the first Tech Week Ireland with 42,000 people taking part all over Ireland.
The ICS has also made representations and submissions to government committees and forums (for example of e-voting), where ICT knowledge is beneficial and on areas of concern for ICT professionals.
The ICS is a nomination body for the Industrial and Commercial Panel for Seanad Éireann. The ICS promotes ICT skills in schools by information campaigns and sponsoring competitions.


== Membership ==
The ICS has different grades of membership dependent on the qualifications.
Student
Affiliate
Associate
Member
Fellow


== External links ==
ICS - Official Home Page
European Computer Driving Licence - ICS Skills Online


== References =="
482,MailSlot,2667574,2975,"A Mailslot is a one-way interprocess communication mechanism, available on the Microsoft Windows operating system, that allows communication between processes both locally and over a network. The use of Mailslots is generally simpler than named pipes or sockets when a relatively small number of relatively short messages are expected to be transmitted, such as for example infrequent state-change messages, or as part of a peer-discovery protocol. The Mailslot mechanism allows for short message broadcasts (""datagrams"") to all listening computers across a given network domain.


== Features ==
Mailslots function as a server-client interface. A server can create a Mailslot, and a client can write to it by name. Only the server can read the mailslot, as such mailslots represent a one-way communication mechanism. A server-client interface could consist of two processes communicating locally or across a network. Mailslots operate over the RPC protocol and work across all computers in the same network domain. Mailslots offer no confirmation that a message has been received. Mailslots are generally a good choice when one client process must broadcast a message to multiple server processes.


== Implementations ==
The most widely known implementation of the Mailslot IPC mechanism is the Windows Messenger service that is part of the Windows NT-line of products, including Windows XP. The Messenger Service, not to be confused with the MSN Messenger internet chat service, is essentially a Mailslot server that waits for a message to arrive. When a message arrives it is displayed in a popup onscreen. The NET SEND command is therefore a type of Mailslot client, because it writes to specified mailslots on a network.
A number of programs also use Mailslots to communicate. Generally these are amateur chat clients and other such programs. Commercial programs usually prefer pipes or sockets.
Examples of Mailslots include:
MAILSLOT\Messngr - Microsoft NET SEND Protocol
MAILSLOT\Browse - Microsoft Browser Protocol
MAILSLOT\Alerter
MAILSLOT\53cb31a0\UnimodemNotifyTSP
MAILSLOT\HydraLsServer - Microsoft Terminal Services Licensing
MAILSLOT\CheyenneDS - CA BrightStor Discovery Service


== External links ==
Mailslots (MSDN Documentation)
Using Mailslots for Interprocess Communication
Using a Mailslot to read/write data over a network"
483,HackNY,33005784,2975,"hackNY is a New York City-based initiative seeking ""to create and empower a community of student-technologists."" hackNYwas created in 2010 by Evan Korth, Hilary Mason, Chris Wiggins. The organization is organized by faculty at both New York University and Columbia University. hackNY holds semiannual, 24-hour ""hackathons,"" and runs a summer Fellows Program which pairs students in the computational sciences with local startup companies.
Each semester, hackNY holds a 24-hour coding marathon, or ""hackathon,"" often at New York University's Courant Institute of Mathematical Sciences, but in Spring 2013 at Columbia's Engineering School. The events are open to all students, not just of Columbia and NYU. The hackathons begin with brief presentations from startups that make their APIs available to the participants. The students are encouraged, though not required, to work together to come up with creative solutions or build new features before the time runs out. The hackathons conclude with a presentation of the students' work, and judging.
hackNY organizes a summer program that pairs quantitative and computational students with startup companies. The students are compensated, receive free housing, and attend a lecture series showing them what it's like to join, or found, a startup company.


== References ==


== External links ==
www.hackny.org
Computer geeks king in job hunt Video on role of hackNY in job market. CNNMoney, Jan 5, 2012."
484,Maurice Wilkes Award,54216361,2948,"The Association for Computing Machinery SIGARCH Maurice Wilkes Award is given annually for outstanding contribution to computer architecture within the last 20 years. The award is named after Maurice Wilkes, a computer scientist credited with several important developments in computing such as microprogramming. The award is presented at the International Symposium on Computer Architecture. Prior recipients include:
2017 – Lieven Eeckhout
2016 – Timothy Sherwood
2015 – Christos Kozyrakis
2014 – Ravi Rajwar
2013 – Parthasarathy (Partha) Ranganathan
2012 – David Brooks
2011 – Kevin Skadron
2010 – Andreas Moshovos
2009 – Shubu Mukherjee
2008 – Sarita Adve
2007 – Todd Austin
2006 – Doug Burger
2005 – Steve Scott
2004 – Kourosh Gharachorloo
2003 – Dirk Meyer
2002 – Glenn Hinton
2001 – Anant Agarwal
2000 – William J. Dally
1999 – Gurindar S. Sohi
1998 – Wen-mei Hwu


== See also ==

ACM Special Interest Group on Computer Architecture
Computer engineering
Computer science
Computing


== References ==


== External links ==
Official page"
485,Online optimization,49914674,2945,"Online optimization is a field of optimization theory, more popular in computer science and operations research, that deals with optimization problems having no or incomplete knowledge of the future (online). These kind of problems are denoted as online problems and are seen as opposed to the classical optimization problems where complete information is assumed (offline). The research on online optimization can be distinguished into online problems where multiple decisions are made sequentially based on a piece-by-piece input and those where a decision is made only once. A famous online problem where a decision is made only once is the Ski rental problem. In general, the output of an online algorithm is compared to the solution of a corresponding offline algorithm which is necessarily always optimal and knows the entire input in advance (competitive analysis).
In many situations, present decisions (for example, resources allocation) must be made with incomplete knowledge of the future or distributional assumptions on the future are not reliable. In such cases, online optimization can be used, which is different from other approaches such as robust optimization, stochastic optimization and Markov decision processes.


== Online problems ==
A problem exemplifying the concepts of online algorithms is the Canadian traveller problem. The goal of this problem is to minimize the cost of reaching a target in a weighted graph where some of the edges are unreliable and may have been removed from the graph. However, that an edge has been removed (failed) is only revealed to the traveller when she/he reaches one of the edge's endpoints. The worst case for this problem is simply that all of the unreliable edges fail and the problem reduces to the usual shortest path problem. An alternative analysis of the problem can be made with the help of competitive analysis. For this method of analysis, the offline algorithm knows in advance which edges will fail and the goal is to minimize the ratio between the online and offline algorithms' performance. This problem is PSPACE-complete.
There are many formal problems that offer more than one online algorithm as solution:
K-server problem
Job shop scheduling problem
List update problem
Bandit problem
Secretary problem
Search games
Ski rental problem
Linear search problem
Portfolio selection problem


== References =="
486,Contract Net Protocol,26070855,2944,"Contract Net Protocol (CNP) is a task-sharing protocol in multi-agent systems, consisting of a collection of nodes or software agents that form the `contract net'. Each node on the network can, at different times or for different tasks, be a manager or a contractor.
When a node gets a `composite task' (or for any reason cannot solve the present task) it breaks the problem down into sub-tasks (if possible) and announces the sub-task to the contract net acting as a manager. Bids are then received from potential contractors and the winning contractor(s) are awarded the job(s).


== Description ==
Task distribution is viewed as a kind of contract negotiation and happens in five stages:
Recognition: An agent recognises it has a problem that it wants help with. The agent has a goal, and either realises it cannot achieve the goal in isolation (does not have the capability to fulfil the goal), or realises it would prefer not to achieve the goal in isolation (typically because of solution quality, deadline, etc.).
Announcement: The agent with the task sends out an announcement of the task which includes a specification of the task to be achieved. The specification must encode a description of the task itself, any constraints, and meta-task information.
Bidding: Agents that receive the announcement decide themselves whether they should bid for the task. Factors that are taken into consideration are that the agent must decide whether it is capable of the expecting task, and that the agent must determine the quality constraints and the price information (if relevant).
Awarding: Agents that send the task announcement must choose among the received bids and decide who to award the contract to. The result of this process is communicated to agents that submitted a bid.
Expediting: This may involve the generation of further contract nets in the form of sub-contracting to complete the task.
An example is in an electronic marketplace, a system in which buyers specify the goods that they want as well as a maximum price that they are willing to pay. The agent programs then would find other user(s) willing to sell the goods within the desired price range. The user with the lowest price would be selected to fulfill the contract. Other constraints could be applied such as delivery time and the location of the goods.


== References ==

The Contract Net Protocol, IEEE Transactions on Computers, Dec 1980. [1]


== See also ==
Multi-agent system
Intelligent agent
Agent-based model"
487,Graph state,4660507,2938,"In quantum computing, a graph state is a special type of multi-qubit state that can be represented by a graph. Each qubit is represented by a vertex of the graph, and there is an edge between every interacting pair of qubits. In particular, they are a convenient way of representing certain types of entangled states.
Graph states are useful in quantum error-correcting codes, entanglement measurement and purification and for characterization of computational resources in measurement based quantum computing models.


== Formal definition ==
Given a graph G = (V, E), with the set of vertices V and the set of edges E, the corresponding graph state is defined as

  
    
      
        
          
            |
            G
            ⟩
          
        
        =
        
          ∏
          
            (
            a
            ,
            b
            )
            ∈
            E
          
        
        
          U
          
            {
            a
            ,
            b
            }
          
        
        
          
            
              |
              +
              ⟩
            
          
          
            ⊗
            V
          
        
      
    
    {\displaystyle {\left|G\right\rangle }=\prod _{(a,b)\in E}U^{\{a,b\}}{\left|+\right\rangle }^{\otimes V}}
  
where the operator 
  
    
      
        
          U
          
            {
            a
            ,
            b
            }
          
        
      
    
    {\displaystyle U^{\{a,b\}}}
   is the controlled-Z interaction between the two vertices (qubits) a, b

  
    
      
        
          U
          
            {
            a
            ,
            b
            }
          
        
        =
        
          [
          
            
              
                
                  
                    1
                  
                
                
                  
                    0
                  
                
                
                  
                    0
                  
                
                
                  
                    0
                  
                
              
              
                
                  
                    0
                  
                
                
                  
                    1
                  
                
                
                  
                    0
                  
                
                
                  
                    0
                  
                
              
              
                
                  
                    0
                  
                
                
                  
                    0
                  
                
                
                  
                    1
                  
                
                
                  
                    0
                  
                
              
              
                
                  
                    0
                  
                
                
                  
                    0
                  
                
                
                  
                    0
                  
                
                
                  
                    −
                    1
                  
                
              
            
          
          ]
        
      
    
    {\displaystyle U^{\{a,b\}}=\left[{\begin{array}{cccc}{1}&{0}&{0}&{0}\\{0}&{1}&{0}&{0}\\{0}&{0}&{1}&{0}\\{0}&{0}&{0}&{-1}\end{array}}\right]}
  
And

  
    
      
        
          
            |
            +
            ⟩
          
        
        =
        
          
            
              
                
                  |
                  0
                  ⟩
                
              
              +
              
                
                  |
                  1
                  ⟩
                
              
            
            
              2
            
          
        
      
    
    {\displaystyle {\left|+\right\rangle }={\frac {{\left|0\right\rangle }+{\left|1\right\rangle }}{\sqrt {2}}}}
  


=== Alternative definition ===
An alternative and equivalent definition is the following.
Define an operator 
  
    
      
        
          K
          
            G
          
          
            (
            a
            )
          
        
      
    
    {\displaystyle K_{G}^{(a)}}
   for each vertex a of G:

  
    
      
        
          K
          
            G
          
          
            (
            a
            )
          
        
        =
        
          σ
          
            x
          
          
            (
            a
            )
          
        
        
          ∏
          
            b
            ∈
            N
            (
            a
            )
          
        
        
          σ
          
            z
          
          
            (
            b
            )
          
        
      
    
    {\displaystyle K_{G}^{(a)}=\sigma _{x}^{(a)}\prod _{b\in N(a)}\sigma _{z}^{(b)}}
  
where N(a) is the neighborhood of a (that is, the set of all b such that 
  
    
      
        (
        a
        ,
        b
        )
        ∈
        E
      
    
    {\displaystyle (a,b)\in E}
  ) and 
  
    
      
        
          σ
          
            x
            ,
            y
            ,
            z
          
        
      
    
    {\displaystyle \sigma _{x,y,z}}
   are the pauli matrices. The graph state 
  
    
      
        
          
            |
            G
            ⟩
          
        
      
    
    {\displaystyle {\left|G\right\rangle }}
   is then defined as the simultaneous eigenstate of the 
  
    
      
        N
        =
        
          |
          V
          |
        
      
    
    {\displaystyle N=\left|V\right|}
   operators 
  
    
      
        
          
            {
            
              K
              
                G
              
              
                (
                a
                )
              
            
            }
          
          
            a
            ∈
            V
          
        
      
    
    {\displaystyle \left\{K_{G}^{(a)}\right\}_{a\in V}}
   with eigenvalue 1:

  
    
      
        
          K
          
            G
          
          
            (
            a
            )
          
        
        
          
            |
            G
            ⟩
          
        
        =
        
          
            |
            G
            ⟩
          
        
      
    
    {\displaystyle K_{G}^{(a)}{\left|G\right\rangle }={\left|G\right\rangle }}
  


== See also ==
Entanglement
Cluster state


== References ==
M. Hein; J. Eisert; H. J. Briegel (2004). ""Multiparty entanglement in graph states"". Physical Review A. 69: 062311. doi:10.1103/PhysRevA.69.062311. 
S. Anders; H. J. Briegel (2006). ""Fast simulation of stabilizer circuits using a graph-state representation"". Physical Review A. 73: 022334. doi:10.1103/PhysRevA.73.022334. 
Graph states on arxiv.org"
488,Computer Russification,13477931,2906,"In computing, Russification involves the localization of computers and software, allowing the user interface of a computer and its software to communicate in the Russian language using Cyrillic script.
Problems associated with Russification before the advent of Unicode included the absence of a single character-encoding standard for Cyrillic (see Cyrillic script#Computer encoding).


== History of the MS-DOS russification ==
The first official Russification of MS-DOS was carried out for MS-DOS 4.01 in 1989/1990, released on April 9, 1990 (1990-04-09). In Microsoft, the Russification project manager and one of its main developers was Nikolai Lyubovny (Николай Любовный). A Russian version of MS-DOS 5.0 was also developed in 1991, released on August 9, 1991 (1991-08-09). Based on an initiative of Microsoft Germany in March 1991, derivates of the Russian MS-DOS 5.0 drivers used for keyboard, display and printer localization support (DISPLAY.SYS, EGA.CPI, EGA2.CPI, KEYB.COM, KEYBOARD.SYS, MSPRINT.SYS, COUNTRY.SYS, ALPHA.EXE) could also be purchased separately (with English messages) as part of Microsoft's AlphabetPlus kit. This enabled English issues of MS-DOS 3.3, 4.01 and 5.0 to be set up for Eastern European countries like Czechoslovakia, Poland, Hungary, Yugoslavia, Romania and Bulgaria.


== Russification of Microsoft Windows ==
A comprehensive instruction set for computer Russification is maintained by Paul Gorodyansky. It is mirrored in many places and recommended by the U.S. Library of Congress.


== See also ==
Cyrillization
GOST 10859
Romanization of Russian
АДОС, unrelated to Russian MS-DOS
PTS-DOS
Mojibake


== References ==


== External links ==
Modern Online (Virtual) Keyboard for Russian (not just alphabet order)
Online Keyboard for Russian
Virtual Russian Online Keyboard with Spellcheck"
489,Doo–Sabin subdivision surface,6584636,2895,"In computer graphics, Doo–Sabin subdivision surface is a type of subdivision surface based on a generalization of bi-quadratic uniform B-splines. It was developed in 1978 by Daniel Doo and Malcolm Sabin.
This process generates one new face at each original vertex, n new faces along each original edge, and n2 new faces at each original face. A primary characteristic of the Doo–Sabin subdivision method is the creation of four faces around every vertex. A drawback is that the faces created at the vertices are not necessarily coplanar.


== Evaluation ==
Doo–Sabin surfaces are defined recursively. Each refinement iteration replaces the current mesh with a smoother, more refined mesh, following the procedure described in. After many iterations, the surface will gradually converge onto a smooth limit surface. The figure below show the effect of two refinement iterations on a T-shaped quadrilateral mesh.

Just as for Catmull–Clark surfaces, Doo–Sabin limit surfaces can also be evaluated directly without any recursive refinement, by means of the technique of Jos Stam. The solution is, however, not as computationally efficient as for Catmull-Clark surfaces because the Doo–Sabin subdivision matrices are not in general diagonalizable.


== See also ==
Expansion - Equivalent geometric operation - truncates vertices and beveling edges.
Conway polyhedron notation - A set of related topological polyhedron and polygonal mesh operators.


== External links ==

Doo–Sabin surfaces"
490,With high probability,45454942,2881,"In mathematics, an event that occurs with high probability (often shortened to w.h.p. or WHP) is one whose probability depends on a certain number n and goes to 1 as n goes to infinity, i.e. it can be made as close as desired to 1 by making n big enough.


== Applications ==
The term WHP is especially used in computer science, in the analysis of probabilistic algorithms. For example, consider a certain probabilistic algorithm on a graph with n nodes. If the probability that the algorithm returns the correct answer is 
  
    
      
        1
        −
        1
        
          /
        
        n
      
    
    {\displaystyle 1-1/n}
  , then when the number of nodes is very large, the algorithm is correct with a probability that is very near 1. This fact is expressed shortly by saying that the algorithm is correct WHP.
Some algorithms in which this term is used are:
Miller–Rabin primality test: a probabilistic algorithm for testing whether a given number n is prime or composite. If n is composite, the test will detect n as composite WHP. There is a small chance that we are unlucky and the test will think that n is prime. But, the probability of error can be reduced indefinitely by running the test many times with different randomizations.
Freivalds' algorithm: a randomized algorithm for verifying matrix multiplication. It runs faster than deterministic algorithms WHP.
Treap: a randomized binary search tree. Its height is logarithmic WHP. Fusion tree is a related data structure.
Online codes: randomized codes which allow the user to recover the original message WHP.
BQP: a complexity class of problems for which there are polynomial-time quantum algorithms which are correct WHP. QMA and QIP are related complexity class.
Probably approximately correct learning: A process for machine-learning in which the learned function has low generalization-error WHP.
Gossip protocols: a communication protocol used in distributed systems to reliably deliver messages to the whole cluster using a constant amount of network resources on each node and ensuring no single point of failure.


== See also ==
randomized algorithm
almost surely


== References ==
Métivier, Y.; Robson, J. M.; Saheb-Djahromi, N.; Zemmari, A. (2010). ""An optimal bit complexity randomized distributed MIS algorithm"". Distributed Computing. 23 (5–6): 331. doi:10.1007/s00446-010-0121-5. 
""Principles of Distributed Computing (lecture 7)"" (PDF). ETH Zurich. Retrieved 21 February 2015."
491,Exponential tree,572903,2873,"An exponential tree is almost identical to a binary search tree, with the exception that the dimension of the tree is not the same at all levels. In a normal binary search tree, each node has a dimension (d) of 1, and has 2d children. In an exponential tree, the dimension equals the depth of the node, with the root node having a d = 1. So the second level can hold four nodes, the third can hold eight nodes, the fourth 16 nodes, and so on.


== Layout ==
""Exponential Tree"" can also refer to a method of laying out the nodes of a tree structure in n (typically 2) dimensional space. Nodes are placed closer to a baseline than their parent node, by a factor equal to the number of child nodes of that parent node (or by some sort of weighting), and scaled according to how close they are. Thus, no matter how ""deep"" the tree may be, there is always room for more nodes, and the geometry of a subtree is unrelated to its position in the whole tree. The whole has a fractal structure.
In fact, this method of laying out a tree can be viewed as an application of the upper half-plane model of hyperbolic geometry, with isometries limited to translations only.


== See also ==
Faster deterministic sorting and searching in linear space (Original paper from '95)
Laying out and Visualizing Large Trees Using a Hyperbolic Space
Implementation and Performance Analysis of Exponential Tree Sorting"
492,Simulation governance,55669746,2871,"Simulation governance is a managerial function concerned with assurance of reliability of information generated by numerical simulation. The term was introduced in 2011  and specific technical requirements were addressed from the perspective of mechanical design in 2012 . Its strategic importance was addressed in 2015 . At the 2017 NAFEMS World Congress in Stockholm simulation governance was identified as the first of eight “big issues” in numerical simulation.
Simulation governance is concerned with (a) selection and adoption of the best available simulation technology, (b) formulation of mathematical models, (c) management of experimental data, (d) data and solution verification procedures, and (e) revision of mathematical models in the light of new information collected from physical experiments and field observations .
A plan for simulation governance has to be tailored to fit the mission of each organization or department within an organization: If that mission is to apply established rules of design and certification then emphasis is on solution verification and standardization. If, on the other hand, that mission is to formulate design rules, or make condition-based maintenance decisions, then verification, validation and uncertainty quantification must be part of the plan.


== References =="
493,Knowledge Interchange Format,5558061,2871,"Knowledge Interchange Format (KIF) is a computer language designed to enable systems to share and re-use information from knowledge-based systems. KIF is similar to frame languages such as KL-One and LOOM but unlike such language its primary role is not intended as a framework for the expression or use of knowledge but rather for the interchange of knowledge between systems. The designers of KIF likened it to PostScript. PostScript was not designed primarily as a language to store and manipulate documents but rather as an interchange format for systems and devices to share documents. In the same way KIF is meant to facilitate sharing of knowledge across different systems that use different languages, formalisms, platforms, etc.
KIF has a declarative semantics. It is meant to describe facts about the world rather than processes or procedures. Knowledge can be described as objects, functions, relations, and rules. It is a formal language, i.e., it can express arbitrary statements in first order logic and can support reasoners that can prove the consistency of a set of KIF statements. KIF also supports non-monotonic reasoning. KIF was created by Michael Genesereth, Richard Fikes and others participating in the DARPA knowledge Sharing Effort.
Although the original KIF group intended to submit to a formal standards body, that did not occur. A later version called Common Logic has since been developed for submission to ISO and has been approved and published. A variant called SUO-KIF is the language in which the Suggested Upper Merged Ontology is written.


== See also ==
Knowledge Query and Manipulation Language


== References ==


== External links ==
Knowledge Interchange Format page at the Stanford AI Lab
Common Logic"
494,Discriminative model,12155912,2836,"Discriminative models, also called conditional models, are a class of models used in machine learning for modeling the dependence of unobserved (target) variables 
  
    
      
        y
      
    
    {\displaystyle y}
   on observed variables 
  
    
      
        x
      
    
    {\displaystyle x}
  . Within a probabilistic framework, this is done by modeling the conditional probability distribution 
  
    
      
        P
        (
        y
        
          |
        
        x
        )
      
    
    {\displaystyle P(y|x)}
  , which can be used for predicting 
  
    
      
        y
      
    
    {\displaystyle y}
   from 
  
    
      
        x
      
    
    {\displaystyle x}
  .
Discriminative models, as opposed to generative models, do not allow one to generate samples from the joint distribution of observed and target variables. However, for tasks such as classification and regression that do not require the joint distribution, discriminative models can yield superior performance (in part because they have fewer variables to compute). On the other hand, generative models are typically more flexible than discriminative models in expressing dependencies in complex learning tasks. In addition, most discriminative models are inherently supervised and cannot easily support unsupervised learning. Application-specific details ultimately dictate the suitability of selecting a discriminative versus generative model.


== Types ==
Examples of discriminative models used in machine learning include:
Logistic regression, a type of generalized linear regression used for predicting binary or categorical outputs (also known as maximum entropy classifiers)
Support vector machines
Boosting (meta-algorithm)
Conditional random fields
Linear regression
Neural networks
Random forests


== See also ==
Generative model


== References =="
495,Alewife (multiprocessor),15484221,2836,"Alewife was a cache coherent multiprocessor developed in the early 1990s by a group led by Anant Agarwal at the Massachusetts Institute of Technology. It was based on a network of up to 512 processing nodes, each of which used the Sparcle computer architecture, which was formed by modifying a Sun Microsystems SPARC CPU to include the APRIL techniques for fast context switches.
The Alewife project was one of two predecessors cited by the creators of the popular Beowulf cluster multiprocessor.


== References ==


== External links ==
MIT Alewife Project"
496,Return code,30873244,2832,"In computer programming, a return code or an error code is an enumerated message that corresponds to the status of a specific software application. They are typically used to identify faults, such as those in faulty hardware, software, or incorrect user input.


== In systems software ==
Exit status is the return code of computer programs running as processes, and is communicated through system calls.


== In networking software ==
Network protocols typically support returning status codes. In the TCP/IP stack, it's a common feature of higher level protocols. For example:
List of HTTP status codes
List of FTP server return codes
Simple Mail Transfer Protocol#Protocol overview


== Error codes and exception handling ==
Error codes are used in various methods of solving the semipredicate problem. They are generally similar to exception handling in programming languages that support it.
Error codes are slowly disappearing from the programmer's environment as modern object-oriented programming languages replace them with exceptions. Exceptions have the advantage of being handled with explicit blocks of code, separate from the rest of the code. While it is considered poor practice in methodologies that use error codes and return codes to indicate failure, programmers often neglect to check return values for error conditions. That negligence can cause undesirable effects, as ignored error conditions often cause more severe problems later in the program. Exceptions are implemented in such a way as to separate the error handling code from the rest of the code. Separating the error handling code from the normal logic makes programs easier to write and understand, since one block of error handling code can service errors from any number of function calls. Exception handling also makes the code more readable than implementations with error codes, since exception handling does not disrupt the flow of the code with frequent checks for error conditions.


== See also ==
errno.h, a header file in C that defines macros for reporting errors
Abort (computing)
Aspect-oriented programming
Failure
Static code analysis


== External links ==
Lists of Linux errno values, both numeric and symbolic
Microsoft system error codes
Microsoft Device Manager error codes"
497,Supnick matrix,10573340,2815,"A Supnick matrix or Supnick array – named after Fred Supnick of the City College of New York, who introduced the notion in 1957 – is a Monge array which is also a symmetric matrix.


== Mathematical definition ==
A Supnick matrix is a square Monge array that is symmetric around the main diagonal.
An n-by-n matrix is a Supnick matrix if, for all i, j, k, l such that if

  
    
      
        1
        ≤
        i
        <
        k
        ≤
        n
      
    
    {\displaystyle 1\leq i<k\leq n}
   and 
  
    
      
        1
        ≤
        j
        <
        l
        ≤
        n
      
    
    {\displaystyle 1\leq j<l\leq n}
  
then

  
    
      
        
          a
          
            i
            j
          
        
        +
        
          a
          
            k
            l
          
        
        ≤
        
          a
          
            i
            l
          
        
        +
        
          a
          
            k
            j
          
        
        
      
    
    {\displaystyle a_{ij}+a_{kl}\leq a_{il}+a_{kj}\,}
  
and also

  
    
      
        
          a
          
            i
            j
          
        
        =
        
          a
          
            j
            i
          
        
        .
        
      
    
    {\displaystyle a_{ij}=a_{ji}.\,}
  
A logically equivalent definition is given by Rudolf & Woeginger who in 1995 proved that
A matrix is a Supnick matrix iff it can be written as the sum of a sum matrix S and a non-negative linear combination of LL-UR block matrices.
The sum matrix is defined in terms of a sequence of n real numbers {αi}:

  
    
      
        S
        =
        [
        
          s
          
            i
            j
          
        
        ]
        =
        [
        
          α
          
            i
          
        
        +
        
          α
          
            j
          
        
        ]
        ;
        
      
    
    {\displaystyle S=[s_{ij}]=[\alpha _{i}+\alpha _{j}];\,}
  
and an LL-UR block matrix consists of two symmetrically placed rectangles in the lower-left and upper right corners for which aij = 1, with all the rest of the matrix elements equal to zero.


== Properties ==
Adding two Supnick matrices together will result in a new Supnick matrix (Deineko and Woeginger 2006).
Multiplying a Supnick matrix by a non-negative real number produces a new Supnick matrix (Deineko and Woeginger 2006).
If the distance matrix in a traveling salesman problem can be written as a Supnick matrix, that particular instance of the problem admits an easy solution (even though the problem is, in general, NP hard).


== References ==
Supnick, Fred (July 1957). ""Extreme Hamiltonian Lines"". Annals of Mathematics. Second Series. 66 (1): 179–201. JSTOR 1970124. 
Woeginger, Gerhard J. (June 2003). ""Computational Problems without Computation"" (PDF). Nieuwarchief. 5 (4): 140–147. 
Deineko, Vladimir G.; Woeginger, Gerhard J. (October 2006). ""Some problems around travelling salesmen, dart boards, and euro-coins"" (PDF). Bulletin of the European Association for Theoretical Computer Science. EATCS. 90: 43–52. ISSN 0252-9742."
498,Computational logic,8619255,2813,"Computational logic is the use of logic to perform or reason about computation. It bears a similar relationship to computer science and engineering as mathematical logic bears to mathematics and as philosophical logic bears to philosophy. It is synonymous with ""logic in computer science"".
The term “Computational Logic” came to prominence with the founding of the ACM Transactions on Computational Logic. However, the term was apparently introduced by J.A. Robinson in a 1970 paper in the Proceedings of the Sixth Annual Machine Intelligence Workshop, Edinburgh, 1970, entitled ""Computational Logic: The Unification Computation"" (Machine Intelligence 6:63-72, Edinburgh University Press, 1971). The expression is used in the second paragraph with a footnote claiming that *computational logic* (the emphasis is in the paper) is ""surely a better phrase than 'theorem proving', for the branch of artificial intelligence which deals with how to make machines do deduction efficiently"". This sounds like coining the term; no reference to a previous use is mentioned. In 1972 the Metamathematics Unit at the University of Edinburgh was renamed “The Department of Computational Logic” in the School of Artificial Intelligence. The term was then used by Robert S. Boyer and J Strother Moore, who worked in the Department in the early 1970s, to describe their work on program verification and automated reasoning. They also founded a company Computational Logic Inc. of the same name.
The term “Computational Logic” has also come to be associated with logic programming, because much of the early work in logic programming in the early 1970s also took place in the Department of Computational Logic in Edinburgh. It was reused in the early 1990s to describe work on extensions of logic programming in the EU Basic Research Project “Compulog” and in the associated Network of Excellence. Krzysztof Apt, who was the co-ordinator of the Basic Research Project Compulog-II, reused and generalized the term when he founded the ACM Transactions on Computational Logic in 2000 and became its first Editor-in-Chief.


== References ==


== Further reading ==
Dov M. Gabbay, Jörg H. Siekmann, John Woods, eds. (2014). Handbook of the History of Logic, Volume 9: Computational Logic. Elsevier. ISBN 978-0-08-093067-1. CS1 maint: Uses editors parameter (link)"
499,Seoul Accord,42971132,2812,"The Seoul Accord is an international accreditation agreement for professional computing and information technology academic degrees, between the bodies responsible for accreditation in its signatory countries. Established in 2008, the signatories as of 2016 are Australia, Canada, Chinese Taipei, Hong Kong China, Japan, Korea, the United Kingdom and the United States. Provisional signatories include Ireland, New Zealand and Philippines.
This agreement mutually recognizes tertiary level computing and IT qualifications between the signatory agencies. Graduates of accredited programs in any of the signatory countries are recognized by the other signatory countries as having met the academic requirements as IT professionals.


== Scope ==
The Seoul Accord covers tertiary undergraduate computing degrees. Engineering and Engineering Technology programs are not covered by the Seoul accord, although some Software engineering programs have dual accreditation with the Washington Accord.


== Signatories ==
The following are the signatory accreditation bodies of the Seoul Accord, their respective countries and territories, and years of admission:
Australia - (Australian Computer Society, 2008)
Canada - (Canadian Information Processing Society, 2008)
Chinese Taipei - (Institute of Engineering Education Taiwan, 2008)
Hong Kong China - (The Hong Kong Institution of Engineers, 2008)
Japan - (JABEE, 2008)
Korea - (Accreditation Board for Engineering Education of Korea, 2008)
United Kingdom - (British Computer Society, 2008)
United States - (ABET, 2008)
The following are Provisional Signatories of the Seoul Accord, along with their respective countries and territories and years of admission:
Ireland - (Engineers Ireland, 2015)
New Zealand - (Institute of IT Professionals, 2015)
Philippines - (The Philippine Information and Computing Accreditation Board, 2015]


== See also ==
Washington Accord - professional engineering degrees
Sydney Accord - engineering technologists
Dublin Accord - engineering technicians
Chartered Engineer
Outcome-based education
Professional Engineer


== References ==


== External links ==
Seoul Accord"
500,Connection string,11519324,2810,"In computing, a connection string is a string that specifies information about a data source and the means of connecting to it. It is passed in code to an underlying driver or provider in order to initiate the connection. Whilst commonly used for a database connection, the data source could also be a spreadsheet or text file.
The connection string may include attributes such as the name of the driver, server and database, as well as security information such as user name and password.


== Examples ==
This example shows a Postgres connection string for connecting to wikipedia.com with SSL and a connection timeout of 180 seconds:

DRIVER={PostgreSQL Unicode};SERVER=www.wikipedia.com;SSL=true;SSLMode=require;DATABASE=wiki;UID=wikiuser;Connect Timeout=180;PWD=ashiknoor

Users of Oracle databases can specify connection strings:
on the command-line (as in: sqlplus scott/tiger@connection_string )
via environment variables ($TWO_TASK in Unix-like environments; %LOCAL% in Microsoft Windows environments)
in local configuration files (such as the default $ORACLE_HOME/network/admin.tnsnames.ora)
in LDAP-capable directory services


== External links ==
ConnectionStrings.com


== References =="
501,Behavior authoring,51887861,2810,"Behavior authoring is a technique that is widely used in crowd simulations and in simulations and computer games that involve multiple autonomous or non-player characters (NPCs). There has been growing academic and industry interest in the behavioral animation of autonomous actors in virtual worlds. However, it remains a considerable challenge to author complicated interactions between multiple actors in a way that balances automation and control flexibility.
Several varieties of behavior authoring systems have been created.


== The BML Sequencer and Smartbody ==
Behavior Markup Language (BML) is a tool for describing autonomous actor behavior in simulations and computer games. SmartBody is a framework for animation of artificial intelligence conversation agents to provide a more lifelike simulation. Combining both of these concepts, the BML sequencer is a tool to allow artists to create SmartBody compliant BML animation sequences for multiple virtual humans. SmartBody allows for complex behavior realization, synchronizing speech recordings with non-verbal behaviors by using the Behavior Markup Language (BML). However, there remain two problems for using BML and SmartBody to achieve the vision that an artist has for animating the character: the authoring problem and multi-party behavior synchronization. The BML Sequencer addresses both.


== Behavior authoring in real-time strategy games ==
Behavior authoring for computer games consists of first writing the behaviors in a programming language, iteratively refining these behaviors, testing the revisions by executing them, identifying new problems and then refining the behaviors again.


== References =="
502,Program status word,3299290,2803,"The program status word (PSW) is an IBM System/360 architecture and successors control register which performs the function of a status register and program counter in other architectures, and more.
Although certain fields within the PSW may be tested or set by using non-privileged instructions, testing or setting the remaining fields may only be accomplished by using privileged instructions.
Contained within the PSW are the two bit condition code, representing zero, positive, negative, overflow, and similar flags of other architectures' status registers. Conditional branch instructions test this encoded as a four bit value, with each bit representing a test of one of the four condition code values, 23 + 22 + 21 + 20. (Since IBM uses big-endian bit numbering, mask value 8 selects code 0, mask value 4 selects code 1, mask value 2 selects code 2, and mask value 1 selects code 3.)
The 64-bit PSW describes (among other things)
Interrupt masks
Privilege states
Condition code
Instruction address
In the early instances of the architecture (System/360 and early System/370), the instruction address was 24 bits; in later instances (XA/370), the instruction address was 31 bits plus a mode bit (24 bit addressing mode if zero; 31 bit addressing mode if one) for a total of 32 bits.
In the present instances of the architecture (z/Architecture), the instruction address is 64 bits and the PSW itself is 128 bits.
The PSW may be loaded by the LOAD PSW instruction (LPSW or LPSWE). Its contents may be examined with the Extract PSW instruction (EPSW).


== References =="
503,Truncation selection,371724,2803,"In animal and plant breeding, truncation selection is a standard method in selective breeding in selecting animals to be bred for the next generation. Animals are ranked by their phenotypic value on some trait such as milk production, and the top percentage is reproduced. The effects of truncation selection for a continuous trait can be modeled by the standard breeder's equation by using heritability and Truncated normal distributions; on a binary trait, it can be modeled easily using the liability threshold model. It is considered an easy and efficient method of breeding.


== Computer science ==
In computer science, truncation selection is a selection method used in genetic algorithms to select potential candidate solutions for recombination modeled after the breeding method.
In truncation selection the candidate solutions are ordered by fitness, and some proportion, p, (e.g. p = 1/2, 1/3, etc.), of the fittest individuals are selected and reproduced 1/p times. Truncation selection is less sophisticated than many other selection methods, and is not often used in practice. It is used in Muhlenbein's Breeder Genetic Algorithm.


== References ==

""Chapter 14: Short-term Changes in the Mean: 2. Truncation and Threshold Selection""
Crow 2010, ""On epistasis: why it is unimportant in polygenic directional selection""
Visscher et al. 2008, ""Heritability in the genomics era - concepts and misconceptions""
Visscher 2016, ""Human Complex Trait Genetics in the 21st Century""
Weight & Harpending 2016, ""Some Uses of Models of Quantitative Genetic Selection in Social Science""
Frost & Harpending 2015, ""Western Europe, state formation, and genetic pacification"""
504,Thomas write rule,217343,2803,"In computer science, particularly the field of databases, the Thomas write rule is a rule in timestamp-based concurrency control. It can be summarized as ignore outdated writes.
It states that, if a more recent transaction has already written the value of an object, then a less recent transaction does not need perform its own write since it will eventually be overwritten by the more recent one.
The Thomas write rule is applied in situations where a predefined logical order is assigned to transactions when they start. For example a transaction might be assigned a monotonically increasing timestamp when it is created. The rule prevents changes in the order in which the transactions are executed from creating different outputs: The outputs will always be consistent with the predefined logical order.
For example consider a database with 3 variables (A, B, C), and two atomic operations C := A (T1), and C := B (T2). Each transaction involves a read (A or B), and a write (C). The only conflict between these transactions is the write on C. The following is one possible schedule for the operations of these transactions:

  
    
      
        
          
            [
            
              
                
                  
                    T
                    
                      1
                    
                  
                
                
                  
                    T
                    
                      2
                    
                  
                
              
              
                
                
                  R
                  e
                  a
                  d
                  (
                  A
                  )
                
              
              
                
                  R
                  e
                  a
                  d
                  (
                  B
                  )
                
                
              
              
                
                
                  W
                  r
                  i
                  t
                  e
                  (
                  C
                  )
                
              
              
                
                  W
                  r
                  i
                  t
                  e
                  (
                  C
                  )
                
                
              
              
                
                  C
                  o
                  m
                  m
                  i
                  t
                
                
              
              
                
                
                  C
                  o
                  m
                  m
                  i
                  t
                
              
            
            ]
          
        
        ⟺
        
          
            [
            
              
                
                  
                    T
                    
                      1
                    
                  
                
                
                  
                    T
                    
                      2
                    
                  
                
              
              
                
                
                  R
                  e
                  a
                  d
                  (
                  A
                  )
                
              
              
                
                  R
                  e
                  a
                  d
                  (
                  B
                  )
                
                
              
              
                
                
                  W
                  r
                  i
                  t
                  e
                  (
                  C
                  )
                
              
              
                
                
              
              
                
                  C
                  o
                  m
                  m
                  i
                  t
                
                
              
              
                
                
                  C
                  o
                  m
                  m
                  i
                  t
                
              
            
            ]
          
        
      
    
    {\displaystyle {\begin{bmatrix}T_{1}&T_{2}\\&Read(A)\\Read(B)&\\&Write(C)\\Write(C)&\\Commit&\\&Commit\end{bmatrix}}\Longleftrightarrow {\begin{bmatrix}T_{1}&T_{2}\\&Read(A)\\Read(B)&\\&Write(C)\\&\\Commit&\\&Commit\\\end{bmatrix}}}
  
If (when the transactions are created) T1 is assigned a timestamp that precedes T2 (i.e., according to the logical order, T1 comes first), then only T2's write should be visible. If, however, T1's write is executed after T2's write, then we need a way to detect this and discard the write.
One practical approach to this is to label each value with a write timestamp (WTS) that indicates the timestamp of the last transaction to modify the value. Enforcing the Thomas write rule only requires checking to see if the write timestamp of the object is greater than the time stamp of the transaction performing a write. If so, the write is discarded
In the example above, if we call TS(T) the timestamp of transaction T, and WTS(O) the write timestamp of object O, then T2's write sets WTS(C) to TS(T2). When T1 tries to write C, it sees that TS(T1) < WTS(C), and discards the write. If a third transaction T3 (with TS(T3) > TS(T2)) were to then write to C, it would get TS(T3) > WTS(C), and the write would be allowed.


== References ==
Robert H. Thomas (1979). ""A majority consensus approach to concurrency control for multiple copy databases"". ACM Transactions on Database Systems. 4 (2): 180–209. doi:10.1145/320071.320076. 

©"
505,Simple precedence grammar,2001331,2802,"A simple precedence grammar is a context-free formal grammar that can be parsed with a simple precedence parser. The concept was first developed by Niklaus Wirth and Helmut Weber from the ideas of Robert Floyd in their paper, EULER: a generalization of ALGOL, and its formal definition, in the Communications of the ACM in 1966.


== Formal definition ==
G = (N, Σ, P, S) is a simple precedence grammar if all the production rules in P comply with the following constraints:
There are no erasing rules (ε-productions)
There are no useless rules (unreachable symbols or unproductive rules)
For each pair of symbols X, Y (X, Y 
  
    
      
        ∈
      
    
    {\displaystyle \in }
   (N ∪ Σ)) there is only one Wirth–Weber precedence relation.
G is uniquely inversible


== Examples ==

  
    
      
        S
        →
        a
        S
        S
        b
        
          |
        
        c
      
    
    {\displaystyle S\to aSSb|c}
  
precedence table:


== Notes ==


== References ==
Alfred V. Aho, Jeffrey D. Ullman (1977). Principles of Compiler Design. 1st Edition. Addison–Wesley.
William A. Barrett, John D. Couch (1979). Compiler construction: Theory and Practice. Science Research Associate.
Jean-Paul Tremblay, P. G. Sorenson (1985). The Theory and Practice of Compiler Writing. McGraw–Hill.


== External links ==
""Simple Precedence Relations"" at Clemson University"
506,Multilevel queue,9310254,2797,"Multi-level queueing, used at least since the late 1950s/early 1960s, is a queue with a predefined number of levels. Unlike the multilevel feedback queue, items get assigned to a particular level at insert (using some predefined algorithm), and thus cannot be moved to another level. Items get removed from the queue by removing all items from a level, and then moving to the next. If an item is added to a level above, the ""fetching"" restarts from there. Each level of the queue is free to use its own scheduling, thus adding greater flexibility than merely having multiple levels in a queue.


== Process Scheduling ==
Multi-level queue  scheduling algorithm is used in scenarios where the processes can be classified into groups based on property like process type, CPU time, IO access, memory size, etc. One general classification of the processes is foreground processes and background processes. In a multi-level queue scheduling algorithm, there will be 'n' number of queues, where 'n' is the number of groups the processes are classified into. Each queue will be assigned a priority and will have its own scheduling algorithm like Round-robin scheduling  or FCFS. For the process in a queue to execute, all the queues of priority higher than it should be empty, meaning the process in those high priority queues should have completed its execution. In this scheduling algorithm, once assigned to a queue, the process will not move to any other queues.
Consider the following table with the arrival time, execute time and type of the process (foreground or background - where foreground processes are given high priority) to understand non pre-emptive and pre-emptive multilevel scheduling in depth with FCFS algorithm for both the queues:


== See also ==
Fair-share scheduling
Lottery scheduling


== References =="
507,Symposium on Computational Geometry,29309436,2793,"The Annual Symposium on Computational Geometry (SoCG) is an academic conference in computational geometry. It was founded in 1985, and in most but not all of its years it has been sponsored by the ACM's SIGACT and SIGGRAPH Special Interest Groups. After a few years of discussion, the computational geometry community decided to leave the ACM and organize the conference on its own. The motivation for this decision was the difficulties to organize something with the ACM outside the United States of America and the possibility of turning to an open-access system of publication. Beginning in 2015 the conference proceedings are published by the Leibniz International Proceedings in Informatics instead of the ACM.
A 2010 assessment of conference quality by the Australian Research Council listed it as ""Rank A"".


== References ==


== External links ==
Symposium on Computational Geometry at DBLP
computational geometry pages
discussion SoCG vs ACM"
508,Straight-line grammar,19282986,2775,"A straight-line grammar (sometimes abbreviated as SLG) is a formal grammar that generates exactly one string. Consequently, it does not branch (every non-terminal has only one associated production rule) nor loop (if non-terminal A appears in a derivation of B, then B does not appear in a derivation of A).
SLGs are of interest in fields like Kolmogorov complexity, Lossless data compression, Structure discovery and Compressed data structures.
The problem of finding a context-free SLG of minimal size that generates a given string is called the smallest grammar problem.


== Formal Definition ==
A context-free grammar G is an SLG if:
1. for every non-terminal N, there is at most one production rule that has N as its left-hand side, and
2. the directed graph G=<V,E>, defined by V being the set of non-terminals and (A,B) ∈ E whenever B appears at the right-hand side of a production rule for A, is acyclic.
An SLG in Chomsky normal form is equivalent to a straight-line program.


== A list of algorithms using SLGs ==
The Sequitur algorithm constructs a straight-line grammar for a given string.
The Lempel-Ziv-Welch algorithm creates a context-free grammar in such a deterministic way that it is necessary to store only the start rule of the generated grammar.
Byte pair encoding


== See also ==
Grammar-based code
Non-recursive grammar - a grammar that doesn't loop, but may branch; generating a finite rather than a singleton language


== References =="
509,Semi-Yao graph,45410332,2774,"The k-semi-Yao graph (k-SYG) of a set of n objects P is a geometric proximity graph, which was first described to present a kinetic data structure for maintenance of all the nearest neighbors on moving objects. It is named for its relation to the Yao graph, which is named after Andrew Yao.


== Construction ==
The k-SYG is constructed as follows. The space around each point p in P is partitioned into a set of polyhedral cones of opening angle 
  
    
      
        θ
      
    
    {\displaystyle \theta }
  , meaning the angle of each pair of rays inside a polyhedral cone emanating from the apex is at most 
  
    
      
        θ
      
    
    {\displaystyle \theta }
  , and then p connects to k points of P in each of the polyhedral cones whose projections on the cone axis is minimum.


== Properties ==
The k-SYG, where k = 1, is known as the theta graph, and is the union of two Delaunay triangulations.
For a small 
  
    
      
        θ
      
    
    {\displaystyle \theta }
   and an appropriate cone axis, the k-SYG gives a supergraph of the k-nearest neighbor graph (k-NNG). For example, in 2D, if we partition the plane around each point into six wedges of equal angles, and pick the cone axes on directions of the cone bisectors, we obtain a k-SYG as a supergraph for the k-NNG.


== See also ==
Geometric spanner


== References =="
510,Graph matching,54414446,2772,"Graph matching is the problem of finding a similarity between graphs.
Graphs are commonly used to encode structural information in many fields, including computer vision and pattern recognition, and graph matching is an important tool in these areas.  In these areas it is commonly assumed that the comparison is between the data graph and the model graph.
The case of exact graph matching is known as the graph isomorphism problem. The problem of exact matching of a graph to a part of another graph is called subgraph isomorphism problem.
The inexact graph matching refers to matching problems when exact matching is impossible, e.g., when the number of vertices in the two graphs are different. In this case it is required to find the best possible match. For example, in image recognition applications, the results of image segmentation in image processing typically produces data graphs with the numbers of vertices much larger than in the model graphs data expected to match against. In the case of attributed graphs, even if the numbers of vertices and edges are the same, the matching still may be only inexact. 
Two categories of search methods are the ones based on identification of possible and impossible pairings of vertices between the two graphs and methods which formulate graph matching as an optimization problem. Graph edit distance is one of similarity measures suggested for graph matching.  A class of algorithms if called error-tolerant graph matching.


== See also ==
String matching
Pattern matching


== References =="
511,SIGCSE Technical Symposium on Computer Science Education,40076829,2769,"The SIGCSE Technical Symposium on Computer Science Education is the main ACM conference for computer science educators. It has been held annually in February or March in the United States since 1970. There are generally around 1300 attendees, who are computer science educators from around the world.


== Nifty Assignments ==
The Nifty Assignments session is one of the most popular sessions at the conference. Started by Nick Parlante in 1999, the session serves as a place for educators to share ideas and materials for successful computer science assignments. Presenters have included Owen Astrachan, Richard E. Pattis, Joseph Zachary, Eric S. Roberts, Cay Horstmann, Mehran Sahami, David Malan, and Mark Guzdial.


== Conferences ==
SIGCSE 2018 - Baltimore, Maryland - February 21-24, 2018 - 49th conference
SIGCSE 2017 - Seattle, Washington - March 8–11, 2017 - 48th conference
SIGCSE 2016 - Memphis, Tennessee - March 2–5, 2016 - 47th conference
SIGCSE 2015 - Kansas City, Missouri - March 4–7, 2015 - 46th conference
SIGCSE 2014 - Atlanta, Georgia - March 5–8, 2014 - 45th conference
SIGCSE 2013 - Denver, Colorado - 44th conference
SIGCSE 2012 - Raleigh, NC - 43rd conference
SIGCSE 2011 - Dallas, Texas - 42nd conference
SIGCSE 2010 - Milwaukee, Wisconsin - 41st conference
SIGCSE 2009 - Chattanooga, Tennessee - 40th conference
SIGCSE 2008 - Portland, Oregon - 39th conference
SIGCSE 2007 - Covington, Kentucky - 38th conference
SIGCSE 2006 - Houston, Texas - 37th conference


== External links ==
Nifty Assignments
SIGCSE Technical Symposium


== References =="
512,Society for the Study of Artificial Intelligence and the Simulation of Behaviour,9855223,2765,"The Society for the Study of Artificial Intelligence and Simulation of Behaviour or SSAISB or AISB is a nonprofit, scientific society devoted to advancing the scientific understanding of the mechanisms underlying thought and intelligent behaviour and their simulation and embodiment in machines. AISB also aims to facilitate co-operation and communication among those interested in the study of artificial intelligence, simulation of behaviour and the design of intelligent systems.


== History ==
Founded in 1964, SSAISB, is the oldest AI society in the world. It is the largest Artificial Intelligence Society in the United Kingdom. The society has an international membership drawn from both academia and industry. It is a member of the European Association for Artificial Intelligence (previously known as European Coordinating Committee for Artificial Intelligence ECCAI).


== Objectives of the Society ==
The objectives of the Society are:
to promote the study of artificial intelligence, simulation of behaviour and the design of intelligent systems.
to facilitate co-operation and communication among those interested in the study of artificial intelligence, simulation of behaviour and the design of intelligent systems.
to hold, or to participate in the holding of, conferences and meetings for the communication of knowledge concerning, and to publicise and disseminate by other means knowledge and views concerning artificial intelligence, simulation of behaviour and the design of intelligent systems.


== Activities of the Society ==
The society hosts an annual convention consisting of parallel symposia covering various specialist topics, loosely organised around a theme. It also runs various events, especially to promote public understanding of AI and cognitive science. The society published the journal AISBJ (no longer published) and continues to publish a quarterly newsletter AISBQ which includes short reports on current AI and cognitive science research.


== External links ==
SSAISB website
Current and past SSAISB conventions
SSAISB Journal
SSAISB events
List of SSAISB Fellows"
513,Message Understanding Conference,6652820,2754,"The Message Understanding Conferences (MUC) were initiated and financed by DARPA (Defense Advanced Research Projects Agency) to encourage the development of new and better methods of information extraction. The character of this competition—many concurrent research teams competing against one another—required the development of standards for evaluation, e.g. the adoption of metrics like precision and recall.


== Topics and exercises ==
Only for the first conference (MUC-1) could the participant choose the output format for the extracted information. From the second conference the output format, by which the participants' systems would be evaluated, was prescribed. For each topic fields were given, which had to be filled with information from the text. Typical fields were, for example, the cause, the agent, the time and place of an event, the consequences etc. The number of fields increased from conference to conference.
At the sixth conference (MUC-6) the task of recognition of named entities and coreference was added. For named entity all phrases in the text were supposed to be marked as person, location, organization, time or quantity.
The topics and text sources, which were processed, show a continuous move from military to civil themes, which mirrored the change in business interest in information extraction taking place at the time.


== Literature ==
Ralph Grishman, Beth Sundheim: Message Understanding Conference - 6: A Brief History. In: Proceedings of the 16th International Conference on Computational Linguistics (COLING), I, Kopenhagen, 1996, 466–471.


== See also ==
DARPA TIPSTER Program


== External links ==
MUC-7
MUC-6
SAIC Information Extraction"
514,Fundamental theorem of linear programming,25844292,2754,"In mathematical optimization, the fundamental theorem of linear programming states, in a weak formulation, that the maxima and minima of a linear function over a convex polygonal region occur at the region's corners. Further, if an extreme value occurs at two corners, then it must also occur everywhere on the line segment between them.


== Statement ==
Consider the optimization problem

  
    
      
        min
        
          c
          
            T
          
        
        x
        
           subject to 
        
        x
        ∈
        P
      
    
    {\displaystyle \min c^{T}x{\text{ subject to }}x\in P}
  
Where 
  
    
      
        P
        =
        {
        x
        ∈
        
          
            R
          
          
            n
          
        
        :
        A
        x
        ≤
        b
        }
      
    
    {\displaystyle P=\{x\in \mathbb {R} ^{n}:Ax\leq b\}}
  . If 
  
    
      
        P
      
    
    {\displaystyle P}
   is a bounded polyhedron (and thus a polytope) and 
  
    
      
        
          x
          
            ∗
          
        
      
    
    {\displaystyle x^{\ast }}
   is an optimal solution to the problem, then 
  
    
      
        
          x
          
            ∗
          
        
      
    
    {\displaystyle x^{\ast }}
   is either an extreme point (vertex) of 
  
    
      
        P
      
    
    {\displaystyle P}
  , or lies on a face 
  
    
      
        F
        ⊂
        P
      
    
    {\displaystyle F\subset P}
   of optimal solutions.


== Proof ==
Suppose, for the sake of contradiction, that 
  
    
      
        
          x
          
            ∗
          
        
        ∈
        
          i
          n
          t
        
        (
        P
        )
      
    
    {\displaystyle x^{\ast }\in \mathrm {int} (P)}
  . Then there exists some 
  
    
      
        ϵ
        >
        0
      
    
    {\displaystyle \epsilon >0}
   such that the ball of radius 
  
    
      
        ϵ
      
    
    {\displaystyle \epsilon }
   centered at 
  
    
      
        
          x
          
            ∗
          
        
      
    
    {\displaystyle x^{\ast }}
   is contained in 
  
    
      
        P
      
    
    {\displaystyle P}
  , that is 
  
    
      
        
          B
          
            ϵ
          
        
        (
        
          x
          
            ∗
          
        
        )
        ⊂
        P
      
    
    {\displaystyle B_{\epsilon }(x^{\ast })\subset P}
  . Therefore,

  
    
      
        
          x
          
            ∗
          
        
        −
        
          
            ϵ
            2
          
        
        
          
            c
            
              
                |
              
              
                |
              
              c
              
                |
              
              
                |
              
            
          
        
        ∈
        P
      
    
    {\displaystyle x^{\ast }-{\frac {\epsilon }{2}}{\frac {c}{||c||}}\in P}
   and

  
    
      
        
          c
          
            T
          
        
        
          (
          
            
              x
              
                ∗
              
            
            −
            
              
                ϵ
                2
              
            
            
              
                c
                
                  
                    |
                  
                  
                    |
                  
                  c
                  
                    |
                  
                  
                    |
                  
                
              
            
          
          )
        
        =
        
          c
          
            T
          
        
        
          x
          
            ∗
          
        
        −
        
          
            ϵ
            2
          
        
        
          
            
              
                c
                
                  T
                
              
              c
            
            
              
                |
              
              
                |
              
              c
              
                |
              
              
                |
              
            
          
        
        =
        
          c
          
            T
          
        
        
          x
          
            ∗
          
        
        −
        
          
            ϵ
            2
          
        
        
          |
        
        
          |
        
        c
        
          |
        
        
          |
        
        <
        
          c
          
            T
          
        
        
          x
          
            ∗
          
        
        .
      
    
    {\displaystyle c^{T}\left(x^{\ast }-{\frac {\epsilon }{2}}{\frac {c}{||c||}}\right)=c^{T}x^{\ast }-{\frac {\epsilon }{2}}{\frac {c^{T}c}{||c||}}=c^{T}x^{\ast }-{\frac {\epsilon }{2}}||c||<c^{T}x^{\ast }.}
  
Hence 
  
    
      
        
          x
          
            ∗
          
        
      
    
    {\displaystyle x^{\ast }}
   is not an optimal solution, a contradiction. Therefore, 
  
    
      
        
          x
          
            ∗
          
        
      
    
    {\displaystyle x^{\ast }}
   must live on the boundary of 
  
    
      
        P
      
    
    {\displaystyle P}
  . If 
  
    
      
        
          x
          
            ∗
          
        
      
    
    {\displaystyle x^{\ast }}
   is not a vertex itself, it must be the convex combination of vertices of 
  
    
      
        P
      
    
    {\displaystyle P}
  , say 
  
    
      
        
          x
          
            1
          
        
        ,
        .
        .
        .
        ,
        
          x
          
            t
          
        
      
    
    {\displaystyle x_{1},...,x_{t}}
  . Then 
  
    
      
        
          x
          
            ∗
          
        
        =
        
          ∑
          
            i
            =
            1
          
          
            t
          
        
        
          λ
          
            i
          
        
        
          x
          
            i
          
        
      
    
    {\displaystyle x^{\ast }=\sum _{i=1}^{t}\lambda _{i}x_{i}}
   with 
  
    
      
        
          λ
          
            i
          
        
        ≥
        0
      
    
    {\displaystyle \lambda _{i}\geq 0}
   and 
  
    
      
        
          ∑
          
            i
            =
            1
          
          
            t
          
        
        
          λ
          
            i
          
        
        =
        1
      
    
    {\displaystyle \sum _{i=1}^{t}\lambda _{i}=1}
  . Observe that

  
    
      
        0
        =
        
          c
          
            T
          
        
        
          (
          
            
              (
              
                
                  ∑
                  
                    i
                    =
                    1
                  
                  
                    t
                  
                
                
                  λ
                  
                    i
                  
                
                
                  x
                  
                    i
                  
                
              
              )
            
            −
            
              x
              
                ∗
              
            
          
          )
        
        =
        
          c
          
            T
          
        
        
          (
          
            
              ∑
              
                i
                =
                1
              
              
                t
              
            
            
              λ
              
                i
              
            
            (
            
              x
              
                i
              
            
            −
            
              x
              
                ∗
              
            
            )
          
          )
        
        =
        
          ∑
          
            i
            =
            1
          
          
            t
          
        
        
          λ
          
            i
          
        
        (
        
          c
          
            T
          
        
        
          x
          
            i
          
        
        −
        
          c
          
            T
          
        
        
          x
          
            ∗
          
        
        )
        .
      
    
    {\displaystyle 0=c^{T}\left(\left(\sum _{i=1}^{t}\lambda _{i}x_{i}\right)-x^{\ast }\right)=c^{T}\left(\sum _{i=1}^{t}\lambda _{i}(x_{i}-x^{\ast })\right)=\sum _{i=1}^{t}\lambda _{i}(c^{T}x_{i}-c^{T}x^{\ast }).}
  
Since 
  
    
      
        
          x
          
            ∗
          
        
      
    
    {\displaystyle x^{\ast }}
   is an optimal solution, all terms in the sum are nonnegative. Since the sum is equal to zero, we must have that each individual term is equal to zero. Hence, 
  
    
      
        
          c
          
            T
          
        
        
          x
          
            ∗
          
        
        =
        
          c
          
            T
          
        
        
          x
          
            i
          
        
      
    
    {\displaystyle c^{T}x^{\ast }=c^{T}x_{i}}
   for each 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
  , so every 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
   is also optimal, and therefore all points on the face whose vertices are 
  
    
      
        
          x
          
            1
          
        
        ,
        .
        .
        .
        ,
        
          x
          
            t
          
        
      
    
    {\displaystyle x_{1},...,x_{t}}
  , are optimal solutions.


== References ==
http://www.linearprogramming.info/fundamental-theorem-of-linear-programming-and-its-properties/
http://demonstrations.wolfram.com/TheFundamentalTheoremOfLinearProgramming/"
515,Read-modify-write,5587875,2750,"In computer science, read-modify-write is a class of atomic operations (such as test-and-set, fetch-and-add, and compare-and-swap) that both read a memory location and write a new value into it simultaneously, either with a completely new value or some function of the previous value. These operations prevent race conditions in multi-threaded applications. Typically they are used to implement mutexes or semaphores. These atomic operations are also heavily used in non-blocking synchronization.
Maurice Herlihy (1991) ranks atomic operations by their consensus numbers, as follows:
∞: memory-to-memory move and swap, augmented queue, compare-and-swap, fetch-and-cons, sticky byte, load-link/store-conditional (LL/SC)
2n - 2: n-register assignment
2: test-and-set, swap, fetch-and-add, queue, stack
1: atomic read and atomic write
It is impossible to implement an operation that requires a given consensus number with only operations with a lower consensus number, no matter how many of such operations one uses. Read-modify-write instructions often produce unexpected results when used on I/O devices, as a write operation may not affect the same internal register that would be accessed in a read operation.
This term is also associated with RAID levels that perform actual write operations as atomic read-modify-write sequences. Such RAID levels include RAID 4, RAID 5 and RAID 6.


== See also ==

Linearizability
Read-erase-modify-write


== References =="
516,Algorithm design,10433312,2741,"Algorithm design is a specific method to create a mathematical process in problem solving processes. Applied algorithm design is algorithm engineering.
Algorithm design is identified and incorporated into many solution theories of operation research, such as dynamic programming and divide-and-conquer. Techniques for designing and implementing algorithm designs are algorithm design patterns, such as template method pattern and decorator pattern, and uses of data structures, and name and sort lists. Some current day uses of algorithm design can be found in internet retrieval processes of web crawling, packet routing and caching.
Mainframe programming languages such as ALGOL (for Algorithmic language), FORTRAN, COBOL, PL/I, SAIL, and SNOBOL are computing tools to implement an ""algorithm design""... but, an ""algorithm design"" (a/d) is not a language. An a/d can be a hand written process, e.g. set of equations, a series of mechanical processes done by hand, an analog piece of equipment, or a digital process and/or processor.
One of the most important aspects of algorithm design is creating an algorithm that has an efficient runtime, also known as its Big O.
Steps in development of Algorithms
Problem definition
Development of a model
Specification of Algorithm
Designing an Algorithm
Checking the correctness of Algorithm
Analysis of Algorithm
Implementation of Algorithm
Program testing
Documentation Preparation


== Common design paradigms ==
Divide and conquer
Dynamic programming
Greedy algorithm
Back Tracking
Brute Force


== Notes ==


== Further reading ==
Algorithm Design Paradigms - Overview by Paul Dunne at the University of Liverpool
Stony Brook Algorithm Repository by Steven S. Skiena, Department of Computer Science, State University of New York
Complete DAA notes pdfs and ppt"
517,List of C++ template libraries,19944154,2739,"The following list of C++ template libraries details the various libraries of templates available for the C++ programming language.
The choice of a typical library depends on a diverse range of requirements such as: desired features (for e.g.: large dimensional linear algebra, parallel computation, partial differential equations), commercial/opensource nature, readability of API, portability or platform/compiler dependence (for e.g.: Linux, Windows, Visual C++, GCC), performance in speed, ease-of-use, continued support from developers, standard compliance, specialized optimization in code for specific application scenarios or even the size of the code-base to be installed.


== General ==
Active Template Library (Windows)
Adaptive Communication Environment
Adobe Source Libraries
AGG (anti-aliased rendering library)
Armadillo C++ Library (linear algebra)
Blitz++ (linear algebra)
Boost
CGAL — Computational Geometry Algorithms Library
Concurrent Collections for C++ (CnC)
Dlib
Eigen Library (linear algebra)
Embedded Template Library (ETL)
Geometry plus Simulation Modules (G+Smo)
IT++
Audio and DSP library with extensive use of template expressions
Loki
Matrix Template Library (linear algebra)
MLPACK C++ library (machine learning)
ODB ORM and Database-Aware Container Template Library
Oracle Template Library
PEGTL — Parsing Expression Grammar Template Library
PAAL
PETSc - Portable, Extensible Toolkit for Scientific Computation
POCO C++ Libraries
SaferCPlusPlus - Safe compatible substitutes for unsafe C++ primitives.
Standard Template Library
STAPL
Template Numerical Toolkit
Templatious
Threading Building Blocks (TBB)
Trilinos (linear algebra)
uSTL
Windows Template Library


== See also ==
List of compilers § C++ compilers


== External links ==
Linear Algebra Libraries (A well-done survey by Claire Mouton, from INRIA, France in 2009)"
518,Stephen M. Watt,48719587,2735,"Stephen M. Watt, a computer scientist and mathematician, is Dean of the Faculty of Mathematics and Professor in the David R. Cheriton School of Computer Science at the University of Waterloo, Ontario, Canada.
He previously held the title of Distinguished University Professor at Western University, Ontario where he served for periods as Chair of the Department of Computer Science and Director of the Ontario Research Centre for Computer Algebra. Prior to this, he held positions at the IBM T.J. Watson Research Center in Yorktown Heights (USA) and INRIA and the University of Nice (France).
Professor Watt's areas of research include algorithms and systems for computer algebra, programming languages and compilers, mathematical handwriting recognition and document analysis. He was one of the original authors of the Maple and Axiom computer algebra systems, principal architect of the Aldor programming language and its compiler at IBM Research, and is co-author of the MathML and editor of the InkML W3C standards.
Watt was a co-founder of Maplesoft in 1988 and served on its board of directors from 1998 to 2009. He served on the board of directors of the Descartes Systems Group from 2001 to 2015, including two periods as board chair. He presently serves on the boards of Waste Diversion Ontario, which oversees the management of all Ontario's recycling programs, and of the McMichael Canadian Art Foundation.
Professor Watt was made Doctor Honoris Causa of the University of the West (Romania) in 2011 and was awarded the J.W. Graham Medal in Computing and Innovation in 2012.


== References =="
519,ARITH Symposium on Computer Arithmetic,47151741,2730,"The IEEE International Symposium on Computer Arithmetic (ARITH) is a conference in the area of computer arithmetic. The symposium was established in 1969, initially as three-year event, then as a biennial event, and, finally, from 2015 as an annual symposium.
ARITH topics span from theoretical aspects and algorithms for operations, to hardware implementations of arithmetic units and applications of computer arithmetic.
ARITH symposia are sponsored by IEEE Computer Society.


== List of ARITH symposia ==


== References =="
520,Computer compatibility,12075392,2730,"A family of computer models is said to be compatible if certain software that runs on one of the models can also be run on all other models of the family. The computer models may differ in performance, reliability or some other characteristic. These differences may affect the outcome of the running of the software.


== Software compatibility ==
Software compatibility can refer to the compatibility that a particular software has running on a particular CPU architecture such as Intel or PowerPC. Software compatibility can also refer to ability for the software to run on a particular operating system. Very rarely is a compiled software compatible with multiple different CPU architectures. Normally, an application is compiled for different CPU architectures and operating systems to allow it to be compatible with the different system. Interpreted software, on the other hand, can normally run on many different CPU architectures and operating systems if the interpreter is available for the architecture or operating system. Software incompatibility occurs many times for new software released for a newer version of an operating system which is incompatible with the older version of the operating system because it may miss some of the features and functionality that the software depends on.


== Hardware compatibility ==
Hardware compatibility can refer to the compatibility of computer hardware components with a particular CPU architecture, bus, motherboard or operating system. Hardware that is compatible may not always run at its highest stated performance, but it can nevertheless work with legacy components. An example is RAM chips, some of which can run at a lower (or sometimes higher) clock rate than rated. Hardware that was designed for one operating system may not work for another, if device or kernel drivers are unavailable. For example, much of the hardware for Mac OS X is proprietary hardware with drivers unavailable for use in operating systems such as Linux.


== Free and open-source software ==


== See also ==
Interchangeability
Forward compatibility
Backward compatibility
Cross-platform
Emulator
List of computer standards
Portability
Plug compatible
Hardware security


== References =="
521,Adversary model,8198634,2720,"In computer science, an online algorithm measures its competitiveness against different adversary models. For deterministic algorithms, the adversary is the same as the adaptive offline adversary. For randomized online algorithms competitiveness can depend upon the adversary model used.


== Common adversaries ==
The three common adversaries are the oblivious adversary, the adaptive online adversary, and the adaptive offline adversary.
The oblivious adversary is sometimes referred to as the weak adversary. This adversary knows the algorithm's code, but does not get to know the randomized results of the algorithm.
The adaptive online adversary is sometimes called the medium adversary. This adversary must make its own decision before it is allowed to know the decision of the algorithm.
The adaptive offline adversary is sometimes called the strong adversary. This adversary knows everything, even the random number generator. This adversary is so strong that randomization does not help against it.


== Important results ==
From S. Ben-David, A. Borodin, R. Karp, G. Tardos, A. Wigderson we have:
If there is a randomized algorithm that is α-competitive against any adaptive offline adversary then there also exists an α-competitive deterministic algorithm.
If G is a c-competitive randomized algorithm against any adaptive online adversary, and there is a randomized d-competitive algorithm against any oblivious adversary, then G is a randomized (c * d)-competitive algorithm against any adaptive offline adversary.


== See also ==
Competitive analysis (online algorithm)
K-server problem
Online algorithm


== References ==
Borodin, A.; El-Yaniv, R. (1998). Online Computation and Competitive Analysis. Cambridge University Press. ISBN 978-0-521-56392-5. 
S. Ben-David; A. Borodin; R. Karp; G. Tardos; A. Wigderson. (1994). ""On the Power of Randomization in On-line Algorithms"" (PDF). Algorithmica. 11: 2–14. doi:10.1007/BF01294260. 


== External links ==
Bibliography of papers on online algorithms"
522,PreScheme,17481163,2720,"PreScheme or Pre-Scheme is a statically-typed dialect of Scheme with the efficiency and low-level machine access of C while retaining many of the desirable features of Scheme.
The PreScheme compiler is a part of Scheme48.


== Variants ==
Macro-Free PreScheme
Obtained from Full PreScheme by expanding all macros.
Pure PreScheme
A tail-recursive, closure-free dialect of PreScheme, obtained from Macro-Free PreScheme by hoisting lambda expressions and beta expansion.
VLISP PreScheme
Simple PreScheme


== Further reading ==
Oliva, Dino P.; Wand, Mitchell (1991). ""A Verified Compiler for Pure PreScheme"". 
A Guide to VLISP, A Verified Programming Language Implementation (1992)
Oliva, Dino P.; Wand, Mitchell (1992). ""A Verified Run-Time Structure for Pure PreScheme"". 
The VLISP PreScheme Front End (1992)
The Revised VLISP PreScheme Front End (1993)
Kelsey, Richard; Rees, Jonathan A. (1993). ""A Tractable Scheme Implementation"". 
Tail-Recursive Stack Disciplines for an Interpreter (1993)
Guttman, Joshua; Swarup, Vipin; Ramsdell, John (1995). ""The VLISP Verified Scheme System"". Lisp and Symbolic Computation. pp. 33–110. 
A Verified Compiler for Multithreaded PreScheme (1996)
Kelsey, Richard A. (1997). ""Pre-Scheme: A Scheme Dialect for Systems Programming""."
523,Instruction step,5713883,2718,"An instruction step is a method of executing a computer program one step at a time to determine how it is functioning. This might be to determine if the correct program flow is being followed in the program during the execution or to see if variables are set to their correct values after a single step has completed.


== Hardware instruction step ==
On earlier computers, a knob on the computer console may have enabled step-by-step execution mode to be selected and execution would then proceed by pressing a ""single step"" or ""single cycle"" button. Program status word / Memory or general purpose register read-out could then be accomplished by observing and noting the console lights.


== Software instruction step ==
On later platforms with multiple users, this method was impractical and so single step execution had to be performed using software techniques.


=== Software techniques ===
Instrumentation - requiring code to be added during compile or assembly to achieve statement stepping. Code can be added manually to achieve similar results in interpretive languages such as javascript.
instruction set simulation - requiring no code modifications for instruction or statement stepping
In some software products which facilitate debugging of High level languages, it is possible to execute an entire HLL statement at a time. This frequently involves many machine instructions and execution pauses after the last instruction in the sequence, ready for the next 'instruction' step. This requires integration with the compilation output to determine the scope of each statement.
Full Instruction set simulators however could provide instruction stepping with or without any source, since they operate at machine code level, optionally providing full trace and debugging information to whatever higher level was available through such integration. In addition they may also optionally allow stepping through each assembly (machine) instruction generated by a HLL statement.
Programs composed of multiple 'modules' compiled from a mixture of compiled languages, and even instructions created ""on-the-fly"" in dynamically allocated memory, could be accommodated using this technique.


=== Examples of programs providing 'Software' instruction step ===
SIMMON an IBM internal test system which provided instruction stepping


== References ==


== See also ==
Instrumentation (computer programming)
Instruction set simulator
Program status word
Instruction cycle"
524,Constrained clustering,33391687,2714,"In computer science, constrained clustering is a class of semi-supervised learning algorithms. Typically, constrained clustering incorporates either a set of must-link constraints, cannot-link constraints, or both, with a Data clustering algorithm. Both a must-link and a cannot-link constraint define a relationship between two data instances. A must-link constraint is used to specify that the two instances in the must-link relation should be associated with the same cluster. A cannot-link constraint is used to specify that the two instances in the cannot-link relation should not be associated with the same cluster. These sets of constraints acts as a guide for which a constrained clustering algorithm will attempt to find clusters in a data set which satisfy the specified must-link and cannot-link constraints. Some constrained clustering algorithms will abort if no such clustering exists which satisfies the specified constraints. Others will try to minimize the amount of constraint violation should it be impossible to find a clustering which satisfies the constraints. Constraints could also be used to guide the selection of a clustering model among several possible solutions. 
A cluster in which the members conform to all must-link and cannot-link constraints is called a chunklet.


== Examples ==
Examples of constrained clustering algorithms include:
COP K-means 
PCKmeans
CMWK-Means 


== References =="
525,Negative cache,234029,2704,"In computer programming, negative cache is a cache that also stores ""negative"" responses, i.e. failures. This means that a program remembers the result indicating a failure even after the cause has been corrected. Usually negative cache is a design choice, but it can also be a software bug.


== Examples ==
Consider a web browser which attempts to load a page while the network is unavailable. The browser will receive an error code indicating the problem, and may display this error message to the user in place of the requested page. However, it is incorrect for the browser to place the error message in the page cache, as this would lead it to display the error again when the user tries to load the same page - even after the network is back up. The error message must not be cached under the page's URL; until the browser is able to successfully load the page, whenever the user tries to load the page, the browser must make a new attempt.
A frustrating aspect of negative caches is that the user may put a great effort into troubleshooting the problem, and then after determining and removing the root cause, the error still does not vanish.
There are cases where failure-like states must be cached. For instance, DNS requires that caching nameservers remember negative responses as well as positive ones. If an authoritative nameserver returns a negative response, indicating that a name does not exist, this is cached. The negative response may be perceived as a failure at the application level; however, to the nameserver caching it, it is not a failure. The cache times for negative and positive caching may be tuned independently.


== Description ==
A negative cache is normally only desired if failure is very expensive and the error condition arises automatically without user's action. It creates a situation where the user is unable to isolate the cause of the failure: despite fixing everything he/she can think of, the program still refuses to work. When a failure is cached, the program should provide a clear indication of what must be done to clear the cache, in addition to a description of the cause of the error. In such conditions a negative cache is an example of a design anti-pattern.
Negative cache still may recover if the cached records expire.


== See also ==
Perl Design Patterns Book


== References =="
526,Humanistic informatics,6837951,2692,"Humanistic Informatics (also known as Humanities informatics) is one of several names chosen for the study of the relationship between human culture and technology. The term is fairly common in Europe, but is little known in the English-speaking world, though Digital Humanities (also known as Humanities computing) is in many cases roughly equivalent.
Humanistic informatics departments were generally started in the 1990s when universities rarely taught humanities-based approaches to the rapidly developing computerized society. For this reason, the field was quite broadly defined, and included courses in humanities computing, basic introductions to how computers work, historical developments of technology, technology and learning, digital art and literature and digital culture. Today several departments have declared more specialized areas of research, such as digital arts and culture at the University of Bergen, and socio-cultural communication with and without technology at the University of Aalborg.
Digital Humanities is a primary topic, and there are several universities in the US and the UK that have Digital Arts and Humanities research and development centers. One aspect of Digital Humanities that will grow will be the intersection of new digital media and the humanities, particularly in the gaming industry which has developed both casual and serious gaming and game design strategies to foster learning in the humanities and all other academic disciplines. A key principle in all digital interactive media or games is the storyline; the narrative or quest or goal of the game is primary to both literary works and games. Characters and players go on the quest, and playing the game becomes the narrative. Game design principles, also relevant in literature and the fine arts, include visual literacy and empowering players/learners to align with great artitsts and writers who believe in the creative process.


== External links ==


=== Departments of humanistic informatics at universities ===
Informatica Umanistica, University of Pisa, Italy
Seksjon for humanistisk informatikk, University of Bergen, Norway
Humanistisk informatikk, University of Oslo (now merged with other departments)
Humanistisk informatik, University of Aalborg"
527,Canopy clustering algorithm,14526742,2692,"The canopy clustering algorithm is an unsupervised pre-clustering algorithm introduced by Andrew McCallum, Kamal Nigam and Lyle Ungar in 2000. It is often used as preprocessing step for the K-means algorithm or the Hierarchical clustering algorithm. It is intended to speed up clustering operations on large data sets, where using another algorithm directly may be impractical due to the size of the data set.
The algorithm proceeds as follows, using two thresholds 
  
    
      
        
          T
          
            1
          
        
      
    
    {\displaystyle T_{1}}
   (the loose distance) and 
  
    
      
        
          T
          
            2
          
        
      
    
    {\displaystyle T_{2}}
   (the tight distance), where 
  
    
      
        
          T
          
            1
          
        
        >
        
          T
          
            2
          
        
      
    
    {\displaystyle T_{1}>T_{2}}
   .
Begin with the set of data points to be clustered.
Remove a point from the set, beginning a new 'canopy'.
For each point left in the set, assign it to the new canopy if the distance is less than the loose distance 
  
    
      
        
          T
          
            1
          
        
      
    
    {\displaystyle T_{1}}
  .
If the distance of the point is additionally less than the tight distance 
  
    
      
        
          T
          
            2
          
        
      
    
    {\displaystyle T_{2}}
  , remove it from the original set.
Repeat from step 2 until there are no more data points in the set to cluster.
These relatively cheaply clustered canopies can be sub-clustered using a more expensive but accurate algorithm.
An important note is that individual data points may be part of several canopies. As an additional speed-up, an approximate and fast distance metric can be used for 3, where a more accurate and slow distance metric can be used for step 4.
Since the algorithm uses distance functions and requires the specification of distance thresholds, its applicability for high-dimensional data is limited by the curse of dimensionality. Only when a cheap and approximative – low-dimensional – distance function is available, the produced canopies will preserve the clusters produced by K-means.


== Benefits ==
The number of instances of training data that must be compared at each step is reduced
There is some evidence that the resulting clusters are improved


== References =="
528,CPM-GOMS,13871782,2691,"GOMS is a specialized human information processor model for human-computer interaction observation that describes a user's cognitive structure on four components. In the book The Psychology of Human Computer Interaction. written in 1983 by Stuart K. Card, Thomas P. Moran and Allen Newell, the authors introduce: ""a set of Goals, a set of Operators, a set of Methods for achieving the goals, and a set of Selections rules for choosing among competing methods for goals."" GOMS is a widely used method by usability specialists for computer system designers because it produces quantitative and qualitative predictions of how people will use a proposed system.


== Overview ==

A GOMS model is composed of methods that are used to achieve specific goals. These methods are then composed of operators at the lowest level. The operators are specific steps that a user performs and are assigned a specific execution time. If a goal can be achieved by more than one method, then selection rules are used to determine the proper Method.
Goals are symbolic structures that define a state of affairs to be achieved and determinate a set of possible methods by which it may be accomplished
Operators are elementary perceptual, motor or cognitive acts, whose execution is necessary to change any aspect of the user's mental state or to affect the task environment
Methods describe a procedure for accomplishing a goal
Control Structure: Selection Rules are needed when a goal is attempted, there may be more than one method available to the user to accomplish it.
There are several different GOMS variations which allow for different aspects of an interface to be accurately studied and predicted.
For all of the variants, the definitions of the major concepts are the same. There is some flexibility for the designer's/analyst's definition of all of the entities. For instance, an operator in one method may be a goal in a different method. The level of granularity is adjusted to capture what the particular evaluator is examining. For a simple applied example see CMN-GOMS.


== Qualification ==


=== Advantages ===
The GOMS approach to user modeling has strengths and weaknesses. While it is not necessarily the most accurate method to measure human-computer interface interaction, it does allow visibility of all procedural knowledge. With GOMS, an analyst can easily estimate a particular interaction and calculate it quickly and easily. This is only possible if the average Methods-Time Measurement data for each specific task has previously been measured experimentally to a high degree of accuracy.


=== Disadvantages ===
GOMS only applies to skilled users. It does not work for beginners or intermediates for errors may occur which can alter the data. Also the model doesn't apply to learning the system or a user using the system after a longer time of not using it. Another big disadvantage is the lack of account for errors, even skilled users make errors but GOMS does not account for errors. Mental workload is not addressed in the model, making this an unpredictable variable. The same applies to fatigue. GOMS only addresses the usability of a task on a system, it does not address its functionality.
User personalities, habits or physical restrictions (for example disabilities) are not accounted for in any of the GOMS models. All users are assumed to be exactly the same. Recently some extensions of GOMS were developed, that allow to formulate GOMS models describing the interaction behavior of disabled users.  


== Variations ==
Basically there are four different GOMS models: the Keystroke-Level Model, CMN-GOMS, NGOMSL and CPM-GOMS. Each model has a different complexity and varies in activities.


=== KLM ===
The Keystroke-Level Model (KLM) is the first and simplest GOMS technique Stuart Card, Thomas P. Moran and Allen Newell created. Estimating an execution time for a task is done by listing the sequence of operators and then totaling the execution times for the individual operators. With KLM the analyst must specify the method used to accomplish each particular task instance. Furthermore, the specified methods are limited to being in sequence form and containing only keystroke-level primitive operators.
KLM's execution part is described in four physical-motor operators:
K keystroking/ keypressing
P pointing with a mouse to a target
H homing the hand on the keyboard
D drawing a line segment on a grid
One mental operator M that stands for the time a user has to mentally prepare himself to do an action, and a system response operator R in with the user has to wait for the system. Execution time is the sum of the times spent executing the different operator types:
Texecute = TK + TP + TH + TD + TM + TR.
Each of these operators has an estimate of execution time, either a single value, a parameterized estimate.


==== Touch Level Model (TLM) ====
GOMS and it variants were designed for keyboard interfaces, nowadays a new type of interface is omnipresent. This addition to the GOMS family, together with updates to the existing KLM operators, is called the Touch Level Model (TLM). Andrew D. Rice and Jonathan W. Lartigue propose this model for the used to model human task performance on a constrained input touchscreen device and, with proper benchmarking, accurately predict actual user performance.
The goal is to provide an instrument for quantitative analysis of touchscreen interfaces.
A number of operators are added for touchscreen interactions:
Distraction (X) a multiplicative operator that is applied to other operators to model real world distractions
Gesture (G) gestures are conceptualized as specialized combinations of finger movements across the device's screen
Pinch (P) refers to the common two-finger gesture
Zoom (Z) the reverse application of the Pinch operator. value in MS = 200 Ms
Initial Act (I) KLM assumed the user is prepared to begin an action, touchscreen devices require users to prepare them for use (home button or password)
Tap (T) operator refers to the physical action of tapping an area on the touchscreen device in order to initiate some change or action
Swipe (S) usually a horizontally or vertically swipe like changing the page in a book. value in MS = 70 Ms
Tilt (L(d)) used with an interacting with a devices equipped with accelerometers.
Rotate (O(d)) gesture in which two or more fingers are placed on the screen and then rotated about a central point
Drag (D) similar to Swipe, Drag also involves tapping a location on the screen and then moving one or more fingers in specific direction


=== CMN-GOMS ===
CMN-GOMS is the original GOMS model proposed by Stuart Card, Thomas P. Moran and Allen Newell.
CMN stands for Card, Moran and Newell and it takes the KLM as its basic and adds subgoals and selection rules. This model can predict operator sequence as well as execution time. A CMN-GOMS model can be represented in program form, making it amenable to analysis as well as execution. CMN-GOMS has been used to model word processors  and CAD systems for ergonomic design(see CAD).  The CMN method can predict the operator sequence and the execution time of a task on a quantitative level and can focus its attention on methods to accomplish goals on a qualitative level.
In the example by Bonnie E. John and David E. Kieras a simple CMN-GOMS on editing a manuscript is shown.


=== NGOMSL ===
NGOMSL is a structured natural language notation for representing GOMS models and a procedure for constructing them. This program form provides predictions of operator sequences, execution time and time to learn methods. An analyst constructs an NGOMSL model by performing a top-down, breadth-first expansion of the user's top-level goals into methods, until the methods contain only primitive operators, typically keystroke-level operators. This model explicitly represents the goal structure just like the CMN-GOMS and can so represent high-level goals.
Shown below is a simple example.


=== CPM-GOMS ===
Bonnie E. John and David Kieras describe four different types of GOMS. CMN-GOMS, KLM and NGOMSL assume that all of the operators occur in sequence and do not contain operators that are below the activity level. CPM-GOMS being the fourth method uses operators at the level of Model Human Processor which assumes that operators of the cognitive processor, perceptual processor, and the motor processor can work in parallel to each other. The most important point of CPM-GOMS is the ability to predict skilled behavior from its ability to model overlapping actions.  
Shown below is a simple copy and paste example.


== GOMS and KLM ==
The biggest difference between GOMS and KLM is how time is assigned to cognitive and perceptual operators when it comes to execution time predictions. Another major difference is that the goal-hierarchy is explicit in GOMS while it was implicit in the KLM. The nature of unobservable operators is another important difference. KLM has a single M operator that precedes each cognitive unit of action. In contrast, GOMS assigns no time to such cognitive overhead. But both models include M-like operators for substantial time-consuming mental actions such as locating information on the screen and verifying entries. Both methods assign roughly the same time to unobservable perceptual and cognitive activities. Also they make different assumptions about unobservable cognitive and perceptual operators and so distribute the time in different ways. 


== Assumptions and Errors ==


=== Importance of Assumptions in GOMS Analysis ===
Accurate assumptions are vital in GOMS analysis. Before applying the average times for detailed functions, it is very important that an experimenter make sure he or she has accounted for as many variables as possible by using assumptions. Experimenters should design their GOMS analysis for the users who will most likely be using the system which is being analyzed. Consider, for example, an experimenter wishes to determine how long it will take an F22 Raptor pilot to interact with an interface he or she has used for years. It can probably be assumed that the pilot has outstanding vision and is in good physical health. In addition, it can be assumed that the pilot can interact with the interface quickly because of the vast hours of simulation and previous use he or she has endured. All things considered, it is fair to use fastman times in this situation. Contrarily, consider an 80-year-old person with no flight experience attempting to interact with the same F22 Raptor interface. It is fair to say that the two people would have much different skill sets and those skill sets should be accounted for subjectively.


=== Accounting for Errors ===
The only way to account for errors in GOMS analysis is to predict where the errors are most likely to occur and measure the time it would take to correct the predicted errors. For example, assume an experimenter thought that in typing the word ""the"" it was likely that a subject would instead incorrectly type ""teh."" The experimenter would calculate the time it takes to type the incorrect word, the time it takes to recognize that a mistake has been made, and the time it takes to correct the recognized error.


== Applications of GOMS ==


=== Workstation Efficiency ===
A successful implementation of CPM-GOMS was in Project Ernestine held by New England Telephone. New ergonomically designed workstations were compared to old workstations in terms of improvement in telephone operators' performance. CPM-GOMS analysis estimated a 3% decrease in productivity. Over the four-month trial 78,240 calls were analysed and it was concluded that the new workstations produced an actual 4% decrease in productivity. As the proposed workstation required less keystrokes than the original it was not clear from the time trials why the decrease occurred. However CPM-GOMS analysis made it apparent that the problem was that the new workstations did not utilize the workers' slack time. Not only did CPM-GOMS give a close estimate, but it provided more information of the situation.


=== CAD ===
GOMS models were employed in the redesign of a CAD (computer-aided design) system for industrial ergonomics. An applied GOMS model shows where the interface needs to be redesigned, as well as provides an evaluation of design concepts and ideas. In Richard Gong's example, when GOMS revealed a frequent goal supported by a very inefficient method, he changed the method to a more efficient one. If GOMS showed that there were goals not supported by any method at all, then new methods were added. GOMS also revealed where similar goals are supported by inconsistent methods, a situation in which users are likely to have problems remembering what to do, and showed how to make the methods consistent.


== Software Tools ==
There exist various tools for the creation and analysis of Goms-Models. A selection is listed in the following:
QGoms (Quick-Goms)
CogTool KLM-based modelling tool
Cogulator Cognitive calculator for GOMS modeling


== See also ==
Human information processor model
KLM-GOMS
CMN-GOMS
NGOMSL
CPM-GOMS


== References ==

A previous version of this article was largely a derivative work of GOMS Analysis Techniques - Final Essay (1997).


== Further reading ==
Kieras, D., John, B., The GOMS Family of Analysis Techniques: Tools for Design and Evaluation, CMU-HCII-94-106, 1994
Judith Reitman Olson, Gary M. Olson: The Growth of Cognitive Modeling in Human-Computer Interaction Since GOMS, in: R. M. Baecker, J. Grudin, W. A. S. Buxton, S. Greenberg: Readings in Human-Computer Interaction: Towards the Year 2000. 1995, San Francisco, CA: Morgan Kaufmann.
Card, S.K.; Moran, T.P.; Newell, A. (1983), The Psychology of Human-Computer Interaction, London: Lawrence Erlbaum Associates, ISBN 0-89859-243-7 
Card, Moran, Newell (1980). The Keystroke-Level Model for User Performance Time With Interactive Systems, Communications of the ACM, July, 23(7), 396-410.
Reason, J. (1990), Human Error, Manchester: Cambridge University Press, ISBN 978-0-521-31419-0 
John, Bonnie E. (1995), Why GOMS?, ACM, ISSN 1072-5520 
Kieras, David (1999), A Guide to GOMS Model Usability Evaluation using GOMSL and GLEAN3, Citeseer 
Gray, Wayne D (1993), Project Ernestine: Validating a GOMS Analysis for Predicting and Explaining Real-World Task Performance, ACM, ISSN 0737-0024 
Haunold, Peter (1994), A Keystroke Level Analysis of a Graphics Application: Manual Map Digitizing, ACM, ISBN 0-89791-650-6"
529,Raster Document Object,11722993,2689,"The .RDO (Raster Document Object) file format is the native format used by Xerox's DocuTech range of hardware and software, that underpins the company's ""Xerox Document On Demand"" ""XDOD"" systems. It is therefore a significant file format for the ""print on demand"" market sector, along with PostScript and PDF.
RDO is a metafile format based on the Open Document Architecture (ODA) specifications: In Xerox's RDO implementation, description and control information is stored within the RDO file, while raster images are stored separately, usually in a separate folder, as TIFF files. The RDO file dictates which bitmap images will be used on each page of a document, and where they will be placed.


== Features and disadvantages ==
This approach has advantages and disadvantages over the monolithic approach used by PDF: The disadvantages of RDO are that it is a largely proprietary format, and the multi-file approach means that file management and orphan control is more of an issue: one cannot tell from a computer's file system whether all the files required for a document to print are present and correct.
In RDO's favor, the multi-file approach allows a networked device to load the small RDO file and then request the larger bitmap files only when necessary: This allows a full job specification to be loaded and installed over a network almost immediately, with the larger bitmap files only having to be transferred as and when needed, allowing more flexibility for managing network traffic loading.
The TIFF file format is highly portable, and Xerox's MakeReady software, supplied with its XDOD systems, readily imports and export postscript files: however, the Xerox ""on demand"" systems typically require a document library to be stored as RDO / TIFF files, and most non-Xerox applications will not read RDO structures directly.


== See also ==
Xerox
DocuTech
Print on demand
Open Document Architecture
Tagged Image File Format
Portable Document Format


== References ==
""Document encoding formats for Phoenix: an example of on-demand publishing"" - Summary Report prepared by South Bank University
Oya Y. Rieger and Anne R. Kenney ""Risk Management of Digital Information Case Study for Image File Format"""
530,Association for Logic Programming,42733501,2680,"Logic programming is a type of programming paradigm which is largely based on formal logic. Any program written in a logic programming language is a set of sentences in logical form, expressing facts and rules about some problem domain. Major logic programming language families include Prolog, Answer set programming (ASP) and Datalog. In all of these languages, rules are written in the form of clauses:
H :- B1, …, Bn.
and are read declaratively as logical implications:
H if B1 and … and Bn.
H is called the head of the rule and B1, …, Bn is called the body. Facts are rules that have no body, and are written in the simplified form:
H.
In the simplest case in which H, B1, …, Bn are all atomic formulae, these clauses are called definite clauses or Horn clauses. However, there exist many extensions of this simple case, the most important one being the case in which conditions in the body of a clause can also be negations of atomic formulae. Logic programming languages that include this extension have the knowledge representation capabilities of a non-monotonic logic.
In ASP and Datalog, logic programs have only a declarative reading, and their execution is performed by means of a proof procedure or model generator whose behaviour is not meant to be under the control of the programmer. However, in the Prolog family of languages, logic programs also have a procedural interpretation as goal-reduction procedures:
to solve H, solve B1, and ... and solve Bn.
Consider, for example, the following clause:
fallible(X) :- human(X).
based on an example used by Terry Winograd to illustrate the programming language Planner. As a clause in a logic program, it can be used both as a procedure to test whether X is fallible by testing whether X is human, and as a procedure to find an X that is fallible by finding an X that is human. Even facts have a procedural interpretation. For example, the clause:
human(socrates).
can be used both as a procedure to show that socrates is human, and as a procedure to find an X that is human by ""assigning"" socrates to X.
The declarative reading of logic programs can be used by a programmer to verify their correctness. Moreover, logic-based program transformation techniques can also be used to transform logic programs into logically equivalent programs that are more efficient. In the Prolog family of logic programming languages, the programmer can also use the known problem-solving behaviour of the execution mechanism to improve the efficiency of programs.


== History ==
The use of mathematical logic to represent and execute computer programs is also a feature of the lambda calculus, developed by Alonzo Church in the 1930s. However, the first proposal to use the clausal form of logic for representing computer programs was made by Cordell Green. This used an axiomatization of a subset of LISP, together with a representation of an input-output relation, to compute the relation by simulating the execution of the program in LISP. Foster and Elcock's Absys, on the other hand, employed a combination of equations and lambda calculus in an assertional programming language which places no constraints on the order in which operations are performed.
Logic programming in its present form can be traced back to debates in the late 1960s and early 1970s about declarative versus procedural representations of knowledge in Artificial Intelligence. Advocates of declarative representations were notably working at Stanford, associated with John McCarthy, Bertram Raphael and Cordell Green, and in Edinburgh, with John Alan Robinson (an academic visitor from Syracuse University), Pat Hayes, and Robert Kowalski. Advocates of procedural representations were mainly centered at MIT, under the leadership of Marvin Minsky and Seymour Papert.
Although it was based on the proof methods of logic, Planner, developed at MIT, was the first language to emerge within this proceduralist paradigm. Planner featured pattern-directed invocation of procedural plans from goals (i.e. goal-reduction or backward chaining) and from assertions (i.e. forward chaining). The most influential implementation of Planner was the subset of Planner, called Micro-Planner, implemented by Gerry Sussman, Eugene Charniak and Terry Winograd. It was used to implement Winograd's natural-language understanding program SHRDLU, which was a landmark at that time. To cope with the very limited memory systems at the time, Planner used a backtracking control structure so that only one possible computation path had to be stored at a time. Planner gave rise to the programming languages QA-4, Popler, Conniver, QLISP, and the concurrent language Ether.
Hayes and Kowalski in Edinburgh tried to reconcile the logic-based declarative approach to knowledge representation with Planner's procedural approach. Hayes (1973) developed an equational language, Golux, in which different procedures could be obtained by altering the behavior of the theorem prover. Kowalski, on the other hand, developed SLD resolution, a variant of SL-resolution, and showed how it treats implications as goal-reduction procedures. Kowalski collaborated with Colmerauer in Marseille, who developed these ideas in the design and implementation of the programming language Prolog.
The Association for Logic Programming was founded to promote Logic Programming in 1986.
Prolog gave rise to the programming languages ALF, Fril, Gödel, Mercury, Oz, Ciao, Visual Prolog, XSB, and λProlog, as well as a variety of concurrent logic programming languages, constraint logic programming languages and datalog.


== Concepts ==


=== Logic and control ===

Logic programming can be viewed as controlled deduction. An important concept in logic programming is the separation of programs into their logic component and their control component. With pure logic programming languages, the logic component alone determines the solutions produced. The control component can be varied to provide alternative ways of executing a logic program. This notion is captured by the slogan
Algorithm = Logic + Control
where ""Logic"" represents a logic program and ""Control"" represents different theorem-proving strategies.


=== Problem solving ===
In the simplified, propositional case in which a logic program and a top-level atomic goal contain no variables, backward reasoning determines an and-or tree, which constitutes the search space for solving the goal. The top-level goal is the root of the tree. Given any node in the tree and any clause whose head matches the node, there exists a set of child nodes corresponding to the sub-goals in the body of the clause. These child nodes are grouped together by an ""and"". The alternative sets of children corresponding to alternative ways of solving the node are grouped together by an ""or"".
Any search strategy can be used to search this space. Prolog uses a sequential, last-in-first-out, backtracking strategy, in which only one alternative and one sub-goal is considered at a time. Other search strategies, such as parallel search, intelligent backtracking, or best-first search to find an optimal solution, are also possible.
In the more general case, where sub-goals share variables, other strategies can be used, such as choosing the subgoal that is most highly instantiated or that is sufficiently instantiated so that only one procedure applies. Such strategies are used, for example, in concurrent logic programming.


=== Negation as failure ===

For most practical applications, as well as for applications that require non-monotonic reasoning in artificial intelligence, Horn clause logic programs need to be extended to normal logic programs, with negative conditions. A clause in a normal logic program has the form:
H :- A1, …, An, not B1, …, not Bn.
and is read declaratively as a logical implication:
H if A1 and … and An and not B1 and … and not Bn.
where H and all the Ai and Bi are atomic formulas. The negation in the negative literals not Bi is commonly referred to as ""negation as failure"", because in most implementations, a negative condition not Bi is shown to hold by showing that the positive condition Bi fails to hold. For example:

Given the goal of finding something that can fly:

there are two candidate solutions, which solve the first subgoal bird(X), namely X = john and X = mary. The second subgoal not abnormal(john) of the first candidate solution fails, because wounded(john) succeeds and therefore abnormal(john) succeeds. However, The second subgoal not abnormal(mary) of the second candidate solution succeeds, because wounded(mary) fails and therefore abnormal(mary) fails. Therefore, X = mary is the only solution of the goal.
Micro-Planner had a construct, called ""thnot"", which when applied to an expression returns the value true if (and only if) the evaluation of the expression fails. An equivalent operator is normally built-in in modern Prolog's implementations. It is normally written as not(Goal) or \+ Goal, where Goal is some goal (proposition) to be proved by the program. This operator differs from negation in first-order logic: a negation such as \+ X == 1 fails when the variable X has been bound to the atom 1, but it succeeds in all other cases, including when X is unbound. This makes Prolog's reasoning non-monotonic: X = 1, \+ X == 1 always fails, while \+ X == 1, X = 1 can succeed, binding X to 1, depending on whether X was initially bound (note that standard Prolog executes goals in left-to-right order).
The logical status of negation as failure was unresolved until Keith Clark [1978] showed that, under certain natural conditions, it is a correct (and sometimes complete) implementation of classical negation with respect to the completion of the program. Completion amounts roughly to regarding the set of all the program clauses with the same predicate on the left hand side, say
H :- Body1.
…
H :- Bodyk.
as a definition of the predicate
H iff (Body1 or … or Bodyk)
where ""iff"" means ""if and only if"". Writing the completion also requires explicit use of the equality predicate and the inclusion of a set of appropriate axioms for equality. However, the implementation of negation by failure needs only the if-halves of the definitions without the axioms of equality.
For example, the completion of the program above is:
canfly(X) iff bird(X), not abnormal(X).
abnormal(X) iff wounded(X).
bird(X) iff X = john or X = mary.
X = X.
not john = mary.
not mary = john.
The notion of completion is closely related to McCarthy's circumscription semantics for default reasoning, and to the closed world assumption.
As an alternative to the completion semantics, negation as failure can also be interpreted epistemically, as in the stable model semantics of answer set programming. In this interpretation not(Bi) means literally that Bi is not known or not believed. The epistemic interpretation has the advantage that it can be combined very simply with classical negation, as in ""extended logic programming"", to formalise such phrases as ""the contrary can not be shown"", where ""contrary"" is classical negation and ""can not be shown"" is the epistemic interpretation of negation as failure.


=== Knowledge representation ===
The fact that Horn clauses can be given a procedural interpretation and, vice versa, that goal-reduction procedures can be understood as Horn clauses + backward reasoning means that logic programs combine declarative and procedural representations of knowledge. The inclusion of negation as failure means that logic programming is a kind of non-monotonic logic.
Despite its simplicity compared with classical logic, this combination of Horn clauses and negation as failure has proved to be surprisingly expressive. For example, it provides a natural representation for the common-sense laws of cause and effect, as formalised by both the situation calculus and event calculus. It has also been shown to correspond quite naturally to the semi-formal language of legislation. In particular, Prakken and Sartor credit the representation of the British Nationality Act as a logic program with being ""hugely influential for the development of computational representations of legislation, showing how logic programming enables intuitively appealing representations that can be directly deployed to generate automatic inferences"".


== Variants and extensions ==


=== Prolog ===

The programming language Prolog was developed in 1972 by Alain Colmerauer. It emerged from a collaboration between Colmerauer in Marseille and Robert Kowalski in Edinburgh. Colmerauer was working on natural language understanding, using logic to represent semantics and using resolution for question-answering. During the summer of 1971, Colmerauer and Kowalski discovered that the clausal form of logic could be used to represent formal grammars and that resolution theorem provers could be used for parsing. They observed that some theorem provers, like hyper-resolution, behave as bottom-up parsers and others, like SL-resolution (1971), behave as top-down parsers.
It was in the following summer of 1972, that Kowalski, again working with Colmerauer, developed the procedural interpretation of implications. This dual declarative/procedural interpretation later became formalised in the Prolog notation
H :- B1, …, Bn.
which can be read (and used) both declaratively and procedurally. It also became clear that such clauses could be restricted to definite clauses or Horn clauses, where H, B1, …, Bn are all atomic predicate logic formulae, and that SL-resolution could be restricted (and generalised) to LUSH or SLD-resolution. Kowalski's procedural interpretation and LUSH were described in a 1973 memo, published in 1974.
Colmerauer, with Philippe Roussel, used this dual interpretation of clauses as the basis of Prolog, which was implemented in the summer and autumn of 1972. The first Prolog program, also written in 1972 and implemented in Marseille, was a French question-answering system. The use of Prolog as a practical programming language was given great momentum by the development of a compiler by David Warren in Edinburgh in 1977. Experiments demonstrated that Edinburgh Prolog could compete with the processing speed of other symbolic programming languages such as Lisp. Edinburgh Prolog became the de facto standard and strongly influenced the definition of ISO standard Prolog.


=== Abductive logic programming ===
Abductive logic programming is an extension of normal Logic Programming that allows some predicates, declared as abducible predicates, to be ""open"" or undefined. A clause in an abductive logic program has the form:
H :- B1, …, Bn, A1, …, An.
where H is an atomic formula that is not abducible, all the Bi are literals whose predicates are not abducible, and the Ai are atomic formulas whose predicates are abducible. The abducible predicates can be constrained by integrity constraints, which can have the form:
false :- B1, …, Bn.
where the Bi are arbitrary literals (defined or abducible, and atomic or negated). For example:

where the predicate normal is abducible.
Problem solving is achieved by deriving hypotheses expressed in terms of the abducible predicates as solutions of problems to be solved. These problems can be either observations that need to be explained (as in classical abductive reasoning) or goals to be solved (as in normal logic programming). For example, the hypothesis normal(mary) explains the observation canfly(mary). Moreover, the same hypothesis entails the only solution X = mary of the goal of finding something that can fly:

Abductive logic programming has been used for fault diagnosis, planning, natural language processing and machine learning. It has also been used to interpret Negation as Failure as a form of abductive reasoning.


=== Metalogic programming ===
Because mathematical logic has a long tradition of distinguishing between object language and metalanguage, logic programming also allows metalevel programming. The simplest metalogic program is the so-called ""vanilla"" meta-interpreter:

where true represents an empty conjunction, and clause(A,B) means there is an object-level clause of the form A :- B.
Metalogic programming allows object-level and metalevel representations to be combined, as in natural language. It can also be used to implement any logic that is specified by means of inference rules. Metalogic is used in logic programming to implement metaprograms, which manipulate other programs, databases, knowledge bases or axiomatic theories as data.


=== Constraint logic programming ===

Constraint logic programming combines Horn clause logic programming with constraint solving. It extends Horn clauses by allowing some predicates, declared as constraint predicates, to occur as literals in the body of clauses. A constraint logic program is a set of clauses of the form:
H :- C1, …, Cn ◊ B1, …, Bn.
where H and all the Bi are atomic formulas, and the Ci are constraints. Declaratively, such clauses are read as ordinary logical implications:
H if C1 and … and Cn and B1 and … and Bn.
However, whereas the predicates in the heads of clauses are defined by the constraint logic program, the predicates in the constraints are predefined by some domain-specific model-theoretic structure or theory.
Procedurally, subgoals whose predicates are defined by the program are solved by goal-reduction, as in ordinary logic programming, but constraints are checked for satisfiability by a domain-specific constraint-solver, which implements the semantics of the constraint predicates. An initial problem is solved by reducing it to a satisfiable conjunction of constraints.
The following constraint logic program represents a toy temporal database of john's history as a teacher:

Here ≤ and < are constraint predicates, with their usual intended semantics. The following goal clause queries the database to find out when john both taught logic and was a professor:
:- teaches(john, logic, T), rank(john, professor, T).
The solution is 2010 ≤ T, T ≤ 2012.
Constraint logic programming has been used to solve problems in such fields as civil engineering, mechanical engineering, digital circuit verification, automated timetabling, air traffic control, and finance. It is closely related to abductive logic programming.


=== Concurrent logic programming ===

Concurrent logic programming integrates concepts of logic programming with concurrent programming. Its development was given a big impetus in the 1980s by its choice for the systems programming language of the Japanese Fifth Generation Project (FGCS).
A concurrent logic program is a set of guarded Horn clauses of the form:

H :- G1, …, Gn | B1, …, Bn.

The conjunction G1, … , Gn is called the guard of the clause, and | is the commitment operator. Declaratively, guarded Horn clauses are read as ordinary logical implications:

H if G1 and … and Gn and B1 and … and Bn.

However, procedurally, when there are several clauses whose heads H match a given goal, then all of the clauses are executed in parallel, checking whether their guards G1, … , Gn hold. If the guards of more than one clause hold, then a committed choice is made to one of the clauses, and execution proceedes with the subgoals B1, …, Bn of the chosen clause. These subgoals can also be executed in parallel. Thus concurrent logic programming implements a form of ""don't care nondeterminism"", rather than ""don't know nondeterminism"".
For example, the following concurrent logic program defines a predicate shuffle(Left, Right, Merge) , which can be used to shuffle two lists Left and Right, combining them into a single list Merge that preserves the ordering of the two lists Left and Right:

Here, [] represents the empty list, and [Head | Tail] represents a list with first element Head followed by list Tail, as in Prolog. (Notice that the first occurrence of | in the second and third clauses is the list constructor, whereas the second occurrence of | is the commitment operator.) The program can be used, for example, to shuffle the lists [ace, queen, king] and [1, 4, 2] by invoking the goal clause:

The program will non-deterministically generate a single solution, for example Merge = [ace, queen, 1, king, 4, 2].
Arguably, concurrent logic programming is based on message passing and consequently is subject to the same indeterminacy as other concurrent message-passing systems, such as Actors (see Indeterminacy in concurrent computation). Carl Hewitt has argued that, concurrent logic programming is not based on logic in his sense that computational steps cannot be logically deduced. However, in concurrent logic programming, any result of a terminating computation is a logical consequence of the program, and any partial result of a partial computation is a logical consequence of the program and the residual goal (process network). Consequently, the indeterminacy of computations implies that not all logical consequences of the program can be deduced.


=== Concurrent constraint logic programming ===

Concurrent constraint logic programming combines concurrent logic programming and constraint logic programming, using constraints to control concurrency. A clause can contain a guard, which is a set of constraints that may block the applicability of the clause. When the guards of several clauses are satisfied, concurrent constraint logic programming makes a committed choice to the use of only one.


=== Inductive logic programming ===

Inductive logic programming is concerned with generalizing positive and negative examples in the context of background knowledge: machine learning of logic programs. Recent work in this area, combining logic programming, learning and probability, has given rise to the new field of statistical relational learning and probabilistic inductive logic programming.


=== Higher-order logic programming ===
Several researchers have extended logic programming with higher-order programming features derived from higher-order logic, such as predicate variables. Such languages include the Prolog extensions HiLog and λProlog.


=== Linear logic programming ===
Basing logic programming within linear logic has resulted in the design of logic programming languages that are considerably more expressive than those based on classical logic. Horn clause programs can only represent state change by the change in arguments to predicates. In linear logic programming, one can use the ambient linear logic to support state change. Some early designs of logic programming languages based on linear logic include LO [Andreoli & Pareschi, 1991], Lolli, ACL, and Forum [Miller, 1996]. Forum provides a goal-directed interpretation of all of linear logic.


=== Object-oriented logic programming ===
F-logic extends logic programming with objects and the frame syntax.
Logtalk extends the Prolog programming language with support for objects, protocols, and other OOP concepts. Highly portable, it supports most standard-compliant Prolog systems as backend compilers.


=== Transaction logic programming ===
Transaction logic is an extension of logic programming with a logical theory of state-modifying updates. It has both a model-theoretic semantics and a procedural one. An implementation of a subset of Transaction logic is available in the Flora-2 system. Other prototypes are also available.


== See also ==
Boolean satisfiability problem
Constraint logic programming
Datalog
Fril
Functional programming
Fuzzy logic
Inductive logic programming
Logic in computer science (includes Formal methods)
Logic programming languages
Programming paradigm
R++
Reasoning system
Rule-based machine learning
Satisfiability


== References ==


=== General introductions ===
Baral, C.; Gelfond, M. (1994). ""Logic programming and knowledge representation"" (PDF). The Journal of Logic Programming. 19–20: 73–148. doi:10.1016/0743-1066(94)90025-6. 
Robert Kowalski. The Early Years of Logic Programming Kowalski, R. A. (1988). ""The early years of logic programming"" (PDF). Communications of the ACM. 31: 38–43. doi:10.1145/35043.35046. 
Lloyd, J. W. (1987). Foundations of Logic Programming. (2nd edition). Springer-Verlag. 


=== Other sources ===
John McCarthy. Programs with common sense Symposium on Mechanization of Thought Processes. National Physical Laboratory. Teddington, England. 1958.
D. Miller, G. Nadathur, F. Pfenning, A. Scedrov. Uniform proofs as a foundation for logic programming, Annals of Pure and Applied Logic, vol. 51, pp 125–157, 1991.
Ehud Shapiro (Editor). Concurrent Prolog MIT Press. 1987.
James Slagle. Experiments with a Deductive Question-Answering Program CACM. December 1965.


== Further reading ==
Carl Hewitt. Procedural Embedding of Knowledge In Planner IJCAI 1971.
Carl Hewitt. The repeated demise of logic programming and why it will be reincarnated
Evgeny Dantsin, Thomas Eiter, Georg Gottlob, Andrei Voronkov: Complexity and expressive power of logic programming. ACM Comput. Surv. 33(3): 374–425 (2001)
Ulf Nilsson and Jan Maluszynski, Logic, Programming and Prolog


== External links ==
Logic Programming Virtual Library entry
Bibliographies on Logic Programming
Association for Logic Programming (ALP)
Theory and Practice of Logic Programming journal
Logic programming in C++ with Castor
Logic programming in Oz
Prolog Development Center
Racklog: Logic Programming in Racket"
531,Iliffe vector,1696737,2676,"In computer programming, an Iliffe vector, also known as a display, is a data structure used to implement multi-dimensional arrays. An Iliffe vector for an n-dimensional array (where n ≥ 2) consists of a vector (or 1-dimensional array) of pointers to an (n − 1)-dimensional array. They are often used to avoid the need for expensive multiplication operations when performing address calculation on an array element. They can also be used to implement jagged arrays, such as triangular arrays, triangular matrices and other kinds of irregularly shaped arrays. The data structure is named after John K. Iliffe.
Their disadvantages include the need for multiple chained pointer indirections to access an element, and the extra work required to determine the next row in an n-dimensional array to allow an optimising compiler to prefetch it. Both of these are a source of delays on systems where the CPU is significantly faster than main memory.
The Iliffe vector for a 2-dimensional array is simply a vector of pointers to vectors of data, i.e., the Iliffe vector represents the columns of an array where each column element is a pointer to a row vector.
Multidimensional arrays in languages such as Java, Python (multidimensional lists), Ruby, Visual Basic .NET, Perl, PHP, JavaScript, Objective-C (when using NSArray, not a row-major C-style array), Swift, and Atlas Autocode are implemented as Iliffe vectors. Lliffe vectors were used to implement sparse multidimensional arrays in the OLAP product Holos.
Iliffe vectors are contrasted with dope vectors in languages such as Fortran, which contain the stride factors and offset values for the subscripts in each dimension.


== References ==

John K. Iliffe (1961). ""The Use of The Genie System in Numerical Calculations"". Annual Review in Automatic Programming. 2: 25. doi:10.1016/S0066-4138(61)80002-5. 


== See also ==
Dope vector
Jagged array


== Further reading ==
""Chapter 3: Data Structure Mappings"". Compiling Techniques. Associates Technology Literature Applications Society. Retrieved 5 May 2015."
532,Numerical linear algebra,7330660,2670,"Numerical linear algebra is the study of algorithms for performing linear algebra computations, most notably matrix operations, on computers. It is often a fundamental part of engineering and computational science problems, such as image and signal processing, telecommunication, computational finance, materials science simulations, structural biology, data mining, bioinformatics, fluid dynamics, and many other areas. Such software relies heavily on the development, analysis, and implementation of state-of-the-art algorithms for solving various numerical linear algebra problems, in large part because of the role of matrices in finite difference and finite element methods.
Common problems in numerical linear algebra include computing the following: LU decomposition, QR decomposition, singular value decomposition, eigenvalues.


== See also ==
Iterative methods
Numerical analysis, of which numerical linear algebra is a subspecialty
Gaussian elimination, an important algorithm in numerical linear algebra
BLAS and LAPACK, highly optimized computer libraries which implement most basic algorithms in numerical linear algebra
List of numerical analysis software
List of numerical libraries


== References ==
Leader, Jeffery J. (2004). Numerical Analysis and Scientific Computation. Addison Wesley. ISBN 0-201-73499-0. 
Bau III, David; Trefethen, Lloyd N. (1997). Numerical linear algebra. Philadelphia: Society for Industrial and Applied Mathematics. ISBN 978-0-89871-361-9. 
J. H. Wilkinson and C. Reinsch, ""Linear Algebra, volume II of Handbook for Automatic Computation"" SIAM Review 14, 658 (1972).
Golub, Gene H.; van Loan, Charles F. (1996), Matrix Computations, 3rd edition, Johns Hopkins University Press, ISBN 978-0-8018-5414-9


== External links ==
Freely available software for numerical algebra on the web, composed by Jack Dongarra and Hatem Ltaief, University of Tennessee
NAG Library of numerical linear algebra routines"
533,International Association for Pattern Recognition,43501035,2670,"Pattern recognition is a branch of machine learning that focuses on the recognition of patterns and regularities in data, although it is in some cases considered to be nearly synonymous with machine learning. Pattern recognition systems are in many cases trained from labeled ""training"" data (supervised learning), but when no labeled data are available other algorithms can be used to discover previously unknown patterns (unsupervised learning).
The terms pattern recognition, machine learning, data mining and knowledge discovery in databases (KDD) are hard to separate, as they largely overlap in their scope. Machine learning is the common term for supervised learning methods and originates from artificial intelligence, whereas KDD and data mining have a larger focus on unsupervised methods and stronger connection to business use. Pattern recognition has its origins in engineering, and the term is popular in the context of computer vision: a leading computer vision conference is named Conference on Computer Vision and Pattern Recognition. In pattern recognition, there may be a higher interest to formalize, explain and visualize the pattern, while machine learning traditionally focuses on maximizing the recognition rates. Yet, all of these domains have evolved substantially from their roots in artificial intelligence, engineering and statistics, and they've become increasingly similar by integrating developments and ideas from each other.
In machine learning, pattern recognition is the assignment of a label to a given input value. In statistics, discriminant analysis was introduced for this same purpose in 1936. An example of pattern recognition is classification, which attempts to assign each input value to one of a given set of classes (for example, determine whether a given email is ""spam"" or ""non-spam""). However, pattern recognition is a more general problem that encompasses other types of output as well. Other examples are regression, which assigns a real-valued output to each input; sequence labeling, which assigns a class to each member of a sequence of values (for example, part of speech tagging, which assigns a part of speech to each word in an input sentence); and parsing, which assigns a parse tree to an input sentence, describing the syntactic structure of the sentence.
Pattern recognition algorithms generally aim to provide a reasonable answer for all possible inputs and to perform ""most likely"" matching of the inputs, taking into account their statistical variation. This is opposed to pattern matching algorithms, which look for exact matches in the input with pre-existing patterns. A common example of a pattern-matching algorithm is regular expression matching, which looks for patterns of a given sort in textual data and is included in the search capabilities of many text editors and word processors. In contrast to pattern recognition, pattern matching is generally a type of machine learning, although pattern-matching algorithms (especially with fairly general, carefully tailored patterns) can sometimes succeed in providing similar-quality output of the sort provided by pattern-recognition algorithms.


== Overview ==
Pattern recognition is generally categorized according to the type of learning procedure used to generate the output value. Supervised learning assumes that a set of training data (the training set) has been provided, consisting of a set of instances that have been properly labeled by hand with the correct output. A learning procedure then generates a model that attempts to meet two sometimes conflicting objectives: Perform as well as possible on the training data, and generalize as well as possible to new data (usually, this means being as simple as possible, for some technical definition of ""simple"", in accordance with Occam's Razor, discussed below). Unsupervised learning, on the other hand, assumes training data that has not been hand-labeled, and attempts to find inherent patterns in the data that can then be used to determine the correct output value for new data instances. A combination of the two that has recently been explored is semi-supervised learning, which uses a combination of labeled and unlabeled data (typically a small set of labeled data combined with a large amount of unlabeled data). Note that in cases of unsupervised learning, there may be no training data at all to speak of; in other words,and the data to be labeled is the training data.
Note that sometimes different terms are used to describe the corresponding supervised and unsupervised learning procedures for the same type of output. For example, the unsupervised equivalent of classification is normally known as clustering, based on the common perception of the task as involving no training data to speak of, and of grouping the input data into clusters based on some inherent similarity measure (e.g. the distance between instances, considered as vectors in a multi-dimensional vector space), rather than assigning each input instance into one of a set of pre-defined classes. Note also that in some fields, the terminology is different: For example, in community ecology, the term ""classification"" is used to refer to what is commonly known as ""clustering"".
The piece of input data for which an output value is generated is formally termed an instance. The instance is formally described by a vector of features, which together constitute a description of all known characteristics of the instance. (These feature vectors can be seen as defining points in an appropriate multidimensional space, and methods for manipulating vectors in vector spaces can be correspondingly applied to them, such as computing the dot product or the angle between two vectors.) Typically, features are either categorical (also known as nominal, i.e., consisting of one of a set of unordered items, such as a gender of ""male"" or ""female"", or a blood type of ""A"", ""B"", ""AB"" or ""O""), ordinal (consisting of one of a set of ordered items, e.g., ""large"", ""medium"" or ""small""), integer-valued (e.g., a count of the number of occurrences of a particular word in an email) or real-valued (e.g., a measurement of blood pressure). Often, categorical and ordinal data are grouped together; likewise for integer-valued and real-valued data. Furthermore, many algorithms work only in terms of categorical data and require that real-valued or integer-valued data be discretized into groups (e.g., less than 5, between 5 and 10, or greater than 10).


=== Probabilistic classifiers ===

Many common pattern recognition algorithms are probabilistic in nature, in that they use statistical inference to find the best label for a given instance. Unlike other algorithms, which simply output a ""best"" label, often probabilistic algorithms also output a probability of the instance being described by the given label. In addition, many probabilistic algorithms output a list of the N-best labels with associated probabilities, for some value of N, instead of simply a single best label. When the number of possible labels is fairly small (e.g., in the case of classification), N may be set so that the probability of all possible labels is output. Probabilistic algorithms have many advantages over non-probabilistic algorithms:
They output a confidence value associated with their choice. (Note that some other algorithms may also output confidence values, but in general, only for probabilistic algorithms is this value mathematically grounded in probability theory. Non-probabilistic confidence values can in general not be given any specific meaning, and only used to compare against other confidence values output by the same algorithm.)
Correspondingly, they can abstain when the confidence of choosing any particular output is too low.
Because of the probabilities output, probabilistic pattern-recognition algorithms can be more effectively incorporated into larger machine-learning tasks, in a way that partially or completely avoids the problem of error propagation.


=== Number of important feature variables ===
Feature selection algorithms attempt to directly prune out redundant or irrelevant features. A general introduction to feature selection which summarizes approaches and challenges, has been given. The complexity of feature-selection is, because of its non-monotonous character, an optimization problem where given a total of 
  
    
      
        n
      
    
    {\displaystyle n}
   features the powerset consisting of all 
  
    
      
        
          2
          
            n
          
        
        −
        1
      
    
    {\displaystyle 2^{n}-1}
   subsets of features need to be explored. The Branch-and-Bound algorithm does reduce this complexity but is intractable for medium to large values of the number of available features 
  
    
      
        n
      
    
    {\displaystyle n}
  . For a large-scale comparison of feature-selection algorithms see .
Techniques to transform the raw feature vectors (feature extraction) are sometimes used prior to application of the pattern-matching algorithm. For example, feature extraction algorithms attempt to reduce a large-dimensionality feature vector into a smaller-dimensionality vector that is easier to work with and encodes less redundancy, using mathematical techniques such as principal components analysis (PCA). The distinction between feature selection and feature extraction is that the resulting features after feature extraction has taken place are of a different sort than the original features and may not easily be interpretable, while the features left after feature selection are simply a subset of the original features.


== Problem statement (supervised version) ==
Formally, the problem of supervised pattern recognition can be stated as follows: Given an unknown function 
  
    
      
        g
        :
        
          
            X
          
        
        →
        
          
            Y
          
        
      
    
    {\displaystyle g:{\mathcal {X}}\rightarrow {\mathcal {Y}}}
   (the ground truth) that maps input instances 
  
    
      
        
          x
        
        ∈
        
          
            X
          
        
      
    
    {\displaystyle {\boldsymbol {x}}\in {\mathcal {X}}}
   to output labels 
  
    
      
        y
        ∈
        
          
            Y
          
        
      
    
    {\displaystyle y\in {\mathcal {Y}}}
  , along with training data 
  
    
      
        
          D
        
        =
        {
        (
        
          
            x
          
          
            1
          
        
        ,
        
          y
          
            1
          
        
        )
        ,
        …
        ,
        (
        
          
            x
          
          
            n
          
        
        ,
        
          y
          
            n
          
        
        )
        }
      
    
    {\displaystyle \mathbf {D} =\{({\boldsymbol {x}}_{1},y_{1}),\dots ,({\boldsymbol {x}}_{n},y_{n})\}}
   assumed to represent accurate examples of the mapping, produce a function 
  
    
      
        h
        :
        
          
            X
          
        
        →
        
          
            Y
          
        
      
    
    {\displaystyle h:{\mathcal {X}}\rightarrow {\mathcal {Y}}}
   that approximates as closely as possible the correct mapping 
  
    
      
        g
      
    
    {\displaystyle g}
  . (For example, if the problem is filtering spam, then 
  
    
      
        
          
            x
          
          
            i
          
        
      
    
    {\displaystyle {\boldsymbol {x}}_{i}}
   is some representation of an email and 
  
    
      
        y
      
    
    {\displaystyle y}
   is either ""spam"" or ""non-spam""). In order for this to be a well-defined problem, ""approximates as closely as possible"" needs to be defined rigorously. In decision theory, this is defined by specifying a loss function or cost function that assigns a specific value to ""loss"" resulting from producing an incorrect label. The goal then is to minimize the expected loss, with the expectation taken over the probability distribution of 
  
    
      
        
          
            X
          
        
      
    
    {\displaystyle {\mathcal {X}}}
  . In practice, neither the distribution of 
  
    
      
        
          
            X
          
        
      
    
    {\displaystyle {\mathcal {X}}}
   nor the ground truth function 
  
    
      
        g
        :
        
          
            X
          
        
        →
        
          
            Y
          
        
      
    
    {\displaystyle g:{\mathcal {X}}\rightarrow {\mathcal {Y}}}
   are known exactly, but can be computed only empirically by collecting a large number of samples of 
  
    
      
        
          
            X
          
        
      
    
    {\displaystyle {\mathcal {X}}}
   and hand-labeling them using the correct value of 
  
    
      
        
          
            Y
          
        
      
    
    {\displaystyle {\mathcal {Y}}}
   (a time-consuming process, which is typically the limiting factor in the amount of data of this sort that can be collected). The particular loss function depends on the type of label being predicted. For example, in the case of classification, the simple zero-one loss function is often sufficient. This corresponds simply to assigning a loss of 1 to any incorrect labeling and implies that the optimal classifier minimizes the error rate on independent test data (i.e. counting up the fraction of instances that the learned function 
  
    
      
        h
        :
        
          
            X
          
        
        →
        
          
            Y
          
        
      
    
    {\displaystyle h:{\mathcal {X}}\rightarrow {\mathcal {Y}}}
   labels wrongly, which is equivalent to maximizing the number of correctly classified instances). The goal of the learning procedure is then to minimize the error rate (maximize the correctness) on a ""typical"" test set.
For a probabilistic pattern recognizer, the problem is instead to estimate the probability of each possible output label given a particular input instance, i.e., to estimate a function of the form

  
    
      
        p
        (
        
          
            l
            a
            b
            e
            l
          
        
        
          |
        
        
          x
        
        ,
        
          θ
        
        )
        =
        f
        
          (
          
            
              x
            
            ;
            
              θ
            
          
          )
        
      
    
    {\displaystyle p({\rm {label}}|{\boldsymbol {x}},{\boldsymbol {\theta }})=f\left({\boldsymbol {x}};{\boldsymbol {\theta }}\right)}
  
where the feature vector input is 
  
    
      
        
          x
        
      
    
    {\displaystyle {\boldsymbol {x}}}
  , and the function f is typically parameterized by some parameters 
  
    
      
        
          θ
        
      
    
    {\displaystyle {\boldsymbol {\theta }}}
  . In a discriminative approach to the problem, f is estimated directly. In a generative approach, however, the inverse probability 
  
    
      
        p
        (
        
          
            x
          
          
            |
          
          
            
              l
              a
              b
              e
              l
            
          
        
        )
      
    
    {\displaystyle p({{\boldsymbol {x}}|{\rm {label}}})}
   is instead estimated and combined with the prior probability 
  
    
      
        p
        (
        
          
            l
            a
            b
            e
            l
          
        
        
          |
        
        
          θ
        
        )
      
    
    {\displaystyle p({\rm {label}}|{\boldsymbol {\theta }})}
   using Bayes' rule, as follows:

  
    
      
        p
        (
        
          
            l
            a
            b
            e
            l
          
        
        
          |
        
        
          x
        
        ,
        
          θ
        
        )
        =
        
          
            
              p
              (
              
                
                  x
                
                
                  |
                
                
                  
                    l
                    a
                    b
                    e
                    l
                    ,
                    
                      θ
                    
                  
                
              
              )
              p
              (
              
                
                  l
                  a
                  b
                  e
                  l
                  
                    |
                  
                  
                    θ
                  
                
              
              )
            
            
              
                ∑
                
                  L
                  ∈
                  
                    all labels
                  
                
              
              p
              (
              
                x
              
              
                |
              
              L
              )
              p
              (
              L
              
                |
              
              
                θ
              
              )
            
          
        
        .
      
    
    {\displaystyle p({\rm {label}}|{\boldsymbol {x}},{\boldsymbol {\theta }})={\frac {p({{\boldsymbol {x}}|{\rm {label,{\boldsymbol {\theta }}}}})p({\rm {label|{\boldsymbol {\theta }}}})}{\sum _{L\in {\text{all labels}}}p({\boldsymbol {x}}|L)p(L|{\boldsymbol {\theta }})}}.}
  
When the labels are continuously distributed (e.g., in regression analysis), the denominator involves integration rather than summation:

  
    
      
        p
        (
        
          
            l
            a
            b
            e
            l
          
        
        
          |
        
        
          x
        
        ,
        
          θ
        
        )
        =
        
          
            
              p
              (
              
                
                  x
                
                
                  |
                
                
                  
                    l
                    a
                    b
                    e
                    l
                    ,
                    
                      θ
                    
                  
                
              
              )
              p
              (
              
                
                  l
                  a
                  b
                  e
                  l
                  
                    |
                  
                  
                    θ
                  
                
              
              )
            
            
              
                ∫
                
                  L
                  ∈
                  
                    all labels
                  
                
              
              p
              (
              
                x
              
              
                |
              
              L
              )
              p
              (
              L
              
                |
              
              
                θ
              
              )
              d
              ⁡
              L
            
          
        
        .
      
    
    {\displaystyle p({\rm {label}}|{\boldsymbol {x}},{\boldsymbol {\theta }})={\frac {p({{\boldsymbol {x}}|{\rm {label,{\boldsymbol {\theta }}}}})p({\rm {label|{\boldsymbol {\theta }}}})}{\int _{L\in {\text{all labels}}}p({\boldsymbol {x}}|L)p(L|{\boldsymbol {\theta }})\operatorname {d} L}}.}
  
The value of 
  
    
      
        
          θ
        
      
    
    {\displaystyle {\boldsymbol {\theta }}}
   is typically learned using maximum a posteriori (MAP) estimation. This finds the best value that simultaneously meets two conflicting objects: To perform as well as possible on the training data (smallest error-rate) and to find the simplest possible model. Essentially, this combines maximum likelihood estimation with a regularization procedure that favors simpler models over more complex models. In a Bayesian context, the regularization procedure can be viewed as placing a prior probability 
  
    
      
        p
        (
        
          θ
        
        )
      
    
    {\displaystyle p({\boldsymbol {\theta }})}
   on different values of 
  
    
      
        
          θ
        
      
    
    {\displaystyle {\boldsymbol {\theta }}}
  . Mathematically:

  
    
      
        
          
            θ
          
          
            ∗
          
        
        =
        arg
        ⁡
        
          max
          
            θ
          
        
        p
        (
        
          θ
        
        
          |
        
        
          D
        
        )
      
    
    {\displaystyle {\boldsymbol {\theta }}^{*}=\arg \max _{\boldsymbol {\theta }}p({\boldsymbol {\theta }}|\mathbf {D} )}
  
where 
  
    
      
        
          
            θ
          
          
            ∗
          
        
      
    
    {\displaystyle {\boldsymbol {\theta }}^{*}}
   is the value used for 
  
    
      
        
          θ
        
      
    
    {\displaystyle {\boldsymbol {\theta }}}
   in the subsequent evaluation procedure, and 
  
    
      
        p
        (
        
          θ
        
        
          |
        
        
          D
        
        )
      
    
    {\displaystyle p({\boldsymbol {\theta }}|\mathbf {D} )}
  , the posterior probability of 
  
    
      
        
          θ
        
      
    
    {\displaystyle {\boldsymbol {\theta }}}
  , is given by

  
    
      
        p
        (
        
          θ
        
        
          |
        
        
          D
        
        )
        =
        
          [
          
            
              ∏
              
                i
                =
                1
              
              
                n
              
            
            p
            (
            
              y
              
                i
              
            
            
              |
            
            
              
                x
              
              
                i
              
            
            ,
            
              θ
            
            )
          
          ]
        
        p
        (
        
          θ
        
        )
        .
      
    
    {\displaystyle p({\boldsymbol {\theta }}|\mathbf {D} )=\left[\prod _{i=1}^{n}p(y_{i}|{\boldsymbol {x}}_{i},{\boldsymbol {\theta }})\right]p({\boldsymbol {\theta }}).}
  
In the Bayesian approach to this problem, instead of choosing a single parameter vector 
  
    
      
        
          
            θ
          
          
            ∗
          
        
      
    
    {\displaystyle {\boldsymbol {\theta }}^{*}}
  , the probability of a given label for a new instance 
  
    
      
        
          x
        
      
    
    {\displaystyle {\boldsymbol {x}}}
   is computed by integrating over all possible values of 
  
    
      
        
          θ
        
      
    
    {\displaystyle {\boldsymbol {\theta }}}
  , weighted according to the posterior probability:

  
    
      
        p
        (
        
          
            l
            a
            b
            e
            l
          
        
        
          |
        
        
          x
        
        )
        =
        ∫
        p
        (
        
          
            l
            a
            b
            e
            l
          
        
        
          |
        
        
          x
        
        ,
        
          θ
        
        )
        p
        (
        
          θ
        
        
          |
        
        
          D
        
        )
        d
        ⁡
        
          θ
        
        .
      
    
    {\displaystyle p({\rm {label}}|{\boldsymbol {x}})=\int p({\rm {label}}|{\boldsymbol {x}},{\boldsymbol {\theta }})p({\boldsymbol {\theta }}|\mathbf {D} )\operatorname {d} {\boldsymbol {\theta }}.}
  


=== Frequentist or Bayesian approach to pattern recognition ===
The first pattern classifier – the linear discriminant presented by Fisher – was developed in the frequentist tradition. The frequentist approach entails that the model parameters are considered unknown, but objective. The parameters are then computed (estimated) from the collected data. For the linear discriminant, these parameters are precisely the mean vectors and the covariance matrix. Also the probability of each class 
  
    
      
        p
        (
        
          
            l
            a
            b
            e
            l
          
        
        
          |
        
        
          θ
        
        )
      
    
    {\displaystyle p({\rm {label}}|{\boldsymbol {\theta }})}
   is estimated from the collected dataset. Note that the usage of 'Bayes rule' in a pattern classifier does not make the classification approach Bayesian.
Bayesian statistics has its origin in Greek philosophy where a distinction was already made between the 'a priori' and the 'a posteriori' knowledge. Later Kant defined his distinction between what is a priori known – before observation – and the empirical knowledge gained from observations. In a Bayesian pattern classifier, the class probabilities 
  
    
      
        p
        (
        
          
            l
            a
            b
            e
            l
          
        
        
          |
        
        
          θ
        
        )
      
    
    {\displaystyle p({\rm {label}}|{\boldsymbol {\theta }})}
   can be chosen by the user, which are then a priori. Moreover, experience quantified as a priori parameter values can be weighted with empirical observations – using e.g., the Beta- (conjugate prior) and Dirichlet-distributions. The Bayesian approach facilitates a seamless intermixing between expert knowledge in the form of subjective probabilities, and objective observations.
Probabilistic pattern classifiers can be used according to a frequentist or a Bayesian approach.


== Uses ==

Within medical science, pattern recognition is the basis for computer-aided diagnosis (CAD) systems. CAD describes a procedure that supports the doctor's interpretations and findings. Other typical applications of pattern recognition techniques are automatic speech recognition, classification of text into several categories (e.g., spam/non-spam email messages), the automatic recognition of handwritten postal codes on postal envelopes, automatic recognition of images of human faces, or handwriting image extraction from medical forms. The last two examples form the subtopic image analysis of pattern recognition that deals with digital images as input to pattern recognition systems.
Optical character recognition is a classic example of the application of a pattern classifier, see OCR-example. The method of signing one's name was captured with stylus and overlay starting in 1990. The strokes, speed, relative min, relative max, acceleration and pressure is used to uniquely identify and confirm identity. Banks were first offered this technology, but were content to collect from the FDIC for any bank fraud and did not want to inconvenience customers..
Artificial neural networks (neural net classifiers) and deep learning have many real-world applications in image processing, a few examples:
identification and authentication: e.g., license plate recognition, fingerprint analysis and face detection/verification;
medical diagnosis: e.g., screening for cervical cancer (Papnet) or breast tumors;
defence: various navigation and guidance systems, target recognition systems, shape recognition technology etc.
For a discussion of the aforementioned applications of neural networks in image processing, see e.g.
In psychology, pattern recognition (making sense of and identifying objects) is closely related to perception, which explains how the sensory inputs humans receive are made meaningful. Pattern recognition can be thought of in two different ways: the first being template matching and the second being feature detection. A template is a pattern used to produce items of the same proportions. The template-matching hypothesis suggests that incoming stimuli are compared with templates in the long term memory. If there is a match, the stimulus is identified. Feature detection models, such as the Pandemonium system for classifying letters (Selfridge, 1959), suggest that the stimuli are broken down into their component parts for identification. For example, a capital E has three horizontal lines and one vertical line.


== Algorithms ==
Algorithms for pattern recognition depend on the type of label output, on whether learning is supervised or unsupervised, and on whether the algorithm is statistical or non-statistical in nature. Statistical algorithms can further be categorized as generative or discriminative.


=== Classification algorithms (supervised algorithms predicting categorical labels) ===

Parametric:
Linear discriminant analysis
Quadratic discriminant analysis
Maximum entropy classifier (aka logistic regression, multinomial logistic regression): Note that logistic regression is an algorithm for classification, despite its name. (The name comes from the fact that logistic regression uses an extension of a linear regression model to model the probability of an input being in a particular class.)
Nonparametric:
Decision trees, decision lists
Kernel estimation and K-nearest-neighbor algorithms
Naive Bayes classifier
Neural networks (multi-layer perceptrons)
Perceptrons
Support vector machines
Gene expression programming


=== Clustering algorithms (unsupervised algorithms predicting categorical labels) ===

Categorical mixture models
Deep learning methods
Hierarchical clustering (agglomerative or divisive)
K-means clustering
Correlation clustering
Kernel principal component analysis (Kernel PCA)


=== Ensemble learning algorithms (supervised meta-algorithms for combining multiple learning algorithms together) ===

Boosting (meta-algorithm)
Bootstrap aggregating (""bagging"")
Ensemble averaging
Mixture of experts, hierarchical mixture of experts


=== General algorithms for predicting arbitrarily-structured (sets of) labels ===
Bayesian networks
Markov random fields


=== Multilinear subspace learning algorithms (predicting labels of multidimensional data using tensor representations) ===
Unsupervised:
Multilinear principal component analysis (MPCA)


=== Real-valued sequence labeling algorithms (predicting sequences of real-valued labels) ===

Supervised (?):
Kalman filters
Particle filters


=== Regression algorithms (predicting real-valued labels) ===

Supervised:
Gaussian process regression (kriging)
Linear regression and extensions
Neural networks and Deep learning methods
Unsupervised:
Independent component analysis (ICA)
Principal components analysis (PCA)


=== Sequence labeling algorithms (predicting sequences of categorical labels) ===
Supervised:
Conditional random fields (CRFs)
Hidden Markov models (HMMs)
Maximum entropy Markov models (MEMMs)
Recurrent neural networks
Unsupervised:
Hidden Markov models (HMMs)


== See also ==


== References ==
This article is based on material taken from the Free On-line Dictionary of Computing prior to 1 November 2008 and incorporated under the ""relicensing"" terms of the GFDL, version 1.3 or later.


== Further reading ==
Fukunaga, Keinosuke (1990). Introduction to Statistical Pattern Recognition (2nd ed.). Boston: Academic Press. ISBN 0-12-269851-7. 
Hornegger, Joachim; Paulus, Dietrich W. R. (1999). Applied Pattern Recognition: A Practical Introduction to Image and Speech Processing in C++ (2nd ed.). San Francisco: Morgan Kaufmann Publishers. ISBN 3-528-15558-2. 
Schuermann, Juergen (1996). Pattern Classification: A Unified View of Statistical and Neural Approaches. New York: Wiley. ISBN 0-471-13534-8. 
Godfried T. Toussaint, ed. (1988). Computational Morphology. Amsterdam: North-Holland Publishing Company. 
Kulikowski, Casimir A.; Weiss, Sholom M. (1991). Computer Systems That Learn: Classification and Prediction Methods from Statistics, Neural Nets, Machine Learning, and Expert Systems. Machine Learning. San Francisco: Morgan Kaufmann Publishers. ISBN 1-55860-065-5. 
Jain, Anil.K.; Duin, Robert.P.W.; Mao, Jianchang (2000). ""Statistical pattern recognition: a review"". IEEE Transactions on Pattern Analysis and Machine Intelligence. 22 (1): 4–37. doi:10.1109/34.824819. 
An introductory tutorial to classifiers (introducing the basic terms, with numeric example)


== External links ==
The International Association for Pattern Recognition
List of Pattern Recognition web sites
Journal of Pattern Recognition Research
Pattern Recognition Info
Pattern Recognition (Journal of the Pattern Recognition Society)
International Journal of Pattern Recognition and Artificial Intelligence
International Journal of Applied Pattern Recognition
Open Pattern Recognition Project, intended to be an open source platform for sharing algorithms of pattern recognition
Improved Fast Pattern Matching Improved Fast Pattern Matching"
534,Two-level scheduling,1068363,2661,"Two-level scheduling is a computer science term to describe a method to more efficiently perform process scheduling that involves swapped out processes.
Consider this problem: A system contains 50 running processes all with equal priority. However, the system's memory can only hold 10 processes in memory simultaneously. Therefore, there will always be 40 processes swapped out written on virtual memory on the hard disk. The time taken to swap out and swap in a process is 50 ms respectively.
With straightforward Round-robin scheduling, every time a context switch occurs, a process would need to be swapped in (because only the 10 least recently used processes are swapped in). Choosing randomly among the processes would diminish the probability to 80% (40/50). If that occurs, then obviously a process also need to be swapped out. Swapping in and out of is costly, and the scheduler would waste much of its time doing unneeded swaps.
That is where two-level scheduling enters the picture. It uses two different schedulers, one lower-level scheduler which can only select among those processes in memory to run. That scheduler could be a Round-robin scheduler. The other scheduler is the higher-level scheduler whose only concern is to swap in and swap out processes from memory. It does its scheduling much less often than the lower-level scheduler since swapping takes so much time.
Thus, the higher-level scheduler selects among those processes in memory that have run for a long time and swaps them out. They are replaced with processes on disk that have not run for a long time. Exactly how it selects processes is up to the implementation of the higher-level scheduler. A compromise has to be made involving the following variables:
Response time: A process should not be swapped out for too long. Then some other process (or the user) will have to wait needlessly long. If this variable is not considered resource starvation may occur and a process may not complete at all.
Size of the process: Larger processes must be subject to fewer swaps than smaller ones because they take longer time to swap. Because they are larger, fewer processes can share the memory with the process.
Priority: The higher the priority of the process, the longer it should stay in memory so that it completes faster.


== References ==
Tanenbaum, Albert Woodhull, Operating Systems: Design and Implementation, p.92"
535,Primary clustering,2343202,2661,"In computer programming, primary clustering is one of two major failure modes of open addressing based hash tables, especially those using linear probing. It occurs after a hash collision causes two of the records in the hash table to hash to the same position, and causes one of the records to be moved to the next location in its probe sequence. Once this happens, the cluster formed by this pair of records is more likely to grow by the addition of even more colliding records, regardless of whether the new records hash to the same location as the first two. This phenomenon causes searches for keys within the cluster to be longer.
For instance, in linear probing, a record involved in a collision is always moved to the next available hash table cell subsequent to the position given by its hash function, creating a contiguous cluster of occupied hash table cells. Whenever another record is hashed to anywhere within the cluster, it grows in size by one cell. Because of this phenomenon, it is likely that a linear-probing hash table with a constant load factor (that is, with the size of the table proportional to the number of items it stores) will have some clusters of logarithmic length, and will take logarithmic time to search for the keys within that cluster.
A related phenomenon, secondary clustering, occurs more generally with open addressing modes including linear probing and quadratic probing in which the probe sequence is independent of the key, as well as in hash chaining. In this phenomenon, a low-quality hash function may cause many keys to hash to the same location, after which they all follow the same probe sequence or are placed in the same hash chain as each other, causing them to have slow access times.
Both types of clustering may be reduced by using a higher-quality hash function, or by using a hashing method such as double hashing that is less susceptible to clustering.


== References =="
536,"IEEE Systems, Man, and Cybernetics Society",22774583,2653,"The IEEE Systems, Man, and Cybernetics Society (IEEE SMC) is a professional society of the IEEE.


== History ==
The earliest incarnation of the IEEE Systems, Man, and Cybernetics Society was the IRE Professional Group on Human Factors in Electronics, formed in 1958. The Group would later change its name to IEEE Professional Technical Group on Human Factors in Electronics (1963), IEEE Human Factors in Electronics Group (1964), Man-Machine Systems Group (1968), Systems, Man and Cybernetics Group (1970), and finally Systems, Man and Cybernetics Society (1972).


== Fields of interest ==
The society's main foci are:
systems engineering and the development of its technology including ""problem definition methods, modeling, and simulation, methods of system experimentation, human factors engineering, data and methods, systems design techniques and test and evaluation methods"".
the integration of ""theories of communications, control, cybernetics, stochastics, optimization, and system structure towards the formulation of a general theory of systems"", and
the application of these theories at ""hardware and software levels to the analysis and design of biological, ecological, socio-economic, social service, computer information, and operational man-machine systems"".


== Publications ==
The IEEE SMC Society publishes three peer-reviewed journals called ""IEEE Transactions on Systems, Man, and Cybernetics"", Part A, Part B, and Part C. Part A is devoted to systems and humans, and Part B to cybernetics.


== Conferences ==
The society sponsors a range of annual conferences around the world that focuses on its Field of Interest.


== References =="
537,Application discovery and understanding,4039688,2651,"Application discovery and understanding (ADU) is the process of automatically analyzing artifacts of a software application and determining metadata structures associated with the application in the form of lists of data elements and business rules. The relationships discovered between this application and a central metadata registry is then stored in the metadata registry itself.


== Business benefits of ADU ==
ADU saves a great deal of time and expense for organizations that are involved in the change control and impact analysis of complex computer systems.
Impact analysis allows managers to know that if specific structures are changed or removed altogether, what the impact of those changes might be to enterprise-wide systems.
The process of gaining application understanding is greatly accelerated when the extracted metadata is displayed using interactive diagrams. When a developer can browse the metadata, and drill down into relevant details on demand, then application understanding is achieved in a way that is natural to the developer. Significant reductions in the effort and time required to perform full impact analysis have been reported when ADU tools are implemented. ADU tools are especially beneficial to newly hired developers. A newly hired developer will be productive much sooner and will require less assistance from the existing staff when ADU tools are in place.


== ADU process ==
ADU software is usually written to scan the following application structures:
Data structures of all kinds
Application source code
User interfaces (searching for labels of forms)
Reports
The output of the ADU process frequently includes:
Lists of previously registered data elements discovered within an application
List of unregistered data elements discovered
Note that a registered data element is any data element that already exists within a metadata registry.


== See also ==
metadata
metadata registry
data element


== Software ==
ASG Becubic
Application Understanding from Advanced
IBM Infosphere
X-Analysis by Fresche


== Related ==
Configuration Management"
538,Traversed edges per second,35635446,2645,"The number of traversed edges per second that can be performed by a supercomputer cluster is a measure of both the communications capabilities and computational power of the machine. This is in contrast to the more standard metric of floating point operations per second (FLOPS), which does not give any weight to the communication capabilities of the machine. The term first entered usage in 2010 with the advent of petascale computing, and has since been measured for many of the world's largest supercomputers.
In this context, an edge is a connection between two vertices on a graph, and the traversal is the ability of the machine to communicate data between these two points. The standardized benchmark associated with Graph500, as of September, 2011, calls for executing graph generation and search algorithms on graphs as large as 1.1 Petabyte.
The ability of an application to utilize a supercomputer cluster effectively depends not only on the raw speed of each processor, but also on the communication network. The importance of communication capability varies from application to application, but it is clear that the LINPACK benchmarks traditionally used for rating the FLOPS of supercomputers do not require the same communications capability as many scientific applications. Therefore, alternative metrics that characterize the performance of a machine in a more holistic manner may be more relevant for many scientific applications, and may be desirable for making purchasing decisions.


== See also ==
TOP500
Graph500
HPCG benchmark
Criticism of LINPACK benchmarks


== References =="
539,Sidney Michaelson,55209041,2641,"Sidney Michaelson FRSE FIMA FSA FBCS (1925–1991) was Scotland's first professor of Computer Science. He was joint founder of the Institute of Mathematics and its Applications. As an author he is remembered for his analysis of the Bible.


== Life ==
He was born on 5 December 1925 in the East End of London into a relatively poor family. Academically brilliant he won a scholarship to Imperial College, London. He studied Mathematics and graduated in 1946. After several years of postgraduate study looking at electrical applications in mathematics he began lecturing at Imperial College in 1949. In 1963 he moved to Edinburgh University as Director of its newly founded Computer Unit and in 1969 became the first Professor of Computer Science.
Notable students included Rosemary Candlin.
In 1969 he was elected a Fellow of the Royal Society of Edinburgh. His proposers were Nicholas Kemmer, David Finney, Sir Michael Swann and Arthur Erdelyi.
He died in Edinburgh on 21 February 1991.


== Family ==
His wife Kitty died in 1955. They had four children.


== Recognition ==
In 1991 Edinburgh University created the Sidney Michaelson Prize in Computer Science his honour.
Michaelson Square in Livingston is named in his memory.


== Publications ==
A Critical Concordance of I and II Corinthians (1979)
A Critical Concordance of the Letter of Paul to the Romans (1977)


== References =="
540,Eckert–Mauchly Award,4466513,2636,"The Eckert–Mauchly Award recognizes contributions to digital systems and computer architecture. It is known as the computer architecture community’s most prestigious award. First awarded in 1979, it was named for John Presper Eckert and John William Mauchly, who between 1943 and 1946 collaborated on the design and construction of the first large scale electronic computing machine, known as ENIAC, the Electronic Numerical Integrator and Computer. A certificate and $5,000 are awarded jointly by the Association for Computing Machinery (ACM) and the IEEE Computer Society for outstanding contributions to the field of computer and digital systems architecture.


== Recipients ==
1979 Robert S. Barton
1980 Maurice V. Wilkes
1981 Wesley A. Clark
1982 Gordon C. Bell
1983 Tom Kilburn
1984 Jack B. Dennis
1985 John Cocke
1986 Harvey G. Cragon
1987 Gene M. Amdahl
1988 Daniel P. Siewiorek
1989 Seymour Cray
1990 Kenneth E. Batcher
1991 Burton J. Smith
1992 Michael J. Flynn
1993 David J. Kuck
1994 James E. Thornton
1995 John Crawford
1996 Yale Patt
1997 Robert Tomasulo
1998 T. Watanabe
1999 James E. Smith
2000 Edward Davidson
2001 John Hennessy
2002 Bantwal Ramakrishna ""Bob"" Rau
2003 Joseph A. (Josh) Fisher
2004 Frederick P. Brooks
2005 Robert P. Colwell
2006 James H. Pomerene
2007 Mateo Valero
2008 David Patterson
2009 Joel Emer
2010 Bill Dally
2011 Gurindar S. Sohi
2012 Algirdas Avizienis
2013 James R. ""Jim"" Goodman
2014 Trevor Mudge
2015 Norman Jouppi
2016 Uri Weiser
2017 Charles P. ""Chuck"" Thacker


== See also ==

ACM Special Interest Group on Computer Architecture
Computer engineering
Computer science
Computing


== References ==
Eckert–Mauchly Award winners
IEEE Computer Society Awards
ACM-IEEE CS Eckert-Mauchly Award"
541,Dynamic data,4169622,2632,"In the context of programming, or if the conversation can safely assume what is the time scale of the data : Dynamic data or transactional data denotes information that is asynchronously changed as further updates to the information becomes available.
The opposite of this is persistent data, which is data that is infrequently accessed and not likely to be modified. Dynamic data is also different from streaming data, in that there is no constant flow of information. Rather, updates may come at any time, with periods of inactivity in between.
In the context of enterprise systems data management, ""dynamic"" data is still likely to be transactional, but not only limited to financial or business transactions. It may also include engineering transactions, such as a revised schematic diagram or architectural document. In this context dynamic data would be the opposite of static. Static data being either unchanged or so rarely changed that it is okay to be in the ""basement"" (far storage), whereas ""dynamic data"" would be reused or changed frequently and therefore needs to be kept in office proper (""near"" storage). An original copy of a wiring schematic can change from ""Dynamic"" to ""Static"" as the new versions make it obsolete. It is still possible to reuse the original, but in the normal flow of business there is no need or very rarely a need to access obsoleted data. While the current version of the wiring schematic is considered ""dynamic"" (changeable and changing).
These two slightly different contexts are not opposed, rather quite similar, the difference being the ""time window"". The original definition was okay but did not address change of classification from Dynamic to Static (persistent). And did not clarify that ""transaction"" includes any type of ""applied work"" on a data object, therefore the reader could have mistakenly believed it was ""business transaction"" or ""financial transaction"".
Persistent data is or is likely to be in the context of the execution of a program. Static data is in the context of the business historical data, regardless of any one application or program. The ""dynamic"" data is the new/updated/revised/deleted data in both cases, but again over different time horizons. Your paycheck stub is dynamic data for 1 week, or 1 day, then it becomes read-only and read-rarely, which would be either and both static and persistent.


== See also ==
Transaction data"
542,Computational geophysics,18539917,2632,"Computational geophysics entails rapid numerical computations that help analyses of geophysical data and observations. High-performance computing is involved, due to the size and complexity of the geophysical data to be processed. The main computing demanding tasks are 3D and 4D images building of the earth sub-surface, Modeling and Migration of complex media, Tomography and inverse problems.
In Canada, Computational geophysics is offered as a university major in the form of a BSc (Hon.) with Co-op at Carleton University. Elsewhere, Rice University has a Center for Computational Geophysics, while Princeton University, the University of Texas and Albert-Ludwigs-Universität have similar programs. Seoul National University has offered postdoctoral positions in the field, while experts, laboratories, projects, internships, undergrad/graduate programs and/or facilities in the program exist at the University of Queensland, Wyoming University, Boston University, Stanford University, Uppsala University, Kansas State University, Kingston University, Australian National University, University of California, San Diego, University of Washington, ETH Zurich, University of Sydney, Appalachian State University, University of Minnesota, University of Tasmania, Bahria University, Boise State University, University of Michigan, University of Oulu, University of Utah, and others.


== References ==


== See also ==
Exploration geophysics
Geochemistry
Numerical weather prediction"
543,Edge loop,3363313,2627,"An edge loop, in computer graphics, can loosely be defined as a set of connected edges across a surface. Usually the last edge meets again with the first edge, thus forming a loop. The set or string of edges can for example be the outer edges of a flat surface or the edges surrounding a 'hole' in a surface.
In a stricter sense an edge loop is defined as a set of edges where the loop follows the middle edge in every 'four way junction'. The loop will end when it encounters another type of junction (three or five way for example). Take an edge on a mesh surface for example, say at one end of the edge it connects with three other edges, making a four way junction. If you follow the middle 'road' each time you would either end up with a completed loop or the edge loop would end at another type of junction.
Edge loops are especially practical in organic models which need to be animated. In organic modeling edge loops play a vital role in proper deformation of the mesh. A properly modeled mesh will take into careful consideration the placement and termination of these edge loops. Generally edge loops follow the structure and contour of the muscles that they mimic. For example, in modeling a human face edge loops should follow the orbicularis oculi muscle around the eyes and the orbicularis oris muscle around the mouth. The hope is that by mimicking the way the muscles are formed they also aid in the way the muscles are deformed by way of contractions and expansions. An edge loop closely mimics how real muscles work, and if built correctly, provides control over contour and silhouette in any position.


== References ==


== External links ==
""Edge Loop"". CG Society. Archived from the original on 2011-03-24. Retrieved 2017-07-04. 
[1], Original Description from 2000"
544,Proteins@home,9783078,2620,"Proteins@home (""Proteins at home"") was one of many distributed computing projects that used the BOINC architecture. The project was run by the Department of Biology at École Polytechnique. The project began on December 28, 2006 and ended in June 2008.


== Purpose ==
Proteins@Home was a large-scale non-profit protein structure prediction project utilizing distributed computing to perform a lot of computations in a small amount of time. From their website:
The amino acid sequence of a protein determines its three-dimensional structure, or 'fold'. Conversely, the three-dimensional structure is compatible with a large, but limited set of amino acid sequences. Enumerating the allowed sequences for a given fold is known as the 'inverse protein folding problem'. We are working to solve this problem for a large number of known protein folds (a representative subset: about 1500 folds). The most expensive step is to build a database of energy functions that describe all these structures. For each structure, we consider all possible sequences of amino acids. Surprisingly, this is computationally tractable, because our energy functions are sums over pairs of interactions. Once this is done, we can explore the space of amino acid sequences in a fast and efficient way, and retain the most favorable sequences. This large-scale mapping of protein sequence space will have applications for predicting protein structure and function, for understanding protein evolution, and for designing new proteins. By joining the project, you will help to build the database of energy functions and advance an important area of science with potential biomedical applications.


== Statistics ==
On March 25, 2007, Proteins@Home had 6040 of 9548 users actively participating with a total of 10405 of 15381 computers contributing. There were approximately 479 active teams spread across 100 countries.


== See also ==
A number of other distributed computing projects are also contributing to the protein folding idea. Some of these projects are:
Folding@home
Predictor@home
Rosetta@home
SIMAP
TANPAKU
Human Proteome Folding Project
POEM@Home


== External links ==
Proteins@home"
545,Attentive user interface,3370995,2617,"Attentive user interfaces (AUI) are user interfaces that manage the user's attention. For instance, an AUI can manage notifications (Horvitz et al. 2003), deciding when to interrupt the user, the kind of warnings, and the level of detail of the messages presented to the user. Attentive user interfaces, by generating only the relevant information, can in particular be used to display information in a way that increase the effectiveness of the interaction (Huberman & Wu 2008).
According to Vertegaal, there are four main types of attentive user interfaces (Vertegaal 2003) (Vertegaal et al. 2006):
Visual attention
Turn management
Interruption decision interfaces
Visual detail management interfaces


== See also ==
Adaptive hypermedia
Attention management
Principles of attention stress


== References ==
Horvitz, E.; Kadie, C. M.; Paek, T.; Hovel, D. (2003). ""Models of Attention in Computing and Communications: From Principles to Applications"". Communications of the ACM. 46 (3): 52–59. doi:10.1145/636772.636798. 
Huberman, Bernardo A.; Wu, Fang (2008). ""The Economics of Attention: Maximizing User Value in Information Rich Environments"" (PDF). Advances in Complex Systems. 11 (4): 487–496. doi:10.1142/S0219525908001830. 
Vertegaal, Roel (2003). ""Attentive User Interfaces"" (PDF). Communications of the ACM. 46 (3): 30. doi:10.1145/636772.636794. Archived from the original (PDF) on 2010-09-29. 
Vertegaal, Roel; Shell, J. S.; Chen, D.; Mamuji, A. (2006). ""Designing for augmented attention: Towards a framework for attentive user interfaces"". Computers in Human Behavior. 22 (4): 771–789. doi:10.1016/j.chb.2005.12.012."
546,International Conference on Document Analysis and Recognition,44808029,2609,"The International Conference on Document Analysis and Recognition (ICDAR) is an international academic conference which is held every two years in a different city. It is about character and symbol recognition, printed/handwritten text recognition, graphics analysis and recognition, document analysis, document understanding, historical documents and digital libraries, document based forensics, camera and video based scene text analysis.


== History ==
ICDAR is held every second year since 1991. The host country changes every time and the conference has taken place on four different continents so far:


== See also ==
International Association for Pattern Recognition


== References =="
547,Mining software repositories,36810432,2609,"The mining software repositories (MSR) field analyzes the rich data available in software repositories, such as version control repositories, mailing list archives, bug tracking systems, issue tracking systems, etc. to uncover interesting and actionable information about software systems, projects and software engineering.


== Definition ==
Herzig and Zeller define ”mining software archives” as a process to ”obtain lots of initial evidence” by extracting data from software repositories. Further they define ”data sources” as product-based artefacts like source code, requirement artefacts or version archives and claim that these sources are unbiased, but noisy and incomplete.


== Techniques ==


=== Coupled Change Analysis ===
The idea in coupled change analysis is that developers change code entities (e.g. files) together frequently for fixing defects or introducing new features. These couplings between the entities are often not made explicit in the code or other documents. Especially developers new on the project do not know which entities need to be changed together. Coupled change analysis aims to extract the coupling out of the version control system for a project. By the commits and the timing of changes, we might be able to identify which entities frequently change together. This information could then be presented to developers about to change one of the entities to support them in their further changes.


== Contradictory Findings ==


== Software Metrics ==


== See also ==
Software analytics
Software maintenance
Software archaeology


== References ==


== External links ==
Working Conference on Mining Software Repositories, the main software engineering conference in the area."
548,Cooperative distributed problem solving,517197,2598,"In computing cooperative distributed problem solving is a network of semi-autonomous processing nodes working together to solve a problem, typically in a multi-agent system. That is concerned with the investigation of problem subdivision, sub-problem distribution, results synthesis, optimisation of problem solver coherence and co-ordination. It is closely related to distributed constraint programming and distributed constraint optimization; see the links below.


== Aspects of CDPS ==
Neither global control or global data storage – no individual CDPS problem solver (agent) has sufficient information to solve the entire problem.
Control and data are distributed
Communication is slower than computation, therefore:
Loose coupling between problem solvers
Efficient protocols (not too much communication overhead)
problems should be modular, coarse grained

Any unique node is a potential bottleneck
Organised behaviour is hard to guarantee since no one node has the complete picture


== See also ==
Multiscale decision making
Distributed constraint optimization
Distributed artificial intelligence
Multi-agent planning


== Some relevant books ==
Faltings, Boi (2006). ""Distributed Constraint Programming"". In Rossi, Francesca; van Beek, Peter; Walsh, Toby. Handbook of Constraint Programming. Elsevier. ISBN 978-0-444-52726-4.  A chapter in an edited book.
Meisels, Amnon (2008). Distributed Search by Constrained Agents. Springer. ISBN 978-1-84800-040-7. 
Shoham, Yoav; Leyton-Brown, Kevin (2009). Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations. New York: Cambridge University Press. ISBN 978-0-521-89943-7.  See Chapters 1 and 2; downloadable free online.
Yokoo, Makoto (2001). Distributed constraint satisfaction: Foundations of cooperation in multi-agent systems. Springer. ISBN 978-3-540-67596-9."
549,Symbolic Manipulation Program,5903016,2597,"Symbolic Manipulation Program, usually called SMP, was a computer algebra system designed by Chris A. Cole and Stephen Wolfram at Caltech circa 1979 and initially developed in the Caltech physics department under Wolfram's leadership with contributions from Geoffrey C. Fox, Jeffrey M. Greif, Eric D. Mjolsness, Larry J. Romans, Timothy Shaw, and Anthony E. Terrano. It was first sold commercially in 1981 by the Computer Mathematics Corporation of Los Angeles which later became part of Inference Corporation; Inference Corp. further developed the program and marketed it commercially from 1983 to 1988. SMP was essentially Version Zero of the more ambitious Mathematica system.
SMP was influenced by the earlier computer algebra systems Macsyma (of which Wolfram was a user) and Schoonschip (whose code Wolfram studied).


== References ==

Chris A. Cole, Stephen Wolfram, ""SMP: A Symbolic Manipulation Program"", Proceedings of the fourth ACM symposium on Symbolic and algebraic computation (SIGSAM), Snowbird, Utah, 1981. full text
Stephen Wolfram with Chris A. Cole, SMP: A Symbolic Manipulation Program, Reference Manual, California Institute of Technology, 1981; Inference Corporation, 1983. full text
Stephen Wolfram, ""Symbolic Mathematical Computation"", Communications of the ACM, April 1985 (Volume 28, Issue 4). Despite the general-sounding title the focus is on an introduction to SMP. Online version of this article
J.M. Greif, ""The SMP Pattern-Matcher"" in B.F. Caviness (editor), Proceedings of EUROCAL 1985, volume 2, pgs. 303-314, Springer-Verlag Lecture Notes in Computer Science, no. 204, ISBN 3-540-15984-3 A discussion, with examples, of the capabilities, tasks, and design philosophy of the pattern-matcher.
SMP's manual ""SMP Handbook""
Stephen Wolfram's blog post on the history of SMP's creation"
550,Computational group theory,618584,2595,"In mathematics, computational group theory is the study of groups by means of computers. It is concerned with designing and analysing algorithms and data structures to compute information about groups. The subject has attracted interest because for many interesting groups (including most of the sporadic groups) it is impractical to perform calculations by hand.
Important algorithms in computational group theory include:
the Schreier–Sims algorithm for finding the order of a permutation group
the Todd–Coxeter algorithm and Knuth–Bendix algorithm for coset enumeration
the product-replacement algorithm for finding random elements of a group
Two important computer algebra systems (CAS) used for group theory are GAP and Magma. Historically, other systems such as CAS (for character theory) and Cayley (a predecessor of Magma) were important.
Some achievements of the field include:
complete enumeration of all finite groups of order less than 2000
computation of representations for all the sporadic groups


== See also ==
Black box group


== References ==
A survey of the subject by Ákos Seress from Ohio State University, expanded from an article that appeared in the Notices of the American Mathematical Society is available online. There is also a survey by Charles Sims from Rutgers University and an older survey by Joachim Neubüser from RWTH Aachen.
There are three books covering various parts of the subject:
Derek F. Holt, Bettina Eick, Eamonn A. O'Brien, ""Handbook of computational group theory"", Discrete Mathematics and its Applications (Boca Raton). Chapman & Hall/CRC, Boca Raton, Florida, 2005. ISBN 1-58488-372-3
Charles C. Sims, ""Computation with Finitely-presented Groups"", Encyclopedia of Mathematics and its Applications, vol 48, Cambridge University Press, Cambridge, 1994. ISBN 0-521-43213-8
Ákos Seress, ""Permutation group algorithms"", Cambridge Tracts in Mathematics, vol. 152, Cambridge University Press, Cambridge, 2003. ISBN 0-521-66103-X."
551,Branch target predictor,1638477,2594,"In computer architecture, a branch target predictor is the part of a processor that predicts the target of a taken conditional branch or an unconditional branch instruction before the target of the branch instruction is computed by the execution unit of the processor.
Branch target prediction is not the same as branch prediction which attempts to guess whether a conditional branch will be taken or not-taken (i.e., sequential).
In more parallel processor designs, as the instruction cache latency grows longer and the fetch width grows wider, branch target extraction becomes a bottleneck. The recurrence is:
Instruction cache fetches block of instructions
Instructions in block are scanned to identify branches
First predicted taken branch is identified
Target of that branch is computed
Instruction fetch restarts at branch target
In machines where this recurrence takes two cycles, the machine loses one full cycle of fetch after every predicted taken branch. As predicted branches happen every 10 instructions or so, this can force a substantial drop in fetch bandwidth. Some machines with longer instruction cache latencies would have an even larger loss. To ameliorate the loss, some machines implement branch target prediction: given the address of a branch, they predict the target of that branch. A refinement of the idea predicts the start of a sequential run of instructions given the address of the start of the previous sequential run of instructions.
This predictor reduces the recurrence above to:
Hash the address of the first instruction in a run
Fetch the prediction for the addresses of the targets of branches in that run of instructions
Select the address corresponding to the branch predicted taken
As the predictor RAM can be 5–10% of the size of the instruction cache, the fetch happens much faster than the instruction cache fetch, and so this recurrence is much faster. If it were not fast enough, it could be parallelized, by predicting target addresses of target branches.


== External links ==
Branch Target Buffers // EE461
Fog, Agner, The microarchitecture of Intel, AMD and VIA CPUs (PDF), retrieved 9 August 2017"
552,Resource contention,13440885,2564,"In computer science, resource contention is a conflict over access to a shared resource such as random access memory, disk storage, cache memory, internal buses or external network devices. A resource experiencing ongoing contention can be described as oversubscribed.
Resolving resource contention problems is one of the basic functions of operating systems. Various low-level mechanisms can be used to aid this, including locks, semaphores, mutexes and queues. The other techniques that can be applied by the operating systems include intelligent scheduling, application mapping decision, and page colouring.
Access to resources is also sometimes regulated by queuing; in the case of computing time on a CPU the controlling algorithm of the task queue is called a scheduler.
Failure to properly resolve resource contention problems may result in a number of problems, including deadlock, livelock, and thrashing.
Resource contention results when multiple processes attempt to use the same shared resource. Access to memory areas is often controlled by semaphores, which allows a pathological situation called a deadlock, when different threads or processes try to allocate resources already allocated by each other. A deadlock usually leads to a program becoming partially or completely unresponsive.
In recent years, research on the contention is more focused on the resources in the memory hierarchy, e.g., last-level caches, front-side bus, memory socket connection.


== References ==


== See also ==
Bus contention
Resource allocation
Collision avoidance (networking)"
553,Odlyzko–Schönhage algorithm,14099326,2555,"In mathematics, the Odlyzko–Schönhage algorithm is a fast algorithm for evaluating the Riemann zeta function at many points, introduced by (Odlyzko & Schönhage 1988). The main point is the use of the fast Fourier transform to speed up the evaluation of a finite Dirichlet series of length N at O(N) equally spaced values from O(N2) to O(N1+ε) steps (at the cost of storing O(N1+ε) intermediate values). The Riemann–Siegel formula used for calculating the Riemann zeta function with imaginary part T uses a finite Dirichlet series with about N = T1/2 terms, so when finding about N values of the Riemann zeta function it is sped up by a factor of about T1/2. This reduces the time to find the zeros of the zeta function with imaginary part at most T from about T3/2+ε steps to about T1+ε steps.
The algorithm can be used not just for the Riemann zeta function, but also for many other functions given by Dirichlet series.
The algorithm was used by Gourdon (2004) to verify the Riemann hypothesis for the first 1013 zeros of the zeta function.


== References ==
Gourdon, X., Numerical evaluation of the Riemann Zeta-function 
Gourdon (2004), The 1013 first zeros of the Riemann Zeta function, and zeros computation at very large height 
Odlyzko, A. (1992), The 1020-th zero of the Riemann zeta function and 175 million of its neighbors  This unpublished book describes the implementation of the algorithm and discusses the results in detail.
Odlyzko, A. M.; Schönhage, A. (1988), ""Fast algorithms for multiple evaluations of the Riemann zeta function"", Trans. Amer. Math. Soc., 309 (2): 797–809, doi:10.2307/2000939, JSTOR 2000939, MR 0961614"
554,Deadline-monotonic scheduling,2596697,2545,"Deadline-monotonic priority assignment is a priority assignment policy used with fixed priority pre-emptive scheduling.
With deadline-monotonic priority assignment, tasks are assigned priorities according to their deadlines; the task with the shortest deadline being assigned the highest priority.
This priority assignment policy is optimal for a set of periodic or sporadic tasks which comply with the following restrictive system model:
All tasks have deadlines less than or equal to their minimum inter-arrival times (or periods).
All tasks have worst-case execution times (WCET) that are less than or equal to their deadlines.
All tasks are independent and so do not block each other's execution (for example by accessing mutually exclusive shared resources).
No task voluntarily suspends itself.
There is some point in time, referred to as a critical instant, where all of the tasks become ready to execute simultaneously.
Scheduling overheads (switching from one task to another) are zero.
All tasks have zero release jitter (the time from the task arriving to it becoming ready to execute).
If restriction 7 is lifted, then ""deadline minus jitter"" monotonic priority assignment is optimal.
If restriction 1. is lifted allowing deadlines greater than periods, then Audsley's optimal priority assignment algorithm may be used to find the optimal priority assignment.
Deadline monotonic priority assignment is not optimal for fixed priority non-pre-emptive scheduling.
A fixed priority assignment policy P is referred to as optimal if no task set exists which is schedulable using a different priority assignment policy which is not also schedulable using priority assignment policy P. Or in other words: Deadline-monotonic priority assignment (DMPA) policy is optimal if any process set, Q, that is schedulable by priority scheme, W, is also schedulable by DMPA


== See also ==
Rate-monotonic scheduling
Dynamic priority scheduling


== References =="
555,Karen Catlin,53052530,2537,"Karen Smith Catlin (born 1963) is an American tech executive and advocate for women working in the technology industry. She most recently served as a Vice President in the Office of the CTO at Adobe Systems. She is a frequent speaker at technology events.
From 1985-1990, Catlin worked as a software developer on the Intermedia system at Brown University. She then joined Macromedia in 1993 as an early employee, initially responsible for product localization. She went on to develop the program management discipline for the company and was promoted to the vice president level, leading shared engineering services across the entire product line. Catlin worked at Macromedia until 2006, when Adobe Systems acquired the company. She was an executive at Adobe Systems from 2006-2012, responsible for shared engineering services.
Catlin co-authored Present! A Techie's Guide to Public Speaking with Poornima Vijayashanker in 2015.


== References ==


== External links ==
Official website"
556,Software Product Line Conference,56613560,2525,"The Software Product Line Conference is an international conference which is held annually.
The conference was started based on previous conferences, a Software Product Line Conference organised by Software Engineering Institute in the US and Product Family Engineering Workshops.
Previous conferences were held in Denver, Bilbao, San Diego, Siena, Boston, Rennes, Baltimore, Kyoto, Limerick, San Francisco, Jeju Island, Munich , Salvador, Tokyo, Florence, Nashville, Beijing, and Sevilla.
The following companies have been elected in the Hall of Fame for their achievement in software product line engineering: Boeing, Robert Bosch GmbH, CelsiusTech, Cummins, Danfoss, Ericsson, FISCAN, General Motors, Hewlett Packard, HomeAway , Lockheed Martin, LSI Logic, Lucent, Market Maker, Nokia, Philips, Salion, Siemens, Toshiba , U.S. Army Live Training Transformation, U.S. Naval Research Laboratory
Since 2016 an award for the most influential paper has been established. 2016 Don Batory was awarded, 2017 the award was given to Krzysztof Czarnecki, Ulrich Eisenecker of the University of Leipzig.


== References =="
557,List of ACM-W Celebrations,49864594,2516,"This is a list of Celebrations of the Association of Computing Machinery Council on Women in Computing (ACM-W). Celebrations are listed by area.


== Canada ==


== Europe ==


== India ==


== Philippines ==


== Puerto Rico ==


== United Kingdom ==


== United States ==


== See also ==
List of organizations for women in science


== References ==


== External links ==
Official website
ACM-W Celebrations of Women in Computing"
558,Observational equivalence,1064136,2516,"Observational equivalence is the property of two or more underlying entities being indistinguishable on the basis of their observable implications. Thus, for example, two scientific theories are observationally equivalent if all of their empirically testable predictions are identical, in which case empirical evidence cannot be used to distinguish which is closer to being correct; indeed, it may be that they are actually two different perspectives on one underlying theory.
In econometrics, two parameter values (or two structures, from among a class of statistical models) are considered observationally equivalent if they both result in the same probability distribution of observable data. This term often arises in relation to the identification problem.
In the formal semantics of programming languages, two terms M and N are observationally equivalent if and only if, in all contexts C[...] where C[M] is a valid term, it is the case that C[N] is also a valid term with the same value. Thus it is not possible, within the system, to distinguish between the two terms. This definition can be made precise only with respect to a particular calculus, one that comes with its own specific definitions of term, context, and the value of a term.


== See also ==
Underdetermination


== References ==

This article is based on material taken from the Free On-line Dictionary of Computing prior to 1 November 2008 and incorporated under the ""relicensing"" terms of the GFDL, version 1.3 or later."
559,Milner Award,34998030,2509,"The Royal Society Milner Award, supported by Microsoft Research, is given for outstanding achievement in computer science by a European researcher. It replaces the Royal Society and Académie des Sciences Microsoft Award and is named in honour of Robin Milner, a pioneer in computer science.
The recipient should be an active researcher in computer science, excepting Microsoft research employees, who has been resident in Europe for at least 12 months prior to their nomination. The winner of the award receives a medal and a personal prize of £5,000. The winner is invited to deliver a public lecture on their research at the Society.
The recipient is chosen by the Council of the Royal Society on the recommendation of the Milner Award Committee. The Committee is made up of Fellows of the Royal Society, Members of the Académie des sciences (France) and Members of Leopoldina (Germany). Nominations are valid for five years after which the candidate cannot be re-nominated until a year after the nomination has expired.


== Winners ==
Source: Royal Society
2018: Marta Kwiatkowska, in recognition of her contribution to the theoretical and practical development of stochastic and quantitative model checking
2017: Andrew Zisserman, in recognition of his exceptional achievements in computer programming
2016: Xavier Leroy, in recognition of his exceptional achievements in computer programming
2015: Thomas A. Henzinger, for fundamental advances in the theory and practice of formal verification and synthesis of reactive, real-time, and hybrid computer systems
2014: Bernhard Schölkopf, for being a pioneer in machine learning whose work defined the field of “kernel machines” which are widely used in all areas of science and industry.
2013: Serge Abiteboul, for his world leading database research.
2012: Gordon Plotkin, for his fundamental research into programming semantics.


== References =="
560,Sign bit,1068955,2505,"In computer science, the sign bit is a bit in a signed number representation that indicates the sign of a number. Although only signed numeric data types have a sign bit, it is invariably located in the most significant bit position, so the term may be used interchangeably with ""most significant bit"" in some contexts.
Almost always, if the sign bit is 0, the number is non-negative (positive or zero). If the sign bit is 1 then the number is negative, although formats other than two's complement integers allow a signed zero: distinct ""positive zero"" and ""negative zero"" representations, the latter of which does not correspond to the mathematical concept of a negative number.
In the two's complement representation, the sign bit has the weight −2w−1 where w is the number of bits. In the ones' complement representation, the most negative value is 1 − 2w−1, but there are two representations of zero, one for each value of the sign bit. In a sign-and-magnitude representation of numbers, the value of the sign bit determines whether the numerical value is positive or negative.
Floating point numbers, such as IEEE format, IBM format, VAX format, and even the format used by the Zuse Z1 and Z3 use a sign-and-magnitude representation.
When using a complement representation, to convert a signed number to a wider format the additional bits must be filled with copies of the sign bit in order to preserve its numerical value, a process called sign extension or sign propagation.


== References =="
561,International Conference on Web Services,11674624,2501,"The International Conference on Web Services or ICWS is an international forum for researchers and industry practitioners focused on Web services. ICWS is sponsored by Services Society and the IEEE Computer Society. It has an 'A' rating in the Conference Portal - Core and an 'A' rating in the Excellence in Research for Australia.


== Areas of focus ==
ICWS features research papers with a wide range of topics, focusing on various aspects of IT services. Some of the topics include Web services specifications and enhancements, Web services discovery and integration, Web services security, Web services standards and formalizations, Web services modeling, Web services-oriented software engineering, Web services-oriented software testing, Web services-based applications and solutions, Web services realizations, semantics in Web services, and all aspects of Service-Oriented Architecture (SOA) infrastructure.


== History ==
The International Conference on Web Services was founded by Dr. Liang-Jie Zhang in June 2003, Las Vegas, USA. Meanwhile, the first ICWS-Europe 2003 (ICWS-Europe'03), founded by Dr. Liang-Jie Zhang with Prof. Mario Jeckle, was held in Germany in Oct, 2003. In 2004, ICWS-Europe was changed to the European Conference on Web Services (ECOWS), held in Erfurt, Germany. In 2012, ECOWS was formally merged into ICWS. Since then, the entire Services Computing community combined the efforts and focused on one prime international forum for web-based services: ICWS.


== References ==


== External links ==
International Conference on Web Services
Services Society
IEEE Computer Society Technical Committee on Services Computing"
562,Dahl–Nygaard Prize,20981570,2496,"The Dahl–Nygaard Prize is awarded annually to a senior researcher with outstanding career contributions and a younger researcher who has demonstrated great potential. The senior prize is recognized as one of the most prestigious prizes in the area of software engineering, though it is a relatively new prize.
The winners of both awards are announced at the European Conference on Object Oriented Programming (ECOOP). The prizes are named after Ole-Johan Dahl and Kristen Nygaard, two Norwegian pioneers in the area of programming and simulation. The prize was created by Association Internationale pour les Technologies Objets (AITO) in 2004.
The recipients of the prize are:
2018, Lars Bak (senior prize) and Guoqing Harry Xu (junior prize)
2017, Barcelona: Gilad Bracha (senior prize) and Ross Tate (junior prize)
2016, Rome: James Noble (senior prize), and Emina Torlak (junior prize)
2015, Prague: Bjarne Stroustrup (senior prize) and Alexander J. Summers (junior prize)
2014, Uppsala: William Cook (senior prize), Robert France (senior prize), and Tudor Gîrba (junior prize)
2013, Montpellier: Oscar Nierstrasz (senior prize) and Matthew Parkinson (junior prize)
2012, Beijing: Gregor Kiczales (senior prize) and Tobias Wrigstad (junior prize)
2011, Lancaster: Craig Chambers (senior prize) and Atsushi Igarashi (junior prize)
2010, Maribor: Doug Lea (senior prize) and Erik Ernst (junior prize)
2009, Genoa: David Ungar (senior prize)
2008, Paphos: Akinori Yonezawa (senior prize) and Wolfgang De Meuter (junior prize)
2007, Berlin: Luca Cardelli (senior prize) and Jonathan Aldrich (junior prize)
2006, Nantes: Erich Gamma, Richard Helm, Ralph Johnson, and (posthumously) John Vlissides
2005, Glasgow: Bertrand Meyer (senior prize) and Gail Murphy (junior prize)


== References ==


== External links ==
Official website"
563,Symposium on Principles of Programming Languages,7251893,2495,"The annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages (POPL) is an academic conference in the field of computer science, with focus on fundamental principles in the design, definition, analysis, and implementation of programming languages, programming systems, and programming interfaces. The venue is jointly sponsored by the two Association for Computing Machinery Special Interest Groups: SIGPLAN and SIGACT.
According to CiteSeer, it is the venue with one of the highest impact factors in the field of computer science.


== History of the conference ==
44th POPL 2017 in Paris, France
43rd POPL 2016 in St. Petersburg, Florida, United States
42nd POPL 2015 in Mumbai, India
41th POPL 2014 in San Diego, California, United States
40th POPL 2013 in Rome, Italy
39th POPL 2012 in Philadelphia, United States
38th POPL 2011 in Austin, United States
37th POPL 2010 in Madrid, Spain
36th POPL 2009 in Savannah, Georgia, United States
35th POPL 2008 in San Francisco, California, United States
34th POPL 2007 in Nice, France
33rd POPL 2006 in Charleston, South Carolina, United States
32nd POPL 2005 in Long Beach, California, United States
31st POPL 2004 in Wien, Austria


== Affiliated events ==
Declarative Aspects of Multicore Programming (DAMP)
Foundations and Developments of Object-Oriented Languages (FOOL/WOOD)
Partial Evaluation and Semantics-Based Program Manipulation (PEPM)
Practical Applications of Declarative Languages (PADL)
Programming Language Technologies for XML (PLAN-X)
Types in Language Design and Implementation (TLDI)
Verification, Model Checking and Abstract Interpretation (VMCAI)


== See also ==
International Conference on Functional Programming (ICFP)
Programming Language Design and Implementation (PLDI)
POPLmark challenge


== References ==


== External links ==
Official website
Acceptance Rates of Compiler Conferences"
564,Recursive transcompiling,51717632,2494,"Recursive Transpiling (or Recursive Transcompiling) is the process of applying the notion of Transpiling recursively, to create a pipeline of transformations (often starting from a Single Source of Truth) which repeatedly turn one technology into another.
By repeating this process, one can turn A -> B -> C -> D -> E -> F and then back into A(v2). Some information will be preserved through this pipeline, from A -> A(v2), and that information (at an abstract level) demonstrates what each of the components A-F agree on.
In each of the different versions that the Transcompiler pipeline produces, that information is preserved. It might take on many different shapes and sizes, but by the time it comes back to A (v2), having been transcompiled 6 times in the pipeline above, the information returns to its original state.
This information which survives the transform through each format, from A-F-A(v2), is (by definition) derivative content or derivative code.
Recursive transpiling takes advantage of the fact that Transpilers may either keep translated code as close to the source code as possible to ease development and debugging of the original source code, or else they may change the structure of the original code so much, that the translated code does not look like the source code. There are also debugging utilities that map the transpiled source code back to the original code; for example, JavaScript source maps allow mapping of the JavaScript code executed by a web browser back to the original source in a transpiled-to-JavaScript language.


== References =="
565,Event (synchronization primitive),20472593,2492,"In computer science, an event (also called event semaphore) is a type of synchronization mechanism that is used to indicate to waiting processes when a particular condition has become true.
An event is an abstract data type with a boolean state and the following operations:
wait - when executed, causes the suspension of the executing process until the state of the event is set to true. If the state is already set to true has no effect.
set - sets the event's state to true, release all waiting processes.
clear - sets the event's state to false.
Different implementations of events may provide different subsets of these possible operations; for example, the implementation provided by Microsoft Windows provides the operations wait (WaitForObject and related functions), set (SetEvent), and clear (ResetEvent). An option that may be specified during creation of the event object changes the behaviour of SetEvent so that only a single thread is released and the state is automatically returned to false after that thread is released.
Events short of reset function, that is, those which can be completed only once, are known as futures. Monitors are, on the other hand, more general since they combine completion signaling with mutex and do not let the producer and consumer to execute simultaneously in the monitor making it an event+critical section.


== References ==


== External links ==
Event Objects, Microsoft Developer Network
Thread Synchronization Mechanisms in Python"
566,European Joint Conferences on Theory and Practice of Software,23821777,2490,"The European Joint Conferences on Theory and Practice of Software (ETAPS) is a confederation of six computer science conferences taking place annually at one conference site, usually end of March or early April. Three of the six conferences (FoSSaCS, FASE, TACAS) are top ranked in software engineering and two (ESOP, CC) are top ranked conferences on programming languages.


== Constituting conferences ==
ETAPS confederates the following six conferences:
Conference on Principles of Security and Trust (POST, since 2012)
European Symposium on Programming (ESOP, since 1998)
Foundations of Software Science and Computation Structures (FoSSaCS, since 1998)
Fundamental Approaches to Software Engineering (FASE, since 1998)
International Conference on Compiler Construction, (CC, since 1998)
Tools and Algorithms for the Construction and Analysis of Systems (TACAS, since 1998)


=== TACAS ===
TACAS (Tools and Algorithms for the Construction and Analysis of Systems) is a conference that focuses on the application of and tool support for various formal methods. It is one of the top ranked conferences for software engineering. It was founded by Bernhard Steffen, Rance Cleaveland, Ed Brinksma, and Kim Larsen. The first TACAS was held in 1995 in Aarhus, Denmark followed by the conferences in 1996 in Passau, Germany and 1997 in Enschede, Netherlands. TACAS was one of the first five constituting conferences of ETAPS in 1998.


== References ==


== External links ==
Official website
List of previous conferences"
567,IEEE John von Neumann Medal,12953311,2490,"The IEEE John von Neumann Medal was established by the IEEE Board of Directors in 1990 and may be presented annually ""for outstanding achievements in computer-related science and technology."" The achievements may be theoretical, technological, or entrepreneurial, and need not have been made immediately prior to the date of the award.
The medal is named after John von Neumann.


== Recipients ==
The following people have received the IEEE John von Neumann Medal:


== See also ==
John von Neumann Theory Prize awarded by the Institute for Operations Research and the Management Sciences (INFORMS).
List of science and technology awards
Prizes named after people


== References =="
568,Timed automaton,35707540,2484,"In automata theory, a timed automaton is a finite automaton extended with a finite set of real-valued clocks. During a run of a timed automaton, clock values increase all with the same speed. Along the transitions of the automaton, clock values can be compared to integers. These comparisons form guards that may enable or disable transitions and by doing so constrain the possible behaviors of the automaton. Further, clocks can be reset. Timed automata are a sub-class of a type hybrid automata.
Timed automata can be used to model and analyse the timing behavior of computer systems, e.g., real-time systems or networks. Methods for checking both safety and liveness properties have been developed and intensively studied over the last 20 years.
It has been shown that the state reachability problem for timed automata is decidable, which makes this an interesting sub-class of hybrid automata. Extensions have been extensively studied, among them stopwatches, real-time tasks, cost functions, and timed games. There exists a variety of tools to input and analyse timed automata and extensions, including the model checkers UPPAAL, Kronos, and the schedulability analyser TIMES. These tools are becoming more and more mature, but are still all academic research tools.


== Formal definition ==
Formally, a timed automaton is a tuple A = (Q,Σ,C,E,q0) that consists of the following components:
Q is a finite set. The elements of Q are called the states of A.
Σ is a finite set called the alphabet or actions of A.
C is a finite set called the clocks of A.
E ⊆ Q×Σ×B(C)×P(C)×Q is a set of edges, called transitions of A, where
B(C) is the set of boolean clock constraints involving clocks from C, and
P(C) is the powerset of C.

q0 is an element of Q, called the initial state.
An edge (q,a,g,r,q') from E is a transition from state q to q' with action a, guard g and clock resets r.


== Notes =="
569,Richard Wexelblat,31175945,2480,"Richard L. Wexelblat, aka Dick Wexelblat is an American computer scientist.
Wexelblat received his BSEE, MSEE (CS), and Ph.D. (CS) from The Moore School of Electrical Engineering at the University of Pennsylvania in 6/1959, 6/1961, and 12/1965 respectively. His doctorate is believed by many and so reported by ACM to have been the first ever awarded by a formally recognized Computer Science department.. (Note: not the first CS doctorate, but the first awarded by a CS department. Note as well that Andy van Dam should share this distinction as he completed his CS dissertation at essentially the same time. RLW 01/20/2012)
He is no longer active in the computer field but is now an artisan woodturner.
His sons, Alan and David, and his brother Paul are also computer scientists though Paul is now mostly retired and David is about halfway through law school.
He is said to be the originator of Wexelblat's scheduling algorithm: ""Choose two of: good, fast, cheap."" [Bob Rosin said I originated this; I'm not sure. He also credited me with having been the first to refer to Occam's Razor as ""The Law of Least Astonishment"". RLW 04/01/2011]


== Selected publications ==
Richard L. Wexelblat (ed.): History of Programming Languages, Academic Press 1981. ISBN 978-0-12-745040-7


== References ==


== External links ==
http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wexelblat:Richard_L=.html
http://dl.acm.org/author_page.cfm?id=81100474947"
570,Peptide computing,7528959,2462,"Peptide computing is a form of computing which uses peptides and molecular biology, instead of traditional silicon-based computer technologies. The basis of this computational model is the affinity of antibodies towards peptide sequences. Similar to DNA computing, the parallel interactions of peptide sequences and antibodies have been used by this model to solve a few NP-complete problems. Specifically, the hamiltonian path problem (HPP) and some versions of the set cover problem are a few NP-complete problems which have been solved using this computational model so far. This model of computation has also been shown to be computationally universal (or Turing complete).
This model of computation has some critical advantages over DNA computing. For instance, while DNA is made of four building blocks, peptides are made of twenty building blocks. The peptide-antibody interactions are also more flexible with respect to recognition and affinity than an interaction between a DNA strand and its reverse complement. However, unlike DNA computing, this model is yet to be practically realized. The main limitation is the availability of specific monoclonal antibodies required by the model.


== See also ==
Biocomputers
Computational gene
Computational complexity theory
DNA computing
Molecular electronics
Parallel computing


== References ==
M. Sakthi Balan; Kamala Krithivasan; Y. Sivasubramanyam (2001). ""Peptide Computing - Universality and Complexity"". Lecture Notes in Computer Science. Lecture Notes in Computer Science. 2340: 290–299. doi:10.1007/3-540-48017-X_27. ISBN 978-3-540-43775-8. 
Hubert Hug & Rainer Schuler (2001). ""Strategies for the development of a peptide computer"". Bioinformatics. 17 (4): 364–368. doi:10.1093/bioinformatics/17.4.364. PMID 11301306."
571,NovoGen,25615280,2462,"NovoGen is a proprietary form of 3D printing technology that allows scientists to assemble living tissue cells into a desired pattern. When combined with an extracellular matrix, the cells can be arranged into complex structures, such as organs. Designed by Organovo, the NovoGen technology has been successfully integrated by Invetech with a production printer that is intended to help develop processes for tissue repair and organ development.


== References =="
572,Partha Niyogi,29288642,2457,"Partha Niyogi (July 31, 1967 – October 1, 2010) was the Louis Block Professor in Computer Science and Statistics at the University of Chicago. He is known for his work in artificial intelligence, especially in the field of manifold learning and evolutionary linguistics. He wrote more than 90 academic publications and two books.


== Notable work ==
Laplacian eigenmaps


== References ==


== External links ==
Personal website
Technical Reports"
573,Adaptive optimization,3959410,2456,"Adaptive optimization is a technique in computer science that performs dynamic recompilation of portions of a program based on the current execution profile. With a simple implementation, an adaptive optimizer may simply make a trade-off between just-in-time compilation and interpreting instructions. At another level, adaptive optimization may take advantage of local data conditions to optimize away branches and to use inline expansion to decrease the cost of procedure calls.
Consider a hypothetical banking application that handles transactions one after another. These transactions may be checks, deposits, and a large number of more obscure transactions. When the program executes, the actual data may consist of clearing tens of thousands of checks without processing a single deposit and without processing a single check with a fraudulent account number. An adaptive optimizer would compile assembly code to optimize for this common case. If the system then started processing tens of thousands of deposits instead, the adaptive optimizer would recompile the assembly code to optimize the new common case. This optimization may include inlining code.
Examples of adaptive optimization include HotSpot and HP's Dynamo system.
In some systems, notably the Java Virtual Machine, execution over a range of bytecode instructions can be provably reversed. This allows an adaptive optimizer to make risky assumptions about the code. In the above example, the optimizer may assume all transactions are checks and all account numbers are valid. When these assumptions prove incorrect, the adaptive optimizer can 'unwind' to a valid state and then interpret the byte code instructions correctly.


== See also ==

Profile-guided optimization
Hot spot (computer science)


== References ==


== External links ==
CiteSeer for ""Adaptive Optimization in the Jalapeño JVM (2000)"" by Matthew Arnold, Stephen Fink, David Grove, Michael Hind, Peter F. Sweeney. Contains links to the full paper in various formats."
574,Internal field separator,7097640,2448,"For many command line interpreters (“shell”) of Unix operating systems, the internal field separator (abbreviated IFS) refers to a variable which defines the character or characters used to separate a pattern into tokens for some operations.
IFS typically includes the space, tab, and the newline.
From the bash man page:

The shell treats each character of $IFS as a delimiter, and splits the results of the other expansions into words on these characters. If IFS is unset, or its value is exactly <space><tab><newline>, the default, then sequences of <space>, <tab>, and <newline> at the beginning and end of the results of the previous expansions are ignored, and any sequence of IFS characters not at the beginning or end serves to delimit words. If IFS has a value other than the default, then sequences of the whitespace characters space and tab are ignored at the beginning and end of the word, as long as the whitespace character is in the value of IFS (an IFS whitespace character). Any character in IFS that is not IFS whitespace, along with any adjacent IFS whitespace characters, delimits a field. A sequence of IFS whitespace characters is also treated as a delimiter. If the value of IFS is null, no word splitting occurs.

IFS was usable as an exploit in some versions of Unix. A program with root permissions could be fooled into executing user-supplied code if it ran (for instance) system(""/bin/mail"") and was called with $IFS set to ""/"", in which case it would run the program ""bin"" (in the current directory and thus writable by the user) with root permissions. This has been fixed by making the shells not inherit the IFS variable.


== External links ==
Some examples on how to apply the use of IFS in Bash scripts:
http://tldp.org/LDP/abs/html/internalvariables.html
http://mindspill.net/computing/linux-notes/using-the-bash-ifs-variable-to-make-for-loops-split-with-non-whitespace-characters/
http://www.cyberciti.biz/faq/bash-for-loop-spaces/"
575,Adleman–Pomerance–Rumely primality test,8198761,2447,"In computational number theory, the Adleman–Pomerance–Rumely primality test is an algorithm for determining whether a number is prime. Unlike other, more efficient algorithms for this purpose, it avoids the use of random numbers, so it is a deterministic primality test. It is named after its discoverers, Leonard Adleman, Carl Pomerance, and Robert Rumely. The test involves arithmetic in cyclotomic fields.
It was later improved by Henri Cohen and Hendrik Willem Lenstra, commonly referred to as APR-CL. It can test primality of an integer n in time:

  
    
      
        (
        ln
        ⁡
        n
        
          )
          
            O
            (
            ln
            
            ln
            
            ln
            ⁡
            n
            )
          
        
        .
      
    
    {\displaystyle (\ln n)^{O(\ln \,\ln \,\ln n)}.}
  


== Software implementations ==
UBASIC provides an implementation under the name APRT-CLE (APR Test CL extended)
A factoring applet that uses APR-CL on certain conditions (source code included)
Pari/GP uses APR-CL conditionally in its implementation of isprime().
mpz_aprcl is an open source implementation using C and GMP.
Jean Penné's LLR uses APR-CL on certain types of prime tests as a fallback option.


== References ==
Adleman, Leonard M.; Pomerance, Carl; Rumely, Robert S. (1983). ""On distinguishing prime numbers from composite numbers"". Annals of Mathematics. 117 (1): 173–206. doi:10.2307/2006975. 
Cohen, Henri; Lenstra, Hendrik W., Jr. (1984). ""Primality testing and Jacobi sums"". Mathematics of Computation. 42 (165): 297–330. doi:10.2307/2007581. 
Riesel, Hans (1994). Prime Numbers and Computer Methods for Factorization. Birkhäuser. pp. 131–136. ISBN 0-8176-3743-5. 
APR and APR-CL"
576,Privacy-preserving computational geometry,41056398,2435,"Privacy-preserving computational geometry is the research area on the intersection of the domains of secure multi-party computation (SMC) and computational geometry. Classical problems of computational geometry reconsidered from the point of view of SMC include shape intersection, private point inclusion problem, range searching, convex hull, and more.
A pioneering work in this area was a 2001 paper by Atallah and Du,  in which the secure point in polygon inclusion and polygonal intersection problems were considered.
Other problems are computation of the distance between two private points and secure two-party point-circle inclusion problem.


== Problem statements ==
The problems use the conventional ""Alice and Bob"" terminology. In all problems the required solution is a protocol of information exchange during which no additional information is revealed beyond what may be inferred from the answer to the required question.
Point-in-polygon: Alice has a point a, and Bob has a polygon B. They need to determine whether a is inside B. 
Polygon pair intersection: Alice has a polygon A, and Bob has a polygon B. They need to determine whether A intersects B. 


== References =="
577,Syntax Definition Formalism,12234455,2435,"The Syntax Definition Formalism (SDF) is a metasyntax used to define context-free grammars: that is, a formal way to describe formal languages. It can express the entire range of context-free grammars. Its current version is SDF3[1]. A parser and parser generator for SDF specifications are provided as part of the free ASF+SDF Meta Environment. These operate using the SGLR (Scannerless GLR parser). An SDF parser outputs parse trees or, in the case of ambiguities, parse forests.


== Overview ==
Features of SDF:
Supports the entire range of context-free languages
Allows modular syntax definitions (grammars can import subgrammars) which enables reuse
Supports annotations


== Examples ==
The following example defines a simple Boolean expression syntax in SDF2:

module basic/Booleans

exports
  sorts Boolean
  context-free start-symbols Boolean

context-free syntax
   ""true""                      -> Boolean
   ""false""                     -> Boolean
   lhs:Boolean ""|"" rhs:Boolean -> Boolean {left}         
   lhs:Boolean ""&"" rhs:Boolean -> Boolean {left}       
   ""not"" ""("" Boolean "")""       -> Boolean           
   ""("" Boolean "")""             -> Boolean

 context-free priorities
   Boolean ""&"" Boolean -> Boolean >
   Boolean ""|"" Boolean -> Boolean


== Program analysis and transformation systems using SDF ==
ASF+SDF Meta Environment provides SDF
RascalMPL
Spoofax/IMP [2]
Stratego/XT
Strafunski


== See also ==
GNU bison
ANTLR


== Further reading ==
A Quick Introduction to SDF, Visser, J. & Scheerder, J. (2000) CWI
The Syntax Definition Formalism SDF, Mark van den Brand, Paul Klint, Jurgen Vinju (2007) CWI


== External links ==
Grammar Deployment Kit
SdfMetz computes metrics for SDF grammars
Download SDF from the ASF+SDF Meta Environment homepage"
578,International Conference on Architectural Support for Programming Languages and Operating Systems,27016205,2432,"The International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS) is an annual interdisciplinary computer science conference organized by the Association for Computing Machinery (ACM).
Reflecting its focus, sponsorship of the conference is made up of 50% by the ACM's Special Interest Group on Computer Architecture (SIGARCH) and 25% by each of the Special Interest Group on Programming Languages (SIGPLAN) and the Special Interest Group on Operating Systems (SIGOPS). It is a high-impact conference in computer architecture and operating systems, but less so in programming languages/software engineering.


== See also ==
List of computer science conferences


== References =="
579,R+ tree,1369821,2431,"R-trees are tree data structures used for spatial access methods, i.e., for indexing multi-dimensional information such as geographical coordinates, rectangles or polygons. The R-tree was proposed by Antonin Guttman in 1984 and has found significant use in both theoretical and applied contexts. A common real-world usage for an R-tree might be to store spatial objects such as restaurant locations or the polygons that typical maps are made of: streets, buildings, outlines of lakes, coastlines, etc. and then find answers quickly to queries such as ""Find all museums within 2 km of my current location"", ""retrieve all road segments within 2 km of my location"" (to display them in a navigation system) or ""find the nearest gas station"" (although not taking roads into account). The R-tree can also accelerate nearest neighbor search for various distance metrics, including great-circle distance.


== R-tree idea ==
The key idea of the data structure is to group nearby objects and represent them with their minimum bounding rectangle in the next higher level of the tree; the ""R"" in R-tree is for rectangle. Since all objects lie within this bounding rectangle, a query that does not intersect the bounding rectangle also cannot intersect any of the contained objects. At the leaf level, each rectangle describes a single object; at higher levels the aggregation of an increasing number of objects. This can also be seen as an increasingly coarse approximation of the data set.
Similar to the B-tree, the R-tree is also a balanced search tree (so all leaf nodes are at the same height), organizes the data in pages, and is designed for storage on disk (as used in databases). Each page can contain a maximum number of entries, often denoted as 
  
    
      
        M
      
    
    {\displaystyle M}
  . It also guarantees a minimum fill (except for the root node), however best performance has been experienced with a minimum fill of 30%–40% of the maximum number of entries (B-trees guarantee 50% page fill, and B*-trees even 66%). The reason for this is the more complex balancing required for spatial data as opposed to linear data stored in B-trees.
As with most trees, the searching algorithms (e.g., intersection, containment, nearest neighbor search) are rather simple. The key idea is to use the bounding boxes to decide whether or not to search inside a subtree. In this way, most of the nodes in the tree are never read during a search. Like B-trees, this makes R-trees suitable for large data sets and databases, where nodes can be paged to memory when needed, and the whole tree cannot be kept in main memory.
The key difficulty of R-tree is to build an efficient tree that on one hand is balanced (so the leaf nodes are at the same height) on the other hand the rectangles do not cover too much empty space and do not overlap too much (so that during search, fewer subtrees need to be processed). For example, the original idea for inserting elements to obtain an efficient tree is to always insert into the subtree that requires least enlargement of its bounding box. Once that page is full, the data is split into two sets that should cover the minimal area each. Most of the research and improvements for R-trees aims at improving the way the tree is built and can be grouped into two objectives: building an efficient tree from scratch (known as bulk-loading) and performing changes on an existing tree (insertion and deletion).
R-trees do not guarantee good worst-case performance, but generally perform well with real-world data. While more of theoretical interest, the (bulk-loaded) Priority R-tree variant of the R-tree is worst-case optimal, but due to the increased complexity, has not received much attention in practical applications so far.
When data is organized in an R-tree, the k nearest neighbors (for any Lp-Norm) of all points can efficiently be computed using a spatial join. This is beneficial for many algorithms based on the k nearest neighbors, for example the Local Outlier Factor. DeLi-Clu, Density-Link-Clustering is a cluster analysis algorithm that uses the R-tree structure for a similar kind of spatial join to efficiently compute an OPTICS clustering.


== Variants ==
R* tree
R+ tree
Hilbert R-tree
X-tree


== Algorithm ==


=== Data layout ===
Data in R-trees is organized in pages, that can have a variable number of entries (up to some pre-defined maximum, and usually above a minimum fill). Each entry within a non-leaf node stores two pieces of data: a way of identifying a child node, and the bounding box of all entries within this child node. Leaf nodes store the data required for each child, often a point or bounding box representing the child and an external identifier for the child. For point data, the leaf entries can be just the points themselves. For polygon data (that often requires the storage of large polygons) the common setup is to store only the MBR (minimum bounding rectangle) of the polygon along with a unique identifier in the tree.


=== Search ===
In range searching, the input is a search rectangle (Query box). Searching is quite similar to searching in a B+ tree. The search starts from the root node of the tree. Every internal node contains a set of rectangles and pointers to the corresponding child node and every leaf node contains the rectangles of spatial objects (the pointer to some spatial object can be there). For every rectangle in a node, it has to be decided if it overlaps the search rectangle or not. If yes, the corresponding child node has to be searched also. Searching is done like this in a recursive manner until all overlapping nodes have been traversed. When a leaf node is reached, the contained bounding boxes (rectangles) are tested against the search rectangle and their objects (if there are any) are put into the result set if they lie within the search rectangle.
For priority search such as nearest neighbor search, the query consists of a point or rectangle. The root node is inserted into the priority queue. Until the queue is empty or the desired number of results have been returned the search continues by processing the nearest entry in the queue. Tree nodes are expanded and their children reinserted. Leaf entries are returned when encountered in the queue. This approach can be used with various distance metrics, including great-circle distance for geographic data.


=== Insertion ===
To insert an object, the tree is traversed recursively from the root node. At each step, all rectangles in the current directory node are examined, and a candidate is chosen using a heuristic such as choosing the rectangle which requires least enlargement. The search then descends into this page, until reaching a leaf node. If the leaf node is full, it must be split before the insertion is made. Again, since an exhaustive search is too expensive, a heuristic is employed to split the node into two. Adding the newly created node to the previous level, this level can again overflow, and these overflows can propagate up to the root node; when this node also overflows, a new root node is created and the tree has increased in height.


==== Choosing the insertion subtree ====
At each level, the algorithm needs to decide in which subtree to insert the new data object. When a data object is fully contained in a single rectangle, the choice is clear. When there are multiple options or rectangles in need of enlargement, the choice can have a significant impact on the performance of the tree.
In the classic R-tree, objects are inserted into the subtree that needs the least enlargement. In the more advanced R*-tree, a mixed heuristic is employed. At leaf level, it tries to minimize the overlap (in case of ties, prefer least enlargement and then least area); at the higher levels, it behaves similar to the R-tree, but on ties again preferring the subtree with smaller area. The decreased overlap of rectangles in the R*-tree is one of the key benefits over the traditional R-tree (this is also a consequence of the other heuristics used, not only the subtree choosing).


==== Splitting an overflowing node ====
Since redistributing all objects of a node into two nodes has an exponential number of options, a heuristic needs to be employed to find the best split. In the classic R-tree, Guttman proposed two such heuristics, called QuadraticSplit and LinearSplit. In quadratic split, the algorithm searches for the pair of rectangles that is the worst combination to have in the same node, and puts them as initial objects into the two new groups. It then searches for the entry which has the strongest preference for one of the groups (in terms of area increase) and assigns the object to this group until all objects are assigned (satisfying the minimum fill).
There are other splitting strategies such as Greene's Split, the R*-tree splitting heuristic (which again tries to minimize overlap, but also prefers quadratic pages) or the linear split algorithm proposed by Ang and Tan (which however can produce very irregular rectangles, which are less performant for many real world range and window queries). In addition to having a more advanced splitting heuristic, the R*-tree also tries to avoid splitting a node by reinserting some of the node members, which is similar to the way a B-tree balances overflowing nodes. This was shown to also reduce overlap and thus increase tree performance.
Finally, the X-tree can be seen as a R*-tree variant that can also decide to not split a node, but construct a so-called super-node containing all the extra entries, when it doesn't find a good split (in particular for high-dimensional data).


=== Deletion ===
Deleting an entry from a page may require updating the bounding rectangles of parent pages. However, when a page is underfull, it will not be balanced with its neighbors. Instead, the page will be dissolved and all its children (which may be subtrees, not only leaf objects) will be reinserted. If during this process the root node has a single element, the tree height can decrease.


=== Bulk-loading ===
Nearest-X – Objects are sorted by their first coordinate (""X"") and then split into pages of the desired size.
Packed Hilbert R-tree – variation of Nearest-X, but sorting using the Hilbert value of the center of a rectangle instead of using the X coordinate. There is no guarantee the pages will not overlap.
Sort-Tile-Recursive (STR): Another variation of Nearest-X, that estimates the total number of leaves required as 
  
    
      
        l
        =
        ⌈
        
          number of objects
        
        
          /
        
        
          capacity
        
        ⌉
      
    
    {\displaystyle l=\lceil {\text{number of objects}}/{\text{capacity}}\rceil }
  , the required split factor in each dimension to achieve this as 
  
    
      
        s
        =
        ⌈
        
          l
          
            1
            
              /
            
            d
          
        
        ⌉
      
    
    {\displaystyle s=\lceil l^{1/d}\rceil }
  , then repeatedly splits each dimensions successively into 
  
    
      
        s
      
    
    {\displaystyle s}
   equal sized partitions using 1-dimensional sorting. The resulting pages, if they occupy more than one page, are again bulk-loaded using the same algorithm. For point data, the leaf nodes will not overlap, and ""tile"" the data space into approximately equal sized pages.
Overlap Minimizing Top-down (OMT): Improvement over STR using a top-down approach which minimizes overlaps between slices and improves query performance.
Priority R-tree


== See also ==
Segment tree
Interval tree – A degenerate R-tree for one dimension (usually time).
Bounding volume hierarchy
Spatial index
GiST


== References ==


== External links ==
 Media related to R-tree at Wikimedia Commons"
580,Echo state network,8887731,2428,"The echo state network (ESN), is a recurrent neural network with a sparsely connected hidden layer (with typically 1% connectivity). The connectivity and weights of hidden neurons are fixed and randomly assigned. The weights of output neurons can be learned so that the network can (re)produce specific temporal patterns. The main interest of this network is that although its behaviour is non-linear, the only weights that are modified during training are for the synapses that connect the hidden neurons to output neurons. Thus, the error function is quadratic with respect to the parameter vector and can be differentiated easily to a linear system.
Alternatively, one may consider a nonparametric Bayesian formulation of the output layer, under which: (i) a prior distribution is imposed over the output weights; and (ii) the output weights are marginalized out in the context of prediction generation, given the training data. This idea has been demonstrated in  by using Gaussian priors, whereby a Gaussian process model with ESN-driven kernel function is obtained. Such a solution was shown to outperform ESNs with trainable (finite) sets of weights in several benchmarks.
Some publicly available implementations of ESNs are: (i) aureservoir: an efficient C++ library for various kinds of echo state networks with python/numpy bindings; and (ii) Matlab code: an efficient matlab for an echo state network.


== See also ==
Liquid-state machine: a similar concept with generalized signal and network.
Reservoir computing


== References =="
581,ΜFluids@Home,3255371,2424,"μFluids@Home is a computer simulation of two-phase flow behavior in microgravity and microfluidics problems at Purdue University, using the Surface Evolver program.


== About ==
The project's purpose is to develop better methods for the management of liquid rocket propellants in microgravity, and to investigate two-phase flow in microelectromechanical systems, taking into account factors like surface tension. Systems can then be designed that use electrowetting, channel geometry, and hydrophobic or hydrophilic coatings to allow the smooth passage of fluids. Such systems would include compact medical devices, biosensors, and fuel cells, to name a few.


== Computing platform ==
It uses the BOINC distributed computing platform.
Application notes
There is no screen saver.
Work unit CPU times are generally less than 20 hours.
Work units average in size around 500 kB.
You have to run many work units to get levels of credit comparable to SETI@Home or Climateprediction.net distributed programs.


== References ==


== External links ==
μFluids@Home Website"
582,Portable object (computing),3955553,2422,"In distributed programming, a portable object is an object which can be accessed through a normal method call while possibly residing in memory on another computer. It is portable in the sense that it moves from machine to machine, irrespective of operating system or computer architecture. This mobility is the end goal of many remote procedure call systems.
The advantage of portable objects is that they are easy to use and very expressive, allowing programmers to be completely unaware that objects reside in other locations. Detractors cite this as a fault, as naïve programmers will not expect network-related errors or the unbounded nondeterminism associated with large networks.


== See also ==
CORBA Common Object Request Broker Architecture, cross-language cross-platform object model
Portable Object Adapter part of the CORBA standard
D-Bus current open cross-language cross-platform Freedesktop.org Object Model
Bonobo deprecated GNOME cross-language Object Model
DCOP deprecated KDE interprocess and software componentry communication system
KParts KDE component framework
XPCOM Mozilla applications cross-platform Component Object Model
COM Microsoft Windows only cross-language Object Model
DCOM Distributed COM, extension making COM able to work in networks
Common Language Infrastructure current .NET cross-language cross-platform Object Model
IBM System Object Model SOM, a component system from IBM used in OS/2
Java Beans
Java Remote Method Invocation (Java RMI)
Internet Communications Engine
Language binding
Foreign function interface
Calling convention
Name mangling
Application programming interface - API
Application Binary Interface - ABI
Comparison of application virtual machines
SWIG open source automatic interfaces bindings generator from many languages to many languages"
583,Symbolic-numeric computation,22288224,2416,"In mathematics and computer science, symbolic-numeric computation is the use of software that combines symbolic and numeric methods to solve problems.


== References ==
Wang, Dongming; Zhi, Lihong (2007). Symbolic-numeric Computation. Springer. ISBN 3-7643-7983-9. 
Mourrain, Bernard; Pavone, Jean-Pascal; Trebuchet, Philippe; Tsigaridas, Elias P.; Wintz, Julien (2008). ""SYNAPS: A Library for Dedicated Applications in Symbolic Numeric Computing"". Software for Algebraic Geometry. The IMA Volumes in Mathematics and its Applications. 148. pp. 81–109. CiteSeerX 10.1.1.135.1680 . doi:10.1007/978-0-387-78133-4_6. 
Grabmeier, Johannes; Kaltofen, Erich; Weispfenning, Volker, eds. (2003). ""Hybrid methods"" (PDF). Computer algebra handbook: foundations, applications, systems, Volume 1. Springer. ISBN 978-3-540-65466-7. 
Robbiano, Lorenzo; Abbott, John (2009). Approximate Commutative Algebra. Springer. ISBN 978-3-211-99313-2. 
Langer, Ulrich; Paule, Peter, eds. (2011). Numerical and Symbolic Scientific Computing. Springer. ISBN 978-3-7091-0793-5. 


== External links ==
""The Fourth International Workshop on Symbolic-Numeric Computation (SNC2011)"". San Jose, California. June 7–9, 2011."
584,CoSy (computer conferencing system),3141171,2414,"CoSy was an early computer conferencing system developed by the University of Guelph in 1983 and 1984. CoSy was selected by Byte Magazine to launch their BIX system in 1985
In addition to BIX, it was used to implement a similar British system named CIX, as well as numerous other installations such as CompuLink Network. CoSy was also chosen for The Open University's ""electronic campus"".
Some rights to the software were later acquired by the British Columbia company SoftWords, who developed it into CoSy400 and added a simple web interface, before losing interest.
When the BIX system closed down, several former ""bixen"" approached University of Guelph and SoftWords and obtained the right to release the original version of CoSy under the GPL. It is now developed as an open source project, and is the basis of the BIX-like NLZero (Noise Level Zero) conferencing service.


== References ==


== External links ==
WebCoSy
CoSy at SourceForge
NLZ website"
585,Floyd's triangle,14705292,2405,"Floyd's triangle is a right-angled triangular array of natural numbers, used in computer science education. It is named after Robert Floyd. It is defined by filling the rows of the triangle with consecutive numbers, starting with a 1 in the top left corner:
Beginning programmers are often assigned the task of writing a program to print out the table in the format shown.
The numbers along the left edge of the triangle are the lazy caterer's sequence and the numbers along the right edge are the triangular numbers. The nth row sums to n(n2 + 1)/2, the constant of an n × n magic square (sequence A006003 in the OEIS).
Summing up the row sums in Floyd's triangle reveals the doubly triangular numbers (triangular numbers with an index that is triangular)(sequence A002817 in the OEIS)
1            = 1 = T(T(1))
1            = 6 = T(T(2))
2 + 3
1
2 + 3     = 21 = T(T(3))
4 + 5 + 6


== See also ==
Pascal's triangle


== References ==


== External links ==
Floyd's triangle at Rosetta code"
586,Dependency relation,10739341,2405,"In mathematics and computer science, a dependency relation is a binary relation that is finite, symmetric, and reflexive; i.e. a finite tolerance relation. That is, it is a finite set of ordered pairs 
  
    
      
        D
      
    
    {\displaystyle D}
  , such that
If 
  
    
      
        (
        a
        ,
        b
        )
        ∈
        D
      
    
    {\displaystyle (a,b)\in D}
   then 
  
    
      
        (
        b
        ,
        a
        )
        ∈
        D
      
    
    {\displaystyle (b,a)\in D}
   (symmetric)
If 
  
    
      
        a
      
    
    {\displaystyle a}
   is an element of the set on which the relation is defined, then 
  
    
      
        (
        a
        ,
        a
        )
        ∈
        D
      
    
    {\displaystyle (a,a)\in D}
   (reflexive)
In general, dependency relations are not transitive; thus, they generalize the notion of an equivalence relation by discarding transitivity.
Let 
  
    
      
        Σ
      
    
    {\displaystyle \Sigma }
   denote the alphabet of all the letters of 
  
    
      
        D
      
    
    {\displaystyle D}
  . Then the independency induced by 
  
    
      
        D
      
    
    {\displaystyle D}
   is the binary relation 
  
    
      
        I
      
    
    {\displaystyle I}
  

  
    
      
        I
        =
        Σ
        ×
        Σ
        ∖
        D
      
    
    {\displaystyle I=\Sigma \times \Sigma \setminus D}
  
That is, the independency is the set of all ordered pairs that are not in 
  
    
      
        D
      
    
    {\displaystyle D}
  . The independency is symmetric and irreflexive.
The pairs 
  
    
      
        (
        Σ
        ,
        D
        )
      
    
    {\displaystyle (\Sigma ,D)}
   and 
  
    
      
        (
        Σ
        ,
        I
        )
      
    
    {\displaystyle (\Sigma ,I)}
  , or the triple 
  
    
      
        (
        Σ
        ,
        D
        ,
        I
        )
      
    
    {\displaystyle (\Sigma ,D,I)}
   (with 
  
    
      
        I
      
    
    {\displaystyle I}
   induced by 
  
    
      
        D
      
    
    {\displaystyle D}
  ) are sometimes called the concurrent alphabet or the reliance alphabet.
The pairs of letters in an independency relation induce an equivalence relation on the free monoid of all possible strings of finite length. The elements of the equivalence classes induced by the independency are called traces, and are studied in trace theory.


== Examples ==

Consider the alphabet 
  
    
      
        Σ
        =
        {
        a
        ,
        b
        ,
        c
        }
      
    
    {\displaystyle \Sigma =\{a,b,c\}}
  . A possible dependency relation is

  
    
      
        
          
            
              
                D
              
              
                
                =
                {
                a
                ,
                b
                }
                ×
                {
                a
                ,
                b
                }
                
                ∪
                
                {
                a
                ,
                c
                }
                ×
                {
                a
                ,
                c
                }
              
            
            
              
              
                
                =
                {
                a
                ,
                b
                
                  }
                  
                    2
                  
                
                ∪
                {
                a
                ,
                c
                
                  }
                  
                    2
                  
                
              
            
            
              
              
                
                =
                {
                (
                a
                ,
                b
                )
                ,
                (
                b
                ,
                a
                )
                ,
                (
                a
                ,
                c
                )
                ,
                (
                c
                ,
                a
                )
                ,
                (
                a
                ,
                a
                )
                ,
                (
                b
                ,
                b
                )
                ,
                (
                c
                ,
                c
                )
                }
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}D&=\{a,b\}\times \{a,b\}\quad \cup \quad \{a,c\}\times \{a,c\}\\&=\{a,b\}^{2}\cup \{a,c\}^{2}\\&=\{(a,b),(b,a),(a,c),(c,a),(a,a),(b,b),(c,c)\}\end{aligned}}}
  
The corresponding independency is

  
    
      
        
          I
          
            D
          
        
        =
        {
        (
        b
        ,
        c
        )
        
        ,
        
        (
        c
        ,
        b
        )
        }
      
    
    {\displaystyle I_{D}=\{(b,c)\,,\,(c,b)\}}
  
Therefore, the letters 
  
    
      
        b
        ,
        c
      
    
    {\displaystyle b,c}
   commute, or are independent of one another."
587,Severin Hacker,40470509,2402,"Severin Hacker (born 1984) is a Swiss computer scientist who is the co-founder and CTO of Duolingo, a popular language-learning platform.


== Biography ==
Hacker was born and raised in Zug and studied at ETH Zurich. He moved to Pittsburgh to study at Carnegie Mellon University where he co-founded Duolingo with Luis von Ahn in 2009.


== References ==


== External links ==
Severin Hacker's official website
Severin Hacker on Twitter 
List of publications from Microsoft Academic
Severin Hacker at DBLP Bibliography Server"
588,LogP machine,19664404,2397,"The LogP machine is a model for parallel computation. It aims at being more practical than the PRAM model while still allowing for easy analysis of computation. The name is not related to the mathematical logarithmic function: Instead, the machine is described by the four parameters 
  
    
      
        L
      
    
    {\displaystyle L}
  , 
  
    
      
        o
      
    
    {\displaystyle o}
  , 
  
    
      
        g
      
    
    {\displaystyle g}
   and 
  
    
      
        P
      
    
    {\displaystyle P}
  .
The LogP machine consists of arbitrarily many processing units with distributed memory. The processing units are connected through an abstract communication medium which allows point-to-point communication. This model is pair-wise synchronous and overall asynchronous.
The machine is described by the four parameters:

  
    
      
        L
      
    
    {\displaystyle L}
  , the latency of the communication medium.

  
    
      
        o
      
    
    {\displaystyle o}
  , the overhead of sending and receiving a message.

  
    
      
        g
      
    
    {\displaystyle g}
  , the gap required between two send/receive operations. A more common interpretation of this quantity is as the inverse of the bandwidth of a processor-processor communication channel.

  
    
      
        P
      
    
    {\displaystyle P}
  , the number of processing units. Each local operation on each machine takes the same time ('unit time'). This time is called a processor cycle.
The units of the parameters 
  
    
      
        L
      
    
    {\displaystyle L}
  , 
  
    
      
        o
      
    
    {\displaystyle o}
   and 
  
    
      
        g
      
    
    {\displaystyle g}
   are measured in multiples of processor cycles.


== See also ==
Bulk synchronous parallel
Parallel programming model


== Notes ==


== References ==
Culler, David; Karp, Richard; Patterson, David; Sahay, Abhijit; Schauser, Klaus Erik; Santos, Eunice; Subramonian, Ramesh; Von Eicken, Thorsten (July 1993), ""LogP: Towards a realistic model of parallel computation"" (PDF), ACM SIGPLAN Notices, 28 (7): 1–12, doi:10.1145/173284.155333"
589,Computational Geometry (journal),48640355,2396,"Computational geometry is a branch of computer science devoted to the study of algorithms which can be stated in terms of geometry. Some purely geometrical problems arise out of the study of computational geometric algorithms, and such problems are also considered to be part of computational geometry. While modern computational geometry is a recent development, it is one of the oldest fields of computing with history stretching back to antiquity.
Computational complexity is central to computational geometry, with great practical significance if algorithms are used on very large datasets containing tens or hundreds of millions of points. For such sets, the difference between O(n2) and O(n log n) may be the difference between days and seconds of computation.
The main impetus for the development of computational geometry as a discipline was progress in computer graphics and computer-aided design and manufacturing (CAD/CAM), but many problems in computational geometry are classical in nature, and may come from mathematical visualization.
Other important applications of computational geometry include robotics (motion planning and visibility problems), geographic information systems (GIS) (geometrical location and search, route planning), integrated circuit design (IC geometry design and verification), computer-aided engineering (CAE) (mesh generation), computer vision (3D reconstruction).
The main branches of computational geometry are:
Combinatorial computational geometry, also called algorithmic geometry, which deals with geometric objects as discrete entities. A groundlaying book in the subject by Preparata and Shamos dates the first use of the term ""computational geometry"" in this sense by 1975.
Numerical computational geometry, also called machine geometry, computer-aided geometric design (CAGD), or geometric modeling, which deals primarily with representing real-world objects in forms suitable for computer computations in CAD/CAM systems. This branch may be seen as a further development of descriptive geometry and is often considered a branch of computer graphics or CAD. The term ""computational geometry"" in this meaning has been in use since 1971.


== Combinatorial computational geometry ==
The primary goal of research in combinatorial computational geometry is to develop efficient algorithms and data structures for solving problems stated in terms of basic geometrical objects: points, line segments, polygons, polyhedra, etc.
Some of these problems seem so simple that they were not regarded as problems at all until the advent of computers. Consider, for example, the Closest pair problem:
Given n points in the plane, find the two with the smallest distance from each other.
One could compute the distances between all the pairs of points, of which there are n(n-1)/2, then pick the pair with the smallest distance. This brute-force algorithm takes O(n2) time; i.e. its execution time is proportional to the square of the number of points. A classic result in computational geometry was the formulation of an algorithm that takes O(n log n). Randomized algorithms that take O(n) expected time, as well as a deterministic algorithm that takes O(n log log n) time, have also been discovered.


=== Problem classes ===
The core problems in computational geometry may be classified in different ways, according to various criteria. The following general classes may be distinguished.


==== Static problems ====
In the problems of this category, some input is given and the corresponding output needs to be constructed or found. Some fundamental problems of this type are:
Convex hull: Given a set of points, find the smallest convex polyhedron/polygon containing all the points.
Line segment intersection: Find the intersections between a given set of line segments.
Delaunay triangulation
Voronoi diagram: Given a set of points, partition the space according to which points are closest to the given points.
Linear programming
Closest pair of points: Given a set of points, find the two with the smallest distance from each other.
Largest empty circle: Given a set of points, find a largest circle with its center inside of their convex hull and enclosing none of them.
Euclidean shortest path: Connect two points in a Euclidean space (with polyhedral obstacles) by a shortest path.
Polygon triangulation: Given a polygon, partition its interior into triangles
Mesh generation
Boolean operations on polygons
The computational complexity for this class of problems is estimated by the time and space (computer memory) required to solve a given problem instance.


==== Geometric query problems ====
In geometric query problems, commonly known as geometric search problems, the input consists of two parts: the search space part and the query part, which varies over the problem instances. The search space typically needs to be preprocessed, in a way that multiple queries can be answered efficiently.
Some fundamental geometric query problems are:
Range searching: Preprocess a set of points, in order to efficiently count the number of points inside a query region.
Point location: Given a partitioning of the space into cells, produce a data structure that efficiently tells in which cell a query point is located.
Nearest neighbor: Preprocess a set of points, in order to efficiently find which point is closest to a query point.
Ray tracing: Given a set of objects in space, produce a data structure that efficiently tells which object a query ray intersects first.
If the search space is fixed, the computational complexity for this class of problems is usually estimated by:
the time and space required to construct the data structure to be searched in
the time (and sometimes an extra space) to answer queries.
For the case when the search space is allowed to vary, see ""Dynamic problems"".


==== Dynamic problems ====
Yet another major class is the dynamic problems, in which the goal is to find an efficient algorithm for finding a solution repeatedly after each incremental modification of the input data (addition or deletion input geometric elements). Algorithms for problems of this type typically involve dynamic data structures. Any of the computational geometric problems may be converted into a dynamic one, at the cost of increased processing time. For example, the range searching problem may be converted into the dynamic range searching problem by providing for addition and/or deletion of the points. The dynamic convex hull problem is to keep track of the convex hull, e.g., for the dynamically changing set of points, i.e., while the input points are inserted or deleted.
The computational complexity for this class of problems is estimated by:
the time and space required to construct the data structure to be searched in
the time and space to modify the searched data structure after an incremental change in the search space
the time (and sometimes an extra space) to answer a query.


==== Variations ====
Some problems may be treated as belonging to either of the categories, depending on the context. For example, consider the following problem.
Point in polygon: Decide whether a point is inside or outside a given polygon.
In many applications this problem is treated as a single-shot one, i.e., belonging to the first class. For example, in many applications of computer graphics a common problem is to find which area on the screen is clicked by a pointer. However, in some applications, the polygon in question is invariant, while the point represents a query. For example, the input polygon may represent a border of a country and a point is a position of an aircraft, and the problem is to determine whether the aircraft violated the border. Finally, in the previously mentioned example of computer graphics, in CAD applications the changing input data are often stored in dynamic data structures, which may be exploited to speed-up the point-in-polygon queries.
In some contexts of query problems there are reasonable expectations on the sequence of the queries, which may be exploited either for efficient data structures or for tighter computational complexity estimates. For example, in some cases it is important to know the worst case for the total time for the whole sequence of N queries, rather than for a single query. See also ""amortized analysis"".


== Numerical computational geometry ==

This branch is also known as geometric modelling and computer-aided geometric design (CAGD).
Core problems are curve and surface modelling and representation.
The most important instruments here are parametric curves and parametric surfaces, such as Bézier curves, spline curves and surfaces. An important non-parametric approach is the level set method.
Application areas of computational geometry include shipbuilding, aircraft, and automotive industries.


== See also ==
List of combinatorial computational geometry topics
List of numerical computational geometry topics
CAD/CAM/CAE
List of geometric algorithms
Solid modeling
Computational topology
Digital geometry
Discrete geometry (combinatorial geometry)
Space partitioning
Tricomplex number
Wikiversity:Topic:Computational geometry
Wikiversity:Computer-aided geometric design


== References ==


== Further reading ==
List of books in computational geometry


=== Journals ===


==== Combinatorial/algorithmic computational geometry ====
Below is the list of the major journals that have been publishing research in geometric algorithms. Please notice with the appearance of journals specifically dedicated to computational geometry, the share of geometric publications in general-purpose computer science and computer graphics journals decreased.
ACM Computing Surveys
ACM Transactions on Graphics
Acta Informatica
Advances in Geometry
Algorithmica
Ars Combinatoria
Computational Geometry: Theory and Applications
Communications of the ACM
Computer Aided Geometric Design
Computer Graphics and Applications
Computer Graphics World
Discrete & Computational Geometry
Geombinatorics
Geometriae Dedicata
IEEE Transactions on Graphics
IEEE Transactions on Computers
IEEE Transactions on Pattern Analysis and Machine Intelligence
Information Processing Letters
International Journal of Computational Geometry and Applications
Journal of Combinatorial Theory, series B
Journal of Computational Geometry
Journal of Differential Geometry
Journal of the ACM
Journal of Algorithms
Journal of Computer and System Sciences
Management Science
Pattern Recognition
Pattern Recognition Letters
SIAM Journal on Computing
SIGACT News; featured the ""Computational Geometry Column"" by Joseph O'Rourke
Theoretical Computer Science
The Visual Computer


== External links ==
Computational Geometry
Computational Geometry Pages
Geometry In Action
""Strategic Directions in Computational Geometry—Working Group Report"" (1996)
Journal of Computational Geometry
(Annual) Winter School on Computational Geometry"
590,Karloff–Zwick algorithm,3199764,2395,"The Karloff–Zwick algorithm, in computational complexity theory, is a randomised approximation algorithm taking an instance of MAX-3SAT Boolean satisfiability problem as input. If the instance is satisfiable, then the expected weight of the assignment found is at least 7/8 of optimal. It provides strong evidence (but not a mathematical proof) that the algorithm performs equally well on arbitrary MAX-3SAT instances. Howard Karloff and Uri Zwick presented the algorithm in 1997.
For the related MAX-E3SAT problem, in which all clauses in the input 3SAT formula are guaranteed to have exactly three literals, the simple randomized approximation algorithm which assigns a truth value to each variable independently and uniformly at random satisfies 7/8 of all clauses in expectation, irrespective of whether the original formula is satisfiable. Further, this simple algorithm can also be easily derandomized using the method of conditional expectations. The Karloff–Zwick algorithm, however, does not require the restriction that the input formula should have three literals in every clause.
Building upon previous work on the PCP theorem, Johan Håstad showed that, assuming P ≠ NP, no polynomial-time algorithm for MAX 3SAT can achieve a performance ratio exceeding 7/8, even when restricted to satisfiable instances of the problem in which each clause contains exactly three literals. Both the Karloff–Zwick algorithm and the above simple algorithm are therefore optimal in this sense.


== References =="
591,Trace table,14264378,2394,"A trace table is a technique used to test algorithms, in order to make sure that no logical errors occur whilst the algorithm is being processed. The table usually takes the form of a multi-column, multi-row table; With each column showing a variable, and each row showing each number input into the algorithm and the subsequent values of the variables.
Trace tables are typically used in schools and colleges when teaching students how to program. They can be an essential tool in teaching students how a certain algorithm works and the systematic process that is occurring when the algorithm is executed. They can also be useful for debugging applications, helping the programmer to easily detect what error is occurring, and why it may be occurring.


== Example ==

This example shows the systematic process that takes place whilst the algorithm is processed. The initial value of x is zero, but i, although defined, has not been assigned a value. Thus, its initial value is unknown. As we execute the program, line by line, the values of i and x change, reflecting each statement of the source code in execution. Their new values are recorded in the trace table. When i reaches the value of 11 because of the i++ statement in the for definition, the comparison i <= 10 evaluates to false, thus halting the loop. As we also reached the end of the program, the trace table also ends.


== See also ==
Algorithms
Programming languages
Debugging


== References ==
http://www.comscigate.com/tutorial/KjellStyle/WilliamChen/trace1.html
http://www.thevickerage.worldonline.co.uk/theteacher/alevel/assem/assem5.htm - archived version
http://portal.newman.wa.edu.au/technology/12infsys/html/KWH2003/TraceTables.htm - archived version"
592,Empirical algorithmics,22553927,2389,"Empirical algorithmics (sometimes also called experimental algorithmics) is the area within computer science that uses empirical methods to study the behaviour of algorithms. It can be used in the analysis of algorithms.
Methods from empirical algorithmics complement theoretical methods for the analysis of algorithms. Through the principled application of empirical methods, particularly from statistics, it is often possible to obtain insights into the behaviour of algorithms that are (currently) inaccessible to theoretical analysis, in particular, high-performance heuristic algorithms for hard combinatorial problems. Empirical methods can also be used to achieve substantial improvements in algorithmic efficiency.
There are two main branches of empirical algorithmics: the first (known as empirical analysis) deals with the analysis and characterisation of the behaviour of algorithms, and the second (known as algorithm design or algorithm engineering) is focused on empirical methods for improving the performance of algorithms. The former uses mostly techniques and tools from statistics, while the latter is based on approaches from statistics, machine learning and optimization.
Research in empirical algorithmics is published in several journals, including the ACM Journal on Experimental Algorithmics (JEA) and the Journal of Artificial Intelligence Research (JAIR), as well as at numerous conferences, including Symposium on Experimental Algorithms (SEA), AAAI, IJCAI, CP and SLS.
Well-known researchers in empirical algorithmics include Giuseppe F. Italiano, Catherine McGeoch, Carla Gomes, Holger H. Hoos, David S. Johnson, Kevin Leyton-Brown, Ruben Ruiz, Bart Selman, Thomas Stützle and Roberto Battiti.


== References =="
593,Laboratory for Foundations of Computer Science,2818171,2381,"The Laboratory for Foundations of Computer Science (LFCS) is a research institute within the School of Informatics at the University of Edinburgh, in Scotland. It was founded in 1987 and is a community of theoretical computer scientists with interests in concurrency, semantics, categories, algebra, types, logic, algorithms, complexity, databases and modelling.


== Full professors ==
Stuart Anderson
Peter Buneman MBE FRS FRSE
Vincent Danos
Wenfei Fan FACM FRSE
Michael Fourman FBCS FRSE
Stephen Gilmore
Andrew Gordon
Jane Hillston FRSE
Aggelos Kiyias
Gordon Plotkin FRS FRSE
Don Sannella FRSE
Perdita Stevens
Colin Stirling
Philip Wadler FACM FRSE


== Selected Past Members ==
Samson Abramsky
Rod Burstall
Luca Cardelli
Matthew Hennessy
Mark Jerrum
Robin Milner FRS
Eugenio Moggi
Faron Moller
Davide Sangiorgi
Chris Tofts
Mads Tofte


== References ==


== External links ==
LFCS website
People"
594,Offset (computer science),3240434,2378,"In computer science, an offset within an array or other data structure object is an integer indicating the distance (displacement) between the beginning of the object and a given element or point, presumably within the same object. The concept of a distance is valid only if all elements of the object are of the same size (typically given in bytes or words).
For example, in A as an array of characters containing ""abcdef"", the fourth element containing the character 'd' has an offset of three from the start of A.


== In assembly language ==
In computer engineering and low-level programming (such as assembly language), an offset usually denotes the number of address locations added to a base address in order to get to a specific absolute address. In this (original) meaning of offset, only the basic address unit, usually the 8-bit byte, is used to specify the offset's size. In this context an offset is sometimes called a relative address.
In IBM System/360 instructions, a 12-bit offset embedded within certain instructions provided a range of between 0 and 4096 bytes. For example, within an unconditional branch instruction (X'47F0Fxxx'), the xxx 12bit hexadecimal offset provided the byte offset from the base register (15) to branch to. An odd offset would cause a program check (unless the base register itself also contained an odd address) - since instructions had to be aligned on half-word boundaries to execute without a program or hardware interrupt.
The previous example describes an indirect way to address to a memory location in the format of segment:offset. For example, assume we want to refer to memory location 0xF867. One way this can be accomplished is by first defining a segment with beginning address 0xF000, and then defining an offset of 0x0867. Further, we are also allowed to shift the hexadecimal segment to reach the final absolute memory address. One thing to note here is that we can reach our final absolute address in many ways."
595,Predictive informatics,11613197,2377,"Predictive informatics (PI) is the combination of predictive modeling and informatics applied to healthcare, pharmaceutical, life sciences and business industries.
Predictive informatics enables researchers, analysts, physicians and decision-makers to aggregate and analyze disparate types of data, recognize patterns and trends within that data, and make more informed decisions in an effort to preemptively alter future outcomes.


== Current uses of PI ==


=== Healthcare ===
Over the past decade the increased usage of electronic health records has produced vast amounts of clinical data that is now computable. Predictive informatics integrates this data with other datasets (e.g., genotypic, phenotypic) in centralized and standardized data repositories upon which predictive analytics may be conducted.


=== Pharmaceuticals ===
The biopharmaceutical industry uses predictive informatics (a superset of chemoinformatics) to integrate information resources to transform data into knowledge in order to make better decisions faster in the area of drug lead identification and optimization.


=== Systems biology ===
Scientists involved in systems biology employ predictive informatics to integrate complex data about the interactions in biological systems from diverse experimental sources.


=== Other uses ===
Predictive informatics and analytics are also used in financial services, insurance, telecommunications, retail, and travel industries.


== See also ==
Predictive analytics
Informatics (academic field)
Predictive modeling
Biomedical informatics
Chemoinformatics


== References ==


== Further reading ==
Christophe Giraud-Carrier, Burdette Pixton, and Roberto A. Rocha. (2009) ""Bariatric surgery performance: A predictive informatics case study"". Intell. Data Anal., 13 (5), 741–754.
Krohn R. (2008) ""Predictive informatics. Why PI is the next great opportunity in healthcare"", J Healthc Inf Manag, 22(1):8–9.


== External links ==
Predictive Informatics: What Is Its Place in Healthcare? Christophe G Giraud-Carrier (2009), Brigham Young University"
596,International Conference on Remote Engineering and Virtual Instrumentation,14777608,2376,"International Conference on Remote Engineering and Virtual Instrumentation (REV) is an annual IAOE conference.
REV is an annual conference covering topics on online & remote engineering, virtual instrumentation and applications. Like other conferences, REV offers various tracks and simultaneous sessions, tutorials and workshops.
The first REV was held in Villach, Austria in 2004. It operates under the auspices of the International Association of Online Engineering (IAOE).
REV’s venue changes every year, and the categories of its program vary. Historically REV has combined the presentation of academic papers with comparatively practical experience reports, panels, workshops and tutorials.


== Locations and organizers ==


== External links ==
Official website"
597,Continuous automaton,1373722,2371,"A continuous automaton can be described as a cellular automaton extended so the valid states a cell can take are not just discrete (for example, the states consist of integers between 0 and 3), but continuous, for example, the real number range [0,1]. The cells however remain discretely separated from each other. One example is called computational verb cellular network (CVCN)   ., of which the states of cells are in the region of [0,1].
Such automata can be used to model certain physical reactions more closely, such as diffusion. One such diffusion model could conceivably consist of a transition function based on the average values of the neighbourhood of the cell. Many implementations of Finite Element Analysis can be thought of as continuous automata, though this degree of abstraction away from the physics of the problem is probably inappropriate.
Continuous spatial automata resemble continuous automata in having continuous values, but they also have a continuous set of locations rather than restricting the values to a discrete grid of cells.


== Reference notes =="
598,PSI Comp 80,8815337,2367,"In 1979, the British magazine Wireless World published the technical details for a ""Scientific Computer"". Shortly afterward the British firm Powertran used this design for their implementation, which they called the PSI Comp 80. It was sold in the form of a kit of parts for a cased single-board home computer system.
The system was based on a Z80 Microprocessor addressing a mixture of 8 KB of system RAM and EPROM, plus 2 KB of Video RAM.
It used a National Semiconductor MM57109N as a mathematical co-processor to speed up calculations.
The monochrome Video Display Controller could simultaneously display combinations of 32 lines of 64 characters, and 128 x 64 resolution graphics by either displaying a normal character or a ""pseudo graphics"" character, with pixel blocks in a 2x2 matrix. A technique similar to the one used in the TRS-80 - It could later be expanded to a higher resolution, although never to colour.
Ahead of its time, it incorporated a number crunching coprocessor and a novel language embedded in EPROM called Basic Using Reverse Polish - BURP.
Add-ons were developed for the system, including memory expansions, floppy and hard disk interfaces, various software packages and a disk operating system, SCIDOS, which was CP/M-compatible but also included features - structured (pathed) disk folders, etc. - now very familiar to modern-day PC users.
During the mid-1980s, the designer of this system, John Adams M.SC., published a new version of the Scientific Computer - the SC84 (Scientific Computer of 1984). It was based upon a backplane and plug-in cards and modules and featuring a Hitachi HD64180 processor, up to 512 kbytes of RAM and a high resolution colour graphics system.


== References ==


== External links ==
a picture of the advertisement for the PSI Comp 80 in Wireless World
Links to Wireless World articles"
599,Thorsten Altenkirch,48765520,2359,"Thorsten Altenkirch () is a German Professor of Computer Science at the University of Nottingham known for his research on logic, type theory, and homotopy type theory. Altenkirch was part of the 2012/2013 special year on univalent foundations at the Institute for Advanced Study. At Nottingham he co-chairs the Functional Programming Laboratory with Graham Hutton.


== Education ==
Altenkirch obtained his PhD from the University of Edinburgh under Rod Burstall.


== Contributions ==
Altenkirch's work includes: Containers, Epigram programming language, and Homotopy Type Theory: Univalent Foundations of Mathematics (The HoTT Book).
Altenkirch has also been a guest on the YouTube channel Computerphile


== References ==


== External links ==
Altenkirch's personal page at Nottingham"
600,Brazilian Computer Society,8161882,2351,"The Brazilian Computer Society (Portuguese: Sociedade Brasileira de Computação, SBC) was established in 1978, as a scientific and educational organization dedicated to the advancement of Computer Science in Brazil and the associated technologies and applications. SBC is a leading forum for researchers, students and computing professionals working in the various fields of Computer Science and Information Technology, being the largest computer society in South America.
It is structurally organized as a Board of Directors, seven regional chapters and a network of 170 institutional representation offices in universities and research institutions throughout Brazil. Research activities are fostered by sixteen Special Interest Groups.


== Newton Faller Award ==
The Newton Faller Award is awarded by the SBC to honor members who have distinguished themselves throughout their lives for services to the SBC. The award is exclusive to current members and founders and is delivered during the opening ceremony of the SBC Congress. It is named in memory of Newton Faller (1947–1996), a Brazilian computer scientist and electrical engineer.
Recipients include:
Source: Brazilian Computer Society
2015: Taisy Silva Weber (UFRGS)
2014: Ricardo Augusto da Luz Reis (UFRGS)
2013: Ricardo de Oliveiro Anido (UNICAMP)
2012: Philippe Alexandre Olivier Navaux (UFRGS) 
2011: Daltro José Nunes (UFRGS) 
2008: Tomasz Kowaltowski (UNICAMP)
2006: Luiz Fernando Gomes Soares (PUC-RJ)
2004: Flávio Rech Wagner (UFRGS)
2001: Siang Wun Song (IME-USP)
2000: Cláudia Maria Bauzer Medeiros (UNICAMP) 


== References ==


== External links ==
SBC Website"
601,Scalable TCP,6979594,2346,"Type of Transmission Control Protocol which is designed to provide much higher throughput and scalability.
Standard TCP recommendations as per RFC 2581 and RFC 5681 call for congestion window to be halved for each packet lost. Effectively, this process keeps halving the throughput until packet loss stops. Once the packet loss subsides, slow start kicks in to ramp the speed back up. When the window sizes are small, say 1 Mbit/s @ 200 ms round trip time and the window is about 20 packets, this recovery time is quite fast—on the order of a few seconds. But as transfer speeds approach 1 Gbit/s, the recovery time becomes half an hour and for 10 Gbit/s it's over 4 hours.
Scalable TCP modifies the congestion control algorithm. Instead of halving the congestion window size, each packet loss decreases the congestion window by a small fraction (a factor of 1/8 instead of Standard TCP's 1/2) until packet loss stops. When packet loss stops, the rate is ramped up at a slow fixed rate (one packet is added for every one hundred successful acknowledgements) instead of the Standard TCP rate that's the inverse of the congestion window size (thus very large windows take a long time to recover). This helps reduce the recovery time on 10 Gbit/s links from 4+ hours (using Standard TCP) to less than 15 seconds when the round trip time is 200 milliseconds.


== External links ==
Scalable TCP details
CERN Paper about Scalable TCP
Oak Ridge National Laboratory Protocol Speed Tests


== See also ==
UDP-based Data Transfer Protocol


== References =="
602,Biological computation,40861214,2344,"The term ""biological computation"" refers, variously, to any of the following:
- the study of the computations performed by natural biota, including the subject matter of systems biology.
- the design of algorithms inspired by the computational methods of biota 
- the design and engineering of manufactured computational devices using synthetic biology components
- computer methods for the analysis of biological data, elsewhere called computational biology
When biological computation refers to using biology to build computers, it is a subfield of computer science and is distinct from the interdisciplinary science of bioinformatics which simply uses computers to better understand biology.


== Books ==
Biological computation (2011) by Ehud Lamm and Ron Unger, CRC Press, ISBN 978-1-4200-8796-3.


== References =="
603,Verlet list,11518816,2338,"A Verlet list (named after Loup Verlet) is a data structure in molecular dynamics simulations to efficiently maintain a list of all particles within a given cut-off distance of each other.
This method may easily be applied to Monte Carlo simulations. For short-range interactions, a cut-off radius is typically used, beyond which particle interactions are considered ""close enough"" to zero to be safely ignored. For each particle, a Verlet list is constructed that lists all other particles within the potential cut-off distance, plus some extra distance so that the list may be used for several consecutive Monte Carlo ""sweeps"" before being updated. If we wish to use the same Verlet list n times before updating, then the cut-off distance for inclusion in the Verlet list should be 
  
    
      
        
          R
          
            c
          
        
        +
        2
        n
        d
      
    
    {\displaystyle R_{c}+2nd}
  , where 
  
    
      
        
          R
          
            c
          
        
      
    
    {\displaystyle R_{c}}
   is the cut-off distance of the potential, and 
  
    
      
        d
      
    
    {\displaystyle d}
   is the maximum Monte Carlo step of a single particle. Thus, we will spend of order 
  
    
      
        
          N
          
            2
          
        
      
    
    {\displaystyle N^{2}}
   time to compute the Verlet lists (
  
    
      
        N
      
    
    {\displaystyle N}
   is the total number of particles), but are rewarded with 
  
    
      
        n
      
    
    {\displaystyle n}
   Monte Carlo ""sweeps"" of order 
  
    
      
        N
        
          n
          
            2
          
        
      
    
    {\displaystyle Nn^{2}}
   (instead of 
  
    
      
        N
        N
      
    
    {\displaystyle NN}
  ). Optimizing our choice of 
  
    
      
        n
      
    
    {\displaystyle n}
  , it can be shown that the 
  
    
      
        O
        (
        
          N
          
            2
          
        
        )
      
    
    {\displaystyle O(N^{2})}
   problem of Monte Carlo sweeps has been converted to an 
  
    
      
        O
        (
        
          N
          
            5
            
              /
            
            3
          
        
        )
      
    
    {\displaystyle O(N^{5/3})}
   problem by using Verlet lists.
Using cell lists to identify the nearest neighbors in 
  
    
      
        O
        (
        N
        )
      
    
    {\displaystyle O(N)}
   further reduces the computational cost.


== See also ==
Cell lists
Verlet integration
Fast multipole method
Molecular mechanics
Software for molecular mechanics modeling


== References ==


== External links ==
Constructing a Neighbour List — from Introduction to Atomistic Simulations course at the University of Helsinki."
604,Logical clock,1376887,2336,"A logical clock is a mechanism for capturing chronological and causal relationships in a distributed system. Distributed systems may have no physically synchronous global clock, so a logical clock allows global ordering on events from different processes in such systems. The first implementation, the Lamport timestamps, was proposed by Leslie Lamport in 1978 (Turing Award in 2013).
In logical clock systems each process has two data structures: logical local time and logical global time. Logical local time is used by the process to mark its own events, and logical global time is the local information about global time. A special protocol is used to update logical local time after each local event, and logical global time when processes exchange data.
Logical clocks are useful in computation analysis, distributed algorithm design, individual event tracking, and exploring computational progress.
Some noteworthy logical clock algorithms are:
Lamport timestamps, which are monotonically increasing software counters.
Vector clocks, that allow for partial ordering of events in a distributed system.
Version vectors, order replicas, according to updates, in an optimistic replicated system.
Matrix clocks, an extension of vector clocks that also contains information about other processes' views of the system.


== References ==


== External links ==
Distributed System Logical Time // Roberto Baldoni, Silvia Bonomi. MIDLAB, Sapienza University of Rome
Chapter 3: Logical Time // Ajay Kshemkalyani and Mukesh Singhal, Distributed Computing: Principles, Algorithms, and Systems, Cambridge University Press, 2008
Distributed Systems 06. Logical Clocks // Paul Krzyzanowski, Rutgers University, Fall 2014"
605,Canterbury corpus,2937023,2333,"The Canterbury corpus is a collection of files intended for use as a benchmark for testing lossless data compression algorithms. It was created in 1997 at the University of Canterbury, New Zealand and designed to replace the Calgary corpus. The files were selected based on their ability to provide representative performance results.


== Contents ==
In its most commonly used form, the corpus consists of 11 files, selected as ""average"" documents from 11 classes of documents, totaling 2,810,784 bytes as follows.


== See also ==
Data compression


== References ==


== External links ==
The Canterbury Corpus"
606,Symposium on Parallelism in Algorithms and Architectures,21235214,2328,"SPAA, the ACM Symposium on Parallelism in Algorithms and Architectures, is an academic conference in the fields of parallel computing and distributed computing. It is sponsored by the Association for Computing Machinery special interest groups SIGACT and SIGARCH, and it is organized in cooperation with the European Association for Theoretical Computer Science (EATCS).


== History ==
SPAA was first organised on 18–21 June 1989, in Santa Fe, New Mexico, United States. In 1989–2002, SPAA was known as Symposium on Parallel Algorithms and Architectures. In 2003, the name changed to Symposium on Parallelism in Algorithms and Architectures to reflect the extended scope of the conference.
In 2003 and 2007, SPAA was part of the Federated Computing Research Conference (FCRC), and in 1998, 2005, and 2009, SPAA was co-located with the ACM Symposium on Principles of Distributed Computing (PODC).


== See also ==
The list of distributed computing conferences contains other academic conferences in parallel and distributed computing.
The list of computer science conferences contains other academic conferences in computer science.


== Notes ==


== External links ==
SPAA proceedings in ACM digital library.
SPAA proceedings information in DBLP."
607,UB-tree,5786138,2325,"The UB-tree as proposed by Rudolf Bayer and Volker Markl is a balanced tree for storing and efficiently retrieving multidimensional data. It is basically a B+ tree (information only in the leaves) with records stored according to Z-order, also called Morton order. Z-order is simply calculated by bitwise interlacing the keys.
Insertion, deletion, and point query are done as with ordinary B+ trees. To perform range searches in multidimensional point data, however, an algorithm must be provided for calculating, from a point encountered in the data base, the next Z-value which is in the multidimensional search range.
The original algorithm to solve this key problem was exponential with the dimensionality and thus not feasible (""GetNextZ-address""). A solution to this ""crucial part of the UB-tree range query"" linear with the z-address bit length has been described later. This method has already been described in an older paper where using Z-order with search trees has first been proposed.


== References =="
608,B-heap,30047903,2324,"A B-heap is a binary heap implemented to keep subtrees in a single page. This reduces the number of pages accessed by up to a factor of ten for big heaps when using virtual memory, compared with the traditional implementation. The traditional mapping of elements to locations in an array puts almost every level in a different page.
There are other heap variants which are efficient in computers using virtual memory or caches, such as cache-oblivious algorithms, k-heaps, and van Emde Boas layouts.


== See also ==
D-ary heap


== References ==


== External links ==
Implementations at https://github.com/varnish/Varnish-Cache/blob/master/lib/libvarnish/binary_heap.c and http://phk.freebsd.dk/B-Heap/binheap.c
Generic heap implementation with B-heap support.
For more on van Emde Boas layouts see Benjamin Sach Descent into Cache-Oblivion or Cache-oblivious data structures."
609,Hyperscale,39395954,2317,"In computing, hyperscale is the ability of an architecture to scale appropriately as increased demand is added to the system. This typically involves the ability to seamlessly provision and add compute, memory, networking, and storage resources to a given node or set of nodes that make up a larger computing, distributed computing, or grid computing environment. Hyperscale computing is necessary in order to build a robust and scalable cloud, big data, map reduce, or distributed storage system and is often associated with the infrastructure required to run large distributed sites such as Facebook, Google, Microsoft, or Amazon. Companies like Ericsson and Intel provide hyperscale infrastructure kits for IT service providers.


== See also ==
Software-defined networking
Software-defined storage


== References ==


== External links =="
610,Data Privacy Lab,41680785,2316,"Data Privacy Lab is a program dedicated to teaching and research in all areas related to privacy technology. The Data Privacy Lab in Harvard University is operating in the Institute for Quantitative Social Science (IQSS). Latanya Sweeney founded the Lab and continues as its Director. The program was first started in 2001 at Carnegie Mellon University in the Heinz College and in 2002, moved to the School of Computer Science, where it operated until 2011 before moving to Harvard. The University of North Carolina at Charlotte is also running a Data Privacy Lab program and it is functioning in the College of Computing and Informatics.
Some of the projects currently underway in the Data Privacy Lab at Harvard School are related to re-identification, discrimination in online ads, privacy-enhanced linking, fingerprint capture, genomic privacy and complex-care patients. The Data Privacy Lab at The University of North Carolina at Charlotte conducts research in various areas like privacy preserving data mining, privacy issues in social networks, privacy aware database generation for software testing and privacy and anonymity in data integration and dissemination.


== References =="
611,PlusCal,28752783,2299,"PlusCal (formerly called +CAL) is a formal specification language created by Leslie Lamport, which transpiles to TLA+. In contrast to TLA+'s action-oriented focus on distributed systems, PlusCal most resembles an imperative programming language and is better-suited to specifying sequential algorithms. PlusCal was designed to replace pseudocode, retaining its simplicity while providing a formally-defined and verifiable language. A one-bit clock is written in PlusCal as follows:


== See also ==
TLA+
Pseudocode


== References ==


== External links ==
PlusCal tools and documentation are found on the PlusCal Algorithm Language page."
612,Bareiss algorithm,17919686,2284,"In mathematics, the Bareiss algorithm, named after Erwin Bareiss, is an algorithm to calculate the determinant or the echelon form of a matrix with integer entries using only integer arithmetic; any divisions that are performed are guaranteed to be exact (there is no remainder). The method can also be used to compute the determinant of matrices with (approximated) real entries, avoiding the introduction any round-off errors beyond those already present in the input.
During the execution of Bareiss algorithm, every integer that is computed is the determinant of a submatrix of the input matrix. This allows, using the Hadamard inequality, to bound the size of these integers. Otherwise, the Bareiss algorithm may be viewed as a variant of Gaussian elimination and needs roughly the same number of arithmetic operations.
It follows that, for an n × n matrix of maximum (absolute) value 2L for each entry, the Bareiss algorithm runs in O(n3) elementary operations with an O(n n/2 2nL) bound on the absolute value of intermediate values needed. Its computational complexity is thus O(n5L2 (log(n)2 + L2)) when using elementary arithmetic or O(n4L (log(n) + L) log(log(n) + L))) by using fast multiplication.
The general Bareiss algorithm is distinct from the Bareiss algorithm for Toeplitz matrices.


== References ==
Bareiss, Erwin H. (1968), ""Sylvester's Identity and multistep integer-preserving Gaussian elimination"" (PDF), Mathematics of Computation, 22 (102): 565–578, doi:10.2307/2004533, JSTOR 2004533 ."
613,Affective design,4857850,2277,"The notion of affective design emerged from the field of human–computer interaction (HCI) and more specifically from the developing area of affective computing. Affective design involves designing interfaces to enable human-computer interactions where emotional information is communicated by the user in a natural and comfortable way - the computer processes the emotional information and may adapt or respond to try to improve the interaction in some way.


== Aims ==
Affective computing aims to deliver affective interfaces capable of eliciting certain emotional experiences from users. Similarly, affective design attempts to define the subjective emotional relationships between consumers and products and to explore the affective properties that products intend to communicate through their physical attributes. It aims to deliver artefacts capable of eliciting maximum physio-psychological pleasure consumers may obtain through all of their senses.


== References =="
614,Priority R-tree,33373595,2269,"The Priority R-tree is a worst-case asymptotically optimal alternative to the spatial tree R-tree. It was first proposed by Arge, De Berg, Haverkort and Yi, K. in an article from 2004. The prioritized R-tree is essentially a hybrid between a k-dimensional tree and a r-tree in that it defines a given object's N-dimensional bounding volume (called Minimum Bounding Rectangles - MBR) as a point in N-dimensions, represented by the ordered pair of the rectangles. The term prioritized arrives from the introduction of four priority-leaves that represents the most extreme values of each dimensions, included in every branch of the tree. Before answering a window-query by traversing the sub-branches, the prioritized R-tree first checks for overlap in its priority nodes. The sub-branches are traversed (and constructed) by checking whether the least value of the first dimension of the query is above the value of the sub-branches. This gives access to a quick indexation by the value of the first dimension of the bounding box.


== Performance ==
Arge et al. writes that the priority tree always answers window-queries with 
  
    
      
        O
        
          (
          
            
              
                (
                
                  
                    N
                    B
                  
                
                )
              
              
                1
                −
                
                  
                    1
                    d
                  
                
              
            
            +
            
              
                T
                B
              
            
          
          )
        
      
    
    {\displaystyle O\left(\left({\frac {N}{B}}\right)^{1-{\frac {1}{d}}}+{\frac {T}{B}}\right)}
   I/Os, where N is the number of d-dimensional (hyper-) rectangles stored in the R-tree, B is the disk block size, and T is the output size.


== Dimensions ==
In the case of N = 2 the rectangle is represented by 
  
    
      
        
        (
        (
        
          x
          
            m
            i
            n
          
        
        ,
        
          y
          
            m
            i
            n
          
        
        )
        ,
        (
        
          x
          
            m
            a
            x
          
        
        ,
        
          y
          
            m
            a
            x
          
        
        )
        )
      
    
    {\displaystyle \,((x_{min},y_{min}),(x_{max},y_{max}))}
   and the MBR thus four corners 
  
    
      
        
        (
        
          x
          
            m
            i
            n
          
        
        ,
        
          y
          
            m
            i
            n
          
        
        ,
        
          x
          
            m
            a
            x
          
        
        ,
        
          y
          
            m
            a
            x
          
        
        )
      
    
    {\displaystyle \,(x_{min},y_{min},x_{max},y_{max})}
  .


== See also ==
Bounding volume hierarchy
B-tree
R-tree


== References =="
615,Morphological gradient,18585770,2263,"In mathematical morphology and digital image processing, a morphological gradient is the difference between the dilation and the erosion of a given image. It is an image where each pixel value (typically non-negative) indicates the contrast intensity in the close neighborhood of that pixel. It is useful for edge detection and segmentation applications.


== Mathematical definition and types ==
Let 
  
    
      
        f
        :
        E
        ↦
        R
      
    
    {\displaystyle f:E\mapsto R}
   be a grayscale image, mapping points from a Euclidean space or discrete grid E (such as R2 or Z2) into the real line. Let 
  
    
      
        b
        (
        x
        )
      
    
    {\displaystyle b(x)}
   be a grayscale structuring element. Usually, b is symmetric and has short-support, e.g.,

  
    
      
        b
        (
        x
        )
        =
        
          {
          
            
              
                
                  0
                  ,
                
                
                  
                    |
                  
                  x
                  
                    |
                  
                  ≤
                  1
                  ,
                
              
              
                
                  −
                  ∞
                  ,
                
                
                  
                    
                      otherwise
                    
                  
                
              
            
          
          
        
      
    
    {\displaystyle b(x)=\left\{{\begin{array}{ll}0,&|x|\leq 1,\\-\infty ,&{\mbox{otherwise}}\end{array}}\right.}
  .
Then, the morphological gradient of f is given by:

  
    
      
        G
        (
        f
        )
        =
        f
        ⊕
        b
        −
        f
        ⊖
        b
      
    
    {\displaystyle G(f)=f\oplus b-f\ominus b}
  ,
where 
  
    
      
        ⊕
      
    
    {\displaystyle \oplus }
   and 
  
    
      
        ⊖
      
    
    {\displaystyle \ominus }
   denote the dilation and the erosion, respectively.
An internal gradient is given by:

  
    
      
        
          G
          
            i
          
        
        (
        f
        )
        =
        f
        −
        f
        ⊖
        b
      
    
    {\displaystyle G_{i}(f)=f-f\ominus b}
  ,
and an external gradient is given by:

  
    
      
        
          G
          
            e
          
        
        (
        f
        )
        =
        f
        ⊕
        b
        −
        f
      
    
    {\displaystyle G_{e}(f)=f\oplus b-f}
  .
The internal and external gradients are ""thinner"" than the gradient, but the gradient peaks are located on the edges, whereas the internal and external ones are located at each side of the edges. Notice that 
  
    
      
        
          G
          
            i
          
        
        +
        
          G
          
            e
          
        
        =
        G
      
    
    {\displaystyle G_{i}+G_{e}=G}
  .
If 
  
    
      
        b
        (
        0
        )
        ≥
        0
      
    
    {\displaystyle b(0)\geq 0}
  , then all the three gradients have non-negative values at all pixels.


== References ==
Image Analysis and Mathematical Morphology by Jean Serra, ISBN 0-12-637240-3 (1982)
Image Analysis and Mathematical Morphology, Volume 2: Theoretical Advances by Jean Serra, ISBN 0-12-637241-1 (1988)
An Introduction to Morphological Image Processing by Edward R. Dougherty, ISBN 0-8194-0845-X (1992)


== External links ==
Morphological gradients, Centre de Morphologie Mathématique, École_des_Mines_de_Paris"
616,Hirschberg–Sinclair algorithm,7817272,2256,"The Hirschberg–Sinclair algorithm is a distributed algorithm designed for leader election problem in a synchronous ring network. It is named after its inventors, Dan Hirschberg and J. B. Sinclair.
The algorithm requires the use of unique IDs (UID) for each process. The algorithm works in phases and sends its UID out in both directions. The message goes out a distance of 2Phase Number hops and then the message heads back to the originating process. While the messages are heading ""out"" each receiving process will compare the incoming UID to its own. If the UID is greater than its own UID then it will continue the message on. Otherwise if the UID is less than its own UID, it will not pass the information on. At the end of a phase, a process can determine if it will send out messages in the next round by if it received both of its incoming messages. Phases continue until a process receives both of its out messages, from both of its neighbors. At this time the process knows it is the largest UID in the ring and declares itself the leader.


== References ==
Hirschberg, D. S.; Sinclair, J. B. (November 1980), ""Decentralized extrema-finding in circular configurations of processors"", Communications of the ACM, 23 (11): 627–628, doi:10.1145/359024.359029 
Lynch, Nancy A. (1996), ""15.1.2 The HS Algorithm"", Distributed Algorithms, Morgan Kaufmann Publishers, Inc., pp. 482–483 
Tel, Gerard (2000), Introduction to Distributed Algorithms, Cambridge University Press, pp. 232–233, ISBN 9780521794831 
Garg, Vijay K. (2002), ""9.4 Hirschberg–Sinclair Algorithm"", Elements of Distributed Computing, John Wiley & Sons, pp. 111–112, ISBN 9780471036005"
617,Computational number theory,511466,2252,"In mathematics and computer science, computational number theory, also known as algorithmic number theory, is the study of algorithms for performing number theoretic computations.


== See also ==
Computational complexity of mathematical operations
SageMath
Number Theory Library
PARI/GP
Fast Library for Number Theory


== Further reading ==
Eric Bach and Jeffrey Shallit, Algorithmic Number Theory, volume 1: Efficient Algorithms. MIT Press, 1996, ISBN 0-262-02405-5
D. M. Bressoud (1989). Factorisation and Primality Testing. Springer-Verlag. ISBN 0-387-97040-1. 
Buhler, J.P.; P., Stevenhagen, eds. (2008). Algorithmic Number Theory: Lattices, Number Fields, Curves and Cryptography. MSRI Publications. 44. Cambridge University Press. ISBN 978-0-521-20833-8. Zbl 1154.11002. 
Henri Cohen, A Course in Computational Algebraic Number Theory, Graduate Texts in Mathematics 138, Springer-Verlag, 1993.
Richard Crandall and Carl Pomerance, Prime Numbers: A Computational Perspective, Springer-Verlag, 2001, ISBN 0-387-94777-9
Riesel, Hans (1994). Prime Numbers and Computer Methods for Factorization. Progress in Mathematics. 126 (second ed.). Boston, MA: Birkhäuser. ISBN 0-8176-3743-5. Zbl 0821.11001. 
Victor Shoup, A Computational Introduction to Number Theory and Algebra. Cambridge, 2005, ISBN 0-521-85154-8
Samuel S. Wagstaff, Jr. (2013). The Joy of Factoring. Providence, RI: American Mathematical Society. ISBN 978-1-4704-1048-3."
618,Object detection,15822591,2251,"Object detection is a computer technology related to computer vision and image processing that deals with detecting instances of semantic objects of a certain class (such as humans, buildings, or cars) in digital images and videos. Well-researched domains of object detection include face detection and pedestrian detection. Object detection has applications in many areas of computer vision, including image retrieval and video surveillance.


== Uses ==
It is used in face detection and face recognition. It is also used in tracking objects, for example tracking a ball during a football match, tracking movement of a cricket bat, tracking a person in a video.


== Concept ==
Every object class has its own special features that helps in classifying the class – for example all circles are round. Object class detection uses these special features. For example, when looking for circles, objects that are at a particular distance from a point (i.e. the center) are sought. Similarly, when looking for squares, objects that are perpendicular at corners and have equal side lengths are needed. A similar approach is used for face identification where eyes, nose, and lips can be found and features like skin color and distance between eyes can be found.


== References ==
""Object Class Detection"". Vision.eecs.ucf.edu. Retrieved 2013-10-09. 
""ETHZ - Computer Vision Lab: Publications"". Vision.ee.ethz.ch. Retrieved 2013-10-09. 


== See also ==
Teknomo–Fernandez algorithm


== External links ==
Multiple object class detection


== References =="
619,Smallest grammar problem,4929352,2248,"In data compression and the theory of formal languages, the smallest grammar problem is the problem of finding the smallest context-free grammar that generates a given string of characters. The size of a grammar is defined by some authors as the number of symbols on the right side of the production rules. Others also add the number of rules to that. The (decision version of the) problem is NP-complete.


== See also ==
Grammar-based code
Kolmogorov Complexity
Lossless data compression
Straight-line grammar


== References ==

Charikar, Moses; Lehman, Eric; Liu, Ding; Panigrahy, Rina; Prabhakaran, Manoj; Rasala, April; Sahai, Amit; Shelat, Abhi (2002). ""Approximating the Smallest Grammar: Kolmogorov Complexity in Natural Models"". Proceedings of the thirty-fourth annual ACM symposium on theory of computing (STOC 2002), Montreal, Quebec, Canada, May 19–21, 2002 (PDF). New York, NY: ACM Press. pp. 792–801. doi:10.1145/509907.510021. ISBN 1-581-13495-9. Zbl 1192.68397."
620,IBM Laboratory Vienna,56216519,2238,"IBM Laboratory Vienna was an IBM research laboratory based in Vienna, Austria.
The laboratory started with a group led by Heinz Zemanek that moved from the Technische Hochschule (now the Technical University of Vienna). Initially, the group worked on computer hardware projects. Later a compiler for the ALGOL 60 programming language was produced. The group built on ideas of Calvin C. Elgot, Peter Landin, and John McCarthy, to create an operational semantics that could define the whole of IBM's PL/I programming language. The meta-language used for this was dubbed by people outside the laboratory as the Vienna Definition Language (VDL). These descriptions were used for compiler design research into compiler design during 1968–70.
The formal method VDM (Vienna Development Method) was a result of research at the laboratory by Dines Bjørner, Cliff Jones, Peter Lucas, and others.


== See also ==
IBM Research


== References =="
621,Qualification principle,17246195,2231,"In programming language theory, the qualification principle states that any semantically meaningful syntactic class may admit local definitions. In other words, it's possible to include a block in any syntactic class, provided that the phrases of that class specify some kind of computation. (Watt 1990)
A common examples for of this principle includes:
block command -- a command containing a local declaration, which is used only for executing this command. In the following excerpt from a C program, tmp variable declared is local to the surrounding block command:

block expression -- an expression containing a local declaration, which is used only for evaluating this expression. In the following excerpt from ML program, local declaration of g can be used only during evaluation of the following expression:

block declaration is one containing a local declaration, the bindings produced by which are used only for elaborating the block declaration. In the following excerpt from ML program, local declaration of function leap, using an auxiliary function multiple:


== References ==
Watt, David A. (1990) [1990]. ""Bindings"". Programming Language Concepts and Paradigms. Prentice Hall. pp. 82–83. ISBN 0-13-728874-3."
622,Programming by example,8880387,2225,"In computer science, programming by example (PbE), also termed programming by demonstration or more generally as demonstrational programming, is an end-user development technique for teaching a computer new behavior by demonstrating actions on concrete examples. The system records user actions and infers a generalized program that can be used on new examples.
PbE is intended to be easier to do than traditional computer programming, which generally requires learning and using a programming language. Many PbE systems have been developed as research prototypes, but few have found widespread real-world application. More recently, PbE has proved to be a useful paradigm for creating scientific work-flows. PbE is used in two independent clients for the BioMOBY protocol: Seahawk and Gbrowse moby. Also the programming by demonstration term has been mostly adopted by robotics researchers for teaching new behaviors to the robot through a physical demonstration of the task.


== See also ==
Example-based machine translation
Inductive programming
Lapis (text editor), which allows simultaneous editing of similar items in multiple selections created by example
Programming by demonstration
Test-driven development


== External links ==
Henry Lieberman's page on Programming by Example
Online copy of Watch What I Do, Allen Cypher's book on Programming by Demonstration
Online copy of Your Wish is My Command, Henry Lieberman's sequel to Watch What I Do
A Visual Language for Data Mapping, John Carlson's description of an Integrated Development Environment (IDE) that used Programming by Example (desktop objects) for data mapping, and an iconic language for recording operations"
623,Block Lanczos algorithm,14572979,2220,"In computer science, the block Lanczos algorithm is an algorithm for finding the nullspace of a matrix over a finite field, using only multiplication of the matrix by long, thin matrices. Such matrices are considered as vectors of tuples of finite-field entries, and so tend to be called 'vectors' in descriptions of the algorithm.
The block Lanczos algorithm is amongst the most efficient methods known for finding nullspaces, which is the final stage in integer factorization algorithms such as the quadratic sieve and number field sieve, and its development has been entirely driven by this application.


== Parallelization issues ==
The algorithm is essentially not parallel: it is of course possible to distribute the matrix–'vector' multiplication, but the whole vector must be available for the combination step at the end of each iteration, so all the machines involved in the calculation must be on the same fast network. In particular, it is not possible to widen the vectors and distribute slices of vectors to different independent machines.
The block Wiedemann algorithm is more useful in contexts where several systems each large enough to hold the entire matrix are available, since in that algorithm the systems can run independently until a final stage at the end.


== History ==
The block Lanczos algorithm was developed by Peter Montgomery and published in 1995; it is based on, and bears a strong resemblance to, the Lanczos algorithm for finding eigenvalues of large sparse real matrices.


== References =="
624,Unicode collation algorithm,641532,2220,"The Unicode collation algorithm (UCA) is an algorithm defined in Unicode Technical Report #10, which defines a customizable method to compare two strings. These comparisons can then be used to collate or sort text in any writing system and language that can be represented with Unicode.
Unicode Technical Report #10 also specifies the Default Unicode Collation Element Table (DUCET). This datafile specifies the default collation ordering. The DUCET is customizable for different languages. Some such customisations can be found in Common Locale Data Repository (CLDR).
An important open source implementation of UCA is included with the International Components for Unicode, ICU. ICU also supports tailoring and the collation tailorings from CLDR are included in ICU. You can see the effects of tailoring and a large number of language specific tailorings in the on-line ICU Locale Explorer.


== See also ==
Collation
ISO/IEC 14651
European ordering rules (EOR)
Common Locale Data Repository (CLDR)


== External links and references ==
Unicode Collation Algorithm: Unicode Technical Standard #10
Mimer SQL Unicode Collation Charts
MySQL UCA-based Unicode Collation Charts


== Tools ==
ICU Locale Explorer An online demonstration of the Unicode Collation Algorithm using International Components for Unicode
msort A sort program that provides an unusual level of flexibility in defining collations and extracting keys.
OpenRTL A library of functions using Unicode collation based on the Unicode collation algorithm. Also supports the customized Unicode collations for the locales defined by CLDR."
625,Input/output completion port,8509979,2219,"Input/output completion port (IOCP) is an API for performing multiple simultaneous asynchronous input/output operations in Windows NT versions 3.5 and later, AIX and on Solaris 10 and later. An input/output completion port object is created and associated with a number of sockets or file handles. When I/O services are requested on the object, completion is indicated by a message queued to the I/O completion port. A process requesting I/O services is not notified of completion of the I/O services, but instead checks the I/O completion port's message queue to determine the status of its I/O requests. The I/O completion port manages multiple threads and their concurrency.


== See also ==
Overlapped I/O
kqueue
epoll


== References ==


== External links ==
Article ""Inside I/O Completion Ports"" at the Wayback Machine (archived November 1, 2010) by Mark Russinovich.
IOCPSOCK - an IOCP implementation of a channel driver for the Tcl language to run on Windows NT/2K/XP/Vista"
626,Ranking (information retrieval),19988623,2215,"Information retrieval (IR) is the activity of obtaining information resources relevant to an information need from a collection of information resources. Searches can be based on full-text or other content-based indexing. Information retrieval is the science of searching for information in a document, searching for documents themselves, and also searching for metadata that describe data, and for databases of texts, images or sounds.
Automated information retrieval systems are used to reduce what has been called information overload. An IR systems is a software that provide access to books, journals and other documents, stores them and manages the document. Web search engines are the most visible IR applications.


== Overview ==
An information retrieval process begins when a user enters a query into the system. Queries are formal statements of information needs, for example search strings in web search engines. In information retrieval a query does not uniquely identify a single object in the collection. Instead, several objects may match the query, perhaps with different degrees of relevancy.
An object is an entity that is represented by information in a content collection or database. User queries are matched against the database information. However, as opposed to classical SQL queries of a database, in information retrieval the results returned may or may not match the query, so results are typically ranked. This ranking of results is a key difference of information retrieval searching compared to database searching.
Depending on the application the data objects may be, for example, text documents, images, audio, mind maps or videos. Often the documents themselves are not kept or stored directly in the IR system, but are instead represented in the system by document surrogates or metadata.
Most IR systems compute a numeric score on how well each object in the database matches the query, and rank the objects according to this value. The top ranking objects are then shown to the user. The process may then be iterated if the user wishes to refine the query.


== History ==
The idea of using computers to search for relevant pieces of information was popularized in the article As We May Think by Vannevar Bush in 1945. It would appear that Bush was inspired by patents for a 'statistical machine' - filed by Emanuel Goldberg in the 1920s and '30s - that searched for documents stored on film. The first description of a computer searching for information was described by Holmstrom in 1948, detailing an early mention of the Univac computer. Automated information retrieval systems were introduced in the 1950s: one even featured in the 1957 romantic comedy, Desk Set. In the 1960s, the first large information retrieval research group was formed by Gerard Salton at Cornell. By the 1970s several different retrieval techniques had been shown to perform well on small text corpora such as the Cranfield collection (several thousand documents). Large-scale retrieval systems, such as the Lockheed Dialog system, came into use early in the 1970s.
In 1992, the US Department of Defense along with the National Institute of Standards and Technology (NIST), cosponsored the Text Retrieval Conference (TREC) as part of the TIPSTER text program. The aim of this was to look into the information retrieval community by supplying the infrastructure that was needed for evaluation of text retrieval methodologies on a very large text collection. This catalyzed research on methods that scale to huge corpora. The introduction of web search engines has boosted the need for very large scale retrieval systems even further.


== Model types ==

For effectively retrieving relevant documents by IR strategies, the documents are typically transformed into a suitable representation. Each retrieval strategy incorporates a specific model for its document representation purposes. The picture on the right illustrates the relationship of some common models. In the picture, the models are categorized according to two dimensions: the mathematical basis and the properties of the model.


=== First dimension: mathematical basis ===
Set-theoretic models represent documents as sets of words or phrases. Similarities are usually derived from set-theoretic operations on those sets. Common models are:
Standard Boolean model
Extended Boolean model
Fuzzy retrieval

Algebraic models represent documents and queries usually as vectors, matrices, or tuples. The similarity of the query vector and document vector is represented as a scalar value.
Vector space model
Generalized vector space model
(Enhanced) Topic-based Vector Space Model
Extended Boolean model
Latent semantic indexing a.k.a. latent semantic analysis

Probabilistic models treat the process of document retrieval as a probabilistic inference. Similarities are computed as probabilities that a document is relevant for a given query. Probabilistic theorems like the Bayes' theorem are often used in these models.
Binary Independence Model
Probabilistic relevance model on which is based the okapi (BM25) relevance function
Uncertain inference
Language models
Divergence-from-randomness model
Latent Dirichlet allocation

Feature-based retrieval models view documents as vectors of values of feature functions (or just features) and seek the best way to combine these features into a single relevance score, typically by learning to rank methods. Feature functions are arbitrary functions of document and query, and as such can easily incorporate almost any other retrieval model as just another feature.


=== Second dimension: properties of the model ===
Models without term-interdependencies treat different terms/words as independent. This fact is usually represented in vector space models by the orthogonality assumption of term vectors or in probabilistic models by an independency assumption for term variables.
Models with immanent term interdependencies allow a representation of interdependencies between terms. However the degree of the interdependency between two terms is defined by the model itself. It is usually directly or indirectly derived (e.g. by dimensional reduction) from the co-occurrence of those terms in the whole set of documents.
Models with transcendent term interdependencies allow a representation of interdependencies between terms, but they do not allege how the interdependency between two terms is defined. They rely an external source for the degree of interdependency between two terms. (For example, a human or sophisticated algorithms.)


== Performance and correctness measures ==
The evaluation of an information retrieval system is the process of assessing how well a system meets the information needs of its users. In general, measurement considers a collection of documents to be searched and a search query. Traditional evaluation metrics, designed for Boolean retrieval or top-k retrieval, include precision and recall. All measures assume a ground truth notion of relevancy: every document is known to be either relevant or non-relevant to a particular query. In practice, queries may be ill-posed and there may be different shades of relevancy.
Virtually all modern evaluation metrics are designed for ranked retrieval without any explicit rank cutoff, taking into account the relative order of the documents retrieved by the search engines and giving more weight to documents returned at higher ranks.


== Timeline ==
Before the 1900s
1801: Joseph Marie Jacquard invents the Jacquard loom, the first machine to use punched cards to control a sequence of operations.
1880s: Herman Hollerith invents an electro-mechanical data tabulator using punch cards as a machine readable medium.
1890 Hollerith cards, keypunches and tabulators used to process the 1890 US Census data.

1920s-1930s
Emanuel Goldberg submits patents for his ""Statistical Machine” a document search engine that used photoelectric cells and pattern recognition to search the metadata on rolls of microfilmed documents.

1940s–1950s
late 1940s: The US military confronted problems of indexing and retrieval of wartime scientific research documents captured from Germans.
1945: Vannevar Bush's As We May Think appeared in Atlantic Monthly.
1947: Hans Peter Luhn (research engineer at IBM since 1941) began work on a mechanized punch card-based system for searching chemical compounds.

1950s: Growing concern in the US for a ""science gap"" with the USSR motivated, encouraged funding and provided a backdrop for mechanized literature searching systems (Allen Kent et al.) and the invention of citation indexing (Eugene Garfield).
1950: The term ""information retrieval"" was coined by Calvin Mooers.
1951: Philip Bagley conducted the earliest experiment in computerized document retrieval in a master thesis at MIT.
1955: Allen Kent joined Case Western Reserve University, and eventually became associate director of the Center for Documentation and Communications Research. That same year, Kent and colleagues published a paper in American Documentation describing the precision and recall measures as well as detailing a proposed ""framework"" for evaluating an IR system which included statistical sampling methods for determining the number of relevant documents not retrieved.
1958: International Conference on Scientific Information Washington DC included consideration of IR systems as a solution to problems identified. See: Proceedings of the International Conference on Scientific Information, 1958 (National Academy of Sciences, Washington, DC, 1959)
1959: Hans Peter Luhn published ""Auto-encoding of documents for information retrieval.""

1960s:
early 1960s: Gerard Salton began work on IR at Harvard, later moved to Cornell.
1960: Melvin Earl Maron and John Lary Kuhns published ""On relevance, probabilistic indexing, and information retrieval"" in the Journal of the ACM 7(3):216–244, July 1960.
1962:
Cyril W. Cleverdon published early findings of the Cranfield studies, developing a model for IR system evaluation. See: Cyril W. Cleverdon, ""Report on the Testing and Analysis of an Investigation into the Comparative Efficiency of Indexing Systems"". Cranfield Collection of Aeronautics, Cranfield, England, 1962.
Kent published Information Analysis and Retrieval.

1963:
Weinberg report ""Science, Government and Information"" gave a full articulation of the idea of a ""crisis of scientific information."" The report was named after Dr. Alvin Weinberg.
Joseph Becker and Robert M. Hayes published text on information retrieval. Becker, Joseph; Hayes, Robert Mayo. Information storage and retrieval: tools, elements, theories. New York, Wiley (1963).

1964:
Karen Spärck Jones finished her thesis at Cambridge, Synonymy and Semantic Classification, and continued work on computational linguistics as it applies to IR.
The National Bureau of Standards sponsored a symposium titled ""Statistical Association Methods for Mechanized Documentation."" Several highly significant papers, including G. Salton's first published reference (we believe) to the SMART system.

mid-1960s:

National Library of Medicine developed MEDLARS Medical Literature Analysis and Retrieval System, the first major machine-readable database and batch-retrieval system.
Project Intrex at MIT.

1965: J. C. R. Licklider published Libraries of the Future.
1966: Don Swanson was involved in studies at University of Chicago on Requirements for Future Catalogs.

late 1960s: F. Wilfrid Lancaster completed evaluation studies of the MEDLARS system and published the first edition of his text on information retrieval.
1968:
Gerard Salton published Automatic Information Organization and Retrieval.
John W. Sammon, Jr.'s RADC Tech report ""Some Mathematics of Information Storage and Retrieval..."" outlined the vector model.
1969: Sammon's ""A nonlinear mapping for data structure analysis"" (IEEE Transactions on Computers) was the first proposal for visualization interface to an IR system.

1970s
early 1970s:

First online systems—NLM's AIM-TWX, MEDLINE; Lockheed's Dialog; SDC's ORBIT.
Theodor Nelson promoting concept of hypertext, published Computer Lib/Dream Machines.

1971: Nicholas Jardine and Cornelis J. van Rijsbergen published ""The use of hierarchic clustering in information retrieval"", which articulated the ""cluster hypothesis.""
1975: Three highly influential publications by Salton fully articulated his vector processing framework and term discrimination model:

A Theory of Indexing (Society for Industrial and Applied Mathematics)
A Theory of Term Importance in Automatic Text Analysis (JASIS v. 26)
A Vector Space Model for Automatic Indexing (CACM 18:11)

1978: The First ACM SIGIR conference.
1979: C. J. van Rijsbergen published Information Retrieval (Butterworths). Heavy emphasis on probabilistic models.
1979: Tamas Doszkocs implemented the CITE natural language user interface for MEDLINE at the National Library of Medicine. The CITE system supported free form query input, ranked output and relevance feedback.

1980s
1980: First international ACM SIGIR conference, joint with British Computer Society IR group in Cambridge.
1982: Nicholas J. Belkin, Robert N. Oddy, and Helen M. Brooks proposed the ASK (Anomalous State of Knowledge) viewpoint for information retrieval. This was an important concept, though their automated analysis tool proved ultimately disappointing.
1983: Salton (and Michael J. McGill) published Introduction to Modern Information Retrieval (McGraw-Hill), with heavy emphasis on vector models.
1985: David Blair and Bill Maron publish: An Evaluation of Retrieval Effectiveness for a Full-Text Document-Retrieval System
mid-1980s: Efforts to develop end-user versions of commercial IR systems.
1985–1993: Key papers on and experimental systems for visualization interfaces.
Work by Donald B. Crouch, Robert R. Korfhage, Matthew Chalmers, Anselm Spoerri and others.

1989: First World Wide Web proposals by Tim Berners-Lee at CERN.

1990s
1992: First TREC conference.
1997: Publication of Korfhage's Information Storage and Retrieval with emphasis on visualization and multi-reference point systems.
1999: Publication of Ricardo Baeza-Yates and Berthier Ribeiro-Neto's Modern Information Retrieval by Addison Wesley, the first book that attempts to cover all IR.
late 1990s: Web search engines implementation of many features formerly found only in experimental IR systems. Search engines become the most common and maybe best instantiation of IR models.


== Major conferences ==
SIGIR: Conference on Research and Development in Information Retrieval
ECIR: European Conference on Information Retrieval
CIKM: Conference on Information and Knowledge Management
WWW: International World Wide Web Conference
WSDM: Conference on Web Search and Data Mining
ICTIR: International Conference on Theory of Information Retrieval


== Awards in the field ==
Tony Kent Strix award
Gerard Salton Award


== Leading IR Research Groups ==
Center for Intelligent Information Retrieval (CIIR) at the University of Massachusetts Amherst 
Information Retrieval Group at the University of Glasgow 
Information and Language Processing Systems (ILPS) at the University of Amsterdam 
Information Retrieval Group (THUIR) at Tsinghua University 


== See also ==


== References ==


== Further reading ==
Ricardo Baeza-Yates, Berthier Ribeiro-Neto. Modern Information Retrieval: The Concepts and Technology behind Search (second edition). Addison-Wesley, UK, 2011.
Stefan Büttcher, Charles L. A. Clarke, and Gordon V. Cormack. Information Retrieval: Implementing and Evaluating Search Engines. MIT Press, Cambridge, Mass., 2010.
Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze. Introduction to Information Retrieval. Cambridge University Press, 2008.


== External links ==
ACM SIGIR: Information Retrieval Special Interest Group
BCS IRSG: British Computer Society - Information Retrieval Specialist Group
Text Retrieval Conference (TREC)
Forum for Information Retrieval Evaluation (FIRE)
Information Retrieval (online book) by C. J. van Rijsbergen
Information Retrieval Wiki
Information Retrieval Facility
Information Retrieval @ DUTH
TREC report on information retrieval evaluation techniques
How eBay measures search relevance
Information retrieval performance evaluation tool @ Athena Research Centre"
627,List of abstractions (computer science),46814283,2212,"This list contains abstractions used in computer programming.


== Notes ==


== References =="
628,Genetic and Evolutionary Computation Conference,49305019,2205,"The Genetic and Evolutionary Computation Conference (GECCO) is the premier conference in the area of genetic and evolutionary computation. GECCO has been held every year since 1999, when it was first established as a recombination of the International Conference on Genetic Algorithms (ICGA) and the Annual Genetic Programming Conference (GP).
GECCO presents the latest high-quality results in genetic and evolutionary computation. Topics of interest include: genetic algorithms, genetic programming, evolution strategies, evolutionary programming, estimation of distribution algorithms, memetic algorithms, hyper-heuristics, evolutionary robotics, evolvable hardware, artificial life, ant colony optimization algorithms, swarm intelligence, artificial immune systems, digital entertainment technologies, evolutionary art, evolutionary combinatorial optimization, metaheuristics, evolutionary multi-objective optimization, evolutionary machine learning, search-based software engineering, theory, real-world applications, and more.
Other important conferences in the field are IEEE Congress on Evolutionary Computation (CEC), Parallel Problem Solving from Nature (PPSN) and EvoStar (a group name for four co-located conferences, EuroGP, EvoCOP, EvoMUSART, and EvoApplications).
GECCO is the main annual conference of the Special Interest Group on Genetic and Evolutionary Computation (SIGEVO), which is a Special Interest Group (SIG) of the Association for Computing Machinery (ACM).


== External links ==
SIGEVO
A list of links to all GECCO websites since 1999"
629,Evolutionary programming,460689,2202,"Evolutionary programming is one of the four major evolutionary algorithm paradigms. It is similar to genetic programming, but the structure of the program to be optimized is fixed, while its numerical parameters are allowed to evolve.
It was first used by Lawrence J. Fogel in the US in 1960 in order to use simulated evolution as a learning process aiming to generate artificial intelligence. Fogel used finite-state machines as predictors and evolved them. Currently evolutionary programming is a wide evolutionary computing dialect with no fixed structure or (representation), in contrast with some of the other dialects. It is becoming harder to distinguish from evolutionary strategies.
Its main variation operator is mutation; members of the population are viewed as part of a specific species rather than members of the same species therefore each parent generates an offspring, using a (μ + μ) survivor selection.


== See also ==
Artificial intelligence
Genetic algorithm
Genetic operator


== References ==
Fogel, L.J., Owens, A.J., Walsh, M.J. (1966), Artificial Intelligence through Simulated Evolution, John Wiley.
Fogel, L.J. (1999), Intelligence through Simulated Evolution : Forty Years of Evolutionary Programming, John Wiley.
Eiben, A.E., Smith, J.E. (2003), Introduction to Evolutionary Computing, Springer. ISBN 3-540-40184-9


== External links ==
The Hitch-Hiker's Guide to Evolutionary Computation: What's Evolutionary Programming (EP)?
Evolutionary Programming by Jason Brownlee (PhD)"
630,System migration,11956097,2202,"System migration involves moving a set of instructions or programs, e.g., PLC (programmable logic controller) programs, from one platform to another, minimizing reengineering.
Migration of systems can also involve downtime, while the old system is replaced with a new one.
Migration can be from a mainframe computer which has a closed architecture, to an open system which employ x86 servers. As well, migration can be from an open system to a Cloud Computing platform. The motivation for this can be the cost savings. Migration can be simplified by tools that can automatically convert data from one form to another. There are also tools to convert the code from one platform to another to be either compiled or interpreted. Vendors of such tools include Micro Focus and Metamining. An alternative to converting the code is the use of software that can run the code from the old system on the new system. Examples are Oracle Tuxedo Application Rehosting Workbench, Morphis - Transformer and products for LINC 4GL.
Migration may also be required when the hardware is no longer available. See JOVIAL.


== See also ==
Data conversion
Data migration
Data transformation
Software migration
Software modernization
List of Linux adopters


== References =="
631,Information Systems Professional,4496564,2200,"Certified Information Systems Security Professional (CISSP) is an independent information security certification granted by the International Information System Security Certification Consortium, also known as (ISC)².
As of 1 January 2018, there are 122,289 (ISC)² members holding the CISSP certification worldwide, in 166 countries with the United States holding the highest member count at 79,617 members. In June 2004, the CISSP designation was accredited under the ANSI ISO/IEC Standard 17024:2003. It is also formally approved by the U.S. Department of Defense (DoD) in both their Information Assurance Technical (IAT) and Managerial (IAM) categories for their DoDD 8570 certification requirement. The CISSP has been adopted as a baseline for the U.S. National Security Agency's ISSEP program. CISSP is a globally recognized certification in the field of IT security.


== History ==
In the mid-1980s, a need arose for a standardized, vendor-neutral certification program that provided structure and demonstrated competence. In November 1988, the Special Interest Group for Computer Security (SIG-CS), a member of the Data Processing Management Association (DPMA), brought together several organizations interested in this goal. The International Information Systems Security Certification Consortium or ""(ISC)²"" formed in mid-1989 as a non-profit organization.
By 1990, the first working committee to establish a Common Body of Knowledge (CBK) had been formed. The first version of the CBK was finalized by 1992, and the CISSP credential was launched by 1994.


== Certification subject matter ==
The CISSP curriculum covers subject matter in a variety of Information Security topics. The CISSP examination is based on what (ISC)² terms the Common Body of Knowledge (or CBK). According to (ISC)², ""the CISSP CBK is a taxonomy – a collection of topics relevant to information security professionals around the world. The CISSP CBK establishes a common framework of information security terms and principles that allow information security professionals worldwide to discuss, debate and resolve matters pertaining to the profession with a common understanding.""
From 2015, the CISSP curriculum is divided into eight domains:
Security and Risk Management
Asset Security
Security Engineering
Communications and Network Security
Identity and Access Management
Security Assessment and Testing
Security Operations
Software Development Security
Before 2015, it covered ten similar domains.


== Requirements ==
Possess a minimum of five years of direct full-time security work experience in two or more of the (ISC)² information security domains (CBK). One year may be waived for having either a four-year college degree, a master's degree in Information Security, or for possessing one of a number of other certifications. A candidate without the five years of experience may earn the Associate of (ISC)² designation by passing the required CISSP examination, valid for a maximum of six years. During those six years a candidate will need to obtain the required experience and submit the required endorsement form for certification as a CISSP. Upon completion of the professional experience requirements the certification will be converted to CISSP status.
Attest to the truth of their assertions regarding professional experience and accept the CISSP Code of Ethics.
Answer questions regarding criminal history and related background.
Pass the multiple choice CISSP exam with a scaled score of 700 points or greater out of 1000 possible points.
Have their qualifications endorsed by another (ISC)² certification holder in good standing.


== Concentrations ==
Holders of CISSP certifications can earn additional certifications in areas of specialty. There are three possibilities:
Information Systems Security Architecture Professional (CISSP-ISSAP)
Information Systems Security Engineering Professional (CISSP-ISSEP), an advanced information security certification issued by (ISC)2 that focuses on the engineering aspects of information security. In October 2014 it was announced that some of its curriculum would be made available to the public by the United States Department of Homeland Security through its National Initiative for Cybersecurity Careers and Studies program. Both ZDNet and Network World have named ISSEP one of tech’s most valuable certifications.
Information Systems Security Management Professional (CISSP-ISSMP), an advanced information security certification issued by (ISC)2 that focuses on the management aspects of information security. In September 2014, Computerworld rated ISSMP one of the top ten most valuable certifications in all of tech.


== Ongoing certification ==
The CISSP credential is valid for three years; most holders renew by submitting Continuing Professional Education (CPE) credits. There is also a yearly membership fee required to maintain certification.


== Value ==
In 2005, Certification Magazine surveyed 35,167 IT professionals in 170 countries on compensation and found that CISSPs led their list of certificates ranked by salary. A 2006 Certification Magazine salary survey also ranked the CISSP credential highly, and ranked CISSP concentration certifications as the top best-paid credentials in IT.
In 2008, another study came to the conclusion that IT professionals with CISSP (or other major security certifications) tend to have salaries $21,000 higher than IT professionals without such certificates. However, there's no proof that there's any cause-and-effect between the certificate and salaries.
As of 2017, a study by CyberSecurityDegrees.com surveyed some 10,000 current and historical cyber security job listings that preferred candidates holding CISSP certifications. CyberSecurityDegrees found that these job openings offered an average salary of $17,526 more than the average cyber security salary.
ANSI certifies that CISSP meets the requirements of ANSI/ISO/IEC Standard 17024, a personnel certification accreditation program.


== References ==


== External links ==
Official website"
632,"Association for Logic, Language and Information",22910426,2195,"The Association for Logic, Language and Information (FoLLI) is an international, especially European, learned society administered from Nancy-Université in France. It was founded in 1991 ""to advance the practicing of research and education on the interfaces between Logic, Linguistics, Computer Science and Cognitive Science and related disciplines."" It publishes the academic journal Journal of Logic, Language and Information (JoLLI), holds an annual academic conference called the European Summer School in Logic, Language and Information (ESSLLI) and awards the E. W. Beth Dissertation Prize to outstanding dissertations in the fields of Logic, Language, and Information. ""Very broadly, FoLLI's basic natural focus is on the phenomenon of information.""


== See also ==
Dynamic semantics
Generalized quantifier
Information theory
Type theory


== References ==


== Bibliography ==
Program for ESSLLI 2012: Opole
Program for ESSLLI 2009: Bordeaux
Program for ESSLLI 2008: Hamburg
Program for ESSLLI 2007: Dublin
Program for ESSLLI 2006: Málaga
Program for ESSLLI 2005: Edinburgh


== External links ==
Association for Logic, Language and Information — FoLLI official home page"
633,Ethash,54463708,2193,"Ethash is the proof-of-work function in Ethereum-based blockchain currencies. It uses Keccak, a hash function eventually standardized to SHA-3. These two are different, and should not be confused. From version 1.0 Ethash has been designed to be ASIC-resistant via memory-hardness (harder to implement in special ASIC chips) and easily verifiable. It also uses a slightly modified version of earlier Dagger and Hashimoto hashes to remove computational overhead. Previously referred to as Dagger-Hashimoto, the Ethash function has evolved over time. Ethash uses an initial 1 GB dataset known as the Ethash DAG and a 16 MB cache for light clients to hold. These are regenerated every 30000 blocks, known as an epoch. Miners grab slices of the DAG to generate mix-hashes using transaction and receipt data, along with a cryptographic nonce to generate a hash below a dynamic target difficulty.


== References =="
634,IJCAI Computers and Thought Award,2088095,2178,"The IJCAI Computers and Thought Award is presented every two years by the International Joint Conferences on Artificial Intelligence (IJCAI), recognizing outstanding young scientists in artificial intelligence. It was originally funded with royalties received from the book Computers and Thought (edited by Edward Feigenbaum and Julian Feldman), and is currently funded by IJCAI.
It is considered to be ""the premier award for artificial intelligence researchers under the age of 35"".


== Award recipients ==
Terry Winograd (1971)
Patrick Winston (1973)
Chuck Rieger (1975)
Douglas Lenat (1977)
David Marr (1979)
Gerald Sussman (1981)
Tom Mitchell (1983)
Hector Levesque (1985)
Johan de Kleer (1987)
Henry Kautz (1989)
Rodney Brooks (1991)
Martha E. Pollack (1991)
Hiroaki Kitano (1993)
Sarit Kraus (1995)
Stuart Russell (1995)
Leslie Kaelbling (1997)
Nicholas Jennings (1999)
Daphne Koller (2001)
Tuomas Sandholm (2003)
Peter Stone (2007)
Carlos Guestrin (2009)
Andrew Ng (2009)
Vincent Conitzer (2011)
Malte Helmert (2011)
Kristen Grauman (2013)
Ariel D. Procaccia (2015)
Percy Liang (2016)
for his contributions to both the approach of semantic parsing for natural language understanding and better methods for learning latent-variable models, sometimes with weak supervision, in machine learning. 
Devi Parikh (2017)


== References ==


== External links ==
IJCAI Awards"
635,Compile time,191766,2176,"In computer science, compile time refers to either the operations performed by a compiler (the ""compile-time operations""), programming language requirements that must be met by source code for it to be successfully compiled (the ""compile-time requirements""), or properties of the program that can be reasoned about during compilation. Compile time refers to the time duration during which the statements written in any programming language are checked for errors.


== Overview ==
The operations performed at compile time usually include syntax analysis, various kinds of semantic analysis (e.g., type checks and instantiation of template) and code generation.
Programming language definitions usually specify compile time requirements that source code must meet to be successfully compiled. For example, languages may stipulate that the amount of storage required by types and variables can be deduced.
Properties of a program that can be reasoned about at compile time include range-checks (e.g., proving that an array index will not exceed the array bounds), deadlock freedom in concurrent languages, or timings (e.g., proving that a sequence of code takes no more than an allocated amount of time).
Compile time occurs before link time (when the output of one or more compiled files are joined together) and runtime (when a program is executed). In some programming languages it may be necessary for some compilation and linking to occur at runtime. There is a trade-off between compile-time and link-time in that many compile time operations can be deferred to link-time without incurring extra run-time.
""Compile time"" can also refer to the amount of time required for compilation.


== See also ==
Link time
Run time (program lifecycle phase)
Compiling
Type system
Just in time compilation"
636,Difference list,11436072,2169,"In computer science, the term difference list may refer to one of two data structures for representing lists. One of these data structures contains two lists, and represents the difference of those two lists. The second data structure is a functional representation of a list with an efficient concatenation operation. In the second approach, difference lists are implemented as single-argument functions, which take a list as argument and prepend to that list. As a consequence, concatenation of difference lists of the second type is implemented essentially as function composition, which is O(1). However, of course the list still has to be constructed eventually (assuming all of its elements are needed), which is plainly at least O(n).


== Difference lists as functions ==
A difference list of the second sort represents lists as a function f, which when given a list x, returns the list that f represents, prepended to x. It is typically used in functional programming languages such as Haskell, although it could be used in imperative languages as well, and is common in the logic-programming language Prolog. Whether this kind of difference list is more efficient than another list representations depends on usage patterns. If an algorithm builds a list by concatenating smaller lists, which are themselves built by concatenating still smaller lists, then use of difference lists can improve performance by effectively ""flattening"" the list building computations.
Examples of use are in the ShowS type in the Prelude of Haskell, and in Donald Bruce Stewart's difference list library for Haskell.


== External links ==
Open Lists and Difference Lists in Prolog
Difference Lists in Haskell (programming language)"
637,ROAM,1177509,2155,"""Roam"" is the fourth single from The B-52's' 1989 hit album Cosmic Thing, following ""(Shake That) Cosmic Thing,"" ""Channel Z,"" and ""Love Shack."" ""Roam"" was a number-three hit on the Billboard Hot 100 singles chart in March 1990, spending a total of 19 weeks on the chart, and was certified Gold by the RIAA. The vocals are sung by Kate Pierson and Cindy Wilson. In February 1991 The B-52s were nominated for a Grammy Award for Best Pop Vocal Performance by a Duo or Group for ""Roam"".


== Track listing U.S. Maxi-Single [CD 9 21441-2] ==
""Roam"" (7"" Remix) - 5:11
""Roam"" (Radio Mix) - 4:13
""Roam"" (12"" Remix) - 8:17
""Bushfire"" (LP Version) - 4:56
""Roam"" (Extended Remix) - 5:25
""Roam"" (Instrumental) - 5:27


== Other Versions ==
""Roam"" (Miami Phunky Break Mix) Time Capsule The Mixes - 7:35
""Roam"" (Indamix Tekno Mix) Time Capsule The Mixes - 10:10
""Roam"" (Single Edit) U.S. Cassette Single - 4:01
""Roam"" (LP Version) Cosmic Thing - 4:54


== Cover versions ==
A parody of the song and video, called ""Comb,"" was a skit on Fast Forward in 1990 as The B-52's began the Australian leg of their Cosmic Tour. The video, which poked fun at Pierson's and Wilson's bouffant wigs, starred Gina Riley, Jane Turner, Michael Veitch and Peter Moon as Kate, Cindy, Fred and Keith respectively.
The song was performed by the cast of the 2002 stage version of Earth Girls Are Easy.
The Yayhoos covered the song on their 2006 album Put The Hammer Down.
The Argentine female singer Marcela Morelo covered the song on her 2009 album ""Otro Plan"" in a Spanish version
Caroline Sunshine covered the song for the soundtrack to the 2012 film Treasure Buddies.


== Chart performance ==


== References =="
638,Symposium on Principles and Practice of Parallel Programming,21525096,2154,"PPoPP, the ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, is an academic conference in the field of parallel programming. PPoPP is sponsored by the Association for Computing Machinery special interest group SIGPLAN.


== History ==
The conference was first organised in 1988 in New Haven, Connecticut, United States; the first conference was called ACM/SIGPLAN Conference on Parallel Programming: Experience with Applications, Languages and Systems (PPEALS). The name changed to the present one when the conference was organised for the second time in 1990.
The conference has been organised biennially in 1991–2005 and annually in 2006–2009. PPoPP was part of the Federated Computing Research Conference (FCRC) in 1993, 1999, and 2003.


== Artifact Evaluation ==
Since 2015 PPoPP features artifact evaluation to validate experiments from accepted papers and improve reproducibility of computer systems research


== See also ==
List of distributed computing conferences.
List of computer science conferences.


== References ==
PPoPP web site.
PPoPP'18 Artifact Evaluation web site.
PPEALS 1988 proceedings in ACM digital library.
PPoPP 1990– proceedings in ACM digital library.
PPoPP proceedings information in DBLP."
639,SIGMOD Edgar F. Codd Innovations Award,18927887,2151,"The ACM SIGMOD Edgar F. Codd Innovations Award is a lifetime research achievement award given by the ACM Special Interest Group on Management of Data, at its yearly flagship conference (also called SIGMOD). According to its homepage, it is given ""for innovative and highly significant contributions of enduring value to the development, understanding, or use of database systems and databases"". The award has been given since 1992.


== Recipients ==


== References =="
640,Pseudo-LRU,1911628,2151,"Pseudo-LRU or PLRU is a family of cache algorithms which improve on the performance of the Least Recently Used (LRU) algorithm by replacing values using approximate measures of age rather than maintaining the exact age of every value in the cache.
PLRU usually refers to two cache replacement algorithms: tree-PLRU and bit-PLRU.


== Tree-PLRU ==
Tree-PLRU is an efficient algorithm to select an item that most likely has not been accessed very recently, given a set of items and a sequence of access events to the items.
This technique is used in the CPU cache of the Intel 486 and in many processors in the Power Architecture (formerly PowerPC) family, such as Freescale's PowerPC G4 used by Apple Computer.
The algorithm works as follows: consider a binary search tree for the items in question. Each node of the tree has a one-bit flag denoting ""go left to find a pseudo-LRU element"" or ""go right to find a pseudo-LRU element"". To find a pseudo-LRU element, traverse the tree according to the values of the flags. To update the tree with an access to an item N, traverse the tree to find N and, during the traversal, set the node flags to denote the direction that is opposite to the direction taken.


== Bit-PLRU ==
Bit-PLRU stores one status bit for each cache line. We call these bits MRU-bits. Every access to a line sets its MRU-bit to 1, indicating that the line was recently used. Whenever the last remaining 0 bit of a set's status bits is set to 1, all other bits are reset to 0. At cache misses, the line with lowest index whose MRU-bit is 0 is replaced.


== See also ==
Cache algorithms


== References ==
https://people.cs.clemson.edu/~mark/464/p_lru.txt
http://www.ipdps.org/ipdps2010/ipdps2010-slides/session-22/2010IPDPS.pdf
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.217.3594&rep=rep1&type=pdf"
641,Belief–desire–intention model,3280462,2141,"The belief–desire–intention (BDI) model of human practical reasoning was developed by Michael Bratman as a way of explaining future-directed intention.
BDI is fundamentally reliant on folk psychology (the 'theory theory'), which is the notion that our mental models of the world are theories. It was used as a basis for developing the belief–desire–intention software model.


== Applications ==
BDI was part of the inspiration behind the BDI software architecture, which Bratman was also involved in developing. Here, the notion of intention was seen as a way of limiting time spent on deliberating about what to do, by eliminating choices inconsistent with current intentions.
BDI has also aroused some interest in psychology. BDI formed the basis for a computational model of childlike reasoning CRIBB. It has been proposed that autistic children do not recognise other people as folk-psychological agents (i.e., agents with their own beliefs, etc.). BDI has been used to develop a rehabilitation strategy to teach autistic children to reason about other people.


== References ==
Bratman, M. E. (1999) [1987]. Intention, Plans, and Practical Reason. CSLI Publications. ISBN 1-57586-192-5. 
Galitsky, Boris (2002). ""Extending the BDI model to accelerate the mental development of autistic patients"". The 2nd International Conference on Development and Learning."
642,Manuel da Silva Rosa,45286110,2138,"Manuel Rosa is an information technology analyst and author born in Pico Island, Azores, Portugal. He is an independent researcher and lecturer on the life of Christopher Columbus, who has published several controversial books and has been featured in Polish documentaries about Columbus. He currently works as an IT analyst at Duke University.


== Selected works ==
O Mistério Colombo Revelado, Ésquilo, Portugal, 2006 (ISBN 978-9728605865)
Unraveling riddles and falsities of Christopher Columbus. His nobility, the location of Natividad, and the Santa Maria’s true fate. e-Spanish Legal History Review, N.º 19 ENERO 2015
A will without a way. A critical review of how the Christopher Columbus Mayorazgo of 1498 continues to perpetrate a fraud against historians and history. e-Spanish Legal History Review, N.º 21 JUNIO 2015
Columbus-The Untold Story, Outwater Media Group, New Jersey, 2016 (ISBN 978-0578179315)


== References ==


== External links ==
e-Spanish Legal History Review, N.º 21 JUNIO 2015 http://www.iustel.com/v2/revistas/detalle_revista.asp?id_noticia=416163
e-Spanish Legal History Review, N.º 19 ENERO 2015 - http://www.iustel.com/v2/revistas/detalle_revista.asp?id_noticia=415524
End of the Enigmatic Christopher Columbus: A Man at Last Emerges to Eradicate the Myth http://www.ancient-origins.net/opinion-guest-authors/end-enigmatic-christopher-columbus-man-last-emerges-eradicate-myth-005934?nopaging=1"
643,Knowledge Collection from Volunteer Contributors,3517589,2138,"A subfield of Knowledge Acquisition within Artificial Intelligence, Knowledge Collection from Volunteer Contributors (KCVC) attempts to drive down the cost of acquiring the knowledge required to support automated reasoning by having the public enter knowledge in computer processable form over the internet. KCVC might be regarded as similar in spirit to Wikipedia, although the intended audience, Artificial Intelligence systems, differs.
What may have been the first research meeting on this topic was The 2005 AAAI Spring Symposium on Knowledge Collection from Volunteer Contributors (KCVC05).
The first large-scale KCVC project was probably the Open Mind Common Sense (OMCS) project, initiated by Push Singh and Marvin Minsky at the MIT Media Lab. In this project, volunteers enter words or simple sentences in English in response to prompts or images. Although the resulting knowledge is not formally represented, it is provided to researchers with parses and other meta-information intended to increase its utility. Later, this group released ConceptNet, which embedded the knowledge contained in the OMCS database as a semantic network.
In late 2005, Cycorp released a KCVC system called FACTory that attempts to acquire knowledge in a form directly usable for automated reasoning. It automatically generates questions in English from an underlying predicate calculus representation of candidate assertions produced by automated reading of web pages, by reviewing information previously entered directly in logical form, and by analogy and abduction.


== External links ==
Open Mind Project
Open Mind Common Sense
ISI's Learner
Cycorp's FACTory"
644,Energy informatics,44689684,2127,"
== Definition ==
Energy Informatics is founded on flow networks that are the major suppliers and consumers of energy. Their efficiency can be improved by collecting and analyzing information. Energy informatics is a research field covering the use of information and communication technology to address energy challenges. Methods used for ""smart"" implementations often combine sensors with artificial intelligence and machine learning.


== Application areas ==
The field among other consider application areas within:
Smart Buildings by developing ICT-centred solutions for improving the energy-efficiency of buildings.
Smart Cities by investigating the synergies between demand patterns and supply availability of energy flows in cities and communities to improve energy efficiency, increase integration of renewable sources, and provide resilience towards system faults caused by extreme situations, like hurricanes and flooding.
Smart Industries including the development of ICT-centred solutions for improving the energy efficiency and predictability of energy intensive industrial processes, without compromising process and product quality.
Smart Energy Networks by developing ICT-centred solutions for coordinating the supply and demand in environmentally sustainable energy networks.


== Research centers ==
Center for Energy Informatics at the University of Southern Denmark
Center For Energy Informatics at USC


== References =="
645,Ueli Maurer (cryptographer),33842913,2126,"Ueli Maurer (born 26 May 1960 in Leimbach, Switzerland) is a professor for cryptography at the Swiss Federal Institute of Technology Zurich (ETH Zurich).
Maurer studied electrical engineering at ETH Zurich and obtained his PhD in 1990, advised by James Massey. Afterwards, he joined Princeton university as a postdoc. In 1992, he became part of the computer science faculty of ETH Zurich.
In a seminal work, he showed that the Diffie-Hellman problem is (under certain conditions) equivalent to solving the discrete log problem.
From 2002 until 2008, Maurer also served on the board of Tamedia AG.
Maurer is editor of the Journal of Cryptology and used to be editor-in-chief.
In 2015, he was named a Fellow of the Association for Computing Machinery ""for contributions to cryptography and information security.""


== References ==


== External links ==
Literature by and about Ueli Maurer (cryptographer) in the German National Library catalogue
Website of Ueli Maurer at ETH Zurich"
646,Internet Technical Committee,6872736,2120,"The Internet Technical Committee (ITC) is a joint committee of the Internet Society (ISOC) and the IEEE Communications Society (ComSoc). The Internet Technical Committee was officially created in December 1994.


== Vision and Goals ==
The Internet Society stimulates interdisciplinary technical exchange and the application of state of the art communications and related technologies to Internet infrastructure and services. The Committee seeks to generate new technical insights from interaction between the Internet and public network communities, contributing to the worldwide emergence of a ubiquitous, multimedia, and high-performance Internet. The Committee sponsors and cosponsors workshops, organizes sessions at conferences of both Societies, and encourages submission of articles to their publications. The scope of the Committee includes evolution of the IP protocol; architectural and scaling issues; addressing, routing, and directory services; protocols and technologies in support of real-time media; dynamic control of quality of service; congestion control and admission policies; signaling and network management; access via diverse local and metropolitan networks; information retrieval and sharing; and international interoperability of services and applications.
The committee meets at the major IEEE Communications Society conferences such as Infocom, Globecom and ICC.


== Notable leaders ==
Notable leaders of the Internet Technical Committee include Henning Schulzrinne, Joe Touch, Charles Kalmanek, Markus Hofmann_(networking), and Dinesh Verma.


== See also ==
Institute of Electrical and Electronics Engineers
IEEE Communications Society


== External links ==
""Internet Technical Committee"". official website."
647,International Journal of Computational Geometry and Applications,25606481,2102,"The International Journal of Computational Geometry and Applications (IJCGA) is a bimonthly journal published since 1991, by World Scientific. It covers the application of computational geometry in design and analysis of algorithms, focusing on problems arising in various fields of science and engineering such as computer-aided geometry design (CAGD), operations research, and others.
The current editors-in-chief are D.-T. Lee of the Institute of Information Science in Taiwan, and Joseph S. B. Mitchell from the Department of Applied Mathematics and Statistics in the State University of New York at Stony Brook.


== Abstracting and indexing ==
Current Contents/Engineering, Computing & Technology
ISI Alerting Services
Science Citation Index Expanded (also known as SciSearch)
CompuMath Citation Index
Mathematical Reviews
INSPEC
DBLP Bibliography Server
Zentralblatt MATH
Computer Abstracts


== References ==


== External links ==
IJCGA Journal Website"
648,C-slowing,4351011,2101,"C-slowing is a technique used in conjunction with retiming to improve throughput of a digital circuit. Each register in a circuit is replaced by a set of C registers (in series). This creates a circuit with C independent threads, as if the new circuit contained C copies of the original circuit. A single computation of the original circuit takes C times as many clock cycles to compute in the new circuit. C-slowing by itself increases latency, but throughput remains the same.
Increasing the number of registers allows optimization of the circuit through retiming to reduce the clock period of the circuit. In the best case, the clock period can be reduced by a factor of C. Reducing the clock period of the circuit reduces latency and increases throughput. Thus, for computations that can be multi-threaded, combining C-slowing with retiming can increase the throughput of the circuit, with little, or in the best case, no increase in latency.
Since registers are relatively plentiful in FPGAs, this technique is typically applied to circuits implemented with FPGAs.


== See also ==
Pipelining
Barrel processor


== Resources ==
PipeRoute: A Pipelining-Aware Router for Reconfigurable Architectures
Simple Symmetric Multithreading in Xilinx FPGAs
Post Placement C-Slow Retiming for Xilinx Virtex (.ppt)
Post Placement C-Slow Retiming for Xilinx Virtex (.pdf)
Exploration of RaPiD-style Pipelined FPGA Interconnects
Time and Area Efficient Pattern Matching on FPGAs"
649,JBOB,19515334,2100,"JBOB, an acronym for Just a Bunch Of Bytes, is a term is used to describe unstructured data that does not have a fixed format. This is a variation on the term JBOD (Just a Bunch Of Disks) that is used to describe standard hard drives that are used in a storage array.
Many computer files have a defined structure such as fixed length records with the data divided into records that are the same length. Structured data might have records of different lengths but each record is prefixed with a RDW (Record Descriptor Word) that indicates the length of that data as well as other attributes. JBOB data has no structure. Records are defined by the presence of characters in the data. For example, a report might have hundreds of records (or lines) but the length of each record is defined by the presence of a Carriage Return (and/or Line Feed). Mainframe computers have traditionally dealt with structured data but unstructured (JBOB) data is much more common in PC environments. The critical difference is that it is difficult, if not impossible, to advance to say, the 100th record without examining every character of the 99 records that precede it. With fixed length records, it is possible to calculate the exact position of a particular record. Even with variable length records, the length of each record is given so navigation is easier.
Since records are determined by the content of the data, metadata is required like what is the record termination character(s) and is usually stored external to the actual data or file. The processing of JBOB data is usually more difficult and may require special knowledge by the computer program. Metadata might also be required for structured data like the fixed record length or the largest variable length record but there usually exist standard utility software to read/write structured data since the format is a known structure."
650,Tensor glyph,32874332,2099,"In scientific visualization a tensor glyph is an object that can visualize all or most of the nine degrees of freedom, such as acceleration, twist, or shear – of a 
  
    
      
        3
        ×
        3
      
    
    {\displaystyle 3\times 3}
   matrix. It is used for tensor field visualization, where a data-matrix is available at every point in the grid. ""Glyphs, or icons, depict multiple data values by mapping them onto the shape, size, orientation, and surface appearance of a base geometric primitive."" Tensor glyphs are a particular case of multivariate data glyphs.
There are certain types of glyphs that are commonly used:
Ellipsoid
Cuboid
Cylindrical
Superquadrics
According to Thomas Schultz and Gordon Kindlmann, specific types of tensor fields ""play a central role in scientific and biomedical studies as well as in image analysis and feature-extraction methods.""


== References ==


== Further reading ==
Superquadric Tensor Glyphs (Images and Examples)
Bertin, Jacques (2010) [1967]. Semiology of Graphics. ISBN 1589482611."
651,Maximal pair,7818178,2097,"In computer science, a maximal pair is a tuple 
  
    
      
        (
        
          p
          
            1
          
        
        ,
        
          p
          
            2
          
        
        ,
        l
        )
      
    
    {\displaystyle (p_{1},p_{2},l)}
  , such that, given a string 
  
    
      
        S
      
    
    {\displaystyle S}
   of length 
  
    
      
        n
      
    
    {\displaystyle n}
  , 
  
    
      
        S
        [
        
          p
          
            1
          
        
        .
        .
        
          p
          
            1
          
        
        +
        l
        −
        1
        ]
        =
        S
        [
        
          p
          
            2
          
        
        .
        .
        
          p
          
            2
          
        
        +
        l
        −
        1
        ]
      
    
    {\displaystyle S[p_{1}..p_{1}+l-1]=S[p_{2}..p_{2}+l-1]}
  , but 
  
    
      
        S
        [
        
          p
          
            1
          
        
        −
        1
        ]
        ≠
        S
        [
        
          p
          
            2
          
        
        −
        1
        ]
      
    
    {\displaystyle S[p_{1}-1]\neq S[p_{2}-1]}
   and 
  
    
      
        S
        [
        
          p
          
            1
          
        
        +
        l
        ]
        ≠
        S
        [
        
          p
          
            2
          
        
        +
        l
        ]
      
    
    {\displaystyle S[p_{1}+l]\neq S[p_{2}+l]}
  . A maximal repeat is a string represented by such tuple. A supermaximal repeat is a maximal repeat never occurring as a proper substring of another maximal repeat. Both maximal pairs, maximal repeats and supermaximal repeats can be found in 
  
    
      
        Θ
        (
        n
        +
        z
        )
      
    
    {\displaystyle \Theta (n+z)}
   time using a suffix tree, if there are 
  
    
      
        z
      
    
    {\displaystyle z}
   such structures.


== Example ==

  
    
      
        (
        2
        ,
        6
        ,
        3
        )
      
    
    {\displaystyle (2,6,3)}
   and 
  
    
      
        (
        6
        ,
        10
        ,
        3
        )
      
    
    {\displaystyle (6,10,3)}
   are maximal pairs as the referenced substrings do not share identical characters to the left or the right.

  
    
      
        (
        2
        ,
        10
        ,
        3
        )
      
    
    {\displaystyle (2,10,3)}
   is not, as the character y follows both substrings.
abc and abcy are maximal repeats, but only abcy is a supermaximal repeat.


== References ==


== External links ==
Project for the computation of all maximal repeats in one ore more strings in Python, using suffix array."
652,Code on demand,661724,2092,"In distributed computing, code on demand is any technology that sends executable software code from a server computer to a client computer upon request from the client's software. Some well-known examples of the code on demand paradigm on the web are Java applets, Adobe’s ActionScript language for the Flash player, and JavaScript.
The program code lies inactive on a web server until a user (client) requests a web page that contains a link to the code using the client's web browser. Upon this request, the web page and the program are transported to the user's machine using HTTP. When the page is displayed, the code is started in the browser and executes locally, inside the user's computer until it is stopped (e.g., by the user leaving the web page).
Code on demand is a specific use of mobile code, within the field of code mobility.


== See also ==
Remote evaluation
Code mobility


== References =="
653,Vortex core line,32872804,2089,"In scientific visualization, a vortex core line is a line-like feature tracing the center of a vortex within a velocity field.


== Detection methods ==
Several methods exist to detect vortex core lines in a flow field. Jiang, Machiraju & Thompson (2004) studied and compared nine methods for vortex detection, including five methods for the identification of vortex core lines. Although this list is incomplete, they considered it representative for the state of the art (as of 2004).
One of these five methods is by Sujudi & Haimes (1995): in a velocity field v(x,t) a vector x lies on a vortex core line if v(x,t) is an eigenvector of the tensor derivative 
  
    
      
        ∇
        
          v
        
        (
        
          x
        
        ,
        t
        )
      
    
    {\displaystyle \nabla {\boldsymbol {v}}({\boldsymbol {x}},t)}
   and the other – not corresponding – eigenvalues are complex.
Another is the Lambda2 method, which is Galilean invariant and thus produces the same results when a uniform velocity field is added to the existing velocity field or when the field is translated.


== See also ==

Flow visualization


== References ==

Jiang, Ming; Machiraju, Raghu; Thompson, David (2004), ""Detection and visualization of vortices"", in Hansen, Charles D.; Johnson, Chris R., Visualization Handbook, Academic Press, pp. 295–309, ISBN 9780123875822 
Sujudi, David; Haimes, Robert (1995), ""Identification of swirling flow in 3-D vector fields"", 12th AIAA Computational Fluid Dynamics Conference, and Open Forum. San Diego, 19–22 June, 1995, pp. 792–799, CiteSeerX 10.1.1.43.7739 "
654,Lamport's distributed mutual exclusion algorithm,14581969,2079,"Lamport's Distributed Mutual Exclusion Algorithm is a contention-based algorithm for mutual exclusion on a distributed system.


== Algorithm ==


=== Nodal properties ===
Every process maintains a queue of pending requests for entering critical section in order. The queues are ordered by virtual time stamps derived from Lamport timestamps.


=== Algorithm ===
Requesting process
Pushing its request in its own queue (ordered by time stamps)
Sending a request to every node.
Waiting for replies from all other nodes.
If own request is at the head of its queue and all replies have been received, enter critical section.
Upon exiting the critical section, remove its request from the queue and send a release message to every process.
Other processes
After receiving a request, pushing the request in its own request queue (ordered by time stamps) and reply with a time stamp.
After receiving release message, remove the corresponding request from its own request queue.


== Message complexity ==
This algorithm creates 3(N − 1) messages per request, or (N − 1) messages and 2 broadcasts. 3(N − 1) messages per request includes:
(N − 1) total number of requests
(N − 1) total number of replies
(N − 1) total number of releases


== Drawbacks ==
There exist multiple points of failure.


== See also ==
Ricart-Agrawala algorithm (an improvement over Lamport's algorithm)
Lamport's Bakery Algorithm
Raymond's Algorithm
Maekawa's Algorithm
Suzuki-Kasami's Algorithm
Naimi-Trehel's Algorithm


== References =="
655,Stevens Award,8881415,2079,"The Stevens Award is a software engineering lecture award given by the Reengineering Forum, an industry association. The international Stevens Award was created to recognize outstanding contributions to the literature or practice of methods for software and systems development. The first award was given in 1995. The presentations focus on the current state of software methods and their direction for the future.
This award lecture is named in memory of Wayne Stevens (1944-1993), a consultant, author, pioneer, and advocate of the practical application of software methods and tools. The Stevens Award and lecture is managed by the Reengineering Forum. The award was founded by International Workshop on Computer Aided Software Engineering (IWCASE), an international workshop association of users and developers of computer-aided software engineering (CASE) technology, which merged into The Reengineering Forum. Wayne Stevens was a charter member of the IWCASE executive board.


== Recipients ==
1995: Tony Wasserman
1996: David Harel
1997: Michael Jackson
1998: Thomas McCabe
1999: Tom DeMarco
2000: Gerald Weinberg
2001: Peter Chen
2002: Cordell Green
2003: Manny Lehman
2004: François Bodart
2005: Mary Shaw, Jim Highsmith
2006: Grady Booch
2007: Nicholas Zvegintzov
2008: Harry Sneed
2009: Larry Constantine
2010: Peter Aiken
2011: Jared Spool, Barry Boehm
2012: Philip Newcomb
2013: Jean-Luc Hainaut
2014: François Coallier
2015: Pierre Bourque


== References =="
656,Degraded mode,36651777,2078,"When a RAID array experiences the failure of one or more disks, it can enter degraded mode, a fallback mode that generally allows the continued usage of the array, but either loses the performance boosts of the RAID technique (such as a RAID-1 mirror across two disks when one of them fails; performance will fall back to that of a normal, single drive) or experiences severe performance penalties due to the necessity to reconstruct the damaged data from error correction data.
Depending on the severity of the problem, the array may be placed into a read-only mode, either automatically or by the system administrator, until it can be corrected. Such corrections may or may not be possible to do on-line (as opposed to an ""off-line"" repair, during which the system is unavailable to users). Many RAID configurations feature spare disks that are already installed and can be automatically added to the array as needed; when this happens, the array may or may not go into degraded mode until the spare is rebuilt, but in any case should not be in degraded mode after the spare is rebuilt. If no spares are available, the array will remain in degraded mode until a spare disk is added, or the array is reconfigured (if that is possible for the configuration in question).
A typical case where a RAID enters degraded mode is a simple two-drive mirror after a power failure – it is unlikely the drives are in sync. Every time blocks are written to the storage elements (physical drives, in this case), certain accounting information is updated after the write. The RAID controller will notice that the storage elements are not in sync, will place the array in degraded mode, and – generally – will start a background resync (rebuild) operation. Simple mirroring solutions will resynchronize the entire array, block by block, across both drives, which can be quite time-consuming; this time can be greatly reduced by the usage of a write intent bitmap."
657,International Joint Conference on Automated Reasoning,4368061,2075,"The International Joint Conference on Automated Reasoning (IJCAR) is a series of conferences on the topics of automated reasoning, automated deduction, and related fields. It is organized semi-regularly as a merger of other meetings. IJCAR replaces those independent conferences in the years it takes place. The conference is organized by CADE Inc., and CADE has always been one of the conferences partaking in IJCAR.
The first IJCAR was held in Siena, Italy in 2001 as a merger of CADE, FTP, and TABLEAUX.
The second IJCAR was held in Cork, Ireland in 2004 as a merger of CADE, FTP, TABLEAUX, FroCoS and CALCULEMUS.
The third IJCAR was held as an independent subconference of the fourth Federated Logic Conference in Seattle, United States, and merged CADE, FTP, TABLEAUX, FroCoS and TPHOLs.
The fourth IJCAR was held in Sydney, Australia in 2008, and merged CADE, FroCoS, FTP and TABLEAUX.
The fifth IJCAR was held in 2010 as an independent subconference of the fifth Federated Logic Conference in Edinburgh, UK, and merged CADE, FTP, TABLEAUX, and FroCoS.
The sixth IJCAR was held in Manchester, UK, as part of the Alan Turing Year 2012, and was collocated with the Alan Turing Centenary Conference. It again merged CADE, FTP, TABLEAUX, and FroCoS.
The seventh IJCAR was held in Vienna, Austria, as part of the Vienna Summer of Logic in 2014, and merged CADE, TABLEAUX, and FroCoS.
The eighth IJCAR was held in Coimbra, Portugal, in 2016, and merged CADE, TABLEAUX, and FroCoS.


== External links ==
IJCAR Home Page
IJCAR-2006 Home Page
IJCAR-2008 Home Page
IJCAR 2016 Home Page"
658,String generation,16637050,2062,"In computer science, string generation is the process of creating a set of strings from a collection of rules. This is an opposite process to that of parsing, which recognises a string based on some collection of rules.
Applications of string generation include test data generation, Captchas and random essay generation.


== Generation methods ==
Methods for generating strings include:
While a deterministic finite automaton is often used to recognize strings it can easily be changed to generate strings.


== Unsolved problems ==
Unsolved problems in string generation include:

Note; It is an undecidable problem to decide whether a given string can be generated by a given W-grammar.


== See also ==
Pretty printing – another process often considered the dual of parsing.


== External links ==
DGL -- Data Generation Language an apparently general facility for addressing this problem
Eli Benderski blog with a demo in Python
Bruce McKenzie paper on a general algorithm
Generate strings matching a regular expression
Generate strings from a yacc grammar
comp.compilers discussion
random essay random essay
Generate random C programs
Generate random string using python"
659,Encrypted function,1399160,2060,"An encrypted function is an attempt to provide mobile code privacy without providing any tamper-resistant hardware. It is a method where in mobile code can carry out cryptographic primitives even though the code
is executed in untrusted environments.
should run autonomously.
Polynomial and rational functions are encrypted such that their transformation can again be implemented as programs consisting of cleartext instructions that a processor or interpreter understands. The processor would not understand the program's function. This field of study is gaining popularity as mobile cryptography.


== Example ==
Scenario: Host A, has an algorithm which computes function f. A wants to send its mobile agent to B which holds input x, to compute f(x). But A doesn't want B to learn anything about f.
Scheme: Function f is encrypted in a way that results in E(f). Host A then creates another program P(E(f)), which implements E(f), and sends it to B through its agent. B then runs the agent, which computes P(E(f))(x) and returns the result to A. A then decrypts this to get f(x).
Drawbacks: Finding appropriate encryption schemes that can transform arbitrary functions is a challenge. The scheme doesn't prevent denial of service, replay, experimental extraction and others.


== References ==
Thomas Sander and Christian F. Tschudin. Protecting Mobile Agents Against Malicious Hosts. In G. Vigna, editor, Mobile agents and security, volume 1419 of Lecture Notes in Computer Science, pages 44–60. Springer-Verlag, New York, NY, 1998. [1]"
660,Fast path,21772235,2059,"Fast path is a term used in computer science to describe a path with shorter instruction path length through a program compared to the 'normal' path. For a fast path to be effective it must handle the most commonly occurring tasks more efficiently than the 'normal' path, leaving the latter to handle uncommon cases, corner cases, error handling, and other anomalies. Fast paths are a form of optimization.
For example dedicated packet routing hardware used to build computer networks will often support software dedicated to handling the most common kinds of packets, with other kinds, for example with control information or packets directed at the device itself instead of being routed elsewhere, put on the metaphorical ""slow path"", in this example usually implemented by software running on the control processor.
Specific implementations of networking software architectures have been developed that leverage the concept of a fast path to maximize the performance of packet processing software. In these implementations, the networking stack is split into two layers and the lower layer, typically called the fast path, processes the majority of incoming packets outside the OS environment without incurring any of the OS overheads that degrade overall performance. Only those rare packets that require complex processing are forwarded to the OS networking stack, which performs the necessary management, signaling and control functions.
Some hardware RAID controllers implement a ""fast path"" for write-through access which bypasses the controller's cache in certain situations. This tends to increase IOPS, particularly for solid-state drives.


== References =="
661,Skin friction line,32909000,2057,"In scientific visualization skin friction lines are used to visualize flows on 3D-surfaces. They are obtained by calculating the streamlines of a derived vector field on the surface, the wall shear stress. Skin friction arises from the friction of the fluid against the ""skin"" of the object that is moving through it and forms a vector at each point on the surface. A skin friction line is a curve on the surface tangent to skin friction vectors. A limit streamline is a streamline where the distance normal to the surface tends to zero. Limit streamlines and skin friction lines coincide.
The lines can be visualized by placing a viscous film on the surface.
The skin friction lines may exhibit a number of different types of singularities: attachment nodes, detachment nodes, Isotropic nodes, Saddle points, and foci.


== See also ==
 Computer science portal


== References ==


=== Additional sources ===
Chapman, G. T. Topological classification of flow separation on three-dimensional bodies AIAA, Aerospace Sciences Meeting, 24th, Reno, NV, Jan. 6–9, 1986. 22 p.[1]
Skin friction lines | CSA
New developments and applications of skin-friction measuring techniques - Abstract - Measurement Science and Technology - IOPscience"
662,CUBIT,17241232,2056,"The cubit is an ancient unit of length that had several definitions according to each of the various different cultures that used the unit. These definitions ranged between 444 mm and 529.2 mm. The unit was based on the forearm length from the tip of the middle finger to the bottom of the elbow. Cubits of various lengths were employed in many parts of the world in antiquity, during the Middle Ages and as recently as Early Modern Times. The term is still used in hedge laying, the length of the forearm being frequently used to determine the interval between stakes placed within the hedge.


== Etymology ==
The English word ""cubit"" comes from the Latin noun cubitus ""elbow"", from the verb cubo, cubare, cubui, cubitum ""to lie down"", from which also comes the adjective ""recumbent"".


== Ancient Egyptian royal cubit ==

The ancient Egyptian royal cubit (meh niswt) is the earliest attested standard measure. Cubit rods were used for the measurement of length. A number of these rods have survived: two are known from the tomb of Maya, the treasurer of the 18th dynasty pharaoh Tutankhamun, in Saqqara; another was found in the tomb of Kha (TT8) in Thebes. Fourteen such rods, including one double cubit rod, were described and compared by Lepsius in 1865. These cubit rods range from 523.5 to 529.2 mm (20.61 to 20.83 in) in length and are divided into seven palms; each palm is divided into four fingers, and the fingers are further subdivided.

Early evidence for the use of this royal cubit comes from the Early Dynastic Period: on the Palermo Stone, the flood level of the Nile river during the reign of the Pharaoh Djer is given as measuring 6 cubits and 1 palm. Use of the royal cubit is also known from Old Kingdom architecture, from at least as early as the construction of the Step Pyramid of Djoser in around 2700 BC.


== Ancient Mesopotamian units of measurement ==
Ancient Mesopotamian units of measurement originated in the loosely organized city-states of Early Dynastic Sumer. Each city, kingdom and trade guild had its own standards until the formation of the Akkadian Empire when Sargon of Akkad issued a common standard. This standard was improved by Naram-Sin, but fell into disuse after the Akkadian Empire dissolved. The standard of Naram-Sin was readopted in the Ur III period by the Nanše Hymn which reduced a plethora of multiple standards to a few agreed upon common groupings. Successors to Sumerian civilization including the Babylonians, Assyrians, and Persians continued to use these groupings.
The Classical Mesopotamian system formed the basis for Elamite, Hebrew, Urartian, Hurrian, Hittite, Ugaritic, Phoenician, Babylonian, Assyrian, Persian, Arabic, and Islamic metrologies. The Classical Mesopotamian System also has a proportional relationship, by virtue of standardized commerce, to Bronze Age Harappan and Egyptian metrologies.

In 1916, during the last years of the Ottoman Empire and in the middle of World War I, the German assyriologist Eckhard Unger found a copper-alloy bar while excavating at Nippur. The bar dates from c. 2650 BC and Unger claimed it was used as a measurement standard. This irregularly formed and irregularly marked graduated rule supposedly defined the Sumerian cubit as about 518.6 mm (20.42 in).


== Biblical cubit ==

The Near Eastern or Biblical cubit is usually estimated as approximately 457 mm (18 in). Epiphanius of Salamis, in his treatise On Weights and Measures, describes how it was customary, in his day, to take the measurement of the biblical cubit: ""The cubit is a measure, but it is taken from the measure of the forearm. For the part from the elbow to the wrist and the palm of the hand is called the cubit, the middle finger of the cubit measure being also extended at the same time and there being added below (it) the span, that is, of the hand, taken all together.""


== Ancient Greece ==
In ancient Greek units of measurement, the standard forearm cubit (Greek: πῆχυς pēkhys) measured approximately 0.46 m (18 in). The short forearm cubit (πυγμή pygmē, lit. ""fist""), from the wrist to the elbow, measured approximately 0.34 m (13 in).


== Ancient Rome ==
In ancient Rome, according to Vitruvius, a cubit was equal to ​1 1⁄2 Roman feet or 6 palm widths (approximately 444 mm or 17.5 in). A 120-centimeter cubit (approximately four feet long), called the Roman ulna, was common in the Roman empire, which cubit was measured from the fingers of the outstretched arm opposite the man's hip.; also, with


== Other systems ==
Other measurements based on the length of the forearm include some lengths of ell, the Chinese chi, the Japanese shaku, the Indian hasta, the Thai sok, the Tamil ""(Mulzham)"", the Telugu ""(Moora)"" (మూర), and the Khmer hat.


== Cubit arm in heraldry ==

A cubit arm in heraldry may be dexter or sinister. It may be vested (with a sleeve) and may be shown in various positions, most commonly erect, but also fesswise (horizontal), bendwise (diagonal) and is often shown grasping objects. It is most often used erect as a crest, for example by the families of Poyntz of Iron Acton, Rolle of Stevenstone and Turton.


== See also ==
History of measurement
List of obsolete units of measurement
System of measurement
Units of measurement


== References ==


== Bibliography ==
Arnold, Dieter (2003). The Encyclopaedia of Ancient Egyptian Architecture. Taurus. ISBN 1-86064-465-1. 
Petrie, Sir Flinders (1881). Pyramids and Temples of Gizeh.
Stone, Mark H., ""The Cubit: A History and Measurement Commentary"", Journal of Anthropology doi:10.1155/2014/489757, 2014


== External links ==
 Media related to Cubit arms at Wikimedia Commons
 The dictionary definition of cubit at Wiktionary"
663,Theory of regions,27115797,2055,"The Theory of regions is an approach for synthesizing a Petri net from a transition system. As such, it aims at recovering concurrent, independent behaviour from transitions between global states. Theory of regions handles elementary net systems as well as P/T nets and other kinds of nets. An important point is that the approach is aimed at the synthesis of unlabeled Petri nets only.


== Definition ==
A region of a transition system 
  
    
      
        (
        S
        ,
        Λ
        ,
        →
        )
      
    
    {\displaystyle (S,\Lambda ,\rightarrow )}
   is a mapping assigning to each state 
  
    
      
        s
        ∈
        S
      
    
    {\displaystyle s\in S}
   a number 
  
    
      
        σ
        (
        s
        )
      
    
    {\displaystyle \sigma (s)}
   (natural number for P/T nets, binary for ENS) and to each transition label a number 
  
    
      
        τ
        (
        ℓ
        )
      
    
    {\displaystyle \tau (\ell )}
   such that consistency conditions 
  
    
      
        σ
        (
        
          s
          ′
        
        )
        =
        σ
        (
        s
        )
        +
        τ
        (
        ℓ
        )
      
    
    {\displaystyle \sigma (s')=\sigma (s)+\tau (\ell )}
   holds whenever 
  
    
      
        (
        s
        ,
        ℓ
        ,
        
          s
          ′
        
        )
        ∈→
      
    
    {\displaystyle (s,\ell ,s')\in \rightarrow }
  .


=== Intuitive explanation ===
Each region represents a potential place of a Petri net.
Mukund: event/state separation property, state separation property.


== References ==
Badouel, E and Darondeau, P. ""Theory of Regions"""
664,Lemke's algorithm,23629372,2051,"In mathematical optimization, Lemke's algorithm is a procedure for solving linear complementarity problems, and more generally mixed linear complementarity problems. It is named after Carlton E. Lemke.
Lemke's algorithm is of pivoting or basis-exchange type. Similar algorithms can compute Nash equilibria for two-person matrix and bimatrix games.


== References ==
Cottle, Richard W.; Pang, Jong-Shi; Stone, Richard E. (1992). The linear complementarity problem. Computer Science and Scientific Computing. Boston, MA: Academic Press, Inc. pp. xxiv+762 pp. ISBN 0-12-192350-9. MR 1150683. 
Murty, K. G. (1988). Linear complementarity, linear and nonlinear programming. Sigma Series in Applied Mathematics. 3. Berlin: Heldermann Verlag. pp. xlviii+629 pp. ISBN 3-88538-403-5. Archived from the original on 2010-04-01.  (Available for download at the website of Professor Katta G. Murty.) MR949214


== External links ==
OMatrix manual on Lemke
Chris Hecker's GDC presentation on MLCPs and Lemke
Linear Complementarity and Mathematical (Non-linear) Programming
Siconos/Numerics open-source GPL implementation in C of Lemke's algorithm and other methods to solve LCPs and MLCPs"
665,Center for Computer-Aided Design,50758484,2049,"Computer-aided design (CAD) is the use of computer systems (or workstations) to aid in the creation, modification, analysis, or optimization of a design. CAD software is used to increase the productivity of the designer, improve the quality of design, improve communications through documentation, and to create a database for manufacturing. CAD output is often in the form of electronic files for print, machining, or other manufacturing operations. The term CADD (for Computer Aided Design and Drafting) is also used.
Its use in designing electronic systems is known as electronic design automation, or EDA. In mechanical design it is known as mechanical design automation (MDA) or computer-aided drafting (CAD), which includes the process of creating a technical drawing with the use of computer software.
CAD software for mechanical design uses either vector-based graphics to depict the objects of traditional drafting, or may also produce raster graphics showing the overall appearance of designed objects. However, it involves more than just shapes. As in the manual drafting of technical and engineering drawings, the output of CAD must convey information, such as materials, processes, dimensions, and tolerances, according to application-specific conventions.
CAD may be used to design curves and figures in two-dimensional (2D) space; or curves, surfaces, and solids in three-dimensional (3D) space.
CAD is an important industrial art extensively used in many applications, including automotive, shipbuilding, and aerospace industries, industrial and architectural design, prosthetics, and many more. CAD is also widely used to produce computer animation for special effects in movies, advertising and technical manuals, often called DCC digital content creation. The modern ubiquity and power of computers means that even perfume bottles and shampoo dispensers are designed using techniques unheard of by engineers of the 1960s. Because of its enormous economic importance, CAD has been a major driving force for research in computational geometry, computer graphics (both hardware and software), and discrete differential geometry.
The design of geometric models for object shapes, in particular, is occasionally called computer-aided geometric design (CAGD).


== Overview of CAD software ==
Starting around the mid 1960s, with the IBM Drafting System, computer-aided design systems began to provide more capability than just an ability to reproduce manual drafting with electronic drafting, the cost-benefit for companies to switch to CAD became apparent. The benefits of CAD systems over manual drafting are the capabilities one often takes for granted from computer systems today; automated generation of Bill of Material, auto layout in integrated circuits, interference checking, and many others. Eventually, CAD provided the designer with the ability to perform engineering calculations. During this transition, calculations were still performed either by hand or by those individuals who could run computer programs. CAD was a revolutionary change in the engineering industry, where draftsmen, designers and engineering roles begin to merge. It did not eliminate departments, as much as it merged departments and empowered draftsman, designers and engineers. CAD is just another example of the pervasive effect computers were beginning to have on industry. Current computer-aided design software packages range from 2D vector-based drafting systems to 3D solid and surface modelers. Modern CAD packages can also frequently allow rotations in three dimensions, allowing viewing of a designed object from any desired angle, even from the inside looking out. Some CAD software is capable of dynamic mathematical modeling.
CAD technology is used in the design of tools and machinery and in the drafting and design of all types of buildings, from small residential types (houses) to the largest commercial and industrial structures (hospitals and factories).
CAD is mainly used for detailed engineering of 3D models or 2D drawings of physical components, but it is also used throughout the engineering process from conceptual design and layout of products, through strength and dynamic analysis of assemblies to definition of manufacturing methods of components. It can also be used to design objects such as jewelry, furniture, appliances, etc. Furthermore, many CAD applications now offer advanced rendering and animation capabilities so engineers can better visualize their product designs. 4D BIM is a type of virtual construction engineering simulation incorporating time or schedule related information for project management.
CAD has become an especially important technology within the scope of computer-aided technologies, with benefits such as lower product development costs and a greatly shortened design cycle. CAD enables designers to layout and develop work on screen, print it out and save it for future editing, saving time on their drawings.


== Uses ==
Computer-aided design is one of the many tools used by engineers and designers and is used in many ways depending on the profession of the user and the type of software in question.
CAD is one part of the whole Digital Product Development (DPD) activity within the Product Lifecycle Management (PLM) processes, and as such is used together with other tools, which are either integrated modules or stand-alone products, such as:
Computer-aided engineering (CAE) and Finite element analysis (FEA)
Computer-aided manufacturing (CAM) including instructions to Computer Numerical Control (CNC) machines
Photorealistic rendering and Motion Simulation.
Document management and revision control using Product Data Management (PDM).
CAD is also used for the accurate creation of photo simulations that are often required in the preparation of Environmental Impact Reports, in which computer-aided designs of intended buildings are superimposed into photographs of existing environments to represent what that locale will be like, where the proposed facilities are allowed to be built. Potential blockage of view corridors and shadow studies are also frequently analyzed through the use of CAD.
CAD has been proven to be useful to engineers as well. Using four properties which are history, features, parameterization, and high-level constraints. The construction history can be used to look back into the model's personal features and work on the single area rather than the whole model. Parameters and constraints can be used to determine the size, shape, and other properties of the different modeling elements. The features in the CAD system can be used for the variety of tools for measurement such as tensile strength, yield strength, electrical or electromagnetic properties. Also its stress, strain, timing or how the element gets affected in certain temperatures, etc.


== Types ==

There are several different types of CAD, each requiring the operator to think differently about how to use them and design their virtual components in a different manner for each.
There are many producers of the lower-end 2D systems, including a number of free and open source programs. These provide an approach to the drawing process without all the fuss over scale and placement on the drawing sheet that accompanied hand drafting since these can be adjusted as required during the creation of the final draft.
3D wireframe is basically an extension of 2D drafting (not often used today). Each line has to be manually inserted into the drawing. The final product has no mass properties associated with it and cannot have features directly added to it, such as holes. The operator approaches these in a similar fashion to the 2D systems, although many 3D systems allow using the wireframe model to make the final engineering drawing views.
3D ""dumb"" solids are created in a way analogous to manipulations of real-world objects (not often used today). Basic three-dimensional geometric forms (prisms, cylinders, spheres, and so on) have solid volumes added or subtracted from them as if assembling or cutting real-world objects. Two-dimensional projected views can easily be generated from the models. Basic 3D solids don't usually include tools to easily allow motion of components, set limits to their motion, or identify interference between components.
There are two types of 3D Solid Modeling
Parametric modeling allows the operator to use what is referred to as ""design intent"". The objects and features created are modifiable. Any future modifications can be made by changing how the original part was created. If a feature was intended to be located from the center of the part, the operator should locate it from the center of the model. The feature could be located using any geometric object already available in the part, but this random placement would defeat the design intent. If the operator designs the part as it functions the parametric modeler is able to make changes to the part while maintaining geometric and functional relationships.
Direct or Explicit modeling provide the ability to edit geometry without a history tree. With direct modeling, once a sketch is used to create geometry the sketch is incorporated into the new geometry and the designer just modifies the geometry without needing the original sketch. As with parametric modeling, direct modeling has the ability to include relationships between selected geometry (e.g., tangency, concentricity).
Top end systems offer the capabilities to incorporate more organic, aesthetics and ergonomic features into designs. Freeform surface modeling is often combined with solids to allow the designer to create products that fit the human form and visual requirements as well as they interface with the machine.


== Technology ==

Originally software for Computer-Aided Design systems was developed with computer languages such as Fortran, ALGOL but with the advancement of object-oriented programming methods this has radically changed. Typical modern parametric feature-based modeler and freeform surface systems are built around a number of key C modules with their own APIs. A CAD system can be seen as built up from the interaction of a graphical user interface (GUI) with NURBS geometry or boundary representation (B-rep) data via a geometric modeling kernel. A geometry constraint engine may also be employed to manage the associative relationships between geometry, such as wireframe geometry in a sketch or components in an assembly.
Unexpected capabilities of these associative relationships have led to a new form of prototyping called digital prototyping. In contrast to physical prototypes, which entail manufacturing time in the design. That said, CAD models can be generated by a computer after the physical prototype has been scanned using an industrial CT scanning machine. Depending on the nature of the business, digital or physical prototypes can be initially chosen according to specific needs.
Today, CAD systems exist for all the major platforms (Windows, Linux, UNIX and Mac OS X); some packages support multiple platforms.
Right now, no special hardware is required for most CAD software. However, some CAD systems can do graphically and computationally intensive tasks, so a modern graphics card, high speed (and possibly multiple) CPUs and large amounts of RAM may be recommended.
The human-machine interface is generally via a computer mouse but can also be via a pen and digitizing graphics tablet. Manipulation of the view of the model on the screen is also sometimes done with the use of a Spacemouse/SpaceBall. Some systems also support stereoscopic glasses for viewing the 3D model.Technologies which in the past were limited to larger installations or specialist applications have become available to a wide group of users. These include the CAVE or HMDs and interactive devices like motion-sensing technology


== Software ==
CAD software enables engineers and architects to design, inspect and manage engineering projects within an integrated graphical user interface (GUI) on a personal computer system. Most applications support solid modeling with boundary representation (B-Rep) and NURBS geometry, and enable the same to be published in a variety of formats. A geometric modeling kernel is a software component that provides solid modeling and surface modeling features to CAD applications.
Based on market statistics, commercial software from Autodesk, Dassault Systems, Siemens PLM Software, and PTC dominate the CAD industry. The following is a list of major CAD applications, grouped by usage statistics.


== History ==
Designers have long used computers for their calculations. Digital computers were used in power system analysis or optimization as early as proto-""Whirlwind"" in 1949. Circuit design theory, or power network methodology would be algebraic, symbolic, and often vector-based. Examples of problems being solved in the mid-1940s to 50s include: servo motors controlled by generated pulse (1949), a digital computer with built-in computer operations to automatically co-ordinate transforms to compute radar related vectors (1951) and the essentially graphic mathematical process of forming a shape with a digital machine tool (1952). These were accomplished with the use of computer software. The man credited with coining the term CAD. Douglas T. Ross stated ""As soon as I saw the interactive display equipment,"" [being used by radar operators 1953] it would be just what his data reduction group needed. With the Lincoln Lab people, they were the only ones who used the big, complex display systems put in for the pre-SAGE, Cape Cod system. But ""we used it for our own personal workstation."".  The designers of these very early computers built utility programs so that programmers could debug programs using flowcharts on a display scope with logical switches that could be opened and closed during the debugging session. They found that they could create electronic symbols and geometric figures to be used to create simple circuit diagrams and flowcharts. They made the pleasant discovery that an object once drawn could be reproduced at will, its orientation, Linkage [ flux, mechanical, lexical scoping ] or scale changed. This suggested numerous possibilities to them. It took ten years of interdisciplinary development work before SKETCHPAD sitting on evolving math libraries emerged from MIT's labs. Additional developments were carried out in the 1960s within the aircraft, automotive, industrial control and electronics industries in the area of 3D surface construction, NC programming, and design analysis, most of it independent of one another and often not publicly published until much later. Some of the mathematical description work on curves was developed in the early 1940s by Robert Issac Newton from Pawtucket, Rhode Island. Robert A. Heinlein in his 1957 novel The Door into Summer suggested the possibility of a robotic Drafting Dan. However, probably the most important work on polynomial curves and sculptured surface was done by Pierre Bézier, Paul de Casteljau (Citroen), Steven Anson Coons (MIT, Ford), James Ferguson (Boeing), Carl de Boor (GM), Birkhoff (GM) and Garibedian (GM) in the 1960s and W. Gordon (GM) and R. Riesenfeld in the 1970s.
The invention of the 3D CAD/CAM is attributed to a French engineer, Pierre Bézier (Arts et Métiers ParisTech, Renault). After his mathematical work concerning surfaces, he developed UNISURF, between 1966 and 1968, to ease the design of parts and tools for the automotive industry. Then, UNISURF became the working base for the following generations of CAD software.
It is argued that a turning point was the development of the SKETCHPAD system at MIT by Ivan Sutherland (who later created a graphics technology company with David Evans). The distinctive feature of SKETCHPAD was that it allowed the designer to interact with his computer graphically: the design can be fed into the computer by drawing on a CRT monitor with a light pen. Effectively, it was a prototype of graphical user interface, an indispensable feature of modern CAD. Sutherland presented his paper Sketchpad: A Man-Machine Graphical Communication System in 1963 at a Joint Computer Conference having worked on it as his PhD thesis paper for a few years. Quoting, ""For drawings where motion of the drawing or analysis of a drawn problem is of value to the user, Sketchpad excels. For highly repetitive drawings or drawings where accuracy is required, Sketchpad is sufficiently faster than conventional techniques to be worthwhile. For drawings which merely communicate with shops, it is probably better to use conventional paper and pencil."" Over time efforts would be directed toward the goal of having the designers drawings communicate not just with shops but with the shop tool itself. This goal would be a long time arriving.
The first commercial applications of CAD were in large companies in the automotive and aerospace industries, as well as in electronics. Only large corporations could afford the computers capable of performing the calculations. Notable company projects were, a joint project of GM (Patrick J. Hanratty) and IBM (Sam Matsa, Doug Ross's MIT APT research assistant) to develop a prototype system for design engineers DAC-1 (Design Augmented by Computer) 1964; Lockheed projects; Bell GRAPHIC 1 and Renault.
One of the most influential events in the development of CAD was the founding of MCS (Manufacturing and Consulting Services Inc.) in 1971 by Patrick J. Hanratty, who wrote the system ADAM (Automated Drafting And Machining) but more importantly supplied code to companies such as McDonnell Douglas (Unigraphics), Computervision (CADDS), Calma, Gerber, Autotrol and Control Data.
As computers became more affordable, the application areas have gradually expanded. The development of CAD software for personal desktop computers was the impetus for almost universal application in all areas of construction.
Other key points in the 1960s and 1970s would be the foundation of CAD systems United Computing, Intergraph, IBM, Intergraph IGDS in 1974 (which led to Bentley Systems MicroStation in 1984).
CAD implementations have evolved dramatically since then. Initially, with 3D in the 1970s, it was typically limited to producing drawings similar to hand-drafted drawings. Advances in programming and computer hardware, notably solid modeling in the 1980s have allowed more versatile applications of computers in design activities.
Key products for 1981 were the solid modeling packages - Romulus (ShapeData) and Uni-Solid (Unigraphics) based on PADL-2 and the release of the surface modeler CATIA (Dassault Systemes). Autodesk was founded 1982 by John Walker, which led to the 2D system AutoCAD. The next milestone was the release of Pro/ENGINEER in 1987, which heralded greater usage of feature-based modeling methods and parametric linking of the parameters of features. Also of importance to the development of CAD was the development of the B-rep solid modeling kernels (engines for manipulating geometrically and topologically consistent 3D objects) Parasolid (ShapeData) and ACIS (Spatial Technology Inc.) at the end of the 1980s and beginning of the 1990s, both inspired by the work of Ian Braid. This led to the release of mid-range packages such as SolidWorks and TriSpective (later known as IRONCAD) in 1995, Solid Edge (then Intergraph) in 1996 and Autodesk Inventor in 1999. An independent geometric modeling kernel has been evolving in Russia since the 1990s.


== See also ==


== References ==


== External links ==
MIT 1982 CAD lab
 Learning materials related to Computer-aided Geometric Design at Wikiversity
 Learning materials related to Computer-aided design at Wikiversity
 Media related to Computer-aided design at Wikimedia Commons
 The dictionary definition of computer-aided design at Wiktionary
Beginning AutoCAD 2018, Industrial Press"
666,BK-tree,3417338,2038,"A BK-tree is a metric tree suggested by Walter Austin Burkhard and Robert M. Keller[1] specifically adapted to discrete metric spaces. For simplicity, let us consider integer discrete metric 
  
    
      
        d
        (
        x
        ,
        y
        )
      
    
    {\displaystyle d(x,y)}
  . Then, BK-tree is defined in the following way. An arbitrary element a is selected as root node. The root node may have zero or more subtrees. The k-th subtree is recursively built of all elements b such that 
  
    
      
        d
        (
        a
        ,
        b
        )
        =
        k
      
    
    {\displaystyle d(a,b)=k}
  . BK-trees can be used for approximate string matching in a dictionary [2].


== See also ==
Levenshtein distance – the distance metric commonly used when building a BK-tree
Damerau–Levenshtein distance – a modified form of Levenshtein distance that allows transpositions


== References ==
^ W. Burkhard and R. Keller. Some approaches to best-match file searching, CACM, 1973
^ R. Baeza-Yates, W. Cunto, U. Manber, and S. Wu. Proximity matching using fixed queries trees. In M. Crochemore and D. Gusfield, editors, 5th Combinatorial Pattern Matching, LNCS 807, pages 198–212, Asilomar, CA, June 1994.
^ Ricardo Baeza-Yates and Gonzalo Navarro. Fast Approximate String Matching in a Dictionary. Proc. SPIRE'98


== External links ==
A BK-tree implementation in Common Lisp with test results and performance graphs.
An explanation of BK-Trees and their relationship to metric spaces [3]
An explanation of BK-Trees with an implementation in C#[4]
A BK-tree implementation in Lua [5]"
667,Poltergeist (computer programming),1777061,2030,"In computer programming, a poltergeist (or gypsy wagon) is a short-lived, typically stateless object used to perform initialization or to invoke methods in another, more permanent class. It is considered an anti-pattern. The original definition is by Michael Akroyd 1996 - Object World West Conference:
""As a gypsy wagon or a poltergeist appears and disappears mysteriously, so does this short lived object. As a consequence the code is more difficult to maintain and there is unnecessary resource waste. The typical cause for this antipattern is poor object design.""
A poltergeist can often be identified by its name; they are often called ""manager_"", ""controller_"", ""start_process"", etc.
Sometimes, poltergeist classes are created because the programmer anticipated the need for a more complex architecture. For example, a poltergeist arises if the same method acts as both the client and invoker in a Command pattern, and the programmer anticipates separating the two phases. However, this more complex architecture may actually never materialize.
Poltergeists should not be confused with long-lived, state-bearing objects of a pattern such as Model-view-controller, or tier-separating patterns such as Business-Delegate.
To remove a poltergeist, delete the class and insert its functionality in the invoked class, possibly by inheritance or as a mixin.


== See also ==
Anti-pattern
YAGNI principle


== References ==
Brown, William J. (1998). ""Chapter 5: Software Development AntiPatterns"". AntiPatterns: Refactoring Software, Architectures, and Projects in Crisis. New York, USA: John Wiley & Sons. ISBN 0-471-19713-0. 


== External links ==
Development AntiPatterns"
668,COSYSMO,2834504,2026,"The Constructive Systems Engineering Cost Model (COSYSMO) was created by Ricardo Valerdi while at the University of Southern California Center for Software Engineering. It gives an estimate of the number of person-months it will take to staff systems engineering resources on hardware and software projects. Initially developed in 2002, the model now contains a calibration data set of more than 50 projects provided by major aerospace and defense companies such as Raytheon, Northrop Grumman, Lockheed Martin, SAIC, General Dynamics, and BAE Systems.
Similar to its predecessor COCOMO, COSYSMO computes effort (and cost) as a function of system functional size and adjusts it based on a number of environmental factors related to systems engineering.
COSYSMO's central cost estimating relationship, or CER is of the form: 
  
    
      
        P
        
          M
          
            N
            S
          
        
        =
        A
        ×
        S
        i
        z
        
          e
          
            E
          
        
        ×
        
          ∏
          
            i
            =
            1
          
          
            n
          
        
        E
        
          M
          
            i
          
        
      
    
    {\displaystyle PM_{NS}=A\times Size^{E}\times \prod _{i=1}^{n}EM_{i}}
  
where ""Size"" is one of four size additive size drivers, and EM represents one of fourteen multiplicative effort multipliers.


== See also ==
Comparison of development estimation software
Software development effort estimation


== Further reading ==
Valerdi, R., Boehm, B., Reifer, D., COSYSMO: A Constructive Systems Engineering Cost Model Coming Age, 13th INCOSE Symposium, July 2003, Crystal City, VA.
Valerdi, R., The Constructive Systems Engineering Cost Estimation Model (COSYSMO), University of Southern California, May 2005.
Valerdi, R., The Constructive Systems Engineering Cost Model (COSYSMO): Quantifying the Costs of Systems Engineering Effort in Complex Systems, VDM Verlag, 2008.


== External links ==
COSYSMO
SystemStar Tool
True COSYSMO
SEER for Systems Engineering"
669,Irwin Sobel,42428054,2022,"Irwin Sobel (born September 12, 1940) is a scientist and researcher in digital image processing.


== Biography ==
Irwin Sobel was born in New York City. He graduated from MIT in 1961 and did his Ph.D. research at the Stanford Artificial Intelligence Project (SAIL) with thesis Camera Models and Machine Perception. He spent most of his career at HP Labs.


== Sobel Operator ==
In 1968, Sobel gave a talk entitled ""An Isotropic 3x3 Image Gradient Operator"" at SAIL; this method became known as the Sobel operator. It should more appropriately be called the Sobel–Feldman operator since it was developed jointly with a colleague, Gary Feldman, also at SAIL.


== References =="
670,Lin–Kernighan heuristic,8818888,2022,"In combinatorial optimization, Lin–Kernighan is one of the best heuristics for solving the symmetric travelling salesman problem. Briefly, it involves swapping pairs of sub-tours to make a new tour. It is a generalization of 2-opt and 3-opt. 2-opt and 3-opt work by switching two or three paths to make the tour shorter. Lin–Kernighan is adaptive and at each step decides how many paths between cities need to be switched to find a shorter tour.


== See also ==
Lin–Kernighan–Johnson
Local search (optimization)


== References ==
Lin, Shen; Kernighan, B. W. (1973). ""An Effective Heuristic Algorithm for the Traveling-Salesman Problem"". Operations Research. 21 (2): 498–516. doi:10.1287/opre.21.2.498. 
K. Helsgaun (2000). ""An Effective Implementation of the Lin-Kernighan Traveling Salesman Heuristic"". European Journal of Operational Research. 126 (1): 106–130. doi:10.1016/S0377-2217(99)00284-2. 
Johnson, David S.; McGeoch, Lyle A. (1997). ""The Traveling Salesman Problem: A Case Study in Local Optimization"". In E. H. L. Aarts and J. K. Lenstra. Local Search in Combinatorial Optimization (PDF). London: John Wiley and Sons. pp. 215–310. CS1 maint: Uses editors parameter (link)


== External links ==
LKH implementation"
671,Captology,13213116,2012,"Captology or behavior design is the study of computers as persuasive technologies. This area of inquiry explores the overlapping space between persuasion in general (influence, motivation, behavior change, etc.) and computing technology. This includes the design, research, and program analysis of interactive computing products (such as the Web, desktop software, specialized devices, etc.) created for the purpose of changing people's attitudes or behaviors.
B. J. Fogg in 1996 derived the term captology from an acronym: Computers As Persuasive Technologies. In 2003, he published the first book on captology, entitled Persuasive Technology: Using Computers to Change What We Think and Do.


== See also ==
Is Google Making Us Stupid?
Persuasive technology


== References ==


== Further reading ==
Fogg, B.J. (2002). Persuasive Technology: Using Computers to Change What We Think and Do. Morgan Kaufmann Publishers. ISBN 1-558-60643-2. 
Teti, A. (2011). PsychoTech, il punto di non ritorno. La tecnologia che controlla la mente (in Italian). Springer-Verlag Italia. ISBN 978-8-847-01814-3. 


== External links ==
The Stanford University Persuasive Technology Lab
The Web Credibility Project"
672,Performance operational analysis,23535285,2010,"In performance engineering, operational analysis is a set of basic quantitative relationships between performance quantities. Basically the Operational Analysis is based on operational laws, e.g. Utilization Law, Service Demand Law, The Forced Flow Law, Little's Law and Interactive Response Time Law and is used to predict the response time, throughput, availability, reliability, security, scalability and extensibility.


== Simple example: utilization law for a single server system ==
Following Denning, consider a single server queuing system. It has a stream of arriving requests, which first go into a queue and then into a server --- eventually completing. This system has four basic quantities that can be observed in a finite period:
T – the length of the period
A – the number of arrivals occurring during the period
B – the total amount of time during which the server is busy during the period
C – the number of completions during the period
From those we can derive some more quantities:
lambda = A/T – the arrival rate
X = C/T – the output rate
U = B/T – the utilization
S = B/C – the mean service time
The utilization law is U = XS. This is established by nothing more than algebra.
There is a corresponding law in more general settings.


== See also ==
Performance engineering
Queueing theory


== References ==


== External links ==
The Operational Analysis of Queueing Network Models"
673,Felix Gers,54036625,2006,"Felix Gers is a professor of computer science at Beuth University of Applied Sciences Berlin. With Jürgen Schmidhuber and Fred Cummins, he introduced the forget gate to the long short-term memory recurrent neural network architecture. This modification of the original architecture has been shown to be crucial to the success of the LSTM at such tasks as speech and handwriting recognition.


== References =="
674,Filer.js,34322002,2003,"Filer.js is a Unix-like wrapper library for the HTML5 filesystem API, written in the javascript programming language. It was first announced on 27 December 2011 by Google Chrome engineer Eric Bidelman on his personal blog.


== Features ==
unix-like commands for interacting with filesystems, such as cp, cd, mkdir, mv amongst others.
Uses asynchronous functions and callbacks to comply with the HTML5 filesystem API.
Accepts multiple types when working with entries. Can accept entries as string paths or as filesystem URLS.


== References ==


== External links ==
Filer on Github
Filer.js sample app"
675,Type erasure,7386145,2003,"In programming languages, type erasure refers to the compile-time process by which explicit type annotations are removed from a program, before it is executed at run-time. Operational semantics that do not require programs to be accompanied by types are called type-erasure semantics, to be contrasted with type-passing semantics. The possibility of giving type-erasure semantics is a kind of abstraction principle, ensuring that the run-time execution of a program does not depend on type information. In the context of generic programming, the opposite of type erasure is called reification.


== Type inference ==

The reverse operation is called type inference. Though type erasure can be used as an easy way to define typing over implicitly typed languages (an implicitly typed term is well-typed if and only if it is the erasure of a well-typed explicitly typed lambda term), it does not always lead to an algorithm to check implicitly typed terms.


== See also ==
Template (C++)
Problems with type erasure (in Generics in Java)
Type polymorphism


== References ==

Crary, Karl; Weirich, Stephanie; Morrisett, Greg (2002). ""Intensional Polymorphism in Type-Erasure Semantics"". Journal of Functional Programming. 12 (6): 567–600. doi:10.1017/S0956796801004282."
676,Amidicity,42777214,2001,"The amidicity scale is a computational method for calculating the strength of an amide bond in an organic compound on a linear scale. It is analogous to aromaticity. It is based on the computed enthalpy of hydrogenation when compared to the reference compounds dimethylacetamide and azaadamantane-2-one. If an amidicity value is close to 100%, then the compound has very good amidic character (and is perfect at 100%); if the value is close to, or below, 0%, then the compound has a lack of amidic character. The scale is not restricted to these values; compounds with weaker amide bonds than azaadamantane-2-one will have amidicities below 0%, whilst compounds with stronger amide bonds than dimethylacetamide will have amidicities of above 100%. If the amidicity value is altered during an acylation, then this will act as a key thermodynamic component of the reaction.


== References =="
677,Woz U,55533945,1999,"Woz U is a tech education platform launched by Apple co-founder Steve Wozniak that focuses on both students and companies. Woz U is Arizona-based with plans to launch physical locations for learning in more than 30 cities across the globe.


== References ==


== External links ==
Woz U – Education. Reprogrammed. – official site"
678,Locality-preserving hashing,2992597,1997,"In computer science, a locality-preserving hashing is a hash function f that maps a point or points in a multidimensional coordinate space to a scalar value, such that if we have three points A, B and C such that

  
    
      
        
          |
        
        A
        −
        B
        
          |
        
        <
        
          |
        
        B
        −
        C
        
          |
        
        ⇒
        
          |
        
        f
        (
        A
        )
        −
        f
        (
        B
        )
        
          |
        
        <
        
          |
        
        f
        (
        B
        )
        −
        f
        (
        C
        )
        
          |
        
        .
        
      
    
    {\displaystyle |A-B|<|B-C|\Rightarrow |f(A)-f(B)|<|f(B)-f(C)|.\,}
  
In other words, these are hash functions where the relative distance between the input values is preserved in the relative distance between of the output hash values; input values that are closer to each other will produce output hash values that are closer to each other.
This is in contrast to cryptographic hash functions and checksums, which are designed to have maximum output difference between adjacent inputs.
Locality preserving hashes are related to space-filling curves and locality-sensitive hashing.


== External links ==
Indyk, Piotr; Motwani, Rajeev; Raghavan, Prabhakar; Vempala, Santosh (1997). ""Locality-preserving hashing in multidimensional spaces"". Proceedings of the twenty-ninth annual ACM symposium on Theory of computing. pp. 618–625. CiteSeerX 10.1.1.50.4927 . doi:10.1145/258533.258656. ISBN 0-89791-888-6. 
Chin, Andrew (1994). ""Locality-preserving hash functions for general purpose parallel computation"" (PDF). Algorithmica. 12 (2–3): 170–181. doi:10.1007/BF01185209."
679,European Symposium on Programming,39166506,1995,"The European Symposium on Programming (ESOP) is an annual conference devoted to fundamental issues in the specification, design, analysis, and implementation of programming languages and systems. It is one of the 10 top conferences in Programming Languages.
Initially a biannual conference, ESOP moved in 1998 into an annual schedule and became one of the founding conferences of the European Joint Conferences on Theory and Practice of Software (ETAPS).


== See also ==
List of computer science conferences
List of computer science conference acronyms
List of publications in computer science
Outline of computer science


== References ==


== Further reading ==
Special issue of Theoretical Computer Science on the European Symposium on Programming
Special issue of ACM Transactions on Programming Languages and Systems on the European Symposium on Programming
Special issue of Science of Computer Programming on the European Symposium on Programming


== External links ==
Official website
DBLP Page of ESOP Conferences"
680,SIGCOMM Award,1609171,1994,"The SIGCOMM Award recognizes lifetime contribution to the field of communication networks. The award is presented in the annual SIGCOMM Technical Conference.
The awardees have been:
2015 Albert Greenberg
2014 George Varghese
2013 Larry Peterson
2012 Nick McKeown
2011 Vern Paxson
2010 Radia Perlman
2009 Jon Crowcroft
2008 Don Towsley
2007 Sally Floyd
2006 Domenico Ferrari
2005 Paul Mockapetris
2004 Simon S. Lam
2003 David Cheriton
2002 Scott Shenker
2001 Van Jacobson
2000 Andre Danthine
1999 Peter Kirstein
1998 Larry Roberts
1997 Jon Postel
1997 Louis Pouzin
1996 Vint Cerf
1995 David J. Farber
1994 Paul Green
1993 Robert Kahn
1992 Sandy Fraser
1991 Hubert Zimmerman
1990 David D. Clark
1990 Leonard Kleinrock
1989 Paul Baran


== References ==


== External links ==
[1]"
681,RDF/XML,13246688,1987,"RDF/XML is a syntax, defined by the W3C, to express (i.e. serialize) an RDF graph as an XML document. RDF/XML is sometimes misleadingly called simply RDF because it was introduced among the other W3C specifications defining RDF and it was historically the first W3C standard RDF serialization format. Although the RDF/XML format is still in use, other RDF serializations are now preferred by many RDF users, both because they are more human-friendly, and because some RDF graphs are not representable in RDF/XML due to restrictions on the syntax of XML QNames.


== References ==


== External links ==
RDF/XML Syntax Specification
RDF Primer
RFC 3870: application/rdf+xml Media Type Registration"
682,Soft state,37081445,1983,"In computer science, soft state is state which is useful for efficiency, but not essential, as it can be regenerated or replaced if needed. The term is often used in network protocol engineering.
It is a term that is used for information that times out (goes away) unless refreshed, which allows protocols to recover from errors in certain services. The term was coined by David D. Clark in his description of the Defense Advanced Research Projects Agency (DARPA) internet protocols.
While in general less efficient than well-designed ""hard state"" protocols when tuned for a particular network regime, soft state protocols behave much better than hard state protocols in an unpredictable network environment such as the Internet.


== References ==


== External links ==
""Soft"" and ""Hard"" State"
683,Fundamental Concepts in Programming Languages,31604290,1983,"Fundamental Concepts in Programming Languages were an influential set of lecture notes written by Christopher Strachey for the International Summer School in Computer Programming at Copenhagen in August, 1967. It introduced much programming language terminology still in use today, including ""R-value"" and ""L-value"", ""ad hoc polymorphism"", ""parametric polymorphism"", and ""referential transparency"".
The lecture notes were reprinted in 2000 in a special issue of Higher-Order and Symbolic Computation in memory of Strachey.


== Bibliography ==
Mosses, Peter D. (2000). ""A Foreword to 'Fundamental Concepts in Programming Languages'"". Higher-Order and Symbolic Computation. 13: 7–9. doi:10.1023/A:1010048229036. 
Strachey, Christopher (2000). ""Fundamental Concepts in Programming Languages"". Higher-Order and Symbolic Computation. 13: 11–49. doi:10.1023/A:1010000313106. 


== See also ==
CPL (programming language)


== External links ==
Higher-Order and Symbolic Computation Volume 13, Issue 1/2 (April 2000) Special Issue in memory of Christopher Strachey
Fundamental Concepts In Programming Languages at the Portland Pattern Repository
ACM Digital Library
Great Works in Programming Languages. Collected by Benjamin C. Pierce."
684,Shotgun debugging,101415,1972,"Shotgun debugging can be defined as:
A process of making relatively un-directed changes to software in the hope that a bug will be perturbed out of existence.
Using the approach of trying several possible solutions of hardware or software problem at the same time, in the hope that one of the solutions (typically source code modifications) will work.
Shotgun debugging has a relatively low success rate and can be very time consuming, except in very simple programs, or when used as an attempt to work around programming language features that one may be using improperly; it usually introduces more bugs.


== Examples ==
Shotgun debugging can occur when working with multi-threaded applications. Attempting to debug a race condition by adding debugging code to the application is likely to change the speed of one thread in relation to another and could cause the problem to disappear. This is known as a Heisenbug. Although apparently a solution to the problem, it is a fix by pure chance and anything else that changes the behavior of the threads could cause it to resurface — for example on a computer with a different scheduler. Code added to any part of the program could easily revert the effect of the ""fix"".


== See also ==
Fuzzing


== References ==

This article is based in part on the Jargon File, which is in the public domain."
685,Knowledge Acquisition and Documentation Structuring,18039001,1970,"Knowledge Acquisition and Documentation Structuring (KADS) is a structured way of developing knowledge-based systems (expert systems). It was developed at the University of Amsterdam as an alternative to an evolutionary approach and is now accepted as the European standard for knowledge based systems.
Its components are:
A methodology for managing knowledge engineering projects.
A knowledge engineering workbench.
A methodology for performing knowledge elicitation.
KADS was further developed into CommonKADS.


== KADS methodology and the industrial development of expert systems ==
A study carried out in 1989 showed that the main reason why expert systems were not being used was an insufficiency of methods for development, especially in the construction of knowledge bases, e.g. the transfer of expertise.
Knowledge Based Systems Analysis and Design Support (KADS) originating in the European ESPRIT project P1098 and representing 75 person-years of work, was one of the most highly developed KBs (Knowledge Based Systems) in the early 90s. This pioneering method provides two types of support for the production of KBs in an industrial approach: firstly, a lifecycle enabling a response to be made to technical and economic constraints (control of the production process, quality assurance of the system,...), and secondly a set of models which structure the production of the system, especially the tasks of analysis and the transformation of expert knowledge into a form exploitable by the machine.


== References ==
This article is based on material taken from the Free On-line Dictionary of Computing prior to 1 November 2008 and incorporated under the ""relicensing"" terms of the GFDL, version 1.3 or later.


== External links ==
CommonKADS website"
686,Deep lambertian networks,42358441,1968,"Deep Lambertian Networks (DLN)  is a combination of Deep belief network and Lambertian reflectance assumption which deals with the challenges posed by illumination variation in visual perception. Lambertian Reflectance model gives an illumination invariant representation which can be used for recognition. The Lambertian reflectance model is widely used for modeling illumination variations and is a good approximation for diffuse object surfaces. The DLN is a hybrid undirected-directed generative model that combines DBNs with the Lambertian reflectance model.
In the DLN, the visible layer consists of image pixel intensities v ∈ RNv, where Nv is the number of pixels in the image. For every pixel i there are two latent variables namely the albedo and surface normal. GRBMs are used to model the albedo and surface normals.
Combining Deep Belief Nets with the Lambertian reflectance assumption, the model can learn good priors over the albedo from 2D images. Illumination variations can be explained by changing only the lighting latent variable. By transferring learned knowledge from similar objects, albedo and surface normals estimation from a single image is also possible. Experiments demonstrate that this model is able to generalize as well as improve over standard baselines in one-shot face recognition.
The model has been successfully applied in reconstruction of shadows facial images, given any set of lighting conditions. The model has also been tested on non-living objects. The method outperforms most other methods and is faster than them.


== References =="
687,Aum Programming Language,41357368,1966,"Aum Programming Language is a multi-paradigm programming language which has been an internal research project at IT Adapter since 2004. It is somewhat similar to modern C# in its feature set adding aspect-oriented-programming, message passing and pattern matching (a la Objective-C, Embarcadero Delphi and Erlang (programming language)).  
The main idea is to build everything around Abstract Syntax Tree. There is no intermediate code a-la Java bytecode or CIL or Dalvik (software) code. Aum modules (compiled assemblies .dll/.class files analogue) are generically serialized abstract-syntax-trees. Aum supports aspect oriented programming with AST pattern matching so aspects may be injected in ""compiled"" code.
Currently we have constructed lexer,parser and most of semantic analyzer. The first code gen is basically an AST-walking interpreter. We have plans to keep working on this project and start using LLVM for machine code JIT compilation. One of the compilation targets is native executable (no VM) just like with C/C++.
Aum is a cross-platform development paradigm, based on a 100% written from scratch runtime library and will be released as Open Source Software targeting Linux, Windows, Android (operating system) and other platforms. 


== See also ==
Embarcadero Delphi
C Sharp (programming language)
Erlang (programming language)
Java (programming language)
Objective-C


== References =="
688,Hooksafe,41280552,1963,"Hooksafe is a hypervisor-based light system that safeguards a computer's kernel from rootkit attacks.
It prevents thousands of kernel hooks in the guest operating system from being hijacked. This is achieved by making a shadow copy of all the kernel hooks at one central place and adding an indirection layer on it to regulate attempts to access the hooks. A prototype of Hooksafe was used on a Linux guest and protected nearly 6000 kernel hooks. It focuses on protecting kernel control data that are function pointers. It provides large scale hook protection with small performance overhead


== History ==
Prior rootkit thwarting systems include: Panorama, Hookfinder and systems focused on analyzing rootkit behavior, Copilot, VMwatcher and systems that detect rootkits based on symptoms, Patagonix, NICKLE and systems aimed to preserve kernel code integrity by preventing malicious rootkit code from executing.


== References ==


== External links ==
VMwatcher
Category:Utility software types"
689,Potato peeling,22021760,1942,"In computational geometry, the potato peeling or convex skull problem is a problem of finding the convex polygon of the largest possible area that lies within a given non-convex polygon. It was posed independently by Goodman and Woo, and solved in polynomial time by Chang and Yap. The exponent of the polynomial time bound is high, but the same problem can also be accurately approximated in near-linear time.


== References =="
690,Path expression,3975986,1938,"In query languages, path expressions identify an object by describing how to navigate to it in some graph (possibly implicit) of objects. For example, the path expression p.Manager.Home.City might refer the city of residence of someone's manager. Path expressions have been extended to support regular expression-like flexibility. XPath is an example of a path expression language.
In concurrency control, path expressions are a mechanism for expressing permitted sequences of execution. For example, a path expression like "" {read}, write"" might specify that either multiple simultaneous executions of read or a single execution of write but not both are allowed at any point in time.


== See also ==
Object database


== References ==
M. Kifer; W. Kim & Y. Sagiv (1992). ""Querying Object-Oriented Databases"". Proc. of the ACM SIGMOD. pp. 393–402. 
Elisa Bertino, Mauro Negri, Giuseppe Pelagatti, and Licia Sbattella (June 1992). ""Object-Oriented Query Languages: The Notion and the Issues"". IEEE Trans. on Knowledge and Data Engineering. 4 (3): 223–236. doi:10.1109/69.142014. CS1 maint: Multiple names: authors list (link)
R. Campbell & R. Kolstad (1979). ""Path Expressions in Pascal"". Proceedings of the 4th International Conference on Software Engineering. 4. pp. 212–219. 
Tony Bloom (1979). ""Evaluating Synchronization Mechanisms"". Proceedings of the seventh ACM symposium on Operating systems principles. pp. 24–32."
691,Coreset,10364657,1930,"In computational geometry, a coreset is a small set of points that approximates the shape of a larger point set, in the sense that applying some geometric measure to the two sets (such as their minimum bounding box volume) results in approximately equal numbers. Many natural geometric optimization problems have coresets that approximate an optimal solution to within a factor of 1 + ε, that can be found quickly (in linear time or near-linear time), and that have size bounded by a function of 1/ε independent of the input size, where ε is an arbitrary positive number. When this is the case, one obtains a linear-time or near-linear time approximation scheme, based on the idea of finding a coreset and then applying an exact optimization algorithm to the coreset. Regardless of how slow the exact optimization algorithm is, for any fixed choice of ε, the running time of this approximation scheme will be O(1) plus the time to find the coreset.


== References =="
692,Dynamic priority scheduling,8355429,1927,"Dynamic priority scheduling is a type of scheduling algorithm in which the priorities are calculated during the execution of the system. The goal of dynamic priority scheduling is to adapt to dynamically changing progress and form an optimal configuration in self-sustained manner. It can be very hard to produce well-defined policies to achieve the goal depending on the difficulty of a given problem.
Earliest deadline first scheduling and Least slack time scheduling are examples of Dynamic priority scheduling algorithms.


== Optimal Schedulable Utilization ==
The idea of real-time scheduling is to confine processor utilization under schedulable utilization of a certain scheduling algorithm, which is scaled from 0 to 1. Higher schedulable utilization means higher utilization of resource and the better the algorithm. In preemptible scheduling, dynamic priority scheduling such as earliest deadline first (EDF) provides the optimal schedulable utilization of 1 in contrast to less than 0.69 with fixed priority scheduling such as rate-monotonic (RM).
In periodic real-time task model, a task's processor utilization is defined as execution time over period. Every set of periodic tasks with total processor utilization less or equal than the schedulable utilization of an algorithm can be feasibly scheduled by that algorithm. Unlike fixed priority, dynamic priority scheduling could dynamically prioritize task deadlines achieving optimal schedulable utilization in the preemtible case.


== Examples ==
Earliest deadline first scheduling
Least slack time scheduling


== References =="
693,U-matrix,28071238,1925,"The U-matrix (unified distance matrix) is a representation of a self-organizing map (SOM) where the Euclidean distance between the codebook vectors of neighboring neurons is depicted in a grayscale image. This image is used to visualize the data in a high-dimensional space using a 2D image.


== Construction procedure ==
Once the SOM is trained using the input data, the final map is not expected to have any twists. If the map is twist-free, the distance between the codebook vectors of neighboring neurons gives an approximation of the distance between different parts of the underlying data. When such distances are depicted in a grayscale image, light colors depict closely spaced node codebook vectors and darker colors indicate more widely separated node codebook vectors. Thus, groups of light colors can be considered as clusters, and the dark parts as the boundaries between the clusters. This representation can help to visualize the clusters in the high-dimensional spaces, or to automatically recognize them using relatively simple image processing techniques.


== References =="
694,SPLASH (conference),29601943,1923,"SPLASH is a programming language-related conference held since 2011, sponsored by the SIGPLAN special interest group of the Association for Computing Machinery (ACM). Its name is an acronym for Systems, Programming, Languages, and Applications: Software for Humanity. SPLASH is an umbrella conference for two longstanding conferences, OOPSLA and Onward! which are now tracks of SPLASH.
SPLASH conferences held so far have been:
Sparks, Nevada, October 17-21, 2010
Portland, Oregon, October 22-27, 2011
Tucson, Arizona, October 19-25, 2012
Indianapolis, Indiana, October 26-31, 2013
Portland, Oregon, October 20-24, 2014
Pittsburgh, Pennsylvania, October 25-30, 2015
Amsterdam, Netherlands, October 30 - November 4, 2016
Vancouver, Canada, October 22 - October 27, 2017
Upcoming SPLASH conferences:
Boston, Massachusetts, November 4 - November 9, 2018


== References ==


== External links ==
Official SPLASH website
History of the conference"
695,Australian Committee on Computation and Automatic Control,28694465,1920,"The Australian Committee on Computation and Automatic Control (ANCCAC) was formed in 1958, with Professor John Bennett as the Foundation Chairman. It ran a computing conferences in Australia from 1960 and in 1961 was accepted as a member of the International Federation for Information Processing (IFIP). The Australian Computer Society took over these roles in 1969 and ANCCAC was dissolved.


== ANCCAC Prize ==
The ANCCAC Prize was established by ACS in 1969, to commemorate Australia's computer pioneers. A medal and a cash prize is awarded each year for the paper published each year in the ACS Journal.


=== ANCCAC Prize recipients ===
1991 Swatman P.A., Swatman P.M.C. and Everett J.E. (1990) ""Stages of Growth of an Innovative Software House: an Additional Criterion for Software Package Selection"", Australian Computer Journal, Vol. 22, No. 7, August, 88-98.
2002 Sale, A. (2001) ""Broadband Internet Access in Regional Australia"", Journal of Research and Practice in Information Technology 33(4): 346-355.
2003 C. A. Middleton (2002) ""Who needs a `Killer App`? Two Perspectives on Content in Residential Broadband Networks"". Journal of Research and Practice in Information Technology 34(2): 67-81.


== External links ==
ACS web site
ACS ANCCAC Award"
696,Perceptual hashing,44284666,1917,"Perceptual hashing is the use of an algorithm that produces a snippet or fingerprint of various forms of multimedia. Perceptual hash functions are analogous if features are similar, whereas cryptographic hashing relies on the avalanche effect of a small change in input value creating a drastic change in output value. Perceptual hash functions are widely used in finding cases of online copyright infringement as well as in digital forensics because of the ability to have a correlation between hashes so similar data can be found (for instance with a differing watermark). For example, Wikipedia could maintain a database of text hashes of popular online books or articles for which the authors hold copyrights to, anytime a Wikipedia user uploads an online book or article that has a copyright, the hashes will be almost exactly the same and could be flagged as plagiarism. This same flagging system can be used for any multimedia or text file.


== References ==


== External links ==
pHash - an open source perceptual hash library
Blockhash.io - an open standard for perceptual hashes
Elog.io - open source blockhash.io implementations
Insight - a perceptual hash tutorial"
697,STRIDE (security),10093926,1916,"STRIDE is a threat classification model developed by Microsoft for thinking about computer security threats. It provides a mnemonic for security threats in six categories.
The threat categories are:
Spoofing of user identity
Tampering
Repudiation
Information disclosure (privacy breach or data leak)
Denial of service (D.o.S)
Elevation of privilege
The STRIDE was initially created as part of the process of threat modelling. STRIDE is a model of threats, used to help reason and find threats to a system. It is used in conjunction with a model of the target system that can be constructed in parallel. This includes a full breakdown of processes, data stores, data flows and trust boundaries.
Today it is often used by security experts to help answer the question ""what can go wrong in this system we're working on?""


== See also ==

DREAD (risk assessment model) - another mnemonic for security threats
Cyber security and countermeasure
OWASP


== References ==


== External links ==
Uncover Security Design Flaws Using The STRIDE Approach"
698,Herbrand Award,2606059,1908,"The Herbrand Award for Distinguished Contributions to Automated Reasoning is an award given by CADE Inc. (although it predates the formal incorporation of CADE) to honour persons or groups for important contributions to the field of automated deduction. The award is named after the French scientist Jacques Herbrand and given at most once per CADE or IJCAR conference. It comes with a prize of US$ 1000. Anyone can be nominated, the award is awarded after a vote among CADE trustees and former recipients, usually with input from the CADE/IJCAR programme committee.
Past recipients of the award are:
Larry Wos (1992)
Woody Bledsoe (1994)
John Alan Robinson (1996)
Wu Wenjun (1997)
Gérard Huet (1998)
Robert S. Boyer and J Strother Moore (1999)
William W. McCune (2000)
Donald W. Loveland (2001)
Mark E. Stickel (2002).
Peter B. Andrews (2003)
Harald Ganzinger (2004)
Martin Davis (2005)
Wolfgang Bibel (2006)
Alan Bundy (2007)
Edmund M. Clarke (2008)
Deepak Kapur (2009)
David Plaisted (2010)
Nachum Dershowitz (2011)
Melvin Fitting (2012)
C Greg Nelson (2013)
Robert L. Constable (2014)
Andrei Voronkov (2015)
Zohar Manna and Richard Waldinger (2016)
Lawrence C. Paulson (2017)


== See also ==
Jacques Herbrand Prize — by the French Academy of Sciences, for mathematics and physics


== External links ==
The Herbrand Award for Distinguished Contributions to Automated Reasoning"
699,Simple HTML Ontology Extensions,4783531,1908,"In the semantic web, Simple HTML Ontology Extensions are a small set of HTML extensions designed to give web pages semantic meaning by allowing information such as class, subclass and property relationships.
SHOE was developed around 1996 by Sean Luke, Lee Spector, James Hendler, Jeff Heflin, and David Rager at the University of Maryland, College Park.


== See also ==
Microformat
Microdata (HTML)


== References ==
Luke, S., Spector, L, and Rager, D. Ontology-Based Knowledge Discovery on the World-Wide Web. Workshop on Internet-Based Information Systems at the 13th National Conference on Artificial Intelligence. 1996.
Luke, S. and Hendler, J. Web Agents that Work. IEEE Multimedia 4:3. 1997.
Luke, S., Spector, L., Rager, D., and Hendler, J. Ontology-based Web Agents. Proceedings of the First International Conference on Autonomous Agents. 1997.
Heflin, J., Hendler, J., and Luke, S. SHOE: A Knowledge Representation Language for Internet Applications. Technical Report CS-TR-4078 (UMIACS TR-99-71), Dept. of Computer Science, University of Maryland at College Park. 1999.
Heflin, J. and Hendler, J. Searching the Web with SHOE. In Artificial Intelligence for Web Search. Papers from the AAAI Workshop. WS-00-01. AAAI Press, Menlo Park, CA, 2000. pp. 35-40.
Heflin, J. Towards the Semantic Web: Knowledge Representation in a Dynamic, Distributed Environment. Ph.D. Thesis, University of Maryland, College Park. 2001.
Heflin, J. and Hendler, J. A Portrait of the Semantic Web in Action. IEEE Intelligent Systems, 16(2):54-59, 2001.


== External links ==
UMD SHOE web page"
700,Applicative functor,50064642,1907,"In functional programming, specifically Haskell, an applicative functor is a structure that is like a monad (return, fmap, join) without join, or like a functor with return. Applicative functors are the programming equivalent of lax monoidal functors with tensorial strength in category theory. In Haskell, applicative functors are implemented in the Applicative type class. Applicative functors were introduced in 2007 by Conor McBride and Ross Paterson in their paper Functional Pearl: applicative programming with effects.


== Relationship to monads ==
Due to historical accident, applicative functors were not implemented as a subclass of Functor and not as a superclass of Monad, but as a separate type class with no overlap. It turned out that in practice, there was very little demand for such a separation, so in 2014, it was proposed to make Applicative retroactively a subclass of Functor.


== See also ==
Current definition of the Applicative class in Haskell


== References =="
701,False nearest neighbor algorithm,2427912,1902,"The false nearest neighbor algorithm is an algorithm for estimating the embedding dimension. The concept was proposed by Kennel et al. The main idea is to examine how the number of neighbors of a point along a signal trajectory change with increasing embedding dimension. In too low an embedding dimension, many of the neighbors will be false, but in an appropriate embedding dimension or higher, the neighbors are real. With increasing dimension, the false neighbors will no longer be neighbors. Therefore, by examining how the number of neighbors change as a function of dimension, an appropriate embedding can be determined.


== See also ==
Time series
Nearest neighbor


== References ==
Rhodes, C.; Morari, M. (1997). ""The false nearest neighbors algorithm: An overview"". Computers & Chemical Engineering. 21: S1149. doi:10.1016/S0098-1354(97)87657-0. 
Hegger, R.; Kantz, H. (1999). ""Improved false nearest neighbor method to detect determinism in time series data"". Physical Review E. 60 (4): 4970. Bibcode:1999PhRvE..60.4970H. doi:10.1103/PhysRevE.60.4970. 
Kennel, M.; Brown, R.; Abarbanel, H. (1992). ""Determining embedding dimension for phase-space reconstruction using a geometrical construction"". Physical Review A. 45 (6): 3403–3411. Bibcode:1992PhRvA..45.3403K. doi:10.1103/PhysRevA.45.3403. PMID 9907388."
702,Leiden Classical,21471659,1901,"Leiden Classical is a distributed computing project run by the Theoretical Chemistry Department of the Leiden Institute of Chemistry at Leiden University. Leiden Classical is part of the BOINC system, and enables scientists or science students to submit their own test simulations of various molecules and atoms in a classical mechanics environment. ClassicalDynamics is a program (and with it a library) completely written in C++. The library is covered by the LGPL license and the main program is covered by the GPL.


== Joining the project ==
Participation is possible via the BOINC manager. Using this software one can create an account in the project. Then someone can make a model of a dynamic system and simulation participating run. There are several models possible, to interactions between molecules or planets.


== User Submitted Calculations ==
To create a personal calculation, your model must have 6 defined variables:
Colors of your molecules
Box in which the model is run
Number of particles in the simulation
Interaction between the particles
Gravity
Coulomb force
Lennard-Jones interaction
Morse interaction
Rydberg interaction
Harmonic spirit
Harmonic bending
Recurrent torsion interactions

Distance conditions
Confirmation parameter(s)


== See also ==
List of distributed computing projects


== References ==

Leiden Classical Forum


== External links ==
Leiden Classical website"
703,Commit (version control),46910960,1900,"In version control systems, a commit adds the latest changes to [part of] the source code to the repository, making these changes part of the head revision of the repository. Unlike commits in data management, commits in version control systems are kept in the repository indefinitely. Thus, when other users do an update or a checkout from the repository, they will receive the latest committed version, unless they specify they wish to retrieve a previous version of the source code in the repository. Version control systems allow rolling back to previous versions easily. In this context, a commit within a version control system is protected as it is easily rolled back, even after the commit has been applied.


== Usage ==


=== Git ===
To commit a change in git, in Terminal (OS X) or command line (assuming git is installed), the following command is run:
git commit -m 'commit message'
This is also assuming that the files within the current directory have been staged as such:
git add .
The above command adds all of the files in the working directory to be staged for the git commit. After the commit has been applied, the last step is to push the commit to the given software repository, in the case below named origin, to the branch master:
git push origin master


== See also ==
Commit (data management)"
704,Security Protocols Open Repository,11271694,1897,"SPORE, the Security Protocols Open Repository, is an online library of security protocols with comments and links to papers. Each protocol is downloadable in a variety of formats, including rules for use with automatic protocol verification tools. All protocols are described using BAN logic or the style used by Clark and Jacob, and their goals. The database includes details on formal proofs or known attacks, with references to comments, analysis & papers. A large number of protocols are listed, including many which have been shown to be insecure.
It is a continuation of the seminal work by John Clark and Jeremy Jacob.
They seek contributions for new protocols, links and comments.


== See also ==

Cryptography
Symmetric-key algorithm
Public-key cryptography
Cryptographic nonce
List of cryptography topics.
Short and long lists of cryptographers.
Important books, papers, and open problems in cryptography.


== External links ==
SPORE - Security Protocols Open Repository


== References =="
705,Successive linear programming,8215226,1894,"Successive Linear Programming (SLP), also known as Sequential Linear Programming, is an optimization technique for approximately solving nonlinear optimization problems.
Starting at some estimate of the optimal solution, the method is based on solving a sequence of first-order approximations (i.e. linearizations) of the model. The linearizations are linear programming problems, which can be solved efficiently. As the linearizations need not be bounded, trust regions or similar techniques are needed to ensure convergence in theory. 
SLP has been used widely in the petrochemical industry since the 1970s. 


== See also ==
Sequential quadratic programming
Sequential linear-quadratic programming
Augmented Lagrangian method


== References ==


== Sources ==
Nocedal, Jorge; Wright, Stephen J. (2006). Numerical Optimization (2nd ed.). Berlin, New York: Springer-Verlag. ISBN 978-0-387-30303-1. 
Bazaraa, Mokhtar S.; Sheraly, Hanif D.; Shetty, C.M. (1993). Nonlinear Programming, Theory and Applications (2nd ed.). John Wiley & Sons. ISBN 0-471-55793-5. 
Palacios-Gomez, F.; Lasdon, L.; Enquist, M. (October 1982). ""Nonlinear Optimization by Successive Linear Programming"". Management Science. 28 (10). doi:10.1287/mnsc.28.10.1106."
706,Incompressible string,6495416,1894,"An incompressible string is a string with Kolmogorov complexity equal to its length, so that it has no shorter encodings.


== Example ==
Suppose we have the string 12349999123499991234, and we are using a compression method that works by putting a special character into the string (say '@') followed by a value that points to an entry in a lookup table (or dictionary) of repeating values. Let's imagine we have an algorithm that examines the string in 4 character chunks. Looking at our string, our algorithm might pick out the values 1234 and 9999 to place into its dictionary. Let's say 1234 is entry 0 and 9999 is entry 1. Now the string can become:
@0@1@0@1@0
Obviously, this is much shorter, although storing the dictionary itself will cost some space. However, the more repeats there are in the string, the better the compression will be.
Our algorithm can do better though, if it can view the string in chunks larger than 4 characters. Then it can put 12349999 and 1234 into the dictionary, giving us:
@0@0@1
Even shorter. Now let's consider another string:
1234999988884321
This string is incompressible by our algorithm. The only repeats that occur are 88 and 99. If we were to store 88 and 99 in our dictionary, we would produce:
1234@1@1@0@04321
Unfortunately this is just as long as the original string, because our placeholders for items in the dictionary are 2 characters long, and the items they replace are the same length. Hence, this string is incompressible by our algorithm.


== References ==


== External links =="
707,DISCUS,4878864,1885,"The discus throw ( pronunciation) is a track and field event in which an athlete throws a heavy disc—called a discus—in an attempt to mark a farther distance than their competitors. It is an ancient sport, as demonstrated by the fifth-century-BC Myron statue, Discobolus. Although not part of the modern pentathlon, it was one of the events of the ancient Greek pentathlon, which can be dated back to at least to 708 BC.


== History ==

Discus is a routine part of most modern track-and-field meets at all levels and is a sport which is particularly iconic of the Olympic Games. The men's competition has been a part of the modern Summer Olympic Games since the first Olympic games in 1896. Images of discus throwers figured prominently in advertising for early modern Games, such as fundraising stamps for the 1896 games and the main posters for the 1920 and 1948 Summer Olympics.
The discus was re-discovered in Magdeburg, Germany, by Christian Georg Kohlrausch and his students in the 1870s. His work around the discus and the earlier throwing techniques have been published since the 1880.
The first modern athlete to throw the discus while rotating the whole body was František Janda-Suk from Bohemia (present Czech Republic). He invented this technique when studying the position of the famous statue of Discobolus. After only one year of developing the technique he gained the olympic silver in 1900.
The women's competition was added to the Olympic program in the 1928 games, although they had been competing at some national and regional levels previously.


== Description ==

The men's discus is a heavy lenticular disc with a weight of 2 kilograms (4.4 lb) and diameter of 22 centimetres (8.7 in), the women's discus has a weight of 1 kilogram (2.2 lb) and diameter of 18 centimetres (7.1 in).
Under IAAF (international) rules, Youth boys (16–17 years) throw the 1.5 kilograms (3.3 lb) discus, the Junior men (18–19 years) throw the unique 1.75 kilograms (3.9 lb) discus, and the girls/women of those ages throw the 1 kilogram (2.2 lb) discus.
In international competition, men throw the 2 kg discus through to age 49. The 1.5 kilograms (3.3 lb) discus is thrown by ages 50–59, and men age 60 and beyond throw the 1 kilogram (2.2 lb) discus. Women throw the 1 kilogram (2.2 lb) discus through to age 74. Starting with age 75, women throw the 0.75 kilograms (1.7 lb) discus.
The typical discus has sides made of plastic, wood, fiberglass, carbon fiber or metal with a metal rim and a metal core to attain the weight. The rim must be smooth, with no roughness or finger holds. A discus with more weight in the rim produces greater angular momentum for any given spin rate, and thus more stability, although it is more difficult to throw. However, a higher rim weight, if thrown correctly, can lead to a farther throw. A solid rubber discus is sometimes used (see in the United States).
To make a throw, the competitor starts in a circle of 2.5 m (8 ft 2​1⁄4 in) diameter, which is recessed in a concrete pad by 20 millimetres (0.79 in). The thrower typically takes an initial stance facing away from the direction of the throw. He then spins anticlockwise (for right-handers) around one and a half times through the circle to build momentum, then releases his throw. The discus must land within a 34.92-degree sector. The rules of competition for discus are virtually identical to those of shot put, except that the circle is larger, a stop board is not used and there are no form rules concerning how the discus is to be thrown.

The basic motion is a forehanded sidearm movement. The discus is spun off the index finger or the middle finger of the throwing hand. In flight the disc spins clockwise when viewed from above for a right-handed thrower, and anticlockwise for a left-handed thrower. As well as achieving maximum momentum in the discus on throwing, the discus' distance is also determined by the trajectory the thrower imparts, as well as the aerodynamic behavior of the discus. Generally, throws into a moderate headwind achieve the maximum distance. Also, a faster-spinning discus imparts greater gyroscopic stability. The technique of discus throwing is quite difficult to master and needs lots of experience to get right, thus most top throwers are 30 years old or more.


== Phases ==
The discus technique can be broken down into phases. The purpose is to transfer from the back to the front of the throwing circle while turning through one and a half circles. The speed of delivery is high, and speed is built up during the throw (slow to fast). Correct technique involves the buildup of torque so that maximum force can be applied to the discus on delivery.

During the wind-up, weight is evenly distributed between the feet, which are about shoulder distance and not overly active. The wind-up sets the tone for the entire throw; the rhythm of the throw is very important.
Focusing on rhythm can bring about the consistency to get in the right positions that many throwers lack. Executing a sound discus throw with solid technique requires perfect balance. This is due to the throw being a linear movement combined with a one and a half rotation and an implement at the end of one arm. Thus, a good discus thrower needs to maintain balance within the circle.
For a right handed thrower, the next stage is to move the weight over the left foot. From this position the right foot is raised, and the athlete 'runs' across the circle. There are various techniques for this stage where the leg swings out to a small or great extent, some athletes turn on their left heel (e.g. Ilke Wylluda) but turning on the ball of the foot is far more common.
The aim is to land in the 'power position', the right foot should be in the center and the heel should not touch the ground at any point. The left foot should land very quickly after the right. Weight should be mostly over the back foot with as much torque as possible in the body—so the right arm is high and far back. This is very hard to achieve. Power position.
The critical stage is the delivery of the discus, from this 'power position' the hips drive through hard, and will be facing the direction of the throw on delivery. Athletes employ various techniques to control the end-point and recover from the throw, such as fixing feet (to pretty much stop dead), or an active reverse spinning onto the left foot (e.g. Virgilijus Alekna).
Sports scientist Richard Ganslen researched the Aerodynamics of the Discus, reporting the discus will stall at an angle of 29°.


== Culture ==

The discus throw has been the subject of a number of well-known ancient Greek statues and Roman copies such as the Discobolus and Discophoros. The discus throw also appears repeatedly in ancient Greek mythology, featured as a means of manslaughter in the cases of Hyacinth, Crocus, Phocus, and Acrisius, and as a named event in the funeral games of Patroclus.
Discus throwers have been selected as a main motif in numerous collectors' coins. One of the recent samples is the €10 Greek Discus commemorative coin, minted in 2003 to commemorate the 2004 Summer Olympics. On the obverse of the coin a modern athlete is seen in the foreground in a half-turned position, while in the background an ancient discus thrower has been captured in a lively bending motion, with the discus high above his head, creating a vivid representation of the sport.


== United States ==
In U.S. high school track and field, boys typically throw a discus weighing 1.6 kg (3 lb 9 oz) and the girls throw the 1 kg (2.2 lb) women's discus. Under USATF Youth rules, boys throw the 1 kg discus between the ages of 11–14, and transition to the 1.6 kg discus as 15- to 18-year-olds. Girls throw the 1 kg discus as 11- to 18-year-olds.
Under US high school rules, if a discus hits the surrounding safety cage and is deflected into the sector, it is ruled a foul. In contrast, under IAAF, WMA, NCAA and USATF rules, it is ruled a legal throw. Additionally, under US high school rules, distances thrown are rounded down to the nearest whole inch, rather than the nearest centimetre.
US high school rules allow the use of a solid rubber discus; it is cheaper and easier to learn to throw (due to its more equal distribution of weight, as opposed to the heavy rim weight of the metal rim/core discus), but less durable.


== Top 25 performers ==

Correct as of July 2017.


=== Men ===


==== Non-Legal Marks ====
Ben Plucknett also threw a world record of 72.34 on 7 July 1981 in Stockholm, but this performance was annulled due to doping offense.
Rickard Bruch also threw 72.18 on 23 July 1974 at an exhibition meeting in Piteå.
John Powell also threw 72.08 on 11 September 1987 in Klagshamn, but the throw was made onto a sloping/downhill sector.
Kamy Keshmiri threw 70.84 on 27 May 1992 in Salinas, but this performance was annulled due to doping offense.


=== Women ===


==== Notes ====
Below is a list of throws equal or superior to 72.94m:
Gabriele Reinsch also threw 74.44 m (1988), 73.42 m (1988).
Ilke Wyludda also threw 74.40 m (1988), 73.04 m (1989).
Diana Sachse also threw 73.90 m (1987), 73.32 m (1987), 73.26 m (1986), 73.24 m (1987), 73.04 m (1987), 72.94 m (1988).
Daniela Costian also threw 73.78 m (1988).
Galina Savinkova also threw 73.26 m (1983), 72.96 m (1985).


==== Non-Legal Marks ====
Martina Hellmann also threw 78.14 at an unofficial meeting in Berlin on 6 September 1988
Ilke Wyludda also threw 75.36 at an unofficial meeting in Berlin on 6 September 1988
Darya Pishchalnikova of Russia threw a best of 70.69 in Cheboksary on 5 July 2012, but this performance was annulled due to doping offense.


== Olympic medalists ==


=== Men ===


=== Women ===


== World Championships medalists ==


=== Men ===


=== Women ===


== Season's bests ==


== See also ==
List of discus throw national champions (men)


== Notes and references ==


== External links ==
World Record
Discus History
IAAF list of discus-throw records in XML"
708,Cartesian Perceptual Compression,3079185,1884,"Cartesian Perceptual Compression (abbreviated CPC, with filename extension .cpc) is a proprietary image file format. It was designed for high compression of black-and-white raster Document Imaging for archival scans.
CPC is lossy, has no lossless mode, and is restricted to bi-tonal images. The company which controls the patented format claims it is highly effective in the compression of text, black-and-white (halftone) photographs, and line art. The format is intended for use in the web distribution of legal documents, design plans, and geographical plot maps.
Viewing and converting documents in the CPC format currently requires the download of proprietary software. Although viewing CPC documents is free, as is converting CPC images to other formats, conversion to CPC format requires a purchase.
JSTOR, a United States-based online system for archiving academic journals, converted its online archives to CPC in 1997. The CPC files are used to reduce storage requirements for its online collection, but are temporarily converted on their servers to GIF for display, and to PDF for printing. JSTOR still scans to TIFF G4 and considers those files its preservation masters.


== See also ==
Image file formats


== External links ==
The company's website Additional technical information about the format
Library of Congress Analysis of format by the LoC.
RLG DigiNews Vol. 7 No. 2 Comparison of rivals to TIFF for Document Imaging purposes"
709,Principles of attention stress,6185605,1882,"The principles of attention stress is a user interface design theory to measure the amount of attention that is required to perform certain tasks in a web application. It is developed by Antradar Software in an attempt to benchmark the ease of use of open source CMS products and to monitor the trend of UI designs.
The attention stress theory is based on many psychological observations, of which the two most important ones are:
attention shift
selection threshold
Attention shift addresses the issue of ""getting lost"", or the experience of a ""broken flow"". It is usually measured by the number of page refreshes or the amount of hand–eye coordination required to complete a task. According to attention shift, new pages cause more stress than pop-ups, and pop-ups are more ""expensive"" than things like inline-editing.
Selection threshold deals with the matter of ""being overwhelmed"". It is observed that when the users are presented more than 4 choices at a time, their decisions tend to base on random guess instead of reasoning. This is especially true with users who suffer minor dyslexic symptoms. A well-known solution to this problem is the ""personal menu"" in Microsoft Office products where rarely used menu items are hidden from the users.
Although the emergence of AJAX provides many ways to reduce attention shift, the paradox between attention shift and selection threshold still cannot be resolved. Because of the nature of some application logic, the overall attention stress bears a lower bound. This limit is termed ""UI capacity"" in the principles of attention stress.


== See also ==
Attention management
Attentive user interface
Cognitive load"
710,Equihash,56432065,1882,"Equihash is a memory-oriented Proof-of-Work algorithm developed by the University of Luxembourg's Interdisciplinary Centre for Security, Reliability and Trust (SnT).


== General ==
Alex Biryukov and Dmitry Khovratovich are part of the University of Luxembourg research group CryptoLUX, which developed Equihash. Equihash was introduced at the Network and Distributed System Security Symposium 2016 in San Diego. The cryptocurrency ZCash integrated Equihash in April 2016, for reasons such as security, privacy, and ASIC miner resistance. Bitcoin Magazine notes, ""According to the CryptoLUX scientists, the algorithm permits avoiding centralization of the mining process in the hands of a few first-class miners with specialized mining hardware, thus contributing to the “democratization” of digital currencies based on Equihash.""


== See also ==
Proof-of-stake


== References =="
711,Linear graph grammar,51752859,1874,"In computer science, a linear graph grammar (also a connection graph reduction system or a port graph grammar) is a class of graph grammar on which nodes have a number of ports connected together by edges and edges connect exactly two ports together. Interaction nets are a special subclass of linear graph grammars in which rewriting is confluent.


== Implementations ==
Bawden introduces linear graphs in the context of a compiler for a fragment of the Scheme programming language. Bawden and Mairson (1998) describe the design of a distributed implementation in which the linear graph is spread across many computing nodes and may freely migrate in order to make rewrites possible.


== Notes ==


== References ==
Bawden, Alan (1986), Connection graphs, In Proceedings of the 1986 ACM conference on LISP and functional programming, pp. 258–265, ACM Press.
Bawden, Alan (1992), Linear graph reduction: confronting the cost of naming, PhD dissertation, MIT.
Bawden, Alan (1993), Implementing Distributed Systems Using Linear Naming, A.I. Technical Report No. 1627, MIT.
Bawden and Mairson (1998), Linear naming: experimental software for optimizing communication protocols, Working paper #1, Dept. Computer Science, Brandeis University."
712,Cubesort,43835553,1870,"Cubesort is a parallel sorting algorithm that builds a self-balancing multi-dimensional array from the keys to be sorted. As the axes are of similar length the structure resembles a cube. After each key is inserted the cube can be rapidly converted to an array.
A cubesort implementation written in C was published in 2014.


== Operation ==
Cubesort's algorithm uses a specialized binary search on each axis to find the location to insert an element. When an axis grows too large it is split. Locality of reference is optimal as only four binary searches are performed on small arrays for each insertion. By using many small dynamic arrays the high cost for insertion on single large arrays is avoided.


== References ==


== External links ==
Cubesort description and implementation in C
Algorithms and Computation: 7th International Symposium, ISAAC '96, Osaka ... edited by Tetsuo Asano et al, pp 187-188, https://books.google.com/books?id=vilOl8JCpFUC&pg=PA188&lpg=PA188&hl=en&f=false (passing mention)"
713,First-class message,38990468,1868,"The British undergraduate degree classification system is a grading structure for undergraduate degrees (bachelor's degrees and integrated master's degrees) in the United Kingdom. The system has been applied (sometimes with significant variations) in other countries and regions, including Australia, Bangladesh, Canada, Hong Kong, India, Ireland, Jamaica, Kenya, Ghana, Malaysia, Malta, Mauritius, Myanmar, Nepal, New Zealand, Nigeria, Pakistan, Singapore, Sri Lanka, South Africa, Trinidad and Tobago, and Zimbabwe.


== History ==
In the 16th century, the Regius Professor of Divinity at the University of Cambridge implemented norm referencing to distinguish the top 25% of candidates, the next 50%, and the bottom 25%.
The classification system as currently used in the United Kingdom was developed in 1918. Honours were then a means to recognise individuals who demonstrated depth of knowledge or originality, as opposed to relative achievement in examination conditions.
Recently, there has been concern over possible grade inflation due to increasing numbers of higher-class honours degrees awarded per annum. The number of first-class honours degrees has reportedly tripled since the 1990s. As with claimed grade inflation of A-levels, prospective employers or educational institutions have observed increased difficulty in selecting candidates. It is, however, unknown whether the rise in the number of first-class degrees is due to grade inflation of whether students are achieving higher levels than in the past, and university leaders have also pointed at the higher A-levels attained by students as evidence that higher degree grades should be expected. On the other hand, the practice of degree classification has been criticised for unduly stigmatising students and being unreflective of a graduate's success or potential for success, particularly in the workplace.


== Degree classification ==
A bachelor's degree can be an honours degree (bachelor's with honours) or an ordinary degree (bachelor's without honours). Honours degrees are classified, usually based on a weighted average (with higher weight given to marks in the later years of the course, and often zero weight to those in the first year) of the marks gained in exams and other assessments. Grade boundaries can vary by institution, but typical values are given below.
First-class honours (1st, 1 or I) – typically 70% or higher
Second-class honours;
Upper division (2:1, 2i or II-1) – typically 60–69%
Lower division (2:2, 2ii or II-2) – typically 50–59%

Third-class honours (3rd, 3 or III) – typically 40–49%
Students who do not achieve honours may be awarded an ordinary degree, sometimes known as a ""pass"". Ordinary degrees, and other exit awards such as the Diploma of Higher Education (DipHE; for completing the first two years of a degree course) and Certificate of Higher Education (CertHE; for completing the first year of a degree course), may be unclassified (pass/fail) or, particularly in Scotland where the ordinary degree is offered as a qualification in its own right, classified into pass, merit and distinction. Foundation degrees are normally classified into pass, merit and distinction.
Integrated master's degrees are usually classified with honours in the same way as a bachelor's honours degree, although some integrated master's degrees are classified like postgraduate taught master's degrees into pass (usually 50%), merit (60%) and distinction (70%).
At most institutions, the system allows a small amount of discretion. A candidate may be elevated to the next degree class if his or her average marks are close to (or the median of their weighted marks achieves) the higher class, and if they have submitted several pieces of work worthy of the higher class. However, even students with a high average mark may be unable to take honours if they have failed part of the course and so have insufficient credits.
In England, Wales and Northern Ireland, a bachelor's degree with honours normally takes three years of full-time study and usually requires 360 credits, of which at least 90 are at level 6 (final year of a bachelor's degree) level, while an ordinary bachelor's degree normally requires 300 credits, of which 60 are at level 6. In Scotland, the honours bachelor's degree takes four years and requires 480 credits with a minimum of 90 at level 10 of the Scottish framework (last year of the honours degree) and 90 at level 9 (penultimate year), while the ordinary degree takes three years and requires 360 credits with a minimum of 60 at level 9 (last year of the ordinary degree).
In Scotland, it is possible to start university a year younger than in the rest of the United Kingdom, as the Scottish Higher exams are often taken at age 16 or 17 (as opposed to 18), so Scottish students often end a four-year course at the same age as a student from elsewhere in the UK taking a three-year course, assuming no gap years.
When a candidate is awarded a degree with honours, ""(Hons)"" may be suffixed to their designatory letters — for example, BA (Hons), BSc (Hons), BMus (Hons), MA (Hons). An MA (Hons) would generally indicate a degree award from certain Scottish universities (c.f. Scottish MA) and is at the same level as a bachelor's degree.


=== Distribution of classes ===
The Higher Education Statistics Agency (HESA) has published the number of degrees awarded with different classifications since 1994/5. The relative proportions of different classes have changed over this period, with increasing numbers of students being awarded higher honours. The table below shows the percentage of classified degrees (i.e. not including fails or unclassified degrees such as MBBS) in each class at five year intervals; note that HESA stopped giving statistics separately for third class honours and pass degree after 2003 and that a small number of undivided second class honours degrees (shown under ""other"" along with ""unknown"", which makes up the bulk of this category) were awarded up to 1996.


=== First-class honours ===
First-class honours, referred to as a ""first"", is the highest honours classification and indicates high academic achievement.
In 2010 and 2011, the Higher Education Statistics Agency (HESA) reported that approximately 15% of all degree candidates graduated with first-class honours. The percentages of graduates achieving a first vary greatly by university and course studied. For example, students of law are least likely to gain a first, whereas students of mathematical sciences are most likely to gain a first. In 2006–2007 and 2010–2011, 5.8% and 8.1% of law students gained a first, respectively; however, in those years, 28.9% and 30.0% of mathematics students gained a first, respectively.
A first class honours degree is sometimes known as a 'Geoff' or 'Damien' after Geoff Hurst or Damien Hirst.


=== Upper second-class honours ===

The upper division is commonly abbreviated to ""2:1"" or ""II.i"" (pronounced two-one). The 2:1 is a minimum requirement for entry to many postgraduate courses in the UK. It is also required for the award of a research council postgraduate studentship in the UK, although possession of a master's degree can render a candidate eligible for an award if their initial degree was below the 2:1 standard. The percentage of candidates who achieve upper second-class honours can vary widely by degree subject, as well as by university.
An upper second class honours degree might be referred to as 'Attila' or 'Don' after 'Attila the Hun' and 'Don Juan'.


=== Lower second-class honours ===
This is the second division of second-class degrees and is abbreviated as ""2:2"" or ""II.ii"" (pronounced two-two).
It has been colloquially referred to in rhyming slang as a ''Desmond"" - after South African social rights activist and retired Anglican archbishop Desmond Tutu (although Tutu himself graduated with an upper second-class honours degree).


=== Third-class honours ===
Third-class honours, referred to as a ""third"", is the lowest honours classification in most modern universities. Historically, the University of Oxford awarded fourth-class honours degrees and, until the late 1970s, did not distinguish between upper and lower second-class honours degrees.
Informally, the third-class honours degree is referred to as a ""gentleman's degree"" (cf. the ""gentleman's C"" in the U.S.).
It has been colloquially known in rhyming slang as a 'Douglas' – after Douglas Hurd, the former Conservative MP (who in reality took first class honours) , or a 'Richard' - after King Richard III.
Approximately 7.2% of students graduating in 2006 with an honours degree received a third-class honours.


=== Ordinary degree ===
While most university bachelor's degree courses lead to honours degrees, some universities offer courses leading to ordinary degrees. Some honours courses permit students who do not gain sufficient credits in a year by a small margin to transfer to a parallel ordinary degree course. Ordinary degrees may also sometimes be awarded to honours degree students who do not pass sufficient credits in their final year to gain an honours degree, but pass enough to earn an ordinary degree.
Some Scottish universities offer three-year ordinary degrees as a qualification in their own right, as well as an honours degree over four years. This is in contrast to English universities that have honours degrees with three years of study. An ordinary degree in Scotland is not a failed honours degree, as in certain English universities. Students can decide, usually at the end of their second or third year, whether or not they wish to complete a fourth honours year. Scottish universities may also award their ordinary degrees with distinction if a student achieves a particularly good grade average, usually 70% or above. A common example of a Scottish ordinary degree is the Bachelor of Laws course taken by graduates of other subjects, as this is sufficient (without honours) for entry into the legal profession.


=== Aegrotat ===
An aegrotat (; from Latin aegrotat, meaning 'he is ill') degree is an honours or ordinary degree without classification, awarded under the presumption that, had a candidate who was unable to undertake their exams due to illness or even death completed those exams, they would have satisfied the standard required for that degree. Aegrotat degrees are often qualified with an appended ""(aegrotat)"".
Following the introduction of current regulations regarding mitigating circumstances, aegrotat degrees are less commonly awarded than they previously were.


=== Variations in classification ===
At the University of Cambridge, undergraduate Tripos examinations are split into three parts (e.g. Part IA, IB, and II), or two parts (Part I and II). Part II is taken at the end of final year. Each student receives a formal classification for each part (i.e. Class I, II.I, II.II, or III). Typically, the Part II grade that corresponds with final examinations is quoted, however officially a grade simply exists for every Part of the degree, not for the overall degree.
At the University of Oxford, a formal degree Class is given, and this is typically based on the final examinations. In Oxford, examinations for Prelims or Honour Moderations are also undertaken in first/second year, however these results do not typically affect the final degree classification. Until the 1970s, the four honours divisions in Oxford's moderations and final examinations were named first, second, third and fourth class, but eventually Oxford gave in and adopted the numbering used by other English universities.


==== Variations of first-class honours ====
At the University of Cambridge, Triposes were previously split into two parts: Part I and Part II. Attaining First Class Honours in both parts would culminate in graduating with a ""Double First"". Most Triposes were later split into three parts: ""Part IA,"" ""Part IB"" and ""Part II"", or ""Part I"", ""Part IIA"" and ""Part IIB"". Attaining a First Class in all three parts culminates in graduating with a ""Triple First"". The frequency of this honour varies with subject, however typically fewer than 3% of students will achieve this distinction. It is possible in some of the humanities Triposes to be awarded a ""Starred First"". The science Triposes do not award Starred Firsts.
Oxford sometimes grants a congratulatory first, which The New York Times described as ""a highly unusual honor in which the examining professors ask no questions about the candidate's written work but simply stand and applaud"", and Martin Amis described as ""the sort where you are called in for a viva and the examiners tell you how much they enjoyed reading your papers"". A ""double first"" at Oxford usually informally refers to first-class honours in both components of an undergraduate degree, i.e. Moderations/Prelims and the Final Honour School, or in both the bachelor's and master's components of an integrated master's degree.
At University College London, candidates who perform well beyond the requirements of a standard First Class Honours may be nominated to the Dean's List. This is generated once per year and recognizes outstanding academic achievement in final examinations. There are no set criteria for nomination to the list, but typically only a nominal number of students from each faculty are nominated per year.


== International comparisons ==


=== France ===
The University of St Andrews gives equivalences between French and British grades for its study-abroad programme. Equivalencies for the purposes of initial teacher training have also been derived by the UK NARIC for 1st, 2:1 and 2:2 degrees, which do not align with St Andrews' table.


=== South Africa ===
The South African Qualifications Authority  (SAQA) compares international degrees with local degrees before any international student continues their studies in that country. While the British degree accreditation and classification system allows students to go straight from a three-year bachelor's degree onto a master's degree (normally requiring a 1st or a 2:1 – those with a 2:2 or a 3rd usually require appropriate professional experience), South Africa does not do so unless the student has proven research capabilities. South African Honours degrees prepare the students to undertake a research-specific degree (in terms of master's), by spending an in-depth year (up to 5 modules) creating research proposals and undertaking a research project of limited scope. This prepares students for the research degrees later in their academic career.


=== Spain ===
The UK NARIC has derived equivalencies for the grades of the Spanish grado and licenciatura degrees for purposes of initial teacher training bursaries.


=== The Netherlands ===
The Netherlands organisation for international cooperation in higher education (NUFFIC) has compared UK degree classification to Dutch degree grades. Dutch equivalencies have also been calculated by the UK NARIC.
Nuffic also noted that the grading culture is different in the Netherlands, so that it is very rare for even the smartest students in the Netherlands to be awarded a 9 or a 10, which represent near perfection and absolute perfection.


=== United States and Canada ===
British honours degrees are sometimes considered equivalent (by British sources) to a US master's degree, with the US bachelor's degree being equivalent to a British pass degree, due to the much higher degree of specialisation in the UK. However, many British institutions accept US bachelor's degrees for admission to postgraduate study (see below) and US comparison services treat British and American degrees as equivalent. When US bachelor's degrees are compared to British honours degrees, equivalencies can be expressed in terms of either US Grade Point Averages (GPAs) or letter grades.
British institutions normally state equivalence in terms of GPAs. Approximate mappings between British classifications and GPAs can be inferred from the graduate admissions criteria used by British universities, which often give international equivalents. For example, University College London (UCL) equates the minimum classification for entrance to GPAs using 1st = 3.6, 2:1 = 3.3 and 2:2 = 3.0. However, different universities convert grades differently: the London School of Economics and Political Science (LSE) considers a GPA (U.S.) of 3.5 or better as equivalent to gaining a 2:1, while the department of English Language and Literature at Oxford considers a GPA of ""about 3.8"" equivalent to a first class degree. Similarly, the UK NARIC gives equivalent GPAs for determining eligibility for teacher training bursaries. In contrast, Durham University's North American Undergraduate Guide 2017 gives a conversion table as a guide to understanding British classifications (rather than for admission to postgraduate study) of 1st = 3.8–4.0, 2:1 = 3.3–3.7, 2:2 = 2.8–3.2 and 3rd = 2.3–2.7. The GPA conversions are summarised in the following table:
Letter grade equivalents are more commonly used by American institutions. World Education Services (WES), a nonprofit organisation which provides qualification conversion services to many universities and employers, gives 1st = A, 2:1 = A-/B+, 2:2 = B, 3rd = B-, Pass = C, which would convert British degrees to higher GPAs than the conversion used by UCL if the guidelines for converting grades to GPA given by Duke University are used. The Fulbright Commission has also created ""an unofficial chart with approximate grade conversions between UK results and US GPA.""
Canadian academic grades may be given as letters, percentages, 12-point GPAs or 4-point GPAs. The 4-point GPAs are sometimes seen to differ from the US but other sources treat them as equivalent. The Durham conversion specifies GPAs for the US and letter grades/percentages for Canada  while the UK NARIC has separate GPA conversions for the four-year bachelor's honours, baccalauréat and professional bachelor's degrees (which differ from their US GPA equivalents by at most 0.1) and the three-year bachelor's degree (which is seen as a lower standard). The British Graduate Admissions Fact Sheet from McGill University uses the conversion 1st = 4.0; 2:1 = 3.0; 2:2 = 2.7; 3rd = 2.0; Pass = 1.0; Fail = 0.0.
Degrees in the UK are mapped to levels of the Frameworks for Higher Education Qualifications of UK Degree-Awarding Bodies (FHEQ), which includes the Framework for Qualifications of Higher Education Institutes in Scotland (FQHEIS), which has an alternative numbering of levels corresponding to those of the Scottish Credit and Qualifications Framework (SCQF). Bachelor's degrees (including the Scottish MA, but not including medical degrees, dentistry degrees or degrees in veterinary science) attained in the UK are at FHEQ level 6/FQHEIS level 9 (ordinary) or 10 (honours); master's degrees (including integrated master's degrees and first degrees in medicine, dentistry and veterinary science) are at FHEQ level 7/FQHEIS level 11, and doctoral degrees are at FHEQ level 8/FQHEIS level 12. Bachelor's, master's and doctoral degrees map to first, second and third cycle qualifications in the Qualifications Framework of the European Higher Education Area.


== Progression to postgraduate study ==
Regulations governing the progression of undergraduate degree graduates to postgraduate programmes vary among universities, and are often flexible. A candidate for a postgraduate master's degree is usually required to have at least a 2:2 bachelor honours degree, although candidates with 2:1s are in a considerably stronger position to gain a place in a postgraduate course and to obtain funding, especially in medical and natural sciences. Some institutions specify a 2:1 minimum for certain types of master's program, such as for a Master of Research course.
Candidates with a Third or an Ordinary degree are sometimes accepted, provided they have acquired satisfactory professional experience subsequent to graduation. A candidate for a doctoral programme who does not hold a master's degree is nearly always required to have a First or 2:1 at bachelor's level.


== Variations ==


=== International degrees ===


==== Australia ====
Some universities, such as those in Australia, offer ordinary or pass degrees, (for instance, as a three-year B.A. or a three-year BSc) by default. High-achieving students may be recognised with an honours classification without further coursework or research, as is often the case in engineering, which often contains a research and thesis component, or law. However, other courses (such as humanities, arts, social sciences, and sciences) and other universities may recognise high-achieving students with an honours classification with further coursework or research, undertaken either concurrently with, and as part of or in addition to, a bachelor's course, or after completion of a bachelor's course requirements and attaining adequately competitive grades.
Some graduate degrees have been or are classified; however, under the Australian Qualifications Framework (AQF), no graduate-level degrees (i.e., master's by coursework, master's by research, or higher research degrees) may be classified. To comply with this standard, some institutions have commenced, or will commence, offering high-achieving graduates with ""distinction"". Notably, this is consistent with British graduate degree classification.


=== British medical and dental degrees ===
In the United Kingdom, medicine is usually taught as an undergraduate course, with graduates being awarded a master's level qualification: normally the conjoined degrees of Bachelor of Medicine, Bachelor of Surgery (MBBS, BM BCh, MB ChB, etc.) although at Queen's University Belfast (and universities in Ireland) Bachelor in the Art of Obstetrics (BAO) is added, and at some universities only the Bachelor of Medicine is awarded - all of these have equal standing. Unlike most undergraduate degrees, the MBBS is not normally considered an honours degree, and thus is not classified into first class honours, etc. Students may be awarded ""Merits"" and ""Distinctions"" for parts of the course or the whole course (depending on the institution) and ""Honours"" may be awarded at some institutions for exceptional performance throughout the course (as a grade above Distinction).
Medical schools split their year groups into one of 10 deciles. These deciles are the major factor in the calculation of Educational Performance Measure (EPM) points used as part of medical students' Foundation Programme applications, with the top decile receiving 43 points, decreasing by a point for each decile (so the lowest gets 34 points); 7 points can be awarded for other educational achievements (other degrees and publications), and the EPM points are combined with up to 50 points from the Situational Judgement Test to give a total out of 100.


== British Grade Point Average ==
Following the recommendation of the Burgess report into the honours degree classification system in 2007, the Higher Education Academy ran a pilot in 2013–2014 in collaboration with 21 institutions delivering higher education (ranging from Russell Group universities to Further Education colleges) to investigate how a Grade Point Average (GPA) system would work best in Britain. Two main weighting systems were tested: an American-style average of all marks, weighted only by credit value, and weighting by ""exit velocity"" in the manner of the honours classification, where modules in the first year are given a low or zero weight and modules in the final year have a higher weight (a third model was only rarely used). Over two thirds of providers preferred exit-velocity weighting to the straight average.
A GPA scale, tied to percentage marks and letter grades, was recommended for use nationally following the study, to run in parallel with the honours degree classification system.


== See also ==
British degree nicknames
Latin Honors


== References =="
714,LU reduction,13305267,1868,"LU reduction is an algorithm related to LU decomposition. This term is usually used in the context of super computing and highly parallel computing. In this context it is used as a benchmarking algorithm, i.e. to provide a comparative measurement of speed for different computers. LU reduction is a special parallelized version of an LU decomposition algorithm, an example can be found in (Guitart 2001). The parallelized version usually distributes the work for a matrix row to a single processor and synchronizes the result with the whole matrix (Escribano 2000).


== Sources ==
J. Oliver, J. Guitart, E. Ayguadé, N. Navarro and J. Torres. Strategies for Efficient Exploitation of Loop-level Parallelism in Java. Concurrency and Computation: Practice and Experience(Java Grande 2000 Special Issue), Vol.13 (8-9), pp. 663–680. ISSN 1532-0634, July 2001, [1], last retrieved on Sept. 14 2007
J. Guitart, X. Martorell, J. Torres, and E. Ayguadé, Improving Java Multithreading Facilities: the Java Nanos Environment, Research Report UPC-DAC-2001-8, Computer Architecture Department, Technical University of Catalonia, March 2001, [2].
Arturo González-Escribano, Arjan J. C. van Gemund, Valentín Cardeñoso-Payo et al., Measuring the Performance Impact of SP-Restricted Programming in Shared-Memory Machines, In Vector and Parallel Processing — VECPAR 2000, Springer Verlag, pp. 128–141, ISBN 978-3-540-41999-0, 2000, [3]"
715,Strings (Unix),3847169,1866,"In computer software, strings is a program in Unix-like operating systems that finds and prints text strings embedded in binary files such as executables. It can be used on object files and core dumps.
Strings are recognized by looking for sequences of at least 4 (by default) printable characters terminating in a NUL character (that is, null-terminated strings). Some implementations provide options for determining what is recognized as a printable character, which is useful for finding non-ASCII and wide character text.
Common usage includes piping its output to grep and fold or redirecting the output to a file.
It is part of the GNU Binary Utilities (binutils), and has been ported to other operating systems including Microsoft Windows.


== Example ==
Using strings to print sequences of characters that are at least 8 characters long (this command prints the system's BIOS information; should be run as root):


== See also ==

cat
GNU Debugger
strip


== References ==


== External links ==
strings – Commands & Utilities Reference, The Single UNIX Specification, Issue 7 from The Open Group"
716,LZJB,7783443,1866,"LZJB is a lossless data compression algorithm invented by Jeff Bonwick to compress crash dumps and data in ZFS. The software is CDDL license licensed. It includes a number of improvements to the LZRW1 algorithm, a member of the Lempel–Ziv family of compression algorithms.. The name LZJB is derived from its parent algorithm and its creator—Lempel Ziv Jeff Bonwick. Bonwick is also one of two architects of ZFS, and the creator of the Slab Allocator.


== References ==


== External links ==
""""compress"" source code"". Archived from the original on 8 June 2012. 
""LZJB source code"". Archived from the original on 7 August 2010. 
LZJB python binding
Javascript port of the LZJB algorithm"
717,Recursive join,8533909,1866,"The recursive join is an operation used in relational databases, also sometimes called a ""fixed-point join"". It is a compound operation that involves repeating the join operation, typically accumulating more records each time, until a repetition makes no change to the results (as compared to the results of the previous iteration).
For example, if a database of family relationships is to be searched, and the record for each person has ""mother"" and ""father"" fields, a recursive join would be one way to retrieve all of a person's known ancestors: first the person's direct parents' records would be retrieved, then the parents' information would be used to retrieve the grandparents' records, and so on until no new records are being found.
In this example, as in many real cases, the repetition involves only a single database table, and so is more specifically a ""recursive self-join"".
Recursive joins can be very time-consuming unless optimized through indexing, the addition of extra key fields, or other techniques.
Recursive joins are highly characteristic of hierarchical data, and therefore become a serious issue with XML data. In XML, operations such as determining whether one element contains another are extremely common, and the recursive join is perhaps the most obvious way to implement them when the XML data is stored in a relational database.
The standard way to define recursive joins in the SQL:1999 standard is by way of recursive common table expressions. Database management systems that support recursive CTEs include Microsoft SQL Server, Oracle, PostgreSQL and others.


== See also ==
Join
Hierarchical and recursive queries in SQL"
718,Jonathan B. Postel Service Award,2467939,1865,"The Jonathan B. Postel Service Award is an award named after Jon Postel. The award has been presented every year since 1999 by the Internet Society to ""honor a person who has made outstanding contributions in service to the data communications community.""
The first recipient of the award was Jon Postel himself (posthumously). The award was created by Vint Cerf as chairman of the Internet Society and announced in ""I remember IANA"" published as RFC 2468.


== Winners ==
2017 kc klaffy
2016 Kanchana Kanchanasut
2015 Rob Blokzijl
2014 Mahabir Pun
2013 Elizabeth J. Feinler
2012 Pierre Ouedraogo
2011 Prof. Kilnam Chon
2010 Dr. Jianping Wu
2009 CSNET - Peter J. Denning, David Farber, Anthony C. Hearn, and Lawrence Landweber
2008 La Fundacion Escuela Latinoamericana de Redes (EsLaRed)
2007 Nii Quaynor
2006 Bob Braden and Joyce K. Reynolds
2005 Jun Murai
2004 Phill Gross
2003 Peter T. Kirstein
2002 Stephen Wolff
2001 Daniel Karrenberg
2000 Scott Bradner
1999 Jon Postel (posthumously)


== References =="
719,Presentation semantics,24209073,1856,"In computer science, particularly in human-computer interaction, presentation semantics specify how a particular piece of a formal language is represented in a distinguished manner accessible to human senses, usually human vision. For example, saying that <bold> ... </bold> must render the text between these constructs using some bold typeface is a specification of presentation semantics for that syntax. An example of interactive presentation semantics is defining the expected behavior of a hypertext link on a suitable syntax.
Many markup languages like HTML, CSS, DSSSL, XSL-FO or troff have presentation semantics, but others like XML, XLink and XPath do not. Character encoding standards like Unicode also have presentation semantics. One of the main goals of style sheet languages like CSS is to separate the syntax used to define structured data from the syntax endowed with presentation semantics that is used to render the data in various ways.


== References =="
720,Sophistication (complexity theory),47697827,1845,"In algorithmic information theory, sophistication is a measure of complexity related to algorithmic entropy.
When K is the Kolmogorov complexity and c is a constant, the sophistication of x can be defined as

  
    
      
        
          Soph
          
            c
          
        
        ⁡
        (
        x
        )
        :=
        inf
        {
        K
        ⁡
        (
        S
        )
        :
        x
        ∈
        S
        ∧
        K
        ⁡
        (
        x
        ∣
        S
        )
        ≥
        
          log
          
            2
          
        
        ⁡
        (
        
          |
        
        S
        
          |
        
        )
        −
        c
        ∧
        
          |
        
        S
        
          |
        
        ∈
        
          
            N
          
          
            +
          
        
        }
        .
      
    
    {\displaystyle \operatorname {Soph} _{c}(x):=\inf\{\operatorname {K} (S):x\in S\land \operatorname {K} (x\mid S)\geq \log _{2}(|S|)-c\land |S|\in \mathbb {N} _{+}\}.}
  
The constant c is called significance. The S variable ranges over finite sets.
Intuitively, sophistication measures the complexity of a set of which the object is a ""generic"" member.


== See also ==
Logical depth


== References ==


=== Further reading ===
Koppel, Moshe (1995). Herken, Rolf, ed. ""Structure"". The Universal Turing Machine (2Nd Ed.). Springer-Verlag New York, Inc.: 403–419. ISBN 3-211-82637-8. 
Antunes, Luís; Fortnow, Lance (August 30, 2007). ""Sophistication Revisited"" (PDF). doi:10.1007/s00224-007-9095-5. 
Luís, Antunes; Bauwens, Bruno; Souto, André; Teixeira, Andreia (2013). ""Sophistication vs Logical Depth"". arXiv:1304.8046 . 


== External links ==
The First Law of Complexodynamics"
721,Computer Aided Verification,6592539,1843,"Computer Aided Verification (CAV) is an annual academic conference on the theory and practice of computer aided formal analysis of software and hardware systems. It is one of the very best conferences in computer science. For example, several important model checking techniques were published in CAV, such as counterexample-guided abstraction refinement and partial order reduction methods.
The first CAV was held in 1989 in Grenoble, France.


== See also ==
The list of computer science conferences contains other academic conferences in computer science.


== References ==


== External links ==
Official web site for CAV
DBLP bibliography for CAV"
722,Concurrency pattern,855068,1840,"In software engineering, concurrency patterns are those types of design patterns that deal with the multi-threaded programming paradigm. Examples of this class of patterns include:
Active Object
Balking pattern
Barrier
Double-checked locking
Guarded suspension
Leaders/followers pattern
Monitor Object
Nuclear reaction
Reactor pattern
Read write lock pattern
Scheduler pattern
Thread pool pattern
Thread-local storage


== See also ==
Design Patterns
Behavioral pattern
Creational pattern
Structural pattern


== References ==


== External links ==
ScaleConf Presentation about concurrency patterns
Recordings about concurrency patterns from Software Engineering Radio:
Episode 12: Concurrency Pt. 1
Episode 19: Concurrency Pt. 2
Episode 29: Concurrency Pt. 3"
723,Automated exception handling,11102669,1839,"Automated exception handling is a computing term referring to the computerized handling of errors. runtime systems (engines) such as those for the Java programming language or .NET Framework lend themselves to an automated mode of exception or error handling. In these environments software errors do not crash the operating system or runtime engine, but rather generate exceptions. Recent advances in these runtime engines enables specialized runtime engine add-on products to provide automated exception handling that is independent of the source code and provides root-cause information for every exception of interest.


== How it works ==
Upon exception, the runtime engine calls an error interception tool that is attached to the runtime engine (e.g., Java Virtual Machine (JVM)). Based on the nature of the exception such as its type and the class and method in which it occurred and based on user preferences, an exception can be either handled or ignored.
If the preference is to handle the exception, then based on handling preferences such as memory search depth, the error interception utility extracts memory values from heap and stack memories. This snapshot then produces the equivalent of a debugger screen (as if there had been a debugger) at the moment of the exception.


== Advantages ==
This mechanism enables the automated handling of software errors independent of the application source code and of its developers. It is a direct artifact of the runtime engine paradigm and it enables unique advantages to the software lifecycle that were unavailable before.


== References =="
724,Visual modeling,5734612,1838,"Visual modeling is the graphic representation of objects and systems of interest using graphical languages. Visual modeling languages may be General-Purpose Modeling (GPM) languages (e.g., UML, Southbeach Notation, IDEF) or Domain-Specific Modeling (DSM) languages (e.g., SysML). They include industry open standards (e.g., UML, SysML, Modelica), as well as proprietary standards, such as the visual languages associated with VisSim, MATLAB and Simulink, OPNET, NetSim, NI Multisim, and Reactive Blocks. Both VisSim and Reactive Blocks provide a royalty-free, downloadable viewer that lets anyone open and interactively simulate their models. The community edition of Reactive Blocks also allows full editing of the models as well as compilation, as long as the work is published under the Eclipse Public License. Visual modeling languages are an area of active research that continues to evolve, as evidenced by increasing interest in DSM languages, visual requirements, and visual OWL (Web Ontology Language).


== See also ==
Discipline-Specific Modeling
Domain-Specific Modeling
Model Driven Engineering
Modeling language


== References ==


== External links ==
Visual Modeling Forum A web community dedicated to visual modeling languages and tools."
725,Two-variable logic,43571383,1834,"In mathematical logic and computer science, two-variable logic is the fragment of first-order logic where formulae can be written using only two different variables. This fragment is usually studied without function symbols.


== Decidability ==
Some important problems about two-variable logic, such as satisfiability and finite satisfiability, are decidable. This result generalizes results about the decidability of fragments of two-variable logic, such as certain description logics; however, some fragments of two-variable logic enjoy a much lower computational complexity for their satisfiability problems.
By contrast, for the three-variable fragment of first-order logic without function symbols, satisfiability is undecidable.


== Counting quantifiers ==
The two-variable fragment of first-order logic with no function symbols is known to be decidable even with the addition of counting quantifiers, and thus of uniqueness quantification. This is a more powerful result, as counting quantifiers for high numerical values are not expressible in that logic.


== References =="
726,Algorithm BSTW,2827733,1834,"The Algorithm BSTW is a data compression algorithm, named after its designers, Bentley, Sleator, Tarjan and Wei in 1986. BSTW is a dictionary-based algorithm that uses a move-to-front transform to keep recently seen dictionary entries at the front of the dictionary. Dictionary references are then encoded using any of a number of encoding methods, usually Elias delta coding or Elias gamma coding.


== References ==

This algorithm was published in the following paper: ""A Locally Adaptive Data Compression Scheme"", Communications of the ACM, 1986, volume 29 number 4, pp. 320–330.
A related idea was published in Ryabko, B. Ya. ""Data compression by means of a book stack"", Problems of Information Transmission, 1980, v. 16: (4), pp. 265–269.
The original name of this code is ""book stack"". The history of discovery of the book stack (or move-to-front) code can be found here: Ryabko, B. Ya.; Horspool, R. Nigel; Cormack, Gordon V. Comments to: ""A locally adaptive data compression scheme"" by J. L. Bentley, D. D. Sleator, R. E. Tarjan and V. K. Wei. Comm. ACM 30 (1987), no. 9, 792–794.


== External links ==
Algorithm BSTW"
727,Rose tree,8163824,1832,"In computing, a multi-way tree or rose tree is a tree data structure with a variable and unbounded number of branches per node. The name rose tree for this structure is prevalent in the functional programming community, e.g., in the context of the Bird–Meertens formalism. It was coined by Lambert Meertens to evoke the similarly-named, and similarly-structured, common rhododendron.


== Definition ==
The following is a definition in Haskell:


== Sources ==


== External links ==
Rose tree on the Haskell wiki
Bayesian Rose Trees
Data.Tree, an implementation of basic rose tree operations in the Haskell containers package"
728,Harry H. Goode Memorial Award,15254255,1824,"The Harry H. Goode Memorial Award is an IEEE Computer Society annual awards in honor of Harry H. Goode for achievements in the information processing field which are considered either a single contribution of theory, design, or technique of outstanding significance, or the accumulation of important contributions on theory or practice over an extended time period, the total of which represent an outstanding contribution.


== Recipient ==
Recipients include:
1964 Howard Aiken
1965 George Stibitz and Konrad Zuse
1966 John Mauchly and J. Presper Eckert
1967 Samuel N. Alexander
1968 Maurice Vincent Wilkes
1974 Edsger W. Dijkstra
1975 Kenneth E. Iverson
1979 Herman Goldstine
1981 C. A. R. Hoare
1983 Gene Amdahl
1985 Carver A. Mead
1992 Edward S. Davidson
1995 Michael J. Flynn
1996 Leonard Kleinrock
1997 James Thornton
1998 Vishwani Agrawal
1999 Ahmed Sameh
2000 John K. Iliife
2001 Oscar H. Ibarra
2002 Ian F. Akyildiz
2003 Peter Chen
2004 Edmund M. Clarke
2005 John Hopcroft
2006 Alan Jay Smith
2007 Guy L. Steele
2008 Dharma Agrawal
2009 Mateo Valero
2010 (no award given)
2011 Moshe Y. Vardi
2012 Arvind
2013 Yale N. Patt
2014 Norman P. Jouppi
2015 David Padua
2016 Giovanni De Micheli


== External links ==
Harry H. Goode Memorial Award - IEEE Computer Society. Info and winner list.
IEEE Computer Society Award List. Small info about the award."
729,Imake,2919168,1824,"imake is a build automation system written for the X Window System. It was used by X from X11R1 (1987) to X11R6.9 (2005), and continued to be used in XFree86 (last commit 2009). It is implemented on top of the C preprocessor and make. The first version was written by Todd Brunhoff at Tektronix.
imake generates makefiles from a template, a set of C preprocessor macro functions, and a per-directory input file called an Imakefile. This allows machine dependencies (such as compiler options, alternate command names, and special make rules) to be kept separate from the descriptions of the various items to be built.
imake was heavily used for X and X-related software through the 1990s, and for unrelated software such as ChorusOS. It was also used for configuration management.
With the release of X.org X11R7.0, it was replaced by GNU Autotools. (X11R6.9 and X11R7.0 were the same codebase with a different build system.). X.Org plans to use Meson in the future instead of Autotools.


== Notes ==


== Sources ==
DuBois, Paul (September 1996). Software Portability with imake (2nd ed.). O'Reilly Media. ISBN 978-1-56592-226-6. 


== External links ==
imake book
imake FAQ"
730,BOOPSI,4418408,1821,"BOOPSI (Basic Object Oriented Programming System for Intuition) is an object-oriented programming system for AmigaOS. It extends the AmigaOS windowing environment (Intuition) with an object-oriented subsystem allowing a hierarchy of object classes in which every class defines a single GUI widget or interface event.
BOOPSI made it easier for developers to create their own system of widgets and create standardized graphical user interfaces. Magic User Interface and ReAction are examples of complete widget toolkits built on BOOPSI. Both toolkits have become popular with Amiga software programmers to generate and maintain graphical user interfaces.
The object-oriented design brings advantages such as straightforward coupling of objects with other objects. For example, a programmer may link a numerical input field and a sliding control, where if the user adjusts the sliding control the numerical value in the input field changes automatically.
BOOPSI was officially introduced with AmigaOS 2.0 and was further extended in later releases.


== References ==
The Amiga ROM Kernel Reference Manual: Libraries, published by Addison Wesley, (1991), ISBN 0-201-56774-1


== External links ==

How it works BOOPSI at Codewiz.org
functioning of BOOPSI at The Flux Research Group, University of Utah."
731,Synonym ring,3553110,1819,"In metadata a synonym ring or synset, is a group of data elements that are considered semantically equivalent for the purposes of information retrieval. These data elements are frequently found in different metadata registries. Although a group of terms can be considered equivalent, metadata registries store the synonyms at a central location called the preferred data element.
According to WordNet, a synset or synonym set is defined as a set of one or more synonyms that are interchangeable in some context without changing the truth value of the proposition in which they are embedded.


== Example ==
The following are considered semantically equivalent and form a synonym ring:

foaf:person
gjxdm:Person
niem:Person
sumo:Human
cyc:Person
umbel:Person

Note that each data element has two components:
Namespace prefix, which is a shorthand for the name of the metadata registry
Data element name, which is the name of the object in each of the distinct metadata registry


== Expressing a synonym ring ==
A synonym ring can be expressed by a series of statements in the Web Ontology Language (OWL) using the classEquivalence or the propertyEquivalence or instance equivalence statement – the sameAs property.


== See also ==
Data Reference Model
Metadata
Vocabulary-based transformation
WordNet


== External links ==
WordNet at Princeton"
732,COmponent Detection Algorithm,28912141,1810,"The Component Detection Algorithm (CODA) is a name for a type of LC-MS and chemometrics software algorithm focused on detecting peaks in noisy chromatograms (TIC) often obtained using the electrospray ionization technique.
The implementation of the algorithm from a mass spectrometry software to another differs. Some implementations need clean chromatograms to substruct background.


== References =="
733,MindModeling@Home,17174845,1805,"MindModeling@Home is a non-profit research project that uses a combination of high performance computing and volunteer distributed computing for the advancement of cognitive science. The research focuses on utilizing computational cognitive process modeling to understand the human mind better. The project aims to improve on the scientific foundations that explain the mechanisms and processes that enable and moderate human performance and learning. MindModeling@Home is hosted by Wright State University and the University of Dayton in Dayton, Ohio.


== See also ==
List of distributed computing projects


== References ==


== External links ==
MindModeling@Home website"
734,Enterprise Data Modeling,5744711,1799,"Enterprise Data Modeling (EDM) is the practice of creating a graphical model of the data used by an enterprise or company. Typical outputs of this activity include an Enterprise Data Model consisting of Entity Relationship Diagrams (ERD), XML Schemas (XSD), and an enterprise wide data dictionary. Producing such a model allows for a business to get a 'helicopter' view of their enterprise. In EAI (Enterprise Application Integration) an EDM allows data to be represented in a single idiom, enabling the use of a common syntax for the XML of services or operations and the physical data model for database schema creation. Data Modeling Tools for ERDs that also allow the user to create a data dictionary are usually used to aid in the development of an EDM.
The implementation of an EDM is closely related to the issues of data governance and data stewardship within an organization.

An Enterprise Data Model (EDM) represents a single integrated definition of data, unbiased of any system or application. It is independent of “how” the data is physically sourced, stored, processed or accessed. The model unites, formalizes and represents the things important to an organization, as well as the rules governing them


== See also ==
Data modeling


== External links ==
Noreen Kendle (July 1, 2005). ""The Enterprise Data Model"". The Data Administration Newsletter. 
Andy Graham 2010. The Enterprise Data Model: A framework for enterprise data architecture. ISBN 978-0956582904."
735,Stable storage,2163561,1797,"Stable storage is a classification of computer data storage technology that guarantees atomicity for any given write operation and allows software to be written that is robust against some hardware and power failures. To be considered atomic, upon reading back a just written-to portion of the disk, the storage subsystem must return either the write data or the data that was on that portion of the disk before the write operations.
Most computer disk drives are not considered stable storage because they do not guarantee atomic write; an error could be returned upon subsequent read of the disk where it was just written to in lieu of either the new or prior data.


== Implementation ==
Multiple techniques have been developed to achieve the atomic property from weakly atomic devices such as disks. Writing data to a disk in two places in a specific way is one technique and can be done by application software.
Most often though, stable storage functionality is achieved by mirroring data on separate disks via RAID technology (level 1 or greater). The RAID controller implements the disk writing algorithms that enable separate disks to act as stable storage. The RAID technique is robust against some single disk failure in an array of disks whereas the software technique of writing to separate areas of the same disk only protects against some kinds of internal disk media failures such as bad sectors in single disk arrangements."
736,Ipke Wachsmuth,31556873,1794,"Ipke Wachsmuth was born 1950. He is a German computer scientist within the fields of artificial intelligence and cognitive science.
Wachsmuth is a professor for artificial intelligence at Bielefeld University and teaches computer science and artificial intelligence since 1989. From 2002 until 2009 he was the managing director of the Center for Interdisciplinary Research (Zentrum für interdisziplinäre Forschung, ZiF) in Bielefeld. His research mainly focuses on human-machine interaction and virtual reality.
Wachsmuth is known for connecting classical symbolic technologies of knowledge representation with elements of dynamic gestures and facial expressions. This is especially exemplified by the development of the embodied agent Max. Since the 1990s Wachsmuth has also been a driving force behind the Interdisciplinary College, an annual spring school in the fields of neurobiology, neural computation, cognitive science, artificial intelligence, robotics and philosophy.


== References ==


== External links ==
Homepage of Ipke Wachsmuth
Homepage of Artificial Intelligence Working Group of Bielefeld University"
737,Flag day (computing),19427518,1791,"A flag day, as used in system administration, is a change which requires a complete restart or conversion of a sizable body of software or data. The change is large and expensive, and—in the event it doesn't work—reversing the change is similarly difficult and expensive.
The situation may arise if there are limitations on backward compatibility and forward compatibility among system components, which then requires that updates be performed almost simultaneously (during a ""flag day cutover"") for the system to function after the upgrade. This contrasts with the method of gradually phased-in upgrades, which avoids the disruption of service caused by en masse upgrades.
This systems terminology originates from a major change in the Multics operating system's definition of ASCII, which was scheduled for the United States holiday, Flag Day, on June 14, 1966.
Another historical flag day was January 1, 1983, when the ARPANET changed from NCP to the TCP/IP protocol suite. This major change required all ARPANET nodes and interfaces to be shut down and restarted across the entire network.


== See also ==
Backward compatibility
Forward compatibility


== References =="
738,Left corner parser,24542773,1788,"In computer science, a left corner parser is a type of chart parser used for parsing context-free grammars. It combines the top-down and bottom-up approaches of parsing. The name derives from the use of the left corner of the grammar's production rules.
An early description of a left corner parser is ""A Syntax-Oriented Translator"" by Peter Zilahy Ingerman.


== References ==
Blackburn, Patrick; Striegnitz, Kristina (August 29, 2002). ""Left-Corner Parsing"". Natural Language Processing Techniques in Prolog. Schenectady, New York: Union College Computer Science department. Retrieved 30 August 2017. 
Specific"
739,Makespan,31501520,1788,"In operations research, the makespan of a project is the total time that elapses from the beginning to the end. The term commonly appears in the context of scheduling. There is a complex project that is composed of several sub-tasks. We would like to assign tasks to workers, such that the project finishes in the shortest possible time.
As an example, suppose the ""project"" is to feed the goats. There are three goats to feed, and there are two children that can feed them: Shmuel feeds each goat in 10 minutes and Shifra feeds each goat in 12 minutes. Several schedules are possible:
If we let Shmuel feed all goats, then the makespan is 30 (3×10 for Shmuel, 0 for Shifra);
If we let Shifra feed one goat and Shmuel two goats, then the makespan is 20 (2×10 for Shmuel, 12 for Shifra);
If we let Shifra feed two goats and Shmuel one goat, then the makespan is 24 (2×12 for Shifra, 10 for Shmuel);
If we let Shifra feed all goats, then the makespan is 36 (3×12 for Shifra).
So in this case, the second schedule attains the shortest makespan, which is 20.


== Types of makespan minimization problems ==
Job shop scheduling – there are n jobs and m identical stations. Each job should be executed on a single machine. This is usually regarded as an online problem.
Open-shop scheduling – there are n jobs and m different stations. Each job should spend some time at each station, in a free order.
Flow shop scheduling – there are n jobs and m different stations. Each job should spend some time at each station, in a pre-determined order.


== References =="
740,Persistent programming language,2595930,1786,"Programming languages that natively and seamlessly allow objects to continue existing after the program has been closed down are called persistent programming languages. JADE is one such language.
A persistent programming language is a programming language extended with constructs to handle persistent data. It is distinguished from embedded SQL in at least two ways:
In a persistent programming language:
The query language is fully integrated with the host language and both share the same type system.
Any format changes required between the host language and the database are carried out transparently.
In Embedded SQL:
Where the host language and data manipulation language have different type systems, code conversion operates outside of the OO type system, and hence has a higher chance of having undetected errors.
Format conversion must be handled explicitly and takes a substantial amount of code.
Using Embedded SQL, a programmer is responsible for writing explicit code to fetch data into memory or store data back to the database. In a persistent programming language, a programmer can manipulate persistent data without having to write such code explicitly.
The drawbacks of persistent programming languages include:
While they are powerful, it is easy to make programming errors that damage the database.
It is harder to do automatic high-level optimization.
They do not support declarative querying well.


== See also ==
Object-relational mapping
Object-oriented database management systems
Object prevalence"
741,Fixed-priority pre-emptive scheduling,7564731,1779,"Fixed-priority preemptive scheduling is a scheduling system commonly used in real-time systems. With fixed priority preemptive scheduling, the scheduler ensures that at any given time, the processor executes the highest priority task of all those tasks that are currently ready to execute.
The preemptive scheduler has a clock interrupt task that can provide the scheduler with options to switch after the task has had a given period to execute—the time slice. This scheduling system has the advantage of making sure no task hogs the processor for any time longer than the time slice. However, this scheduling scheme is vulnerable to process or thread lockout: since priority is given to higher-priority tasks, the lower-priority tasks could wait an indefinite amount of time. One common method of arbitrating this situation is aging, which gradually increments the priority of waiting processes and threads, ensuring that they will all eventually execute. Most Real-time operating systems (RTOSs) have preemptive schedulers. Also turning off time slicing effectively gives you the non-preemptive RTOS.
Preemptive scheduling is often differentiated with cooperative scheduling, in which a task can run continuously from start to end without being preempted by other tasks. To have a task switch, the task must explicitly call the scheduler. Cooperative scheduling is used in a few RTOS such as Salvo or TinyOS."
742,Protected procedure,12330336,1772,"In computer science, the concept of protected procedure, first introduced as protected service routine in 1965, is necessary when two computations A and B use the same routine S; a protected procedure is such if makes not possible for a malfunction of one of the two computation to cause incorrect execution to the other.
One of the most important aspects of Dennis and Van Horn (hypothetical) system ""supervisor"" was the inclusion of a description of protected procedure.
In a global environment system (where there's some shared variable), the protected procedure mechanism allows the enforcement of the principle of least privilege and the avoidance of side effects in resources management (see Denning principles).


== Footnotes ==


== References ==
Dennis, J. B., and Glasee, E. The structure of on-line information processing systems. Information Systems Sciences: Proc. Second Cong., Spartan Books, Baltimore, 1965, pp. 1–11
J. B. Dennis and E. C. Van Horn. Programming Semantics for Multiprogrammed Computations. Communications of the ACM 9(3), March 1966.
Levy, Henry M. (1984). ""3"". Capability-based computer systems. Maynard, Mass: Digital Press. ISBN 0-932376-22-3."
743,Shannon (unit),41465868,1769,"The shannon (symbol: Sh), more commonly known as the bit, is a unit of information and of entropy defined by IEC 80000-13. One shannon is the information content of an event occurring when its probability is one half. It is also the entropy of a system with two equiprobable states. If a message is made of a sequence of a given number of bits, with all possible bit strings being equally likely, the message's information content expressed in shannons is equal to the number of bits in the sequence. For this and historical reasons, the shannon is more commonly known as the bit. The introduction of the term shannon provides an explicit distinction between the amount of information that is expressed and the quantity of data that may be used to represent the information. IEEE Std 260.1-2004 still defines the unit for this meaning as the bit, with no mention of the shannon.
The shannon can be converted to other information units according to
1 Sh = 1 bit ≈ 0.693 nat ≈ 0.301 Hart.
The shannon is named after Claude Shannon, the founder of information theory.


== See also ==
hartley


== References =="
744,Implicit invocation,2505607,1765,"Implicit invocation is a term used by some authors for a style of software architecture in which a system is structured around event handling, using a form of callback. It is closely related to inversion of control and what is known informally as the Hollywood principle.
Implicit invocation is the core technique behind the observer pattern.


== See also ==
Mach-II, an implicit invocation framework for the ColdFusion Markup Language (CFML)
Spring Framework


== External links ==
An Introduction to Software Architecture by David Garlan and Mary Shaw
An Introduction to Implicit Invocation Architectures by Benjamin Edwards"
745,European Conference on Computer Vision,30126422,1764,"ECCV, the European Conference on Computer Vision, is a biennial research conference with the proceedings published by Springer Science+Business Media. Similar to ICCV in scope and quality, it is held those years which ICCV is not. Like ICCV and CVPR, it is considered an important conference in computer vision, with an 'A' rating from the Australian Ranking of ICT Conferences and an 'A1' rating from the Brazilian ministry of education. The acceptance rate for ECCV 2010 was 24.4% posters and 3.3% oral presentations.
Like other top computer vision conferences, ECCV has tutorial talks, technical sessions, and poster sessions. The conference is usually spread over five to six days with the main technical program occupying three days in the middle, and tutorial and workshops, focussed on specific topics, being held in the beginning and at the end.


== References =="
746,Re-order buffer,390468,1761,"A re-order buffer (ROB) is used in a Tomasulo algorithm for out-of-order instruction execution. It allows instructions to be committed in-order.
Normally, there are three stages of instructions: ""Issue"", ""Execute"", ""Write Result"". In Tomasulo algorithm, there is an additional stage ""Commit"". In this stage, the results of instructions will be stored in a register or memory. In the ""Write Result"" stage, the results are just put in the re-order buffer. All contents in this buffer can then be used when executing other instructions depending on these.
There are additional fields in every entry of the buffer:
Instruction type (jump, store to memory, store to register)
Destination (either memory address or register number)
Result (value that goes to destination or indication of a (un)successful jump)
Validity (does the result already exist?)
Additional benefits of the re-order buffer include precise exceptions and easy rollback control of target address mispredictions (branch or jump). The ROB works by storing instructions in their original fetched order. The ROB can also be accessed from the side since each reservation station (in Tomasulo algorithm) has an additional parameter that points to instruction in the ROB. When jump prediction is not correct or a nonrecoverable exception is encountered in the instruction stream, the ROB is cleared of all instructions and reservation stations are re-initialized.


== External links ==
Reorder Buffer"
747,Diversity in computing,49863282,1759,"Computer science is one of the STEM fields, (science, technology, engineering, and mathematics), and like many STEM fields, it has a problem maintaining a strong and diverse STEM pipeline. Since 2009, when the number of undergraduate computer science (CS) graduates hit a low, there have been many efforts to increase the supply to meet an ever-increasing demand.


== See also ==

Association for Computing Machinery
Black Girls Code
Coalition to Diversify Computing
STEM pipeline
Women in computing
EarSketch
CASInclude


== References ==


== External links ==
Coalition for Cultural Diversity
UK Coalition for Cultural Diversity
Black Girls Code website
Computer science’s diversity gap starts early
More Students—But Few Girls, Minorities—Took AP Computer Science Exams
AP Archived Data 2014
Top and Bottom Five States for Minorities in Computing"
748,Stride scheduling,7755182,1759,"The stride scheduling is a type of scheduling mechanism that has been introduced as a simple concept to achieve proportional CPU capacity reservation among concurrent processes. Stride scheduling aims to sequentially allocate a resource for the duration of standard time-slices (quantum) in a fashion, that performs periodic recurrences of allocations. Thus, a process p1 which has reserved twice the share of a process p2 will be allocated twice as often as p2. In particular, process p1 will even be allocated two times every time p2 is waiting for allocation, assuming that neither of the two processes performs a blocking operation.


== See also ==


== References =="
749,Ontology Definition MetaModel,20237887,1759,"The Ontology Definition MetaModel (ODM) is an Object Management Group (OMG) specification to make the concepts of Model-Driven Architecture applicable to the engineering of ontologies. Hence, it links Common Logic (CL), the Web Ontology Language (OWL), and the Resource Description Framework (RDF).
OWL and RDF were initially defined to provide an XML-based machine to machine interchange of metadata and semantics. ODM now integrates these into visual modeling, giving a standard well-defined process for modeling the ontology, as well as, allowing for interoperability with other modeling based on languages like UML, SysML and UPDM.


== See also ==
Web Ontology Language
Unified Modeling Language


== External links ==
Ontology Definition Metamodel (ODM) Version 1.0, Object Management Group, May 2009
W3C OWL ontology


== Resources ==
Eclipse ODM
Enterprise Architect ODM MDG Technology
Formal Modelling, Knowledge Representation and Reasoning for Design and Development of User-centric Pervasive Software: A Meta-review, article in International Journal of Metadata, Semantics and Ontologies, Vol 6, No 2 (2011), 96-125, by Ahmet Soylu, Patrick De Causmaecker, Davy Preuveneers, Yolande Berbers, and Piet Desmet"
750,Tbox,4070243,1758,"In Computer Science, a TBox is a ""terminological component""—a conceptualization associated with a set of facts, known as an ABox.
The terms ABox and TBox are used to describe two different types of statements in ontologies. TBox statements describe a conceptualization, a set of concepts and properties for these concepts. ABox are TBox-compliant statements about individuals belonging to those concepts. For instance, a specific tree is an individual for the concept of ""Tree"", while it can be stated that trees as a concept are material beings that have to be positioned on some location it is possible to state the specific location that a tree takes at some specific time.
Together ABox and TBox statements make up a knowledge base. A TBox is a set of definitions and specializations.
A definition is an equality with an atomic concept on the left hand, for example: a bachelor is a student who is undergraduate.
A specialization is an inclusion with an atomic concept on the left hand, for example: the set of students is a subset of the people who are studying.


== See also ==
ABox
Description Logic Modeling
metadata
Web Ontology Language


== References =="
751,Semantic Geospatial Web,47806420,1753,"The Semantic Geospatial Web is a vision to include geospatial information at the core of the Semantic Web to facilitate information retrieval and information integration. This vision requires the definition of geospatial ontologies, semantic gazetteers, and shared technical vocabularies to describe geographic phenomena. The Semantic Geospatial Web is part of geographic information science.


== External links ==
W3C Geospatial Semantic Web Community Group


== References =="
752,Pantelides algorithm,21814084,1748,"Pantelides algorithm gives a systematic method for reducing high-index systems of differential-algebraic equations to lower index, by selectively adding differentiated forms of the equations already present in the system. It is possible for the algorithm to fail in some instances.
Pantelides algorithm is implemented in several significant equation-based simulation programs such as gPROMS, Modelica and EMSO.


== References =="
753,Forest of stars,10054330,1743,"A forest of stars is a set of star worlds whose adjacency matrix is a tree. This means that no intersecting star worlds create a cycle, or hole, in the overall space. If an object or space can be represented by a forest of stars, it can be mapped onto a sphere-world by mapping each star world onto the boundary of its parent star world in the adjacency tree. The root of an adjacency tree can be picked arbitrarily.
All star worlds in a forest of stars must have intersections that are also star worlds with respect to their center point.
Forests of stars are used in robot navigation to create navigation functions such as artificial potential functions. A forest of stars is used to represent robots or obstacles that have shapes which can be approximated by the union of separate stars.


== Relation to sphere worlds ==
A sphere world is a space whose boundary is a sphere of the same dimension as the space. A star world is any world whose boundary can be mapped onto the boundary of a sphere world. Since a forest of stars is the union of a number of star worlds, the forest can be recursively mapped onto a single sphere world, and then navigation techniques for sphere worlds can be used.


== See also ==
Navigation function
Sphere world
Star world
Topology


== References ==
E. Rimon, D. Koditschek Exact Robot Navigation Using Artificial Potential Functions IEEE Transactions on Robotics and Automation, Vol 8, No 5, Oct 1992"
755,IEEE Transactions on Control Systems and Technology,12953353,1739,"The IEEE Transactions on Control Systems Technology is published bimonthly by the IEEE Control Systems Society. The journal publishes papers, letters, tutorials, surveys, and perspectives on control systems technology. The editor-in-chief is Prof. Andrea Serrani (The Ohio State University). According to the Journal Citation Reports, the journal has a 2013 impact factor of 2.521.


== References ==


== External links ==
Official website"
756,METIS,41457976,1731,"The Métis are members of ethnic groups native to Canada and parts of the United States that trace their descent to indigenous North Americans and European settlers. The Métis in Canada are recognized as an aboriginal people under the Constitution Act of 1982; they number 451,795 as of 2011. Smaller communities identifying as Métis exist in the U.S.


== Etymology ==
The word derives from the French adjective métis, also spelled metice, referring to a hybrid, or someone of mixed ancestry. In the 16th century, métis came to be used as a noun for people of mixed European and indigenous American parentage. It is related to the Spanish term ""Mestizo"", which has the same meaning. It later came to be used for people of mixed European and indigenous backgrounds in other French colonies, including Guadeloupe in the Caribbean; African countries like Senegal; Algeria; and the former French Indochina.
In Latin America, a similar word is mestizo in Spanish-speaking countries, and in Portuguese-speaking countries, mestiço is also used. The English word mestee is a corruption of the Middle French mestis (the letters 's' both pronounced at the start of the Middle French period, and both silent at the end of the Middle French period).
It has also been used to refer to people of mixed race born generally to indigenous women and French men in New France and La Louisiane. The Métis in Canada married within their own group, and over time, created a distinct culture of their own.
The term mestee was widely used in the antebellum United States for mixed-race individuals, according to Jack D. Forbes, used for people of European and Native American ancestry, as well as European and African, or tri-racial. In the 19th century, the census takers recorded people of color as mulatto, also meaning mixed race. In former French colonies, a group known as free people of color had developed from unions between African or mixed-race women and French male colonists; often the men freed their children.
After the American Civil War, the term ""mestee"" gradually fell into disuse when the millions of slaves were made freedmen. As Americans worked to re-establish the unification of the country during and after Reconstruction, they passed laws after the turn of the 20th century to enforce the ""one-drop rule"". By this anyone with any known Sub-Saharan African ancestry was legally ""Black"", a more restrictive definition than had previously operated in the South, especially on the frontier. Native American scholar Jack D. Forbes has attempted to revive ""mestee"" as a term for the mixed-race peoples established as free before the Civil War.
Worldwide, the word has been adapted since the early 20th century for a number of purposes. ""Metisaje"" was used from the 1920s to the 1960s in some Latin American countries to indicate cultural hybridity, and at times to invoke a nationalist sentiment. Cultural ""Hybridity"" theorists have used the term ""métissage"" to examine postcolonial themes, including Françoise Lionnet. Creolité is a cultural and literary movement that has common threads with ""métis"" identity, and has been a counterpoint to the Négritude movement, although it has also been used to indicate ""race and gender specific"" themes as well.


== Métis people in Canada ==

The Canadian Encyclopedia indicates that there is no complete consensus as on the definition of Métis in Canada. The Canadian Encyclopedia's definition of Métis was not developed in consultation with Métis people or communities. It uses the following definition:

Written with a small m, métis is an old French word meaning ""mixed"", and it is used here in a general sense for people of dual Indian-White ancestry. Capitalized, Métis generally refers to people of the post-contact indigenous people, the Métis Nation. It may variously refer to a distinctive socio cultural heritage, a means of ethnic self-identification, and sometimes a political and legal category, more or less narrowly defined.


=== History of Métis in Canada ===
While some have argued that Métis people's history begins in the 17th century with the unions of various French colonists, typically trappers and traders, and Algonquian women, including but not limited to Mi'kmaq, Algonquin, Ojibwe, and Cree peoples, in actually, the Métis Nation is indigenous to what is known as the ""Métis Homeland,"" stretching from what is presently called Northwestern Ontario and westward across the prairies. The Métis Nation is rooted in its maternal homeland and is principally connected to its Cree and Saulteaux relations through a long history of alliance.
People of ""mixed ancestry,"" although not of the Métis Nation, have a distinct history of their own. These unions began in the east, extending from the Atlantic coast to the Great Lakes. The fur trade and colonial development drew French voyageurs and coureurs des bois to the west, along with the later Hudson's Bay Company employees. Wintering partners of the fur trading companies typically took country wives for their months away from the eastern cities.
After the fall of New France in 1763, many mixed-race populations continued to establish themselves, often specializing in the fur trade and related hunting. Some served as interpreters as they often were fluent in both indigenous and European languages. English and Scottish traders also married indigenous women, often the daughters of high-ranking chiefs, forming an elite mixed society. As the eighteenth century ended, the fur trade moved westwards into the Plains.
The Métis Nation fomented their distinct and unique Indigenous identity in 1812, Cuthbert Grant led a battle in the Pemmican War, flying the Métis flag. Many treaties throughout Canada were being negotiated in the nineteenth century, including in Ontario with the Robinson–Huron treaty. In 1870 the Métis at Red River, led by Louis Riel, resisted the colonial efforts of Canada, and negotiated entry into Canada as the province of Manitoba with promises to protect their rights. In 1885, the Métis were resisting Canadian colonialism with the North-West Rebellion. The Métis were defeated and Riel was hanged as a traitor to Canada, but his role in history is controversial.
Métis in the Métis Nation homeland faced scrip after 1885, and many were considered ""Road Allowance people"". Racism towards Métis peoples in the west was a large part of the late nineteenth and twentieth centuries.
In 1982, Métis were included as indigenous people in the Canadian constitution. They are defined as an ethnic group with their own culture, distinct from First Nations and Inuit peoples. Métis peoples have formed a variety of political organizations to promote their interests, and lobby the federal government through their primary national political association, Métis National Council (MNC).
In 2003, the R. v. Powley 2003 SCC 43 ruled that a family of Métis people in Ontario had the right to hunt moose as part of their Métis aboriginal rights. This case was funded by the Métis Nation of Ontario (MNO), a provincial affiliate of the MNC. The case established the Métis history in Ontario, which was long debated by many people. The case also established the Powley test, which helps to define who is Métis, and therefore eligible to rights as an aboriginal person.
On April 14, 2016, the Supreme Court in Daniels v Canada (Indian Affairs and Northern Development) 2016 SCC 12 reached a landmark decision.


== Métis people in the United States ==


== See also ==
Multiracial
Mestizo
Anglo-Métis
Baster
Half-caste
Kahnawake surnames
NunatuKavut people


== References ==


== Bibliography ==
Barkwell, Lawrence J.; Dorion, Leah; Hourie, Audreen (2006). ""Métis legacy Michif culture, heritage, and folkways"". Métis legacy series. 2. Saskatoon: Gabriel Dumont Institute. ISBN 0-920915-80-9. 
Barkwell, Lawrence J.; Dorion, Leah; Prefontaine, Darren (2001). Métis Legacy: A Historiography and Annotated Bibliography. Winnipeg: Pemmican Publications Inc. and Saskatoon: Gabriel Dumont Institute. ISBN 1-894717-03-1. 


== External links ==
The Rupertsland Institute (Alberta) – A service dedicated to the research and development, education, and training and employment of Metis individuals. It is affiliated with the Metis Nations of Alberta. Along with providing financial aid, the Rupertsland Institute helps Metis individuals acquire essential skills for employment."
757,Presburger Award,31823220,1719,"The Presburger Award, started in 2010, is awarded each year by the European Association for Theoretical Computer Science (EATCS) to ""a young scientist for outstanding contributions in theoretical computer science, documented by a published paper or a series of published papers."" The award is named after Mojżesz Presburger who accomplished his path-breaking work on decidability of the theory of addition (which today is called Presburger arithmetic) as a student in 1929.
Past recipients of the award are:
Mikołaj Bojańczyk (2010)
Patricia Bouyer-Decitre (2011)
Venkatesan Guruswami and Mihai Pătraşcu (2012)
Erik Demaine (2013) 
David Woodruff (2014) 
Xi Chen (2015)
Mark Braverman (2016)
Alexandra Silva (2017)


== References =="
758,Local maximum intensity projection,32861600,1719,"In scientific visualization, a local maximum intensity projection (LMIP) is a volume rendering method for 3D data, that is proposed as an improvement to the maximum intensity projection (MIP). Where the MIP projects the maximum intensity that falls in the way of parallel rays traced from the viewpoint, LMIP takes the first local maximum value, that is above a certain threshold.
Local maximum intensity projection has been proposed as in visualization of data from computerized tomography and magnetic resonance imaging. It also can be used to extract a 3-dimensional vascular network from the data from a knife-edge scanning microscope at a significantly reduced computational demand (65% reduced).


== Footnotes =="
759,Computational Statistics & Data Analysis,34877794,1717,"Computational Statistics & Data Analysis is a monthly peer-reviewed scientific journal covering research on and applications of computational statistics and data analysis. The journal was established in 1983 and is the official journal of the International Association for Statistical Computing, a section of the International Statistical Institute.


== See also ==
List of statistics journals


== References ==


== External links ==
Official website"
760,Beier–Neely morphing algorithm,35815287,1714,"Image morphing is a technique to synthesize a fluid transformation from one image (source image) to another (destination image). Source image can be one or more than one images. There are two parts in the image morphing implementation. The first part is warping and the second part is cross-dissolving.
The algorithm of Beier and Neely is a method to compute a mapping of coordinates between 2 images from a set of lines; i.e., the warp is specified by a set of line pairs where the start-points and end-points are given for both images. The algorithm is widely used within morphing software.
Also noteworthy, this algorithm only discussed about the situation with at most 2 source images as there are other algorithms introducing multiple source images.


== See also ==
Morphing
Image warping
Image processing


== References ==


== External links ==
Description of the algorithm by Evan Wallace of Brown University"
761,Interrupt coalescing,39658426,1711,"Interrupt coalescing, also known as interrupt moderation, is a technique in which events which would normally trigger a hardware interrupt are held back, either until a certain amount of work is pending, or a timeout timer triggers. Used correctly, this technique can reduce interrupt load by up to an order of magnitude, while only incurring relatively small latency penalties. Interrupt coalescing is typically combined with either a hardware FIFO or direct memory access, to allow for continued data throughput while interrupts are being held back.
Interrupt coalescing is a common feature of modern network cards, but the technique dates back to early computer UARTs such as the 16550 UART chip used in the IBM PC's serial interface, at a time when even servicing the interrupt rates required by the low data rate serial data streams of the day was taxing for contemporary CPUs.
Interrupt coalescing can also be implemented without support in hardware, by disabling interrupts in the interrupt controller and using timer-based polling.


== See also ==
I/O processor
Timer coalescing


== References =="
762,Powernex,55218888,1702,"PowerNex is a computer kernel written in D programming language. The first iteration was produced in 2015. The open source code (Mozilla Public License 2.0) is hosted here. The sole developer is Dan Printzell.
The projects goal is to have a whole OS written in D


== See also ==
TempleOS – another operating system developed largely from scratch
ToaruOS – a Unix-like independently developed OS
Redox OS – a Unix-like independently developed OS


== References ==


== External links ==
Official website
PowerNex on GitHub"
763,EPAM,12838512,1693,"EPAM Systems, Inc., also known as EPAM, is a global provider of software engineering and IT consulting services headquartered in Newtown, Pennsylvania, United States. The company has software development centers and branch offices in North America, Europe, Asia and Australia.


== History ==


=== Early years and IPO ===
The company was founded as EPAM by Belarus natives Arkadiy Dobkin in Princeton, New Jersey, and Leo Lozner in Minsk, Belarus in 1993. It incorporated as EPAM Systems on December 18, 2002. The company has since grown to approximately 25,900 tech employees, as of March 2018.
EPAM initially stood for ""Effective Programming for America"", though the company simply uses the acronym in all its marketing and information materials.
On January 24, 2012, EPAM announced the launch of an IPO on the New York Stock Exchange under the ticker EPAM. This is the first IPO that comes from the outsourcing industry in Eastern Europe.


=== Acquisitions ===


== Corporate social responsibility ==
The company has collaborated with the United Nations in local and global initiatives that promote the sustainable development and social responsibility.


=== Education ===
EPAM joined the Business Call to Action initiative by United Nations Development Programme (UNDP) with a commitment to prepare 5,000 students in Central Europe to enter the IT sector by 2020. The company pledged to meet these goals through its University Program. Launched in 2004 to meet increased industry demand, the program provides specialized IT training for university students in Central and Eastern Europe.
To diversify the IT talent pool and attract girls to technology, the company launched the e-Kids program, a 10-week coding curriculum across three levels focused around the Scratch programming language.


=== Environment ===
EPAM is committed to sustainable environment through a collaboration with the United Nations in local communities.


== Recent awards and honours ==


=== Awards ===


=== Recognition ===


== References ==


== External links ==
EPAM Systems website
List of Institutional Investors"
764,Factor oracle,33736410,1692,"A factor oracle is a finite state automaton that can efficiently search for factors (substrings) in a body of text. Older techniques, such as suffix trees, were time-efficient but required significant amounts of memory. Factor oracles, by contrast, can be constructed in linear time and space in an incremental fashion.


== Overview ==
Older techniques for matching strings include: suffix arrays, suffix trees, suffix automata or directed acyclic word graphs, and factor automata (Allauzen, Crochemore, Raffinot, 1999). In 1999, Allauzen, Crochemore, and Raffinot, presented the factor oracle algorithm as a memory efficient improvement upon these older techniques for string matching and compression. Starting in the mid-2000s, factor oracles have found application in computer music, as well.


== Implementations ==
The Computer Audition Laboratory provides a Matlab implementation of the factor oracle algorithm.


== See also ==
Suffix array
Generalised suffix tree


== References =="
765,IJCAI Award for Research Excellence,2731564,1688,"The IJCAI Award for Research Excellence is a biannual award before given at the IJCAI conference to researcher in artificial intelligence as a recognition of excellence of their career. Beginning in 2016, the conference is held annually and so is the award.


== Laureates ==
The recipients of this award have been:
John McCarthy (1985)
Allen Newell (1989)
Marvin Minsky (1991)
Raymond Reiter (1993)
Herbert A. Simon (1995)
Aravind Joshi (1997)
Judea Pearl (1999)
Donald Michie (2001)
Nils Nilsson (2003)
Geoffrey E. Hinton (2005)
Alan Bundy (2007)
Victor R. Lesser (2009)
Robert Kowalski (2011)
Hector Levesque (2013)
Barbara Grosz (2015)
for her pioneering research in Natural Language Processing and in theories and applications of Multiagent Collaboration. 
Michael I. Jordan (2016)
for his groundbreaking and impactful research in both the theory and application of statistical machine learning. 


== Winners of also Turing Award ==
John McCarthy (1971)
Allen Newell (1975)
Marvin Minsky (1969)
Herbert A. Simon (1975)
Judea Pearl (2011)


== See also ==
Turing award


== References ==


== External links ==
http://www.ijcai.org/awards/"
766,Trace-based simulation,36010141,1684,"In computer science, trace-based simulation refers to system simulation performed by looking at traces of program execution or system component access with the purpose of performance prediction.
Trace-based simulation may be used in a variety of applications, from the analysis of solid state disks to the message passing performance on very large computer clusters.
Traced-based simulators usually have two components: one that executes actions and stores the results (i.e. traces) and another which reads the log files of traces and interpolates them to new (and often more complex) scenarios.
For instance, in the case of large computer cluster design, the execution takes place on a small number of nodes, and traces are left in log files. The simulator reads those log files and simulates performance on a much larger number of nodes, thus providing a view of the performance of very large applications, based on the execution traces on a much smaller number of nodes.


== See also ==
BIGSIM


== References =="
767,Milorad Simić,52945137,1683,"Milorad Simić (Serbian Cyrillic: Милорад Симић; born 5 June 1946) is a Serbian philologist, linguist, lexicographer and computer scientist. He was born in Obadi (Bosnia and Herzegovina) and finished gymnasium in Srebrenica, College of Pedagogy in Šabac, and Faculty of Philology and magister studies in Belgrade. Since 1972 he is employed at the Institute of Serbian Language at the Serbian Academy of Science and Arts (SANU). He is an editor of the SANU Dictionary, and founder of the Srbosof agency specialized in linguistical computer science. He is a member of the council of Project Rastko since 1997. He has authored digital dictionaries and linguistical software. He was awarded the Order of Despot Stefan Lazarević by the Serbian Orthodox Church in March 2015.


== References ==
""ИНТЕРВЈУ: Милорад Симић, добитник Ордена Светог деспота Стефана Лазаревића"". Pouke."
768,Layer (object-oriented design),6018908,1681,"In object-oriented design, a layer is a group of classes that have the same set of link-time module dependencies to other modules. In other words, a layer is a group of reusable components that are reusable in similar circumstances. In programming languages, the layer distinction is often expressed as ""import"" dependencies between software modules.
Layers are often arranged in a tree-form hierarchy, with dependency relationships as links between the layers. Dependency relationships between layers are often either inheritance, composition or aggregation relationships, but other kinds of dependencies can also be used.
Layers is an architectural pattern described in many books, for example Pattern-Oriented Software Architecture


== Notes ==


== References ==
John Lakos, Large-scale C++ software design. Addison-Wesley, 1997.


== See also ==
Abstraction layer
Common layers in an information system logical architecture
Shearing layers"
769,Featherstone's algorithm,4340134,1679,"Featherstone's algorithm is a technique used for computing the effects of forces applied to a structure of joints and links (an ""open kinematic chain"") such as a skeleton used in ragdoll physics.
The Featherstone's algorithm uses a reduced coordinate representation. This is in contrast to the more popular Lagrange multiplier method, which uses maximal coordinates. Brian Mirtich's PhD Thesis has a very clear and detailed description of the algorithm. Baraff's paper ""Linear-time dynamics using Lagrange multipliers"" has a discussion and comparison of both algorithms.


== References ==
Featherstone, R. (1987). Robot Dynamics Algorithms. Boston: Kluwer. ISBN 0-89838-230-0. 


== External links ==
Featherstone Multibody in Bullet Physics engine
Featherstone's algorithm implementation in the Moby rigid body dynamics simulator
Source code for implementation of Featherstone's algorithm
Description and references
Mirtich's Thesis
Baraff's Lagrange multiplier method
Roy Featherstone's home page"
770,Time-utility function,15735043,1678,"Time-utility functions (TUFs, also called time-value functions) are needed for real-time computing when a deadline occurs.
They were introduced by E. Douglas Jensen in 1977 as a way to overcome the limited expressiveness in classic deadline constraints in real-time systems. In a graphical interpretation, the utility (positive for reward, negative for penalty) is plotted over the time. A deadline then represents the point where the utility changes from positive to negative. In computer science and programming, this is when a task must be terminated. If not, an exception occurs, which usually leads to an abortion. As such, a TUF is a generalization of deadline constraints in everyday life. With TUF time constraints, timeliness optimality criteria can be specified.


== External links ==
time-utility-functions and real-time computing
website on real-time computing by E. Douglas Jensen
Adaptive Time-utility Function Scheme for Downlink Packet Scheduling in IEEE 802.16e/WiMAX Networks
Time Utility Functions for Modeling and Evaluating Resource Allocations in a Heterogeneous Computing System"
771,IEEE Transactions on Computers,12953121,1678,"IEEE Transactions on Computers is a monthly peer-reviewed scientific journal covering all aspects of computer design. It was established in 1952 and is published by the IEEE Computer Society. The editor-in-chief is Albert Y. Zomaya (University of Sydney). According to the Journal Citation Reports, the journal has a 2013 impact factor of 1.473.


== References ==


== External links ==
Official website"
772,Beam stack search,3210817,1669,"Beam Stack Search is a search algorithm that combines chronological backtracking (that is, depth-first search) with beam search and is similar to Depth-First Beam Search. Both search algorithms are anytime algorithms that find good but likely sub-optimal solutions quickly, like beam search, then backtrack and continue to find improved solutions until convergence to an optimal solution.


== Implementation ==
Beam Stack Search uses the beam stack as a data structure to integrate chronological backtracking with beam search and can be combined with the divide and conquer algorithm technique, resulting in divide-and-conquer beam-stack search.


== Alternatives ==
Beam Search Using Limited Discrepancy Backtracking (BULB) is a search algorithm that combines limited discrepancy search with beam search and thus performs non-chronological backtracking, which often outperforms the chronological backtracking done by Beam Stack Search and Depth-First Beam Search.


== References =="
773,Pratt parser,30892451,1667,"In computer science, a Pratt parser is an improved recursive descent parser that associates semantics with tokens instead of grammar rules. It was first described by Vaughan Pratt in the 1973 paper ""Top down operator precedence"", and was treated in much more depth in a Masters Thesis under his supervision. Pratt designed the parser originally to implement the CGOL programming language. Douglas Crockford used the technique to build JSLint.


== References ==


== See also ==
Operator Precedence Parser


== External links ==
Pratt Parsers: Expression Parsing Made Easy
A Pratt Parser implementation in Python"
774,Design By Numbers,20877791,1667,"Design By Numbers (or DBN) was an influential experiment in teaching programming initiated at the MIT Media Lab during the 1990s. Led by John Maeda and his students they created software aimed at allowing designers, artists and other non-programmers to easily start computer programming. The software itself could be run in a browser and published alongside the software was a book and courseware.
Design By Numbers is no longer an active project but has gone on to influence many other projects aimed at making computer programming more accessible to non-technical people. Its most public result is Processing, created by Maeda's students Casey Reas and Ben Fry, who built on the work of DBN and has gone on to international success.


== Further reading ==
Maeda, John (October 1, 2001). Design By Numbers. MIT Press. p. 256. ISBN 0-262-63244-6. 


== See also ==
NodeBox
Processing
Smile software


== External links ==
Official website"
775,Relativistic programming,31865194,1660,"Relativistic programming (RP) is a style of concurrent programming where instead of trying to avoid conflicts between readers and writers (or writers and writers in some cases) the algorithm is designed to tolerate them and get a correct result regardless of the order of events. Also, relativistic programming algorithms are designed to work without the presences of a global order of events. That is, there may be some cases where one thread sees two events in a different order than another thread (hence the term relativistic because in Einstein's theory of special relativity the order of events is not always the same to different viewers).
Relativistic programming provides advantages in performance compared to other concurrency paradigms because it does not require one thread to wait for another nearly as often. Because of this, forms of it (Read-Copy-Update for instance) are now used extensively in the Linux kernel (over 9,000 times as of March 2014 and has grown from nothing to 8% of all locking primitives in about a decade).


== See also ==
Non-blocking algorithm


== References ==


== External links ==
Relativistic Programming at Portland State University"
776,Services computing,1820849,1660,"Services Computing has become a cross-discipline that covers the science and technology of bridging the gap between business services and IT services. The underneath breaking technology suite includes Web services and service-oriented architecture (SOA), cloud computing, business consulting methodology and utilities, business process modeling, transformation and integration. This scope of Services Computing covers the whole life-cycle of services innovation research that includes business componentization, services modeling, services creation, services realization, services annotation, services deployment, services discovery, services composition, services delivery, service-to-service collaboration, services monitoring, services optimization, as well as services management. The goal of Services Computing is to enable IT services and computing technology to perform business services more efficiently and effectively.


== References ==


== External links ==
IEEE Services Computing Online Community
Technical Committee on Services Computing, IEEE Computer Society
International Conference on Services Computing
IEEE Transactions on Services Computing
Services Society
International Conference on Web Services"
777,"International Conference on Acoustics, Speech, and Signal Processing",22318852,1651,"ICASSP, the International Conference on Acoustics, Speech, and Signal Processing, is an annual flagship conference organized of IEEE Signal Processing Society. All papers included in its proceedings have been indexed by Ei Compendex.
The first ICASSP was held in 1976 in Philadelphia, Pennsylvania based on the success of a conference in Massachusetts four years earlier that had focused specifically on speech signals.
As ranked by Google Scholar's h-index metric in 2016, ICASSP has the highest h-index of any conference in Signal Processing field.
Also, It is considered a high level conference in signal processing and, for example, obtained an 'A1' rating from the Brazilian ministry of education based on its H-index.


== References =="
778,Joseph Glickauf,44364819,1633,"Joseph Glickauf Jr. (January 15, 1912 – July 9, 2005), was an American-born engineer, inventor and corporate executive known as one of the first advocates of the use of computers in business and industry and the “father” of the computer consulting industry.
Joseph Glickauf was hired by Arthur Andersen Co. immediately after serving in the US Navy and was tasked with initiating the use of the freshly invented computer for his employer. Glickauf became familiar with the capabilities of the UNIVAC and immediately saw the far-reaching implications of computers for business. To demonstrate the computer to Arthur Andersen’s employees he built the Arthur Andersen Demonstration Computer known as “Glickiac”. The company management was quick to see the potential and made resources available for future development.
In 1953 General Electric Appliance Park hired Arthur Andersen to automate GE’s payroll. Glickauf lead the effort and recommended GE the installation of a UNIVAC I computer and printer. The project was initially a failure but it started what is now known as the “computer consulting”.


== References =="
779,Hard space,933852,1633,"In typesetting and text editors, the term hard space has several meanings, all related to a special way of representing the space between characters.
The most commonly used meaning is the same as non-breaking space, a special space character used by a word processor that forbids an automatic line breaking (line wrap) at its position.
In earlier days of text editors that worked with text mode CRT displays, when a paragraph had to be justified, this was achieved by means of inserting extra soft spaces at whitespaces. The soft spaces were so called because they could be ""compressed"" away during further editing. By contrast, ordinary spaces were called hard or incompressible spaces.
Also, in some older text editors, the hard spaces were both non-expandable—i.e., no soft spaces could be added to them—and non-breaking ones.
In many term programs and game parsers, a hard space was a special kind of field delimiter, against which a filename could be examined or listed, or a semantic thought or consideration could be interpreted.
In the Commodore directory system, a hard space usually terminated the spelling of a filename, and was replaced with a quotation mark when listed to the user.


== See also ==
Control character
Space (punctuation)
Soft hyphen"
780,Flick (time),56389209,1629,"A flick is a unit of time equivalent to exactly 1/705,600,000 of a second. The figure was chosen so that frequencies of 24, 25, 30, 48, 50, 60, 90, 100 and 120 Hz, as well as 1/1000 divisions of all those, can be represented with integers. The unit was launched in January 2018 by Facebook. A flick is approximately 1.42 x 10−9 s, which makes it larger than a nanosecond but much smaller than a microsecond.
A similar unit for integer representation of temporal points was proposed in 2004 under the name TimeRef, splitting a second into 14,112,000 parts. This makes 1 TimeRef equivalent to 50 Flicks.


== Etymology ==
The word flick is a portmanteau of frame (as in e.g. animation frame) and tick (as in computer instruction cycle).


== References ==


== External links ==
Why Did Facebook Invent A New Unit Of Time? The ""Flick"" Explained With Math. YouTube video: [1]"
781,High Performance Knowledge Bases,4782884,1625,"The High Performance Knowledge Bases (HPKB) was a DARPA research program to advance the technology of how computers acquire, represent and manipulate knowledge. The successor of the HPKB project was the Rapid Knowledge Formation (RKF) project.
The primary results of the HPKB project was to focus further research on the Knowledge acquisition bottleneck problem.
HPKB was divided programmatically into three groups:
Integrators
Technology developers
Challenge problem developers


== See also ==
Knowledge base
Cyc - commercial knowledge base
OpenCyc - Open Source version of Cyc
Electronic Directory Research (EDR) - Japanese large knowledge base effort
Project Halo - Ultimate successor project
Rapid Knowledge Formation (RKF)- follow-on project
SUMO - Suggested Upper Merged Ontology
Wikipedia - example of large knowledge base that is not yet semantically parsable
WordNet - a semantic network of words, terms used in the English language


== External links ==
[1] DARPA HPKB Home Page
Cohen,P., Schrag, R., Jones, E., Pease, A., Lin, A., Starr, B., Gunning, D. and Burke, M. DARPA High-Performance Knowledge Bases Project AI Magazine Volume 19 Number 4 (1998)


== References ==
Web Intelligence: First Asia-Pacific Conference, Wi 2001, Maebashi City, Japan, October 23–26, by N Zhong, Y Yao, J Liu"
782,Lava flow (programming),587106,1621,"Lava is molten rock generated by geothermal energy and expelled through fractures in planetary crust or in an eruption, usually at temperatures from 700 to 1,200 °C (1,292 to 2,192 °F). The resulting structures after solidification and cooling are also sometimes described as lava. The molten rock is formed in the interior of some planets, including Earth, and some of their satellites, though such material located below the crust is referred to by other terms.
A lava flow is a moving outpouring of lava created during a non-explosive effusive eruption. When it has stopped moving, lava solidifies to form igneous rock. The term lava flow is commonly shortened to lava. Although lava can be up to 100,000 times more viscous than water, lava can flow great distances before cooling and solidifying because of its thixotropic and shear thinning properties.
Explosive eruptions produce a mixture of volcanic ash and other fragments called tephra, rather than lava flows. The word lava comes from Italian, and is probably derived from the Latin word labes which means a fall or slide. The first use in connection with extruded magma (molten rock below the Earth's surface) was apparently in a short account written by Francesco Serao on the eruption of Vesuvius between May 14 and June 4, 1737. Serao described ""a flow of fiery lava"" as an analogy to the flow of water and mud down the flanks of the volcano following heavy rain.


== Lava composition ==

The composition of almost all lava of the Earth's crust is dominated by silicate minerals, mostly feldspars, olivine, pyroxenes, amphiboles, micas and quartz.


=== Silicate lavas ===
Igneous rocks, which form lava flows when erupted, can be classified into three chemical types; felsic, intermediate, and mafic (four if one includes the super-heated ultramafic). These classes are primarily chemical; however, the chemistry of lava also tends to correlate with the magma temperature, its viscosity and its mode of eruption.


==== Felsic lava ====
Felsic or silicic lavas such as rhyolite and dacite typically form lava spines, lava domes or ""coulees"" (which are thick, short lava flows) and are associated with pyroclastic (fragmental) deposits. Most silicic lava flows are extremely viscous, and typically fragment as they extrude, producing blocky autobreccias. The high viscosity and strength are the result of their chemistry, which is high in silica, aluminium, potassium, sodium, and calcium, forming a polymerized liquid rich in feldspar and quartz, and thus has a higher viscosity than other magma types. Felsic magmas can erupt at temperatures as low as 650 to 750 °C (1,202 to 1,382 °F). Unusually hot (>950 °C; >1,740 °F) rhyolite lavas, however, may flow for distances of many tens of kilometres, such as in the Snake River Plain of the northwestern United States.


==== Intermediate lava ====
Intermediate or andesitic lavas are lower in aluminium and silica, and usually somewhat richer in magnesium and iron. Intermediate lavas form andesite domes and block lavas, and may occur on steep composite volcanoes, such as in the Andes. Poorer in aluminium and silica than felsic lavas, and also commonly hotter (in the range of 750 to 950 °C (1,380 to 1,740 °F)), they tend to be less viscous. Greater temperatures tend to destroy polymerized bonds within the magma, promoting more fluid behaviour and also a greater tendency to form phenocrysts. Higher iron and magnesium tends to manifest as a darker groundmass, and also occasionally amphibole or pyroxene phenocrysts.


==== Mafic lava ====
Mafic or basaltic lavas are typified by their high ferromagnesian content, and generally erupt at temperatures in excess of 950 °C (1,740 °F). Basaltic magma is high in iron and magnesium, and has relatively lower aluminium and silica, which taken together reduces the degree of polymerization within the melt. Owing to the higher temperatures, viscosities can be relatively low, although still thousands of times higher than water. The low degree of polymerization and high temperature favors chemical diffusion, so it is common to see large, well-formed phenocrysts within mafic lavas. Basalt lavas tend to produce low-profile shield volcanoes or ""flood basalt fields"", because the fluidal lava flows for long distances from the vent. The thickness of a basalt lava, particularly on a low slope, may be much greater than the thickness of the moving lava flow at any one time, because basalt lavas may ""inflate"" by supply of lava beneath a solidified crust. Most basalt lavas are of ʻAʻā or pāhoehoe types, rather than block lavas. Underwater, they can form pillow lavas, which are rather similar to entrail-type pahoehoe lavas on land.


==== Ultramafic lava ====
Ultramafic lavas such as komatiite and highly magnesian magmas that form boninite take the composition and temperatures of eruptions to the extreme. Komatiites contain over 18% magnesium oxide, and are thought to have erupted at temperatures of 1,600 °C (2,910 °F). At this temperature there is no polymerization of the mineral compounds, creating a highly mobile liquid with viscosity as low as that of water. Most if not all ultramafic lavas are no younger than the Proterozoic, with a few ultramafic magmas known from the Phanerozoic. No modern komatiite lavas are known, as the Earth's mantle has cooled too much to produce highly magnesian magmas.


=== Unusual lavas ===
Some lavas of unusual composition have erupted onto the surface of the Earth. These include:
Carbonatite and natrocarbonatite lavas are known from Ol Doinyo Lengai volcano in Tanzania, which is the sole example of an active carbonatite volcano.
Iron oxide lavas are thought to be the source of the iron ore at Kiruna, Sweden which formed during the Proterozoic. Iron oxide lavas of Pliocene age occur at the El Laco volcanic complex on the Chile-Argentina border. Iron oxide lavas are thought to be the result of immiscible separation of iron oxide magma from a parental magma of calc-alkaline or alkaline composition.
Sulfur lava flows up to 250 metres (820 feet) long and 10 metres (33 feet) wide occur at Lastarria volcano, Chile. They were formed by the melting of sulfur deposits at temperatures as low as 113 °C (235 °F).
Olivine nephelinite lavas are thought to have come from much deeper in the mantle of the Earth than other lavas.
The term ""lava"" can also be used to refer to molten ""ice mixtures"" in eruptions on the icy satellites of the Solar System's gas giants. (See cryovolcanism).


== Lava behavior ==

In general, the composition of a lava determines its behavior more than the temperature of its eruption. The viscosity of lava is important because it determines how the lava will behave. Lavas with high viscosity are rhyolite, dacite, andesite and trachyte, with cooled basaltic lava also quite viscous; those with low viscosities are freshly erupted basalt, carbonatite and occasionally andesite.
Highly viscous lava shows the following behaviors:
tends to flow slowly, clog, and form semi-solid blocks which resist flow
tends to entrap gas, which form vesicles (bubbles) within the rock as they rise to the surface
correlates with explosive or phreatic eruptions and is associated with tuff and pyroclastic flows
Highly viscous lavas do not usually flow as liquid, and usually form explosive fragmental ash or tephra deposits. However, a degassed viscous lava or one which erupts somewhat hotter than usual may form a lava flow.
Lava with low viscosity shows the following behaviors:
tends to flow easily, forming puddles, channels, and rivers of molten rock
tends to easily release bubbling gases as they are formed
eruptions are rarely pyroclastic and are usually quiescent
volcanoes tend to form broad shields rather than steep cones
Lavas also may contain many other components, sometimes including solid crystals of various minerals, fragments of exotic rocks known as xenoliths and fragments of previously solidified lava.


== Lava morphology ==

The physical behavior of lava creates the physical forms of a lava flow or volcano. More fluid basaltic lava flows tend to form flat sheet-like bodies, whereas viscous rhyolite lava flows forms knobbly, blocky masses of rock.
General features of volcanology can be used to classify volcanic edifices and provide information on the eruptions which formed the lava flow, even if the sequence of lavas have been buried or metamorphosed.

The ideal lava flow will have a brecciated top, either as pillow lava development, autobreccia and rubble typical of ʻaʻā and viscous flows, or a vesicular or frothy carapace such as scoria or pumice. The top of the lava will tend to be glassy, having been flash frozen in contact with the air or water.
The centre of a lava flow is commonly massive and crystalline, flow banded or layered, with microscopic groundmass crystals. The more viscous lava forms tend to show sheeted flow features, and blocks or breccia entrained within the sticky lava. The crystal size at the centre of a lava will in general be greater than at the margins, as the crystals have more time to grow.
The base of a lava flow may show evidence of hydrothermal activity if the lava flowed across moist or wet substrates. The lower part of the lava may have vesicles, perhaps filled with minerals (amygdules). The substrate upon which the lava has flowed may show signs of scouring, it may be broken or disturbed by the boiling of trapped water, and in the case of soil profiles, may be baked into a brick-red terracotta.
Discriminating between an intrusive sill and a lava flow in ancient rock sequences can be difficult. However, some sills do not usually have brecciated margins, and may show a weak metamorphic aureole on both the upper and lower surface, whereas a lava will only bake the substrate beneath it. However, it is often difficult in practice to identify these metamorphic phenomenon because they are usually weak and restricted in size. Peperitic sills, intruded into wet sedimentary rocks, commonly do not bake upper margins and have upper and lower autobreccias, closely similar to lavas.


=== ʻAʻā ===

ʻAʻā is one of three basic types of flow lava. ʻAʻā is basaltic lava characterized by a rough or rubbly surface composed of broken lava blocks called clinker. The Hawaiian word was introduced as a technical term in geology by Clarence Dutton.
The loose, broken, and sharp, spiny surface of an ʻaʻā flow makes hiking difficult and slow. The clinkery surface actually covers a massive dense core, which is the most active part of the flow. As pasty lava in the core travels downslope, the clinkers are carried along at the surface. At the leading edge of an ʻaʻā flow, however, these cooled fragments tumble down the steep front and are buried by the advancing flow. This produces a layer of lava fragments both at the bottom and top of an ʻaʻā flow.
Accretionary lava balls as large as 3 metres (10 feet) are common on ʻaʻā flows. ʻAʻā is usually of higher viscosity than pāhoehoe. Pāhoehoe can turn into ʻaʻā if it becomes turbulent from meeting impediments or steep slopes.
The sharp, angled texture makes ʻaʻā a strong radar reflector, and can easily be seen from an orbiting satellite (bright on Magellan pictures).
ʻAʻā lavas typically erupt at temperatures of 1,000 to 1,100 °C (1,830 to 2,010 °F).
The word is also spelled aa, aʻa, ʻaʻa, and a-aa, and pronounced . It originates from Hawaiian where it is pronounced [ʔəˈʔaː], meaning ""stony rough lava"", but also to ""burn"" or ""blaze"".


=== Pāhoehoe ===

Pāhoehoe (; from Hawaiian [paːˈhoweˈhowe], meaning ""smooth, unbroken lava""), also spelled pahoehoe, is basaltic lava that has a smooth, billowy, undulating, or ropy surface. These surface features are due to the movement of very fluid lava under a congealing surface crust. The Hawaiian word was introduced as a technical term in geology by Clarence Dutton.
A pāhoehoe flow typically advances as a series of small lobes and toes that continually break out from a cooled crust. It also forms lava tubes where the minimal heat loss maintains low viscosity. The surface texture of pāhoehoe flows varies widely, displaying all kinds of bizarre shapes often referred to as lava sculpture. With increasing distance from the source, pāhoehoe flows may change into ʻaʻā flows in response to heat loss and consequent increase in viscosity. Pahoehoe lavas typically have a temperature of 1,100 to 1,200 °C (2,010 to 2,190 °F).
On the Earth, most lava flows are less than 10 km (6.2 mi) long, but some pāhoehoe flows are more than 50 km (31 mi) long.
The rounded texture makes pāhoehoe a poor radar reflector, and is difficult to see from an orbiting satellite (dark on Magellan picture).


=== Block lava flows ===
Block lava flows are typical of andesitic lavas from stratovolcanoes. They behave in a similar manner to ʻaʻā flows but their more viscous nature causes the surface to be covered in smooth-sided angular fragments (blocks) of solidified lava instead of clinkers. Like in ʻaʻā flows, the molten interior of the flow, which is kept insulated by the solidified blocky surface, overrides the rubble that falls off the flow front. They also move much more slowly downhill and are thicker in depth than ʻaʻā flows.


=== Domes and coulées ===
Lava domes and coulées are associated with felsic lava flows ranging from dacite to rhyolite. The very viscous nature of these lava cause them to not flow far from the vent, causing the lava to form a lava dome at the vent. When a dome forms on an inclined surface it can flow in short thick flows called coulées (dome flows). These flows often travel only a few kilometers from the vent.


=== Pillow lava ===

Pillow lava is the lava structure typically formed when lava emerges from an underwater volcanic vent or subglacial volcano or a lava flow enters the ocean. However, pillow lava can also form when lava is erupted beneath thick glacial ice. The viscous lava gains a solid crust on contact with the water, and this crust cracks and oozes additional large blobs or ""pillows"" as more lava emerges from the advancing flow. Since water covers the majority of Earth's surface and most volcanoes are situated near or under bodies of water, pillow lava is very common.


== Lava landforms ==
Because it is formed from viscous molten rock, lava flows and eruptions create distinctive formations, landforms and topographical features from the macroscopic to the microscopic.


=== Volcanoes ===

Volcanoes are the primary landforms built by repeated eruptions of lava and ash over time. They range in shape from shield volcanoes with broad, shallow slopes formed from predominantly effusive eruptions of relatively fluid basaltic lava flows, to steeply-sided stratovolcanoes (also known as composite volcanoes) made of alternating layers of ash and more viscous lava flows typical of intermediate and felsic lavas.
A caldera, which is a large subsidence crater, can form in a stratovolcano, if the magma chamber is partially or wholly emptied by large explosive eruptions; the summit cone no longer supports itself and thus collapses in on itself afterwards. Such features may include volcanic crater lakes and lava domes after the event. However, calderas can also form by non-explosive means such as gradual magma subsidence. This is typical of many shield volcanoes.


=== Cinder and spatter cones ===

Cinder cones and spatter cones are small-scale features formed by lava accumulation around a small vent on a volcanic edifice. Cinder cones are formed from tephra or ash and tuff which is thrown from an explosive vent. Spatter cones are formed by accumulation of molten volcanic slag and cinders ejected in a more liquid form.


=== Kīpukas ===

Another Hawaiian English term derived from the Hawaiian language, a kīpuka denotes an elevated area such as a hill, ridge or old lava dome inside or downslope from an area of active volcanism. New lava flows will cover the surrounding land, isolating the kīpuka so that it appears as a (usually) forested island in a barren lava flow.


=== Lava domes ===

Lava domes are formed by the extrusion of viscous felsic magma. They can form prominent rounded protuberances, such as at Valles Caldera. As a volcano extrudes silicic lava, it can form an inflation dome, gradually building up a large, pillow-like structure which cracks, fissures, and may release cooled chunks of rock and rubble. The top and side margins of an inflating lava dome tend to be covered in fragments of rock, breccia and ash.
Examples of lava dome eruptions include the Novarupta dome, and successive lava domes of Mount St Helens.


=== Lava tubes ===

Lava tubes are formed when a flow of relatively fluid lava cools on the upper surface sufficiently to form a crust. Beneath this crust, which being made of rock is an excellent insulator, the lava can continue to flow as a liquid. When this flow occurs over a prolonged period of time the lava conduit can form a tunnel-like aperture or lava tube, which can conduct molten rock many kilometres from the vent without cooling appreciably. Often these lava tubes drain out once the supply of fresh lava has stopped, leaving a considerable length of open tunnel within the lava flow.
Lava tubes are known from the modern day eruptions of Kīlauea, and significant, extensive and open lava tubes of Tertiary age are known from North Queensland, Australia, some extending for 15 kilometres (9 miles).


=== Lava fountains ===

A lava fountain is a volcanic phenomenon in which lava is forcefully but non-explosively ejected from a crater, vent, or fissure. The highest lava fountains recorded were during the 1999 eruption of Mount Etna in Italy, which reached heights of 2,000 m (6,562 ft). However, lava fountains observed during Mount Vesuvius' 1779 eruption are believed to have reached at least 3,000 m (9,843 ft). Lava fountains may occur as a series of short pulses, or a continuous jet of lava. They are commonly associated with Hawaiian eruptions.


=== Lava lakes ===

Rarely, a volcanic cone may fill with lava but not erupt. Lava which pools within the caldera is known as a lava lake. Lava lakes do not usually persist for long, either draining back into the magma chamber once pressure is relieved (usually by venting of gases through the caldera), or by draining via eruption of lava flows or pyroclastic explosion.
There are only a few sites in the world where permanent lakes of lava exist. These include:
Mount Erebus, Antarctica
Puʻu ʻŌʻō and Halemaumau Crater on Kilauea volcano, Hawaii
Erta Ale, Ethiopia
Nyiragongo, Democratic Republic of Congo
Ambrym, Vanuatu.


=== Lava delta ===

Lava deltas form wherever sub-aerial flows of lava enter standing bodies of water. The lava cools and breaks up as it encounters the water, with the resulting fragments filling in the seabed topography such that the sub-aerial flow can move further offshore. Lava deltas are generally associated with large-scale, effusive type basaltic volcanism.


== Hazards ==
Lava flows are enormously destructive to property in their path. However, casualties are rare since flows are usually slow enough for people and animals to escape, though this is dependent on the viscosity of the lava. Nevertheless, injuries and deaths have occurred, either because they had their escape route cut off, because they got too close to the flow or, more rarely, if the lava flow front travels too quickly. This notably happened during the eruption of Nyiragongo in Zaire (now Democratic Republic of the Congo). On the night of 10 January 1977 a crater wall was breached and a fluid lava lake drained out in under an hour. The resulting flow sped down the steep slopes at up to 100 km/h (62 mph), and overwhelmed several villages while residents were asleep. As a result of this disaster, the mountain was designated a Decade Volcano in 1991.
Deaths attributed to volcanoes frequently have a different cause, for example volcanic ejecta, pyroclastic flow from a collapsing lava dome, lahars, poisonous gases that travel ahead of lava, or explosions caused when the flow comes into contact with water. A particularly dangerous area is called a lava bench. This very young ground will typically break-off and fall into the sea.
Areas of recent lava flows continue to represent a hazard long after the lava has cooled. Where young flows have created new lands, land is more unstable and can break-off into the sea. Flows often crack deeply, forming dangerous chasms, and a fall against 'a'a lava is similar to falling against broken glass. Rugged hiking boots, long pants, and gloves are recommended when crossing lava flows.
Diverting a lava flow is extremely difficult, but it can be accomplished in some circumstances, as was once partially achieved in Vestmannaeyjar, Iceland.


== Towns destroyed by lava flows ==

Kalapana, Hawaii was destroyed by the eruption of the Kīlauea volcano in 1990. (abandoned)
Koae and Kapoho, Hawaii were both destroyed by the same eruption of Kīlauea in January, 1960. (abandoned)
Keawaiki, Hawaii 1859 (abandoned)
San Sebastiano al Vesuvio, Italy Destroyed in 1944 by the most recent eruption of Mount Vesuvius during the Allies' occupation of southern Italy. (rebuilt)
Cagsawa, Philippines buried by lava erupted from Mayon Volcano in 1814.
The Nisga'a villages of Lax Ksiluux and Wii Lax K'abit in northwestern British Columbia, Canada were destroyed by thick lava flows during the eruption of Tseax Cone in the 1700s.


== Towns damaged by lava flows ==
Catania, Italy, in the eruption of Mount Etna in 1669 (rebuilt)
Goma, Democratic Republic of Congo, in the eruption of Nyiragongo in 2002
Heimaey, Iceland, in the 1973 Eldfell eruption (rebuilt)
Royal Gardens, Hawaii, by the eruption of Kilauea in 1986–87 (abandoned)
Parícutin (village after which the volcano was named) and San Juan Parangaricutiro, Mexico, by Parícutin from 1943 to 1952.
Sale'aula, Samoa, by eruptions of Mt Matavanu between 1905 and 1911.


== Towns destroyed by tephra ==
Tephra is volcanic ash, lapilli, volcanic bombs or volcanic blocks.
Pompeii, Italy in the eruption of Mount Vesuvius in 79 AD
Herculaneum, Italy in the eruption of Mount Vesuvius in 79 AD
Sumbawa Island, Indonesia in the eruption of Mount Tambora in 1815 AD
Cerén, El Salvador in the eruption of Ilopango between 410 and 535 AD
Plymouth, Montserrat, in 1995. Plymouth was the capital and only port of entry for Montserrat and had to be completely abandoned, along with over half of the island. It is still the de jure capital.


== References ==

 This article incorporates text from a publication now in the public domain: Chisholm, Hugh, ed. (1911). ""Lava"". Encyclopædia Britannica (11th ed.). Cambridge University Press. 


== External links ==
USGS definition of ʻAʻā
USGS definition of Pāhoehoe
USGS definition of Ropy Pāhoehoe
Volcanic landforms of Hawaii
USGS hazards associated with lava flows
Hawaiian Volcano Observatory Volcano Watch newsletter article on Nyiragongo eruptions, 31 January 2002
National Geographic lava video Retrieved 23 August 2007"
783,Semantic computing,23754861,1618,"Semantic computing is a field of computing that combines elements of semantic analysis, natural language processing, data mining and related fields.
Semantic computing addresses three core problems:
Understanding the (possibly naturally-expressed) intentions (semantics) of users and expressing them in a machine-processable format
Understanding the meanings (semantics) of computational content (of various sorts, including, but is not limited to, text, video, audio, process, network, software and hardware) and expressing them in a machine-processable format
Mapping the semantics of user with that of content for the purpose of content retrieval, management, creation, etc.
The IEEE has held an International Conference on Semantic Computing since 2007.


== See also ==
Computational semantics
Semantic audio
Semantic compression
Semantic Computing, eds. P. Sheu, H. Yu, C.V. Ramamoorthy, A. Joshi and L.A. Zadeh, IEEE/Wiley, 2010


== References ==


== External links ==
IEEE International Conference on Semantic Computing
IEEE International School on Semantic Computing
International Journal of Semantic Computing
Institute for Semantic Computing
Semantic Computing Research Group
Semantic Link Network"
784,Computational X,52286308,1611,"Computational X is a term used to describe the various fields of study that have emerged from the applications of informatics and big data to specific disciplines. Examples include computational biology, computational neuroscience, computational physics, and computational linguistics.


== See also ==
Computational thinking


== References ==
From computational science to Internetics: Integration of science with computer science by Geoffrey C. Fox, Computational Science, Mathematics, and Software: Proceedings of the International Symposium on Computational Science in Celebration of the 65th Birthday of John R. Rice, eds. Ronald F. Boisvert, Elias N. Houstis, Purdue University Press, 2002, 22-26 May, 1999, Volume 1.
Computing the Future: A Broader Agenda for Computer Science and Engineering by Committee to Assess the Scope and Direction of Computer Science and Technology, Computer Science and Telecommunications Board, National Research Council, 1992.
Scientific Basis for Computational Science by Raúl Valdés-Pérez, 1993.
How to Teach Computational Thinking by Stephen Wolfram, September 7, 2016."
785,SIGSAM,5903241,1610,"SIGSAM is the ACM Special Interest Group on Symbolic and Algebraic Manipulation. It publishes the ACM Communications in Computer Algebra and often sponsors the International Symposium on Symbolic and Algebraic Computation (ISSAC).


== External links ==
ACM Official SIGSAM web site
ISSAC 2009, Seoul, Korea
ISSAC 2008, (""RISC Linz""), Hagenberg, Austria
ISSAC 2007, Waterloo, Ontario
ISSAC 2006, Genoa
ISSAC 2005, Beijing
ISSAC 2004, Santander, Cantabria
ISSAC 2003, Philadelphia
ISSAC 2002, Lille
ISSAC 2001, London, Ontario
ISSAC 2000, St. Andrews
ISSAC 1999, Vancouver
ISSAC 1998, Rostock
ISSAC 1997, Maui"
786,ALF (proof assistant),32680774,1605,"ALF (""Another logical framework"") is a structure editor for monomorphic Martin-Löf type theory developed at Chalmers University. It is a predecessor of the Alfa, Agda, Cayenne and Coq proof assistants and dependently typed programming languages. It was the first language to support inductive families and dependent pattern matching.


== References ==


== Further reading ==
Lena Magnusson and Bengt Nordström. ""The ALF proof editor and its proof engine"".
Thorsten Altenkirch, Veronica Gaspes, Bengt Nordström and Björn von Sydow. ""A user's guide to ALF"".


== External links ==
Alfa"
787,Program comprehension,3733920,1604,"Program comprehension (""program understanding,"" ""source code comprehension"") is a domain of computer science concerned with the ways software engineers maintain existing source code. The cognitive and other processes involved are identified and studied. The results are used to develop tools and training.
Software maintenance tasks have five categories: adaptive maintenance, corrective maintenance, perfective maintenance, code reuse, and code leverage.


== Theories of program comprehension ==
Titles of works on program comprehension include
Using a behavioral theory of program comprehension in software engineering
The concept assignment problem in program understanding, and
Program Comprehension During Software Maintenance and Evolution.
Computer scientists pioneering program comprehension include Ruven Brooks, Ted J. Biggerstaff, and Anneliese von Mayrhauser.


== Organizations ==
The International Conference on Program Comprehension [1] is dedicated to research on program comprehension.


== See also ==
Program analysis (computer science)
Program slicing"
788,Commentz-Walter algorithm,33386298,1604,"In computer science, the Commentz-Walter algorithm is a string searching algorithm invented by Beate Commentz-Walter. Like the Aho–Corasick string matching algorithm, it can search for multiple patterns at once. It combines ideas from Aho–Corasick with the fast matching of the Boyer–Moore string search algorithm. For a text of length n and maximum pattern length of m, its worst-case running time is O(mn), though the average case is often much better.
GNU grep implements a string matching algorithm very similar to Commentz-Walter.


== References ==


== External links ==
http://www.hs-albsig.de/studium/wirtschaftsinformatik/Documents/commentzwalterextab.pdf"
789,LINDO,43628228,1600,"LINDO (Linear, Interactive, and Discrete Optimizer) is a software package for linear programming, integer programming, nonlinear programming, stochastic programming and global optimization.
Lindo also creates ""What'sBest!"" which is an add-in for linear, integer and nonlinear optimization. First released for Lotus 1-2-3 and later also for Microsoft Excel.


== References ==


== External links ==
Official website [1]"
790,International Symposium on Memory Management,31172163,1598,"The International Symposium on Memory Management (ISMM) is an ACM SIGPLAN symposium on memory management. Before becoming a conference it was known as the International Workshop on Memory Management (IWMM).


== History ==
1992: http://www.informatik.uni-trier.de/~ley/db/conf/iwmm/iwmm92.html
1995: http://www.informatik.uni-trier.de/~ley/db/conf/iwmm/iwmm95.html
1998: https://www.sfu.ca/~burton/ismm98.html
2000: http://www.cs.kent.ac.uk/events/conf/2000/ismm2000/
2002: http://www.hpl.hp.com/personal/Hans_Boehm/ismm/
2004: http://www.research.ibm.com/ismm04/
2006: http://www.cs.technion.ac.il/~erez/ismm06/
2007: http://www.eecs.harvard.edu/~greg/ismm07/
2008: http://www.cs.kent.ac.uk/~rej/ismm2008
2009: http://sysrun.haifa.il.ibm.com/hrl/ISMM2009/
2010: http://www.cs.purdue.edu/ISMM10/
2011: http://www.hpl.hp.com/personal/Hans_Boehm/ismm11/
2012: http://ismm12.cs.purdue.edu/
2013: http://www.cs.technion.ac.il/~erez/ismm13/
2014: http://ismm2014.cs.tufts.edu/
2015: http://conf.researchr.org/home/ismm-2015
2016: http://conf.researchr.org/home/ismm-2016


== External links ==
SIGPLAN
ACM
DBLP"
791,Nullary constructor,8364397,1594,"In computer programming, a nullary constructor is a constructor that takes no arguments. Also known as a 0-argument constructor or no-argument constructors.


== Object-oriented constructors ==
In object-oriented programming, a constructor is code that is run when an object is created. Default constructors of objects are usually nullary.


=== Java example ===


== Algebraic data types ==
In algebraic data types, a constructor is one of many tags that wrap data. If a constructor does not take any data arguments, it is nullary.


=== Haskell example ==="
792,EACSL,38979342,1591,"The European Association for Computer Science Logic (EACSL), founded 14 July 1992, is an international professional non-profit organization representing the interests of its members and promoting computer science logic in the areas of scientific research and education. It supports both basic and application oriented research to advance the connections between basic research and industrial applications. The current president is Prof. Anuj Dawar (University of Cambridge, UK).
Each year, the EACSL organizes the international conference Computer Science Logic (CSL)  and publishes the associated proceedings, it supports several workshops and summer schools  and sponsors the Ackermann Award, the EACSL Outstanding Dissertation Award for Logic in Computer Science. The annual general meeting of members takes place each year during the annual international conference CSL.


== References ==


== External links ==
[4] — EACSL official home page."
793,Cristina Lopes,20779471,1588,"Cristina Videira Lopes is a Professor of Informatics and Computer Science at University of California, Irvine. Prior to being a professor, she was a Research Scientist at the Xerox Palo Alto Research Center (PARC). While at PARC, she was most known as a founder of the group that developed Aspect-Oriented Programming (AOP) and started aspectj.org. More recently, she has been working in ubiquitous computing, with a focus in communication mechanisms that are pervasive, secure and intuitive for humans to perceive and interact with.


== List of publications ==
Papers by Cristina Lopes
Videira Lopes, Cristina (2014). Exercises in Programming Style. Chapman and Hall/CRC. ISBN 978-1482227376. 


== References =="
794,Typed assembly language,5115749,1572,"In computer science, a typed assembly language (TAL) is an assembly language that is extended to include a method of annotating the datatype of each value that is manipulated by the code. These annotations can then be used by a program (type checker) that processes the assembly language code in order to analyse how it will behave when it is executed. Specifically, such a type checker can be used to prove the type safety of code that meets the criteria of some appropriate type system.
Typed assembly languages usually include a high-level memory management system based on garbage collection.
A typed assembly language with a suitably expressive type system can be used to enable the safe execution of untrusted code without using an intermediate representation like bytecode, allowing features similar to those currently provided by virtual machine environments like Java and .NET.


== See also ==
Proof-carrying code


== Further reading ==
Greg Morrisett. ""Typed assembly language"" in Advanced Topics in Types and Programming Languages. Editor: Benjamin C. Pierce.


== External links ==
TALx86, a research project from Cornell University which has implemented a typed assembler for the Intel IA-32 architecture."
795,Voronoi pole,28412014,1560,"In geometry, the positive and negative Voronoi poles of a cell in a Voronoi diagram are certain vertices of the diagram.


== Definition ==
Let 
  
    
      
        
          V
          
            p
          
        
      
    
    {\displaystyle V_{p}}
   be the Voronoi cell of the site 
  
    
      
        p
        ∈
        P
      
    
    {\displaystyle p\in P}
  . If 
  
    
      
        
          V
          
            p
          
        
      
    
    {\displaystyle V_{p}}
   is bounded then its positive pole is the Voronoi vertex in 
  
    
      
        
          V
          
            p
          
        
      
    
    {\displaystyle V_{p}}
   with maximal distance to the sample point 
  
    
      
        p
      
    
    {\displaystyle p}
  . Furthermore, let 
  
    
      
        
          
            
              u
              ¯
            
          
        
      
    
    {\displaystyle {\bar {u}}}
   be the vector from 
  
    
      
        p
      
    
    {\displaystyle p}
   to the positive pole. If the cell is unbounded, then a positive pole is not defined, and 
  
    
      
        
          
            
              u
              ¯
            
          
        
      
    
    {\displaystyle {\bar {u}}}
   is defined to be a vector in the average direction of all unbounded Voronoi edges of the cell.
The negative pole is the Voronoi vertex 
  
    
      
        v
      
    
    {\displaystyle v}
   in 
  
    
      
        
          V
          
            p
          
        
      
    
    {\displaystyle V_{p}}
   with the largest distance to 
  
    
      
        p
      
    
    {\displaystyle p}
   such that the vector 
  
    
      
        
          
            
              u
              ¯
            
          
        
      
    
    {\displaystyle {\bar {u}}}
   and the vector from 
  
    
      
        p
      
    
    {\displaystyle p}
   to 
  
    
      
        v
      
    
    {\displaystyle v}
   make an angle larger than 
  
    
      
        
          
            π
            2
          
        
      
    
    {\displaystyle {\frac {\pi }{2}}}
  .


== Example ==

Here 
  
    
      
        x
      
    
    {\displaystyle x}
   is the positive pole of 
  
    
      
        
          V
          
            p
          
        
      
    
    {\displaystyle V_{p}}
   and 
  
    
      
        y
      
    
    {\displaystyle y}
   its negative. As the cell corresponding to 
  
    
      
        q
      
    
    {\displaystyle q}
   is unbounded only the negative pole 
  
    
      
        z
      
    
    {\displaystyle z}
   exists.


== References ==
Boissonnat, Jean-Daniel (2007). Effective Computational Geometry for Curves and Surfaces. Berlin: Springer. ISBN 978-3-540-33258-9."
796,Sequence step algorithm,14206817,1557,"A sequence step algorithm (SQS-AL) is an algorithm implemented in a discrete event simulation system to maximize resource utilization. This is achieved by running through two main nested loops: A sequence step loop and a replication loop. For each sequence step, each replication loop is a simulation run that collects crew idle time for activities in that sequence step. The collected crew idle times are then used to determine resource arrival dates for user-specified confidence levels. The process of collecting the crew idle times and determining crew arrival times for activities on a considered sequence step is repeated from the first to the last sequence step.


== See also ==
Linear scheduling method


== Further reading ==
Photios G. Ioannou and Chachrist Srisuwanrat Sequence Step Algorithm for Continuous Resource Utilization in Probabilistic Repetitive Projects
Chachrist Srisuwanrat and Photios G. Ioannou The Investigation of Lead-Time Buffering under Uncertainty Using Simulation and Cost Optimization"
797,Satplan,2955843,1557,"Satplan (better known as Planning as Satisfiability) is a method for automated planning. It converts the planning problem instance into an instance of the Boolean satisfiability problem, which is then solved using a method for establishing satisfiability such as the DPLL algorithm or WalkSAT.
Given a problem instance in planning, with a given initial state, a given set of actions, a goal, and a horizon length, a formula is generated so that the formula is satisfiable if and only if there is a plan with the given horizon length. This is similar to simulation of Turing machines with the satisfiability problem in the proof of Cook's theorem. A plan can be found by testing the satisfiability of the formulas for different horizon lengths. The simplest way of doing this is to go through horizon lengths sequentially, 0, 1, 2, and so on.


== See also ==
Graphplan


== References ==
H. A. Kautz and B. Selman (1992). Planning as satisfiability. In Proceedings of the Tenth European Conference on Artificial Intelligence (ECAI'92), pages 359-363.
H. A. Kautz and B. Selman (1996). Pushing the envelope: planning, propositional logic, and stochastic search. In Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI'96), pages 1194-1201.
J. Rintanen (2009). Planning and SAT. In A. Biere, H. van Maaren, M. Heule and Toby Walsh, Eds., Handbook of Satisfiability, pages 483-504, IOS Press."
798,Secure Operations Language,4536722,1553,"The Secure Operations Language (SOL) was developed jointly by the United States Naval Research Laboratory and Utah State University in the United States. SOL is a domain-specific synchronous programming language for developing distributed applications and is based on software engineering principles developed in the Software Cost Reduction project at the Naval Research Laboratory in the late 1970s and early 1980s. SOL is intended to be a domain-specific language for developing service-based systems. Concurrently, a domain-specific extension of Java (SOLj) is being developed (FTDCS 2007) Application domains include sensor networks, defense and space systems, healthcare delivery, power control, etc.
The investigators of the project are Dr. Ramesh Bharadwaj from the Naval Research Laboratory and Dr. Supratik Mukhopadhyay from Utah State University.


== References ==
Bharadwaj, Ramesh (2002). ""SOL: A Verifiable Synchronous Language for Reactive Systems"". Electronic Notes in Theoretical Computer Science. 65 (5): 140. doi:10.1016/S1571-0661(05)82565-4."
799,Disruptor (software),42963753,1552,"Disruptor is a library for the Java programming language that provides a concurrent ring buffer data structure of the same name, developed at LMAX Exchange. It is designed to provide a low-latency, high-throughput work queue in asynchronous event processing architectures. It ensures that any data is owned by only one thread for write access, therefore reducing write contention compared to other structures.


== See also ==

Concurrent data structure


== References ==


== External links ==
https://lmax-exchange.github.io/disruptor/
https://bitbucket.org/hmbd/hmbdc-rel/overview"
800,Astronomical algorithm,853888,1549,"Astronomical algorithms are the algorithms used to calculate ephemerides, calendars, and positions (as in celestial navigation or satellite navigation). Examples of large and complex astronomical algorithms are those used to calculate the position of the Moon. A simple example is the calculation of the Julian day.
Numerical model of solar system discusses a generalized approach to local astronomical modeling. The variations séculaires des orbites planétaires describes an often used model.


== See also ==
Astrodynamics
Celestial mechanics
Charge-coupled device (a data-collecting device that is sometimes aimed at the sky and requires algorithms to process its output)
Doomsday rule
List of algorithms
List of astronomical objects
Meeus, Jean
Transformation from spherical coordinates to rectangular coordinates
An implementation of Astronomical Algorithms. Calculate MJD, Equation of Time and Solar Declination in Excel, CAD or your other programs. The Sun API is free and extremely accurate. For Windows Computers.
astronomy API An API to the Universe"
801,Pattern directed invocation programming language,2076159,1547,"In computer science, pattern-directed invocation programming languages are programming languages in which procedures are invoked indirectly by specifying a pattern for a set of procedures as opposed to specifying one directly by name, pointer, URL, etc., as in conventional programming. These languages were initiated by Planner which featured high level plans invoked by patterns for assertions and goals. They include logic programming languages such as Prolog as a special case. Ether was a pattern-directed invocation language introduced in the Scientific Community Metaphor.
Pattern-directed invocation is a commonly used and more practical alternative to first-order theorem proving in automated theorem proving, and is the primary method of implementing a blackboard system for automated reasoning in general. In it, a database of facts (a la Prolog) is augmented with a set of procedures called demons. A demon is automatically invoked whenever a term matching the demon's pattern is added to the database.
Pattern-directed invocation is related to rule-based programming.


== Notes =="
802,Qualified Security Assessor,18060754,1546,"Qualified Security Assessor (QSA) is a designation conferred by the PCI Security Standards Council to those individuals that meet specific information security education requirements, have taken the appropriate training from the PCI Security Standards Council, are employees of a Qualified Security Assessor (QSA) company approved PCI security and auditing firm, and will be performing PCI compliance assessments as they relate to the protection of credit card data.
The term QSA can be implied to identify an individual qualified to perform payment card industry compliance auditing and consulting or the firm itself. QSA companies are sometimes differentiated from QSA individuals by the initialism 'QSAC'.
The primary goal of an individual with the PCI QSA certification is to perform an assessment of a firm that handles credit card data against the high-level control objectives of the PCI Data Security Standard (PCI DSS).


== References ==


== External links ==
PCI Security Standards Council"
803,Occam-π,2079775,1544,"In computer science, occam-π (or occam-pi) is the name of a variant of the programming language occam developed by the Kent Retargetable occam Compiler (KRoC) team at the University of Kent. The name reflects the introduction of elements of π-calculus into occam, especially concepts involving mobile agents (processes) and data. The language contains several extensions to occam 2.1, including:
Nested protocols
Run-time process creation
Mobile channels, data, and processes
Recursion
Protocol inheritance
Array constructors
Extended rendezvous


== See also ==
occam (programming language)
Transputer
KRoC
Transterpreter


== References ==


== External links ==
Official website
University of Kent Occam-pi project page
Tock Occam compiler
Parallel programming users group on Occam-pi"
804,Database-as-IPC,23170217,1544,"In computer programming, Database-as-IPC is an anti-pattern where a database is used as the message queue for routine interprocess communication in a situation where a lightweight IPC mechanism such as sockets would be more suitable. British computer scientist, Junade Ali, defined the Database-as-IPC Anti-Pattern as using a database to ""schedule jobs or queue up tasks to be completed"", noting that this anti-pattern centres around using a database for temporary messages instead of persistent data.
Using a database for this kind of message passing is extremely inefficient compared to other IPC methods and often introduces serious long-term maintenance issues, but this method enjoys a measure of popularity because the database operations are more widely understood than ""proper"" IPC mechanisms.


== References =="
805,Block contention,21675914,1543,"In database management systems, block contention refers to multiple processes or instances competing for access to the same index or data block at the same time. In general this can be caused by very frequent index or table scans, or frequent updates. Concurrent statement executions by two or more instances may also lead to contention, and subsequently busy waiting for the process without the lock.


== Solutions ==
To reduce contention for table blocks due to delete, select or update statements, reduce the number of rows per block. This can be done by using a smaller block size.
To reduce contention for table blocks due to insert statements, increase the number of freelists, or buffer frames.
To reduce contention for index blocks the best strategy is to implement a reverse index.
In most situations the goal is to spread queries over a greater number of blocks, to avoid concentrating on any single one.


== References =="
806,Code rate,7981806,1541,"In telecommunication and information theory, the code rate (or information rate) of a forward error correction code is the proportion of the data-stream that is useful (non-redundant). That is, if the code rate is k/n, for every k bits of useful information, the coder generates a total of n bits of data, of which n-k are redundant.
If R is the gross bitrate or data signalling rate (inclusive of redundant error coding), the net bitrate (the useful bit rate exclusive of error-correction codes) is ≤ R•k/n.
For example: The code rate of a convolutional code will typically be 1/2, 2/3, 3/4, 5/6, 7/8, etc., corresponding to one redundant bit inserted after every single, second, third, etc., bit. The code rate of the Reed Solomon block code denoted RS(204,188) is 188/204, meaning that 204 - 188 = 16 redundant bits are added to each block of 188 bits of useful information.
A few error correction codes do not have a fixed code rate—rateless erasure codes.
Note that bit/s is a more widespread unit of measurement for the information rate, implying that it is synonymous with net bit rate or useful bit rate exclusive of error-correction codes.


== See also ==
Information rate
Source information rate (Entropy rate)


== References =="
807,Fragmented distribution attack,25594776,1540,"Fragmented distribution attack in computer security is a malware or virus distribution technique aiming at bypassing protection systems by sending fragments of code over the network.
This technique has been first described in a paper published on Virus Bulletin 2009 annual conference by Anoirel Issa, malware Analyst for the Symantec Hosted Services, formerly MessageLabs.


== Method of attack ==
A malware is split into several fragments and are embedded in an innocent file, and these segments are sent over a protected network. The fragmented malware successfully bypasses firewalls, IDS and anti-virus undetected, then is re-assembled on victim's system. The re-assembler is a separate program, which is not necessarily a malware thus can evade security measures, locates malware fragment carriers and pre-assemble the malware in memory. The re-assembler may write the code to disk then executes the re-assembled code on either in memory or on disk.


== Consequences ==
If successfully achieved, an FDA attack can result to some serious consequences depends on the victim's level of protection. Consequence not easily predictable but can lead to:
Data, intellectual property leakage
Government, military, industrial espionage
Irreversible financial losses


== External links ==
Virus bulletin conference White paper"
808,Driver wrapper,1774245,1537,"A driver wrapper is software that functions as an adapter between an operating system and a driver, such as a device driver, that was not designed for that operating system. It can enable the use of devices for which no drivers for the particular operating system are available. In particular, as of 2010 Microsoft Windows is the dominant family of operating systems for IBM PC compatible computers, and many devices are supplied with drivers for Windows but not other operating systems.


== Windows driver wrappers for Linux ==
Several open source software projects allow using Microsoft Windows drivers under another operating system, such as Linux.
Examples include network drivers for wireless cards (such as NDISwrapper for Linux or Project Evil for FreeBSD) and the NTFS file system (see Captive NTFS).
The common thread among these examples is the use of wrapper technology, which allows execution of the drivers in a foreign environment. Limitations for driver wrappers include the lack of an ability to function at real time. An example of this limitation includes latency problems as those associated with attempts to make compatible with Linux the ZoomR16 audio DAW sound recorder and control surface.


== See also ==


== References =="
809,Activity-centered design,29451101,1534,"Activity-centered design (ACD) is an extension of the Human-centered design paradigm in interaction design. ACD features heavier emphasis on the activities that a user would perform with a given piece of technology. ACD has its theoretical underpinnings in activity theory, from which activities can be defined as actions taken by a user to achieve a goal.
When working with activity-centered design, the designers use research to get insights of the users. Observations and interviews are typical approaches to learn more about the users' behavior. By mapping users' activities and tasks, the designer may notice missing tasks for the activity to become more easy to perform, and thus design solutions to accomplish those tasks.


== References ==
Saffer, Dan. 2010. Designing for interaction.
Gay, Geri and Helene Hembrooke. 2004. Activity-Centered Design: An Ecological Approach to Designing Smart Tools and Usable Systems.
Norman, Don. 2015. The Design of Everyday Things: Revised and Expanded Edition.


== Notes =="
810,Superior (hierarchy),307680,1532,"In a hierarchy or tree structure of any kind, a superior is an individual or position at a higher level in the hierarchy than another (a ""subordinate"" or ""inferior""), and thus closer to the apex. In business, superiors are people who are supervisors and in the military, superiors are people who are higher in the chain of command (superior officer). Superiors are given, sometimes supreme, authority over others under their command. When an order is given, one must follow that order and obey it or punishment may be issued.
A Religious Superior is the person to whom a cleric is immediately responsible under canon law. For monks, it would be the Abbot (or the Abbess for nuns); for friars, it would be the Prior, or, for Franciscans, the Guardian (Custos), for Minims, the Corrector; for diocesan priests, it would be the local Bishop. In religious orders with a hierarchy above the local community, there will also be superiors general and possibly provincial superiors above the local abbot, prior, or Mother Superior. The priest in charge a ""mission sui iuris"" is called an ecclesiastical superior.


== See also ==
Parent node
Seniority"
811,Fuzzy transportation,15639656,1530,"The aim of fuzzy transportation is to find the least transportation cost of some commodities through a capacitated network when the supply and demand of nodes and the capacity and cost of edges are represented as fuzzy numbers. This problem is a new branch in combinatorial optimization and network flow problems. Combinatorial algorithms can be provided to solve fuzzy transportation problem to find the fuzzy optimal flow(s). Such methods are capable of handling the decision maker's risk taking. Some application of such standpoint were presented in industries. Liu and Kao pursued this attempt to find better solution for this problem (Network flow problems with fuzzy arc lengths, IEEE Transactions on Systems, Man and Cybernetics Part B: Cybernetics, 34 (2004) 765-769).
It is interesting to check that which methods in traditional fuzzy optimization problem can be extended to combinatorial optimization problems e.g., transformation that they maintain the nice structure of problem. Then, valuable algorithms can be proposed for fuzzy combinatorial optimization to take the uncertainty of real problems into account.
By using fuzzy transportation, it is a reasonable attempt to find special solutions for hazardous material transportation because of the possibility of implementing the optimistic and pessimistic concepts into account."
812,Virtual Object System,3509339,1530,"The Virtual Object System (VOS) is a computer software technology for creating distributed object systems. The sites hosting Vobjects are typically linked by a computer network, such as a local area network or the Internet. Vobjects may send messages to other Vobjects over these network links (remotely) or within the same host site (locally) to perform actions and synchronize state. In this way, VOS may also be called an object-oriented remote procedure call system. In addition, Vobjects may have a number of directed relations to other Vobjects, which allows them to form directed graph data structures.
VOS is patent free, and its implementation is Free Software. The primary application focus of VOS is general purpose, multiuser, collaborative 3D virtual environments or virtual reality. The primary designer and author of VOS is Peter Amstutz.


== External links ==
Interreality.org official site"
813,Maximum common edge subgraph,36037560,1522,"Given two graphs 
  
    
      
        G
      
    
    {\displaystyle G}
   and 
  
    
      
        
          G
          ′
        
      
    
    {\displaystyle G'}
  , the maximum common edge subgraph problem is the problem of finding a graph 
  
    
      
        H
      
    
    {\displaystyle H}
   with as many edges as possible which is isomorphic to both a subgraph of 
  
    
      
        G
      
    
    {\displaystyle G}
   and a subgraph of 
  
    
      
        
          G
          ′
        
      
    
    {\displaystyle G'}
  .
The maximum common edge subgraph problem on general graphs is NP-complete as it is a generalization of subgraph isomorphism: a graph 
  
    
      
        H
      
    
    {\displaystyle H}
   is isomorphic to a subgraph of another graph 
  
    
      
        G
      
    
    {\displaystyle G}
   if and only if the maximum common edge subgraph of 
  
    
      
        G
      
    
    {\displaystyle G}
   and 
  
    
      
        H
      
    
    {\displaystyle H}
   has the same number of edges as 
  
    
      
        H
      
    
    {\displaystyle H}
  . Unless the two inputs 
  
    
      
        G
      
    
    {\displaystyle G}
   and 
  
    
      
        
          G
          ′
        
      
    
    {\displaystyle G'}
   to the maximum common edge subgraph problem are required to have the same number of vertices, the problem is APX-hard.


== See also ==
Maximum common subgraph isomorphism problem
Subgraph isomorphism problem
Induced subgraph isomorphism problem


== References =="
814,Episode (app),55302613,1519,"Episode is an interactive story app developed by American game developer Pocket Gems.


== References ==


== External links ==
Official website"
815,Operability,5393194,1519,"Operability is the ability to keep an equipment, a system or a whole industrial installation in a safe and reliable functioning condition, according to pre-defined operational requirements.
In a computing systems environment with multiple systems this includes the ability of products, systems and business processes to work together to accomplish a common task such as finding and returning availability of inventory for flight.
In the gas turbine engine business, engine operability is the ability of the engine to operate without compressor stall or surge, combustor flame-out or other power loss. Operability engineers work in the fields of engine and compressor modeling, control and test to ensure the engine meets its ignition, starting, acceleration, deceleration and over-speed requirements under the most extreme operating conditions.
Operability is considered one of the ilities and is closely related to reliability, supportability and maintainability.
Operability also refers to whether or not a surgical operation can be performed to treat a patient with a reasonable degree of safety and chance of success.


== References ==


== External links ==
Software Operability"
816,General-purpose modeling,5734285,1516,"General-purpose modelling (GPM) is the systematic use of a general-purpose modelling language to represent the various facets of an object or a system. Examples of GPM languages are:
The Unified Modelling Language (UML), an industry standard for modelling software-intensive systems
EXPRESS (ISO 10303-11), an international standard for the specification of data models
IDEF, a group of languages from the 1970s that aimed to be neutral, generic and reusable
Gellish, an industry standard natural language oriented modeling language for storage and exchange of data and knowledge, published in 2005
Lisp, a functional programming language designed for symbol processing, later extended with imperative abilities
XML, a data modelling language now beginning to be used to model code (MetaL, Microsoft .Net [1])
Contrast GPM languages with dedicated domain-specific modelling (DSM) languages, which like domain-specific languages (DSLs), are maturing and becoming a viable alternative to GPM languages.


== See also ==
Domain-specific modeling (DSM)
Model-driven engineering (MDE)
Unified Modelling Language (UML)
ISO 10303-11 EXPRESS
IDEF
LISP
Southbeach Notation
XML"
817,Mirror (programming),38035521,1515,"In computer programming, a mirror is a reflection mechanism that is completely decoupled from the object whose structure is being introspected. This is as opposed to traditional reflection, for example in Java, where one introspects an object using methods from the object itself (e.g. getClass()).
Mirrors adhere to the qualities of encapsulation, stratification and ontological correspondence.


== Benefits ==
Decoupling the reflection mechanism from the objects themselves allows for a few benefits:
The object's interface is not polluted, so there is no danger of breaking reflection by overriding a reflective method.
There can be different mirror systems.
The mirror system can be removed entirely (potentially allowing for optimizations).
A mirror system can operate on remote code, since it is not coupled with a particular machine.


== Languages that use mirrors ==
Dart, via its reflect function.
Scala
Swift, via its reflect function.


== References =="
818,Probabilistic analysis of algorithms,15383889,1514,"In analysis of algorithms, probabilistic analysis of algorithms is an approach to estimate the computational complexity of an algorithm or a computational problem. It starts from an assumption about a probabilistic distribution of the set of all possible inputs. This assumption is then used to design an efficient algorithm or to derive the complexity of a known algorithm.
This approach is not the same as that of probabilistic algorithms, but the two may be combined.
For non-probabilistic, more specifically, for deterministic algorithms, the most common types of complexity estimates are the average-case complexity (expected time complexity) and the almost always complexity. To obtain the average-case complexity, given an input distribution, the expected time of an algorithm is evaluated, whereas for the almost always complexity estimate, it is evaluated that the algorithm admits a given complexity estimate that almost surely holds.
In probabilistic analysis of probabilistic (randomized) algorithms, the distributions or averaging for all possible choices in randomized steps are also taken into an account, in addition to the input distributions.


== See also ==
Amortized analysis
Average-case complexity
Best, worst and average case
Random self-reducibility
Principle of deferred decision"
819,Service-oriented development of applications,2796616,1509,"In the field of software application development, service-oriented development of applications (or SODA) is a way of producing service-oriented architecture applications. Use of the term SODA was first used by the Gartner research firm.
SODA represents one possible activity for company to engage in when making the transition to service-oriented architecture (SOA). However, it has been argued that an overreliance on SODA can reduce overall system flexibility, reuse, and business agility. This danger is greater for sites that use an application server, which could diminish flexibility in redeployment and composition of services.


== See also ==
Enterprise service bus
Service-oriented modeling


== References ==


== External links ==
Gartner articles on the ROI aspects of SODA (Registration and fee required.)
Pillars of Service-Oriented development
What's the Big Deal About SOA"
820,COMMIT (SQL),46362717,1505,"A COMMIT statement in SQL ends a transaction within a relational database management system (RDBMS) and makes all changes visible to other users. The general format is to issue a BEGIN WORK statement, one or more SQL statements, and then the COMMIT statement. A COMMIT statement will also release any existing savepoints that may be in use. This means that once a COMMIT statement is issued, you can not rollback the transaction.
In terms of transactions, the opposite of commit is to discard the tentative changes of a transaction, a rollback.


== See also ==
Commit (data management)
Atomic commit
Two-phase commit protocol
Three-phase commit protocol"
821,Penny Crane Award for Distinguished Service,44419959,1497,"The Penny Crane Award for Distinguished Service is an award issued by the Association for Computing Machinery's Special Interest Group on University and College Computing Services. It was established in 2000 to recognise individuals who have made significant contributions to the Special Interest Group, and to computing in higher education.


== Recipients ==
2014 Cynthia Dooling
2013 Terris Wolff
2012 No Recipient
2011 Leila Lyonsi
2010 Lida Larsen
2009 Robert Paterson
2008 Jerry Smith
2007 Dennis Mar
2006 Jennifer Fajman
2005 J. Michael Yohe
2004 Linda Hutchison
2003 Russell Vaught
2002 John Bucher
2001 John H. (Jack) Esbin
2000 Jane Caviness


== See also ==
See Qualifications and Nominations page, at the ACM SIGUCCS Web Page.
Penny Crane Award Web Page at ACM/SIGUCCS
Penny Crane memory book


== References =="
822,Abox,4070216,1495,"In Computer Science, an ABox is an ""assertion component""—a fact associated with a terminological vocabulary within a knowledge base.
The terms ABox and TBox are used to describe two different types of statements in ontologies. TBox statements describe a system in terms of controlled vocabularies, for example, a set of classes and properties. ABox are TBox-compliant statements about that vocabulary.
TBox statements are sometimes associated with object-oriented classes and ABox statements associated with instances of those classes.
Together ABox and TBox statements make up a knowledge base.


== Examples of ABox and TBox statements ==
ABox statements typically have the form:

 A is an instance of B

or

 John is a Person

This should be contrasted with TBox statements (or statements about terminology) such as:

 All Students are Persons

or

 There are two types of Persons: Students and Teachers

TBox statements tend to be more permanent within a knowledge base and tend to be stored in a data model or a metadata registry. In contrast, ABox statements are much more dynamic in nature and tend to be stored as instance data within transactional systems within databases.


== See also ==
TBox
Description Logic#Modeling
metadata
Web Ontology Language"
823,Data-oriented parsing,9250822,1493,"Data-oriented parsing (DOP, also data-oriented processing) is a probabilistic model in computational linguistics. DOP was conceived by Remko Scha in 1990 with the aim of developing a performance-oriented grammar framework. Unlike other probabilistic models, DOP takes into account all subtrees contained in a treebank rather than being restricted to, for example, 2-level subtrees (like PCFGs).
Several variants of DOP have been developed. The initial version developed by Rens Bod was based on tree-substitution grammar, while more recently, DOP has been combined with lexical-functional grammar (LFG). The resulting DOP-LFG finds an application in machine translation. Other work on learning and parameter estimation for DOP has also found its way into machine translation.


== References ==
Remko Scha Research on DOP
DOP Homepage
Khalil Sima'an: Learning DOP models from treebanks; Computational Complexity
Andy Way (1999). A hybrid architecture for robust MT using LFG-DOP. Journal of Experimental and Theoretical Artificial Intelligence 11(3):441–471."
824,RRQR factorization,31372766,1486,"An RRQR factorization or rank-revealing QR factorization is a matrix decomposition algorithm based on the QR factorization which can be used to determine the rank of a matrix. The SVD can be used to generate an RRQR, but it is not an efficient method to do so. An RRQR implementation is available in MATLAB.


== References =="
825,Mark Weiser Award,48558046,1483,"The ACM SIGOPS (Special Interest Group on Operating Systems) Mark Weiser Award is awarded to an individual who has shown creativity and innovation in operating system research. The recipients began their career no earlier than 20 years prior to nomimation, and hence the award is analogical to the Fields Medal. The award was created in 2001 and is named after Mark Weiser, the father of ubiquitous computing.
The winners of this award have been:
2016: Antony Rowstron, Microsoft Research Cambridge
2015: Yuanyuan Zhou
2014: Eddie Kohler
2013: Stefan Savage
2012: Jeff Dean, Sanjay Ghemawat
2011: Miguel Castro
2010: Robert Tappan Morris
2009: Eric Brewer
2008: Peter Druschel
2007: Peter M. Chen
2006: Dawson Engler
2005: Thomas E. Anderson
2004: Brian N. Bershad
2003: Mike Burrows
2002: Mendel Rosenblum
2001: Frans Kaashoek


== See also ==
List of prizes, medals and awards
List of prizes named after people


== References =="
826,SDCH,48186564,1483,"SDCH (Shared Dictionary Compression for HTTP) is a data compression algorithm created by Google, based on VCDIFF (RFC 3284). It was supported natively in recent versions of Google Chrome, Chromium and Android, as well as on Google websites.
SDCH achieves its effectiveness by using pre-negotiated dictionaries to ""warm up"" its internal state prior to encoding or decoding. These may either be already stored locally, or uploaded from a source and then cached.
SDCH compression was removed from Google Chrome, and other Chromium products, in version 59.


== See also ==
Brotli
HTTP compression


== References =="
827,Semantic analysis (computational),29116756,1483,"Semantic analysis (computational) is a composite of the ""semantic analysis"" and the ""computational"" components.
""Semantic analysis"" refers to a formal analysis of meaning, and ""computational"" refer to approaches that in principle support effective implementation.


== See also ==
Computational semantics
Natural language processing
Semantic analytics
Semantic Web
SemEval


== References ==


== Further reading ==
Chris Fox (2010), Computational Semantics, In Alexander Clark, Chris Fox, and Shalom Lappin, editors. The Handbook of Computational Linguistics and Natural Language Processing. Wiley-Blackwellis.15:394–428
Agirre, Eneko, Lluis Marquez & Richard Wincentowski (2009), Computational Semantic analysis of language: SemEval-2007 and beyond, Language Resources and Evaluation 43(2):97-104"
828,Australian Partnership for Advanced Computing,12885090,1473,"The Australian Partnership for Advanced Computing was an Australian organisation established in 1998 to provide advanced computing and grid infrastructure for Australian research communities. In 2007 APAC was replaced with the National Computational Infrastructure.


== APAC Partners ==
AC3 - Australian Centre for Advanced Computing and Communications in NSW
ANU - The Australian National University
CSIRO
iVEC - Organisation for advanced computing in Western Australia
QCIF - Queensland Cyber Infrastructure Foundation
SAPAC - South Australian Partnership for Advanced Computing
TPAC - University of Tasmania acting as host for the Tasmanian Partnership for Advanced Computing
VPAC - Victorian Partnership for Advanced Computing


== See also ==
Backing Australia's Ability
Systemic Infrastructure Initiative


== External links ==
{{official website|https://web.archive.org/web/20090120211513/http://www.apac.edu.au/%7D%7D; Archive"
829,Global Document Type Identifier,23245152,1473,"The Global Document Type Identifier (GDTI) is part of the GS1 system of standards. It is a simple tool to identify a document by type and can identify documents uniquely where required.
The term “document” is applied broadly to cover any official or private papers that confer a right (e.g. a proof of ownership) or obligation (e.g., notification or call for military service) upon the bearer. The issuer of the document is normally responsible for all the information contained upon the document, both bar coded and Human Readable Interpretation. Such documents typically require storage of the appropriate information contained on the document. Examples of the kind of documents that could have a GDTI are tax demands, proof of shipment forms, insurance policies, internal invoices etc. A company or business will issue a GDTI where it is important to maintain a record of the document. The GDTI will provide a link to the database that holds the ‘master’ copy of the document. The GDTI may be produced as a GS1-128 bar code and printed on the document as a method of identification or for detail or information retrieval.


== External links ==
Global Document Type Identifier at GS1 website"
830,Context tree weighting,14313430,1463,"The context tree weighting method (CTW) is a lossless compression and prediction algorithm by Willems, Shtarkov & Tjalkens 1995. The CTW algorithm is among the very few such algorithms that offer both theoretical guarantees and good practical performance (see, e.g. Begleiter, El-Yaniv & Yona 2004). The CTW algorithm is an “ensemble method,” mixing the predictions of many underlying variable order Markov models, where each such model is constructed using zero-order conditional probability estimators.


== References ==
Willems; Shtarkov; Tjalkens (1995), The Context-Tree Weighting Method: Basic Properties, 41, IEEE Transactions on Information Theory 
Begleiter; El-Yaniv; Yona (2004), On Prediction Using Variable Order Markov Models (PDF), 22, Journal of Artificial Intelligence Research: Journal of Artificial Intelligence Research, pp. 385–421 


== External links ==
Relevant CTW papers and implementations
CTW Official Homepage"
831,IEEE Southwest Symposium on Image Analysis and Interpretation,42091495,1462,"The IEEE Southwest Symposium on Image Analysis and Interpretation is the IEEE biennial conference on image analysis, computer vision and pattern recognition. [1] [2] It is considered, together with CVPR, the major conference in interpretation of images and video in the United States. [3] It was first held in San Antonio, TX in April, 1996. It is indexed by IEEE [4] and the Institute for Scientific Information (ISI). [5] It is also included in Scopus and Scimago. [6]


== External links ==
conference website
Scimago and Scopus
IEEE website"
832,Semi-structured model,5876874,1461,"The semi-structured model is a database model where there is no separation between the data and the schema, and the amount of structure used depends on the purpose.
The advantages of this model are the following:
It can represent the information of some data sources that cannot be constrained by schema.
It provides a flexible format for data exchange between different types of databases.
It can be helpful to view structured data as semi-structured (for browsing purposes).
The schema can easily be changed.
The data transfer format may be portable.
The primary trade-off being made in using a semi-structured database model is that queries cannot be made as efficiently as in a more constrained structure, such as in the relational model. Typically the records in a semi-structured database are stored with unique IDs that are referenced with pointers to their location on disk. This makes navigational or path-based queries quite efficient, but for doing searches over many records (as is typical in SQL), it is not as efficient because it has to seek around the disk following pointers.
The Object Exchange Model (OEM) is one standard to express semi-structured data, another way is XML.


== See also ==
Semi-structured data"
833,Available expression,12274025,1458,"In the field of compiler optimizations, available expressions is an analysis algorithm that determines for each point in the program the set of expressions that need not be recomputed. Those expressions are said to be available at such a point. To be available on a program point, the operands of the expression should not be modified on any path from the occurrence of that expression to the program point.
The analysis is an example of a forward data flow analysis problem. A set of available expressions is maintained. Each statement is analysed to see whether it changes the operands of one or more available expressions. This yields sets of available expressions at the end of each basic block, known as the outset in data flow analysis terms. An expression is available at the start of a basic block if it is available at the end of each of the basic block's predecessors. This gives a set of equations in terms of available sets, which can be solved by an iterative algorithm.
Available expression analysis is used to do global common subexpression elimination (CSE). If an expression is available at a point, there is no need to re-evaluate it.


== References ==
Aho, Sethi & Ullman: Compilers - Principles, Techniques, and Tools Addison-Wesley Publishing Company 1986"
834,Derivative code,51717770,1453,"Derivative Code or Chameleon Code is source code which has been derived entirely from one or more other machine readable file formats. If Recursive Transcompiling is used in the development process, some code will survive all the way through the pipeline from beginning to end, and then back to the beginning again.
This code is, by definition, derivative code. The following procedure can be used to easily test if any source code is derivative code or not.
Delete the code in question
Build (or compile) the project
If the build process simply replaces the source code which has been deleted, it is (obviously) code which has been derived from something else and is therefore, by definition, derivative code.
If the build process fails, and a human being needs to re-create the deleted code by hand this is again, by definition, hand code.
Ironically, the transcompilers and other tools which create derivative code, are usually themselves either in part, or entirely hand code.


== References =="
835,RFPolicy,26369,1448,"The RFPolicy states a method of contacting vendors about security vulnerabilities found in their products. It was originally written by hacker and security consultant Rain Forest Puppy.
The policy gives the vendor five working days to respond to the reporter of the bug. If the vendor fails to contact the reporter in those five days, the issue is recommended to be disclosed to the general community. The reporter should help the vendor reproduce the bug and work out a fix. The reporter should delay notifying the general community about the bug if the vendor provides feasible reasons for requiring so.
If the vendor fails to respond or shuts down communication with the reporter of the problem in more than five working days, the reporter should disclose the issue to the general community. When issuing an alert or fix, the vendor should give the reporter proper credits about reporting the bug.


== External links ==
RFPolicy v2.0"
836,Gabbay's separation theorem,19835615,1446,"In mathematical logic and computer science, Gabbay's separation theorem, named after Dov Gabbay, states that any arbitrary temporal logic formula can be rewritten in a logically equivalent ""past → future"" form. I.e. the future becomes what must be satisfied. This form can be used as execution rules; a MetateM program is a set of such rules.


== References =="
837,Nested loop join,7806299,1443,"A nested loop join is a naive algorithm that joins two sets by using two nested loops. Join operations are important for database management.


== Algorithm ==
Two relations 
  
    
      
        R
      
    
    {\displaystyle R}
   and 
  
    
      
        S
      
    
    {\displaystyle S}
   are joined as follows:

  For each tuple r in R do
     For each tuple s in S do
        If r and s satisfy the join condition
           Then output the tuple <r,s>

This algorithm will involve nr*bs+ br block transfers and nr+br seeks, where br and bs are number of blocks in relations R and S respectively, and nr is the number of tuples in relation R.
The algorithm runs in 
  
    
      
        O
        (
        
          |
        
        R
        
          |
        
        
          |
        
        S
        
          |
        
        )
      
    
    {\displaystyle O(|R||S|)}
   I/Os, where 
  
    
      
        
          |
        
        R
        
          |
        
      
    
    {\displaystyle |R|}
   and 
  
    
      
        
          |
        
        S
        
          |
        
      
    
    {\displaystyle |S|}
   is the number of tuples contained in 
  
    
      
        R
      
    
    {\displaystyle R}
   and 
  
    
      
        S
      
    
    {\displaystyle S}
   respectively and can easily be generalized to join any number of relations.
The block nested loop join algorithm is a generalization of the simple nested loops algorithm that takes advantage of additional memory to reduce the number of times that the 
  
    
      
        S
      
    
    {\displaystyle S}
   relation is scanned.


== References =="
838,Data processing inequality,46772144,1438,"The Data processing inequality is an information theoretic concept which states that the information content of a signal cannot be increased via a local physical operation. This can be expressed concisely as 'post-processing cannot increase information'. As explained by Kinney and Atwal, the DPI means that information is generally lost (never gained) when transmitted through a noisy channel.


== Example ==
Let 
  
    
      
        X
        →
        Y
        →
        Z
      
    
    {\displaystyle X\rightarrow Y\rightarrow Z}
   be a Markov chain; then,
  
    
      
        I
        (
        x
        ;
        y
        )
        ⩾
        I
        (
        x
        ;
        z
        )
      
    
    {\displaystyle I(x;y)\geqslant I(x;z)}
   with
  
    
      
        I
        (
        x
        ;
        y
        )
        =
        I
        (
        x
        ;
        z
        )
      
    
    {\displaystyle I(x;y)=I(x;z)}
   if and only if 
  
    
      
        X
        →
        Z
        →
        Y
      
    
    {\displaystyle X\rightarrow Z\rightarrow Y}
  
where 
  
    
      
        I
        (
        x
        ;
        y
        )
      
    
    {\displaystyle I(x;y)}
   is the mutual information.


== See also ==
Garbage in, garbage out


== References ==


== External links ==
http://www.scholarpedia.org/article/Mutual_information"
839,Generation gap (pattern),3158152,1438,"A generation gap or generational gap, is a difference of opinions between one generation and another regarding beliefs, politics, or values. In today's usage, ""generation gap"" often refers to a perceived gap between younger people and their parents or grandparents.


== History ==
Early sociologists such as Karl Mannheim noted differences across generations in how the youth transits into adulthood. and studied the ways in which generations separate themselves from one another, in the home and in social situations and areas (such as churches, clubs, senior centers, and youth centers).
The sociological theory of a generation gap first came to light in the 1960s, when the younger generation (later known as Baby Boomers) seemed to go against everything their parents had previously believed in terms of music, values, governmental and political views. Sociologists now refer to ""generation gap"" as ""institutional age segregation"". Usually, when any of these age groups is engaged in its primary activity, the individual members are physically isolated from people of other generations, with little interaction across age barriers except at the nuclear family level.


== Distinguishing generation gaps ==
There are several ways to make distinctions between generations. For example, names are given to major groups (Baby boomers, Gen X, etc.) and each generation sets its own trends and has its own cultural impact.


=== Language use ===
It can be distinguished by the differences in their language use. The generation gap has created a parallel gap in language that can be difficult to communicate across. This issue is one visible throughout society, creating complications within day to day communication at home, in the work place, and within schools. As new generations seek to define themselves as something apart from the old, they adopt new lingo and slang, allowing a generation to create a sense of division from the previous one. This is a visible gap between generations we see every day. ""Man's most important symbol is his language and through this language he defines his reality.""


==== Slang ====
Slang is an ever-changing set of colloquial words and phrases that speakers use to establish or reinforce social identity or cohesiveness within a group or with a trend in society at large. As each successive generation of society struggles to establish its own unique identity among its predecessors it can be determined that generational gaps provide a large influence over the continual change and adaptation of slang. As slang is often regarded as an ephemeral dialect, a constant supply of new words is required to meet the demands of the rapid change in characteristics. And while most slang terms maintain a fairly brief duration of popularity, slang provides a quick and readily available vernacular screen to establish and maintain generational gaps in a societal context.


==== Technological influences ====
Every generation develops new slang, but with the development of technology, understanding gaps have widened between the older and younger generations. ""The term 'communication skills,' for example, might mean formal writing and speaking abilities to an older worker. But it might mean e-mail and instant-messenger savvy to a twenty something."" People often have private conversations in secret in a crowded room in today's age due to the advances of mobile phones and text messaging. Among ""texters"" a form of slang or texting lingo has developed, often keeping those not as tech savvy out of the loop. ""Children increasingly rely on personal technological devices like cell phones to define themselves and create social circles apart from their families, changing the way they communicate with their parents. Cell phones, instant messaging, e-mail and the like have encouraged younger users to create their own inventive, quirky and very private written language. That has given them the opportunity to essentially hide in plain sight. They are more connected than ever, but also far more independent. Text messaging, in particular, has perhaps become this generation's version of pig Latin.""
While in the case with language skills such as shorthand, a system of stenography popular during the twentieth century, technological innovations occurring between generations have made these skills obsolete. Older generations used shorthand to be able to take notes and write faster using abbreviated symbols, rather than having to write each word. However, with new technology and keyboards, newer generations no longer need these older communication skills, like Gregg shorthand. Although over 20 years ago, language skills such as shorthand classes were taught in many high schools, now students have rarely seen or even heard of forms like shorthand.
The transitions from each level of lifespan development have remained the same throughout history. They have all shared the same basic milestones in their travel from childhood, through midlife and into retirement. However, while the pathways remain the same—i.e. attending school, marriage, raising families, retiring—the actual journey varies not only with each individual, but with each new generation. For instance, as time goes on, technology is being introduced to individuals at younger and younger ages. While the Baby Boomers had to introduce Atari and VCRs to their parents, Generation Y’ers had to teach their parents how to maneuver such things as DVRs, cell phones and social media. There is a vast difference in Generation Y’ers and the Baby Boomers when it comes to technology. In 2011, the National Sleep Foundation conducted a poll that focused on sleep and the use of technology; 95% of those polled admitted to using some form of technology within the last hour before going to bed at night. The study compared the difference in sleep patterns in those who watched TV or listened to music prior to bedtime compared to those who used cell phones, video games and the Internet.
The study looked at Baby Boomers (born 1946-1964), Generation X’ers (born 1965-1980), Generation Y’ers (born 1981-2000) and Generation Z’ers (born 2000 to present). The research, as expected, showed generational gaps between the different forms of technology used. The largest gap was shown between texting and talking on the phone; 56% of Gen Z’ers and 42% of Gen Y’ers admitted to sending, receiving, reading text messages every night within one hour prior to bedtime, compared to only 15% of Gen X’ers (born 1965-1980), and 5% of Baby Boomers. Baby Boomers (born 1946-1964), were more likely to watch TV within the last hour prior to bedtime, 67%, compared to Gen Y’ers (born 1981-2000), who came in at 49%. When asked about computer/internet use within the last hour prior to bedtime, 70% of those polled admitted to using a computer ""a few times a week"", and from those, 55% of the Gen Z’ers (born mid-1990s or 2000 to present), said they ""surf the web"" every night before bed.


==== Language brokering ====
Another phenomenon within language that works to define a generation gap occurs within families in which different generations speak different primary languages. In order to find a means to communicate within the household environment, many have taken up the practice of language brokering, which refers to the ""interpretation and translation performed in everyday situations by bilinguals who have had no special training"". In immigrant families where the first generation speaks primarily in their native tongue, the second generation primarily in the language of the country in which they now live while still retaining fluency in their parent's dominant language, and the third generation primarily in the language of the country they were born in while retaining little to no conversational language in their grandparent's native tongue, the second generation family members serve as interpreters not only to outside persons, but within the household, further propelling generational differences and divisions by means of linguistic communication.
Furthermore, in some immigrant families and communities, language brokering is also used to integrate children into family endeavors and into civil society. Child integration has become very important to form linkages between new immigrant communities and the predominant culture and new forms of bureaucratic systems. In addition, it also serves towards child development by learning and pitching in.


==== Workplace attitudes ====
USA Today reported that younger generations are ""entering the workplace in the face of demographic change and an increasingly multi-generational workplace"". Multiple engagement studies show that the interests shared across the generation gap by members of this increasingly multi-generational workplace can differ substantially.
A popular belief held by older generations is that the characteristics of Millennials can potentially complicate professional interactions. To some managers, this generation is a group of coddled, lazy, disloyal, and narcissistic young people, who are incapable of handling the simplest task without guidance. For this reason, when millennials first enter a new organization, they are often greeted with wary coworkers. Career was an essential component of the identities of Baby boomers; they made many sacrifices, working 55 to 60 hour weeks, patiently waiting for promotions. Millennials, on the other hand, are not workaholics and do not place such a strong emphasis on their careers. Even so, they expect all the perks, in terms of good pay and benefits, rapid advancement, work-life balance, stimulating work, and giving back to their community. Studies have found that millennials are usually exceptionally confident in their abilities and, as a result, fail to prove themselves by working hard, seeking key roles in significant projects early on in their careers, which frustrates their older coworkers.
Most of these inflated expectations are direct results of the generation's upbringing. During the Great Recession, millennials watched first-hand as their parents worked long hours, only to fall victim to downsizing and layoffs. Many families could not withstand these challenges, leading to high divorce rates and broken families. Millennials do not want to be put in the same position as their parents, so they have made their personal lives a main priority. In fact, fifty-nine percent of Millennials say the Great Recession negatively impacted their career plans, while only 35% of mature workers feel the same way. For these reasons, millennials are more likely to negotiate the terms of their work. Though some boomers view this as lazy behavior, others have actually been able to learn from millennials, reflecting on whether the sacrifices that they had made in their lives provided them with the happiness that they had hoped for.
Growing up, millennials looked to parents, teachers, and coaches as a source of praise and support. They were a part of an educational system with inflated grades and Standardized tests, in which they were skilled at performing well. They were brought up believing they could be anything and everything they dreamed of. As a result, millennials developed a strong need for frequent, positive feedback from supervisors. Today, managers find themselves assessing their subordinates’ productivity quite frequently, despite the fact that they often find it burdensome. Additionally, millennials’ salaries and Employee benefits give this generation an idea of how well they are performing. Millennials crave success, and good paying jobs have been proven to make them feel more successful.
Additionally, studies show that promotions are very important to millennials, and when they do not see opportunities for rapid advancement at one organization, they are quick to quit in an effort to find better opportunities. They have an unrealistic timeline for these promotions, however, which frustrates older generations. They also have a low tolerance for unchallenging work; when work is not stimulating, they often perform poorly out of boredom. As a result, managers must constantly provide millennials with greater responsibility so that they feel more involved and needed in the organization.
Because group projects and presentations were commonplace during the schooling of millennials, this generation enjoys collaborating and even developing close friendships with colleagues. While working as part of a team enhances innovation, enhances productivity, and lowers personnel costs, downsides still exist. Supervisors find that millennials avoid risk and independent responsibility by relying on team members when making decisions, which prevents them from showcasing their own abilities.
Perhaps the most commonly cited difference between older and younger generations is technological proficiency. Studies have shown that their reliance on technology has made millennials less comfortable with face-to-face interaction and deciphering verbal cues. However, technological proficiency also has its benefits; millennials are far more effective in multitasking, responding to visual stimulation, and filtering information than older generations.
However, according to the engagement studies, mature workers and the new generations of workers share similar thoughts on a number of topics across the generation gap. Their opinions overlap on flexible working hours/arrangements, promotions/bonuses, the importance of computer proficiency, and leadership. Additionally, the majority of Millennials and mature workers enjoy going to work every day, and feel inspired to do their best.


=== Generational consciousness ===
Generational consciousness is another way of distinguishing among generations that was worked on by social scientist Karl Mannheim. Generational consciousness is when a group of people become mindful of their place in a distinct group identifiable by their shared interests and values. Social, economic, or political changes can bring awareness to these shared interests and values for similarly-aged people who experience these events together, and thereby form a generational consciousness. These types of experiences can impact individuals' development at a young age and enable them to begin making their own interpretations of the world based on personal encounters that set them apart from other generations.


=== Intergenerational living ===
""Both social isolation and loneliness in older men and women are associated with increased mortality, according to a 2012 Report by the National Academy of Sciences of the United States of America"". Intergenerational living is one method being used currently worldwide as a means of combating such feelings. A nursing home in Deventer, The Netherlands, developed a program wherein students from a local university are provided small, rent-free apartments within the nursing home facility. In exchange, the students volunteer a minimum of 30 hours per month to spend time with the seniors. The students will watch sports with the seniors, celebrate birthdays, and simply keep them company during illnesses and times of distress. Programs similar to the Netherlands’ program were developed as far back as the mid-1990s in Barcelona, Spain. In Spain's program, students were placed in seniors’ homes, with a similar goal of free/cheap housing in exchange for companionship for the elderly. That program quickly spread to 27 other cities throughout Spain, and similar programs can be found in Lyons, France, and Cleveland, Ohio.


=== Demographics ===
In order for sociologists to understand the transition into adulthood of children in different generation gaps, they compare the current generation to both older and earlier generations at the same time. Not only does each generation experience their own ways of mental and physical maturation, but they also create new aspects of attending school, forming new households, starting families and even creating new demographics. The difference in demographics regarding values, attitudes and behaviors between the two generations are used to create a profile for the emerging generation of young adults.
Following the thriving economic success that was a product of the Second World War, America's population skyrocketed between the years 1940-1959, to which the new American generation was called the Baby Boomers. Today, as of 2017, many of these Baby Boomers have celebrated their 60th birthdays and in the next few years America's senior citizen population will boost exponentially due to the population of people who were born during the years 1940 and 1959. The generation gap, however, between the Baby Boomers and earlier generations is growing due to the Boomers population post-war. There is a large demographic difference between the Baby Boomer generation and earlier generations, where earlier generations are less racially and ethnically diverse than the Baby Boomers’ population. Where this drastic racial demographic difference occurs also holds to a continually growing cultural gap as well; baby boomers have had generally higher education, with a higher percentage of women in the labor force and more often occupying professional and managerial positions. These drastic culture and generation gaps create issues of community preferences as well as spending.


== See also ==
Achievement gap
Ageism
Digital divide
Income gap
Inter-generational contract
Intergenerational equity
List of Generations
Marriage gap
Moral panic
Student activism
Student voice
Transgenerational design
Youth activism
Youth voice
Slang
Technology


== References ==

Bennis, W. and Thomas, R. (2002) Geeks and Geezers: how era, values and defining moments shape leaders, Harvard Business School Publishing
Employee Evolution: the Voice of Millennials at Work"
840,Information Systems International Conference,46460707,1431,"Information Systems International Conference (ISICO) is an AISINDO AIS Indonesia Chapter affiliated international conference in which administered by the Department of Information Systems, Institut Teknologi Sepuluh Nopember, Indonesia. ISICO is a biannual conference which has been run since 2011. ISICO 2013 was held in Bali and invited Prof. Doug Vogel (Association for Information Systems (AIS) Immediate Past President) from City University of Hong Kong and Prof. Don Kerr (President of Australasian AIS chapter) from University of the Sunshine Coast, attended by 340 participants from 9 countries and established the new AIS Chapter of Indonesia (named: AISINDO).
In 2015, ISICO has collaborated with Procedia Computer Science from Elsevier to publish all ISICO full papers into the journal.
External Link
ISICO 2015
ISICO 2015
ISICO 2013
Open Access Journals of Information Systems"
841,Rule induction,7517319,1428,"Rule induction is an area of machine learning in which formal rules are extracted from a set of observations. The rules extracted may represent a full scientific model of the data, or merely represent local patterns in the data.


== Paradigms ==
Some major rule induction paradigms are:
Association rule learning algorithms (e.g., Agrawal)
Decision rule algorithms (e.g., Quinlan 1987)
Hypothesis testing algorithms (e.g., RULEX)
Horn clause induction
Version spaces
Rough set rules
Inductive Logic Programming
Boolean decomposition (Feldman)


== Algorithms ==
Some rule induction algorithms are:
Charade
Rulex
Progol
CN2


== References ==

Quinlan, J. R. (1987). ""Generating production rules from decision trees"" (PDF). In McDermott, John. Proceedings of the Tenth International Joint Conference on Artificial Intelligence (IJCAI-87). Milan, Italy. pp. 304–307."
842,Bidirectional map,10070867,1426,"In computer science, a bidirectional map, or hash bag, is an associative data structure in which the 
  
    
      
        (
        k
        e
        y
        ,
        v
        a
        l
        u
        e
        )
      
    
    {\displaystyle (key,value)}
   pairs form a one-to-one correspondence. Thus the binary relation is functional in each direction: 
  
    
      
        v
        a
        l
        u
        e
      
    
    {\displaystyle value}
   can also act as a key to 
  
    
      
        k
        e
        y
      
    
    {\displaystyle key}
  . A pair 
  
    
      
        (
        a
        ,
        b
        )
      
    
    {\displaystyle (a,b)}
   thus provides a unique coupling between 
  
    
      
        a
      
    
    {\displaystyle a}
   and 
  
    
      
        b
      
    
    {\displaystyle b}
   so that 
  
    
      
        b
      
    
    {\displaystyle b}
   can be found when 
  
    
      
        a
      
    
    {\displaystyle a}
   is used as a key and 
  
    
      
        a
      
    
    {\displaystyle a}
   can be found when 
  
    
      
        b
      
    
    {\displaystyle b}
   is used as a key.


== External links ==
Boost.org
Commons.apache.org
Cablemodem.fibertel.com.ar (archived version)
Codeproject.com
BiMap in the Google Guava library
bidict (bidirectional map implementation for Python)"
843,Brodal queue,33238984,1425,"In computer science, the Brodal queue is a heap/priority queue structure with very low worst case time bounds: 
  
    
      
        O
        (
        1
        )
      
    
    {\displaystyle O(1)}
   for insertion, find-minimum, meld (merge two queues) and decrease-key and 
  
    
      
        O
        (
        
          l
          o
          g
        
        (
        n
        )
        )
      
    
    {\displaystyle O(\mathrm {log} (n))}
   for delete-minimum and general deletion. They are the first heap variant to achieve these bounds without resorting to amortization of operational costs. Brodal queues are named after their inventor Gerth Stølting Brodal.
While having better asymptotic bounds than other priority queue structures, they are, in the words of Brodal himself, ""quite complicated"" and ""[not] applicable in practice."" Brodal and Okasaki describe a persistent (purely functional) version of Brodal queues.


== Summary of running times ==
In the following time complexities O(f) is an asymptotic upper bound and Θ(f) is an asymptotically tight bound (see Big O notation). Function names assume a min-heap.


== References =="
844,Fundamental pattern,855048,1423,"Fundamental patterns are one of the types of design patterns. They are termed fundamental as they form the basic building blocks of the other patterns. Most of the other patterns and most modern applications draw on these patterns in one way or another.
Examples of this class of patterns include:
Delegation pattern: an object outwardly expresses certain behaviour but in reality delegates responsibility
Functional design: strives for each modular part of a computer program has only one responsibility and performs that with minimum side effects
Interface pattern: method for structuring programs so that they're simpler to understand
Proxy pattern: an object functions as an interface to another, typically more complex, object
Facade pattern: provides a simplified interface to a larger body of code, such as a class library.
Composite pattern: defines Composite object (e.g. a shape) designed as a composition of one-or-more similar objects (other kinds of shapes/geometries), all exhibiting similar functionality. The Composite object then exposes properties and methods for child objects manipulation as if it were a simple object.


== See also ==
Design pattern (computer science)"
845,Commit (data management),1626958,1422,"In computer science and data management, a commit is the making of a set of tentative changes permanent. A popular usage is at the end of a transaction. A commit is an act of committing.


== Data management ==
A COMMIT statement in SQL ends a transaction within a relational database management system (RDBMS) and makes all changes visible to other users. The general format is to issue a BEGIN WORK statement, one or more SQL statements, and then the COMMIT statement. Alternatively, a ROLLBACK statement can be issued, which undoes all the work performed since BEGIN WORK was issued. A COMMIT statement will also release any existing savepoints that may be in use.
In terms of transactions, the opposite of commit is to discard the tentative changes of a transaction, a rollback.


== See also ==
Commit (version control)
Atomic commit
Two-phase commit protocol
Three-phase commit protocol"
846,Screened-subnet firewall,8307689,1418,"In network security, a screened subnet firewall is a variation of the dual-homed gateway and screened host firewall. It can be used to separate components of the firewall onto separate systems, thereby achieving greater throughput and flexibility, although at some cost to simplicity. As each component system of the screened subnet firewall needs to implement only a specific task, each system is less complex to configure.
A screened subnet firewall is often used to establish a demilitarized zone (DMZ).
Although it is available in education, and IT audit textbooks, the legacy concept is not viable to the modern multitier architecture with firewall between each tier. That clearly demonstrate the control separation, and the compromise of single device will not forgoes security entirely (dual control). 


== References ==

http://www.vtcif.telstra.com.au/pub/docs/security/800-10/node58.html


== See also ==
Firewall (networking)"
847,"(a,b)-tree",4436119,1417,"In computer science, a B-tree is a self-balancing tree data structure that keeps data sorted and allows searches, sequential access, insertions, and deletions in logarithmic time. The B-tree is a generalization of a binary search tree in that a node can have more than two children. Unlike self-balancing binary search trees, the B-tree is optimized for systems that read and write large blocks of data. B-trees are a good example of a data structure for external memory. It is commonly used in databases and filesystems.


== Overview ==

In B-trees, internal (non-leaf) nodes can have a variable number of child nodes within some pre-defined range. When data is inserted or removed from a node, its number of child nodes changes. In order to maintain the pre-defined range, internal nodes may be joined or split. Because a range of child nodes is permitted, B-trees do not need re-balancing as frequently as other self-balancing search trees, but may waste some space, since nodes are not entirely full. The lower and upper bounds on the number of child nodes are typically fixed for a particular implementation. For example, in a 2-3 B-tree (often simply referred to as a 2-3 tree), each internal node may have only 2 or 3 child nodes.
Each internal node of a B-tree contains a number of keys. The keys act as separation values which divide its subtrees. For example, if an internal node has 3 child nodes (or subtrees) then it must have 2 keys: a1 and a2. All values in the leftmost subtree will be less than a1, all values in the middle subtree will be between a1 and a2, and all values in the rightmost subtree will be greater than a2.
Usually, the number of keys is chosen to vary between 
  
    
      
        d
      
    
    {\displaystyle d}
   and 
  
    
      
        2
        d
      
    
    {\displaystyle 2d}
  , where 
  
    
      
        d
      
    
    {\displaystyle d}
   is the minimum number of keys, and 
  
    
      
        d
        +
        1
      
    
    {\displaystyle d+1}
   is the minimum degree or branching factor of the tree. In practice, the keys take up the most space in a node. The factor of 2 will guarantee that nodes can be split or combined. If an internal node has 
  
    
      
        2
        d
      
    
    {\displaystyle 2d}
   keys, then adding a key to that node can be accomplished by splitting the hypothetical 
  
    
      
        2
        d
        +
        1
      
    
    {\displaystyle 2d+1}
   key node into two 
  
    
      
        d
      
    
    {\displaystyle d}
   key nodes and moving the key that would have been in the middle to the parent node. Each split node has the required minimum number of keys. Similarly, if an internal node and its neighbor each have 
  
    
      
        d
      
    
    {\displaystyle d}
   keys, then a key may be deleted from the internal node by combining it with its neighbor. Deleting the key would make the internal node have 
  
    
      
        d
        −
        1
      
    
    {\displaystyle d-1}
   keys; joining the neighbor would add 
  
    
      
        d
      
    
    {\displaystyle d}
   keys plus one more key brought down from the neighbor's parent. The result is an entirely full node of 
  
    
      
        2
        d
      
    
    {\displaystyle 2d}
   keys.
The number of branches (or child nodes) from a node will be one more than the number of keys stored in the node. In a 2-3 B-tree, the internal nodes will store either one key (with two child nodes) or two keys (with three child nodes). A B-tree is sometimes described with the parameters 
  
    
      
        (
        d
        +
        1
        )
      
    
    {\displaystyle (d+1)}
   — 
  
    
      
        (
        2
        d
        +
        1
        )
      
    
    {\displaystyle (2d+1)}
   or simply with the highest branching order, 
  
    
      
        (
        2
        d
        +
        1
        )
      
    
    {\displaystyle (2d+1)}
  .
A B-tree is kept balanced by requiring that all leaf nodes be at the same depth. This depth will increase slowly as elements are added to the tree, but an increase in the overall depth is infrequent, and results in all leaf nodes being one more node farther away from the root.
B-trees have substantial advantages over alternative implementations when the time to access the data of a node greatly exceeds the time spent processing that data, because then the cost of accessing the node may be amortized over multiple operations within the node. This usually occurs when the node data are in secondary storage such as disk drives. By maximizing the number of keys within each internal node, the height of the tree decreases and the number of expensive node accesses is reduced. In addition, rebalancing of the tree occurs less often. The maximum number of child nodes depends on the information that must be stored for each child node and the size of a full disk block or an analogous size in secondary storage. While 2-3 B-trees are easier to explain, practical B-trees using secondary storage need a large number of child nodes to improve performance.


=== Variants ===
The term B-tree may refer to a specific design or it may refer to a general class of designs. In the narrow sense, a B-tree stores keys in its internal nodes but need not store those keys in the records at the leaves. The general class includes variations such as the B+ tree and the B* tree.
In the B+ tree, copies of the keys are stored in the internal nodes; the keys and records are stored in leaves; in addition, a leaf node may include a pointer to the next leaf node to speed sequential access.
The B* tree balances more neighboring internal nodes to keep the internal nodes more densely packed. This variant requires non-root nodes to be at least 2/3 full instead of 1/2 (Knuth 1998, p. 488). As the most costly part of operation of inserting the node in B-tree is splitting the node, B*-trees are created to postpone splitting operation as long as they can. To maintain this, instead of immediately splitting up a node when it gets full, its keys are shared with a node next to it. This spill operation is less costly to do than split, because it requires only shifting the keys between existing nodes, not allocating memory for a new one. For inserting, first it is checked whether the node has some free space in it, and if so, the new key is just inserted in the node. However, if the node is full (it has m - 1 keys, where m is the order of the tree as maximum number of pointers to subtrees from one node), it needs to be checked whether the right sibling exists and has some free space. If the right sibling has j < m - 1 keys, then keys are redistributed between the two sibling nodes as evenly as possible. For this purpose, m keys from the current node plus the new key inserted, one key from the parent node and j keys from the sibling node are seen as an ordered array of m + j + 1 keys. The array becomes split by half, so that ⌊(m + j + 1)/2⌋ lowest keys stay in the current node, the next (middle) key is inserted in the parent and the rest go to the right sibling. (The newly inserted key may be finally placed in either of sibling nodes or in their parent node.) Situation when right sibling is full, and left isn't is analogous. When both the sibling nodes are full, then the two nodes (current node and a sibling) are split into three and one more key is shifted up the tree, to the parent node. If the parent is full, then spill/split operation propagates towards the root node. Deleting nodes is somewhat more complex than inserting however.
B-trees can be turned into order statistic trees to allow rapid searches for the Nth record in key order, or counting the number of records between any two records, and various other related operations.


=== Etymology ===
Rudolf Bayer and Ed McCreight invented the B-tree while working at Boeing Research Labs in 1971 (Bayer & McCreight 1972), but they did not explain what, if anything, the B stands for. Douglas Comer explains:

The origin of ""B-tree"" has never been explained by the authors. As we shall see, ""balanced,"" ""broad,"" or ""bushy"" might apply. Others suggest that the ""B"" stands for Boeing. Because of his contributions, however, it seems appropriate to think of B-trees as ""Bayer""-trees. (Comer 1979, p. 123 footnote 1)

Donald Knuth speculates on the etymology of B-trees in his May, 1980 lecture on the topic ""CS144C classroom lecture about disk storage and B-trees"", suggesting the ""B"" may have originated from Boeing or from Bayer's name.
Ed McCreight answered a question on B-tree's name in 2013:

Bayer and I were in a lunchtime where we get to think [of] a name. And ... B is, you know ... We were working for Boeing at the time, we couldn't use the name without talking to lawyers. So, there is a B. [The B-tree] has to do with balance, another B. Bayer was the senior author, who [was] several years older than I am and had many more publications than I did. So there is another B. And so, at the lunch table we never did resolve whether there was one of those that made more sense than the rest. What really lives to say is: the more you think about what the B in B-trees means, the better you understand B-trees.""


== B-tree usage in databases ==


=== Time to search a sorted file ===
Usually, sorting and searching algorithms have been characterized by the number of comparison operations that must be performed using order notation. A binary search of a sorted table with N records, for example, can be done in roughly ⌈ log2 N ⌉ comparisons. If the table had 1,000,000 records, then a specific record could be located with at most 20 comparisons: ⌈ log2 (1,000,000) ⌉ = 20.
Large databases have historically been kept on disk drives. The time to read a record on a disk drive far exceeds the time needed to compare keys once the record is available. The time to read a record from a disk drive involves a seek time and a rotational delay. The seek time may be 0 to 20 or more milliseconds, and the rotational delay averages about half the rotation period. For a 7200 RPM drive, the rotation period is 8.33 milliseconds. For a drive such as the Seagate ST3500320NS, the track-to-track seek time is 0.8 milliseconds and the average reading seek time is 8.5 milliseconds. For simplicity, assume reading from disk takes about 10 milliseconds.
Naively, then, the time to locate one record out of a million would take 20 disk reads times 10 milliseconds per disk read, which is 0.2 seconds.
The time won't be that bad because individual records are grouped together in a disk block. A disk block might be 16 kilobytes. If each record is 160 bytes, then 100 records could be stored in each block. The disk read time above was actually for an entire block. Once the disk head is in position, one or more disk blocks can be read with little delay. With 100 records per block, the last 6 or so comparisons don't need to do any disk reads—the comparisons are all within the last disk block read.
To speed the search further, the first 13 to 14 comparisons (which each required a disk access) must be sped up.


=== An index speeds the search ===
A significant improvement can be made with an index. In the example above, initial disk reads narrowed the search range by a factor of two. That can be improved substantially by creating an auxiliary index that contains the first record in each disk block (sometimes called a sparse index). This auxiliary index would be 1% of the size of the original database, but it can be searched more quickly. Finding an entry in the auxiliary index would tell us which block to search in the main database; after searching the auxiliary index, we would have to search only that one block of the main database—at a cost of one more disk read. The index would hold 10,000 entries, so it would take at most 14 comparisons. Like the main database, the last 6 or so comparisons in the aux index would be on the same disk block. The index could be searched in about 8 disk reads, and the desired record could be accessed in 9 disk reads.
The trick of creating an auxiliary index can be repeated to make an auxiliary index to the auxiliary index. That would make an aux-aux index that would need only 100 entries and would fit in one disk block.
Instead of reading 14 disk blocks to find the desired record, we only need to read 3 blocks. Reading and searching the first (and only) block of the aux-aux index identifies the relevant block in aux-index. Reading and searching that aux-index block identifies the relevant block in the main database. Instead of 150 milliseconds, we need only 30 milliseconds to get the record.
The auxiliary indices have turned the search problem from a binary search requiring roughly log2 N disk reads to one requiring only logb N disk reads where b is the blocking factor (the number of entries per block: b = 100 entries per block in our example; log100 1,000,000 = 3 reads).
In practice, if the main database is being frequently searched, the aux-aux index and much of the aux index may reside in a disk cache, so they would not incur a disk read.


=== Insertions and deletions ===
If the database does not change, then compiling the index is simple to do, and the index need never be changed. If there are changes, then managing the database and its index becomes more complicated.
Deleting records from a database is relatively easy. The index can stay the same, and the record can just be marked as deleted. The database remains in sorted order. If there are a large number of deletions, then searching and storage become less efficient.
Insertions can be very slow in a sorted sequential file because room for the inserted record must be made. Inserting a record before the first record requires shifting all of the records down one. Such an operation is just too expensive to be practical. One solution is to leave some spaces. Instead of densely packing all the records in a block, the block can have some free space to allow for subsequent insertions. Those spaces would be marked as if they were ""deleted"" records.
Both insertions and deletions are fast as long as space is available on a block. If an insertion won't fit on the block, then some free space on some nearby block must be found and the auxiliary indices adjusted. The hope is that enough space is available nearby, such that a lot of blocks do not need to be reorganized. Alternatively, some out-of-sequence disk blocks may be used.


=== Advantages of B-tree usage for databases ===
The B-tree uses all of the ideas described above. In particular, a B-tree:
keeps keys in sorted order for sequential traversing
uses a hierarchical index to minimize the number of disk reads
uses partially full blocks to speed insertions and deletions
keeps the index balanced with a recursive algorithm
In addition, a B-tree minimizes waste by making sure the interior nodes are at least half full. A B-tree can handle an arbitrary number of insertions and deletions.


== Technical description ==


=== Terminology ===
The literature on B-trees is not uniform in its terminology (Folk & Zoellick 1992, p. 362).
Bayer & McCreight (1972), Comer (1979), and others define the order of B-tree as the minimum number of keys in a non-root node. Folk & Zoellick (1992) points out that terminology is ambiguous because the maximum number of keys is not clear. An order 3 B-tree might hold a maximum of 6 keys or a maximum of 7 keys. Knuth (1998, p. 483) avoids the problem by defining the order to be maximum number of children (which is one more than the maximum number of keys).
The term leaf is also inconsistent. Bayer & McCreight (1972) considered the leaf level to be the lowest level of keys, but Knuth considered the leaf level to be one level below the lowest keys (Folk & Zoellick 1992, p. 363). There are many possible implementation choices. In some designs, the leaves may hold the entire data record; in other designs, the leaves may only hold pointers to the data record. Those choices are not fundamental to the idea of a B-tree.
There are also unfortunate choices like using the variable k to represent the number of children when k could be confused with the number of keys.
For simplicity, most authors assume there are a fixed number of keys that fit in a node. The basic assumption is the key size is fixed and the node size is fixed. In practice, variable length keys may be employed (Folk & Zoellick 1992, p. 379).


=== Definition ===
According to Knuth's definition, a B-tree of order m is a tree which satisfies the following properties:
Every node has at most m children.
Every non-leaf node (except root) has at least ⌈m/2⌉ children.
The root has at least two children if it is not a leaf node.
A non-leaf node with k children contains k−1 keys.
All leaves appear in the same level
Each internal node’s keys act as separation values which divide its subtrees. For example, if an internal node has 3 child nodes (or subtrees) then it must have 2 keys: a1 and a2. All values in the leftmost subtree will be less than a1, all values in the middle subtree will be between a1 and a2, and all values in the rightmost subtree will be greater than a2.
Internal nodes
Internal nodes are all nodes except for leaf nodes and the root node. They are usually represented as an ordered set of elements and child pointers. Every internal node contains a maximum of U children and a minimum of L children. Thus, the number of elements is always 1 less than the number of child pointers (the number of elements is between L−1 and U−1). U must be either 2L or 2L−1; therefore each internal node is at least half full. The relationship between U and L implies that two half-full nodes can be joined to make a legal node, and one full node can be split into two legal nodes (if there’s room to push one element up into the parent). These properties make it possible to delete and insert new values into a B-tree and adjust the tree to preserve the B-tree properties.
The root node
The root node’s number of children has the same upper limit as internal nodes, but has no lower limit. For example, when there are fewer than L−1 elements in the entire tree, the root will be the only node in the tree with no children at all.
Leaf nodes
Leaf nodes have the same restriction on the number of elements, but have no children, and no child pointers.
A B-tree of depth n+1 can hold about U times as many items as a B-tree of depth n, but the cost of search, insert, and delete operations grows with the depth of the tree. As with any balanced tree, the cost grows much more slowly than the number of elements.
Some balanced trees store values only at leaf nodes, and use different kinds of nodes for leaf nodes and internal nodes. B-trees keep values in every node in the tree, and may use the same structure for all nodes. However, since leaf nodes never have children, the B-trees benefit from improved performance if they use a specialized structure.


== Best case and worst case heights ==
Let h be the height of the classic B-tree. Let n > 0 be the number of entries in the tree. Let m be the maximum number of children a node can have. Each node can have at most m−1 keys.
It can be shown (by induction for example) that a B-tree of height h with all its nodes completely filled has n= mh+1−1 entries. Hence, the best case height of a B-tree is:

  
    
      
        ⌈
        
          log
          
            m
          
        
        ⁡
        (
        n
        +
        1
        )
        ⌉
        −
        1
      
    
    {\displaystyle \lceil \log _{m}(n+1)\rceil -1}
  
Let 
  
    
      
        d
      
    
    {\displaystyle d}
   be the minimum number of children an internal (non-root) node can have. For an ordinary B-tree, 
  
    
      
        d
        =
        
          ⌈
          
            m
            
              /
            
            2
          
          ⌉
        
        .
      
    
    {\displaystyle d=\left\lceil m/2\right\rceil .}
  
Comer (1979, p. 127) and Cormen et al. (2001, pp. 383–384) give the worst case height of a B-tree (where the root node is considered to have height 0) as

  
    
      
        h
        ≤
        
          ⌊
          
            
              log
              
                d
              
            
            ⁡
            
              (
              
                
                  
                    n
                    +
                    1
                  
                  2
                
              
              )
            
          
          ⌋
        
        .
      
    
    {\displaystyle h\leq \left\lfloor \log _{d}\left({\frac {n+1}{2}}\right)\right\rfloor .}
  


== Algorithms ==


=== Search ===
Searching is similar to searching a binary search tree. Starting at the root, the tree is recursively traversed from top to bottom. At each level, the search reduces its field of view to the child pointer (subtree) whose range includes the search value. A subtree's range is defined by the values, or keys, contained in its parent node. These limiting values are also known as separation values.
Binary search is typically (but not necessarily) used within nodes to find the separation values and child tree of interest.


=== Insertion ===

All insertions start at a leaf node. To insert a new element, search the tree to find the leaf node where the new element should be added. Insert the new element into that node with the following steps:
If the node contains fewer than the maximum allowed number of elements, then there is room for the new element. Insert the new element in the node, keeping the node's elements ordered.
Otherwise the node is full, evenly split it into two nodes so:
A single median is chosen from among the leaf's elements and the new element.
Values less than the median are put in the new left node and values greater than the median are put in the new right node, with the median acting as a separation value.
The separation value is inserted in the node's parent, which may cause it to be split, and so on. If the node has no parent (i.e., the node was the root), create a new root above this node (increasing the height of the tree).

If the splitting goes all the way up to the root, it creates a new root with a single separator value and two children, which is why the lower bound on the size of internal nodes does not apply to the root. The maximum number of elements per node is U−1. When a node is split, one element moves to the parent, but one element is added. So, it must be possible to divide the maximum number U−1 of elements into two legal nodes. If this number is odd, then U=2L and one of the new nodes contains (U−2)/2 = L−1 elements, and hence is a legal node, and the other contains one more element, and hence it is legal too. If U−1 is even, then U=2L−1, so there are 2L−2 elements in the node. Half of this number is L−1, which is the minimum number of elements allowed per node.
An improved algorithm supports a single pass down the tree from the root to the node where the insertion will take place, splitting any full nodes encountered on the way. This prevents the need to recall the parent nodes into memory, which may be expensive if the nodes are on secondary storage. However, to use this improved algorithm, we must be able to send one element to the parent and split the remaining U−2 elements into two legal nodes, without adding a new element. This requires U = 2L rather than U = 2L−1, which accounts for why some textbooks impose this requirement in defining B-trees.


=== Deletion ===
There are two popular strategies for deletion from a B-tree.
Locate and delete the item, then restructure the tree to retain its invariants, OR
Do a single pass down the tree, but before entering (visiting) a node, restructure the tree so that once the key to be deleted is encountered, it can be deleted without triggering the need for any further restructuring
The algorithm below uses the former strategy.
There are two special cases to consider when deleting an element:
The element in an internal node is a separator for its child nodes
Deleting an element may put its node under the minimum number of elements and children
The procedures for these cases are in order below.


==== Deletion from a leaf node ====
Search for the value to delete.
If the value is in a leaf node, simply delete it from the node.
If underflow happens, rebalance the tree as described in section ""Rebalancing after deletion"" below.


==== Deletion from an internal node ====
Each element in an internal node acts as a separation value for two subtrees, therefore we need to find a replacement for separation. Note that the largest element in the left subtree is still less than the separator. Likewise, the smallest element in the right subtree is still greater than the separator. Both of those elements are in leaf nodes, and either one can be the new separator for the two subtrees. Algorithmically described below:
Choose a new separator (either the largest element in the left subtree or the smallest element in the right subtree), remove it from the leaf node it is in, and replace the element to be deleted with the new separator.
The previous step deleted an element (the new separator) from a leaf node. If that leaf node is now deficient (has fewer than the required number of nodes), then rebalance the tree starting from the leaf node.


==== Rebalancing after deletion ====
Rebalancing starts from a leaf and proceeds toward the root until the tree is balanced. If deleting an element from a node has brought it under the minimum size, then some elements must be redistributed to bring all nodes up to the minimum. Usually, the redistribution involves moving an element from a sibling node that has more than the minimum number of nodes. That redistribution operation is called a rotation. If no sibling can spare an element, then the deficient node must be merged with a sibling. The merge causes the parent to lose a separator element, so the parent may become deficient and need rebalancing. The merging and rebalancing may continue all the way to the root. Since the minimum element count doesn't apply to the root, making the root be the only deficient node is not a problem. The algorithm to rebalance the tree is as follows:
If the deficient node's right sibling exists and has more than the minimum number of elements, then rotate left
Copy the separator from the parent to the end of the deficient node (the separator moves down; the deficient node now has the minimum number of elements)
Replace the separator in the parent with the first element of the right sibling (right sibling loses one node but still has at least the minimum number of elements)
The tree is now balanced

Otherwise, if the deficient node's left sibling exists and has more than the minimum number of elements, then rotate right
Copy the separator from the parent to the start of the deficient node (the separator moves down; deficient node now has the minimum number of elements)
Replace the separator in the parent with the last element of the left sibling (left sibling loses one node but still has at least the minimum number of elements)
The tree is now balanced

Otherwise, if both immediate siblings have only the minimum number of elements, then merge with a sibling sandwiching their separator taken off from their parent
Copy the separator to the end of the left node (the left node may be the deficient node or it may be the sibling with the minimum number of elements)
Move all elements from the right node to the left node (the left node now has the maximum number of elements, and the right node – empty)
Remove the separator from the parent along with its empty right child (the parent loses an element)
If the parent is the root and now has no elements, then free it and make the merged node the new root (tree becomes shallower)
Otherwise, if the parent has fewer than the required number of elements, then rebalance the parent

Note: The rebalancing operations are different for B+ trees (e.g., rotation is different because parent has copy of the key) and B*-tree (e.g., three siblings are merged into two siblings).


=== Sequential access ===
While freshly loaded databases tend to have good sequential behavior, this behavior becomes increasingly difficult to maintain as a database grows, resulting in more random I/O and performance challenges.


=== Initial construction ===
In applications, it is frequently useful to build a B-tree to represent a large existing collection of data and then update it incrementally using standard B-tree operations. In this case, the most efficient way to construct the initial B-tree is not to insert every element in the initial collection successively, but instead to construct the initial set of leaf nodes directly from the input, then build the internal nodes from these. This approach to B-tree construction is called bulkloading. Initially, every leaf but the last one has one extra element, which will be used to build the internal nodes.
For example, if the leaf nodes have maximum size 4 and the initial collection is the integers 1 through 24, we would initially construct 4 leaf nodes containing 5 values each and 1 which contains 4 values:
We build the next level up from the leaves by taking the last element from each leaf node except the last one. Again, each node except the last will contain one extra value. In the example, suppose the internal nodes contain at most 2 values (3 child pointers). Then the next level up of internal nodes would be:
This process is continued until we reach a level with only one node and it is not overfilled. In the example only the root level remains:


== In filesystems ==
In addition to its use in databases, the B-tree (or § Variants) is also used in filesystems to allow quick random access to an arbitrary block in a particular file. The basic problem is turning the file block 
  
    
      
        i
      
    
    {\displaystyle i}
   address into a disk block (or perhaps to a cylinder-head-sector) address.
Some operating systems require the user to allocate the maximum size of the file when the file is created. The file can then be allocated as contiguous disk blocks. In that case, to convert the file block address 
  
    
      
        i
      
    
    {\displaystyle i}
   into a disk block address, the operating system simply adds the file block address 
  
    
      
        i
      
    
    {\displaystyle i}
   to the address of the first disk block constituting the file. The scheme is simple, but the file cannot exceed its created size.
Other operating systems allow a file to grow. The resulting disk blocks may not be contiguous, so mapping logical blocks to physical blocks is more involved.
MS-DOS, for example, used a simple File Allocation Table (FAT). The FAT has an entry for each disk block, and that entry identifies whether its block is used by a file and if so, which block (if any) is the next disk block of the same file. So, the allocation of each file is represented as a linked list in the table. In order to find the disk address of file block 
  
    
      
        i
      
    
    {\displaystyle i}
  , the operating system (or disk utility) must sequentially follow the file's linked list in the FAT. Worse, to find a free disk block, it must sequentially scan the FAT. For MS-DOS, that was not a huge penalty because the disks and files were small and the FAT had few entries and relatively short file chains. In the FAT12 filesystem (used on floppy disks and early hard disks), there were no more than 4,080  entries, and the FAT would usually be resident in memory. As disks got bigger, the FAT architecture began to confront penalties. On a large disk using FAT, it may be necessary to perform disk reads to learn the disk location of a file block to be read or written.
TOPS-20 (and possibly TENEX) used a 0 to 2 level tree that has similarities to a B-tree. A disk block was 512 36-bit words. If the file fit in a 512 (29) word block, then the file directory would point to that physical disk block. If the file fit in 218 words, then the directory would point to an aux index; the 512 words of that index would either be NULL (the block isn't allocated) or point to the physical address of the block. If the file fit in 227 words, then the directory would point to a block holding an aux-aux index; each entry would either be NULL or point to an aux index. Consequently, the physical disk block for a 227 word file could be located in two disk reads and read on the third.
Apple's filesystem HFS+, Microsoft's NTFS, AIX (jfs2) and some Linux filesystems, such as btrfs and Ext4, use B-trees.
B*-trees are used in the HFS and Reiser4 file systems.
DragonFly BSD's HAMMER file system uses a modified B+-tree.


== Variations ==


=== Access concurrency ===
Lehman and Yao showed that all the read locks could be avoided (and thus concurrent access greatly improved) by linking the tree blocks at each level together with a ""next"" pointer. This results in a tree structure where both insertion and search operations descend from the root to the leaf. Write locks are only required as a tree block is modified. This maximizes access concurrency by multiple users, an important consideration for databases and/or other B-tree-based ISAM storage methods. The cost associated with this improvement is that empty pages cannot be removed from the btree during normal operations. (However, see  for various strategies to implement node merging, and source code at.)
United States Patent 5283894, granted in 1994, appears to show a way to use a 'Meta Access Method'  to allow concurrent B+ tree access and modification without locks. The technique accesses the tree 'upwards' for both searches and updates by means of additional in-memory indexes that point at the blocks in each level in the block cache. No reorganization for deletes is needed and there are no 'next' pointers in each block as in Lehman and Yao.


== See also ==
B+tree
R-tree
Red–black tree
2–3 tree
2–3–4 tree


== Notes ==


== References ==

General
Bayer, R.; McCreight, E. (1972), ""Organization and Maintenance of Large Ordered Indexes"" (PDF), Acta Informatica, 1 (3): 173–189, doi:10.1007/bf00288683 
Comer, Douglas (June 1979), ""The Ubiquitous B-Tree"", Computing Surveys, 11 (2): 123–137, doi:10.1145/356770.356776, ISSN 0360-0300 .
Cormen, Thomas; Leiserson, Charles; Rivest, Ronald; Stein, Clifford (2001), Introduction to Algorithms (Second ed.), MIT Press and McGraw-Hill, pp. 434–454, ISBN 0-262-03293-7 . Chapter 18: B-Trees.
Folk, Michael J.; Zoellick, Bill (1992), File Structures (2nd ed.), Addison-Wesley, ISBN 0-201-55713-4 
Knuth, Donald (1998), Sorting and Searching, The Art of Computer Programming, Volume 3 (Second ed.), Addison-Wesley, ISBN 0-201-89685-0 . Section 6.2.4: Multiway Trees, pp. 481–491. Also, pp. 476–477 of section 6.2.3 (Balanced Trees) discusses 2-3 trees.


=== Original papers ===
Bayer, Rudolf; McCreight, E. (July 1970), Organization and Maintenance of Large Ordered Indices, Mathematical and Information Sciences Report No. 20, Boeing Scientific Research Laboratories .
Bayer, Rudolf (1971), Binary B-Trees for Virtual Memory, Proceedings of 1971 ACM-SIGFIDET Workshop on Data Description, Access and Control, San Diego, California .


== External links ==
B-tree lecture by David Scot Taylor, SJSU
B-Tree visualisation (click ""init"")
B-tree and UB-tree on Scholarpedia Curator: Dr Rudolf Bayer
B-Trees: Balanced Tree Data Structures
NIST's Dictionary of Algorithms and Data Structures: B-tree
B-Tree Tutorial
The InfinityDB BTree implementation
Cache Oblivious B(+)-trees
Dictionary of Algorithms and Data Structures entry for B*-tree
Open Data Structures - Section 14.2 - B-Trees
Counted B-Trees
B-Tree .Net, a modern, virtualized RAM & Disk implementation"
848,Pruning (morphology),2145137,1417,"The pruning algorithm is a technique used in digital image processing based on mathematical morphology. It is used as a complement to the skeleton and thinning algorithms to remove unwanted parasitic components. In this case 'parasitic' components refer to branches of a line which are not key to the overall shape of the line and should be removed. These components can often be created by edge detection algorithms or digitisation.
The standard pruning algorithm will remove all branches shorter than a given number of points. The algorithm starts at the end points and recursively removes a given number (n) of points from each branch. After this step it will apply dilation on the new end points with a (2N+1)(2N+1) structuring element of 1’s and will intersect the result with the original image. If a parasitic branch is shorter than four points and we run the algorithm with n = 4 the branch will be removed. The second step ensures that the main trunks of each line are not shortened by the procedure.


== See also ==
Morphological image processing


== External links ==
Morphological Pruning function in Mathematica"
849,ACM/IEEE Virtual Reality International Conference,42136536,1417,"SC (formerly Supercomputing), the International Conference for High Performance Computing, Networking, Storage and Analysis, is the name of the annual conference established in 1988 by the Association for Computing Machinery and the IEEE Computer Society. In 2016, about 11,000 people participated overall. The not-for-profit conference is run by a committee of approximately 600 volunteers who spend roughly three years organizing each conference.
Not to be confused with the International Supercomputing Conference.


== Sponsorship and Governance ==
SC is sponsored by the Association for Computing Machinery and the IEEE Computer Society. From its formation through 2011, ACM sponsorship was managed through ACM's Special Interest Group on Computer Architecture (SIGARCH). Sponsors are listed on each proceedings page in the ACM DL; see for example. Beginning in 2012, ACM began the process of transitioning sponsorship from SIGARCH to the recently formed Special Interest Group on High Performance Computing (SIGHPC). This transition was completed after SC15, and for SC16 ACM sponsorship was vested exclusively in SIGHPC (IEEE sponsorship remained unchanged). The conference is non-profit.
The conference is governed by a steering committee that includes representatives of the sponsoring societies, the current conference general chair, the general chairs of the preceding two years, the general chairs of the next two conference years, and a number of elected members. All steering committee members are volunteers, with the exception of the two representatives of the sponsoring societies, who are employees of those societies. The committee selects the conference general chair, approves each year's conference budget, and is responsible for setting policy and strategy for the conference.


== Conference Components ==
Although each conference committee introduces slight variations on the program each year, the core components of the conference remain largely unchanged from year to year.


=== Technical Program ===
The SC Technical Program is competitive with an acceptance rate around 20% for papers (see History). Traditionally, the program includes invited talks, panels, research papers, tutorials, workshops, posters, and Birds of a Feather (BoF) sessions.


=== Awards ===
Each year, SC hosts the following conference and sponsoring society awards:
ACM Gordon Bell Prize
ACM/IEEE-CS George Michael Memorial HPC Fellowship
ACM/IEEE-CS Ken Kennedy Award
ACM SIGHPC Computational & Data Science Fellowships
IEEE-CS Seymour Cray Computer Engineering Award
IEEE-CS Sidney Fernbach Memorial Award
IEEE CS TCHPC Award for Excellence for Early Career Researchers in HPC
Test of Time Award


=== Exhibits ===
In addition to the technical program, SC hosts a research exhibition each year that includes universities, state-sponsored computing research organizations (such as the Federal labs in the US), and vendors of HPC-related hardware and software from many countries around the world. There were 353 exhibitors at SC16 in Salt Lake City, UT.


=== Student Program ===
SC's program for students has gone through a variety of changes and emphases over the years. Beginning with SC15 the program is called ""Students@SC"", and is oriented toward undergraduate and graduate students in computing related fields, and computing-oriented students in science and engineering. The program includes professional development programs, opportunities to learn from mentors, and engagement with SC’s technical sessions.


=== SCinet ===
SCinet is SC’s research network. Started in 1991, SCinet features emerging technologies for very high bandwidth, low latency wide area network communications in addition to operational services necessary to provide conference attendees with connectivity to the commodity Internet and to many national research and engineering networks.


== Name changes ==
Since its establishment in 1988, and until 1995, the full name of the conference was the ""ACM/IEEE Supercomputing Conference"" (sometimes: ""ACM/IEEE Conference on Supercomputing""). The conference's abbreviated (and more commonly used) formal name was ""Supercomputing 'XY"", where XY denotes the last two digits of the year. In 1996, according to the archived front matter of the conference proceedings, the full name was changed to the ACM/IEEE ""International Conference on High Performance Computing and Communications"". The latter document further announced that, as of 1997, the conference will undergo a name change and will be called ""SC97: High Performance Networking and Computing"". The document explained that

1997 [will mark] the first use of ""SC97"" as the name of the annual conference you've known as ""Supercomputing 'XY"". This change reflects our growing attention to networking, distributed computing, data-intensive applications, and other emerging technologies that push the frontiers of communications and computing.

A 1997 HPCwire article discussed at length the reasoning, considerations, and concerns that accompanied the decision to change the name of the conference series from ""Supercomputing 'XY"" to ""SC 'XY"", stating that

It's official: the age of supercomputing has ended. At any rate, the word ""supercomputing"" has been excised from the title of the annual trade shows, sponsored by the IEEE and ACM, that have been known for almost ten years as ""Supercomputing '(final two digits of year)"". The next event, to be held in San Jose next November, has been redesignated ""SC '97."" Like Lewis Carroll's Cheshire Cat, ""supercomputing"" has faded steadily away until only the smile, nose, and whiskers remain. [...] The loss is a real one. An enormous range of ordinary people had some idea, however vague, what ""supercomputing"" meant. No-caf, local alternatives like ""SC"" and ""HPC"" lack this authority. This is not a trivial issue. In these days of rapid change, passing technofancies, and information overload, a rose with the wrong name is just another thorn -- or forgotten immediately. After all, how can businessmen, ordinary consumers, and taxpayers be expected to pay money for something they can't comprehend? More important, will investors and grant-givers hand over money to support further R&D on something whose only identity is an arbitrary clump of capital letters?

Despite these concerns, the abbreviated name of the conference, ""SC"", is still used today, a reminiscent of the abbreviation of the conference's original name—""Supercomputing Conference"".
The full name, in contrast, underwent several changes. Between 1997 and 2003, the name ""High Performance Networking and Computing"" was specified in the front matter of the archived conference proceedings in some years (1997, 1998, 2000, 2002), whereas in other years it was omitted altogether in favor of the abbreviated name (1999, 2001, 2003). In 2004, the stated front matter full name was changed to ""High Performance Computing, Networking and Storage Conference"". In 2005, this name was replaced by the original name of the conference—""supercomputing""— in the front matter. Finally, in 2006, the current full name, as used today, emerged: ""The International Conference for High Performance Computing, Networking, Storage and Analysis"".
Despite all of the name variances in the proceedings through the years, the digital library of ACM, the co-sponsoring society, records the name of the conference as ""The ACM/IEEE Conference on Supercomputing"" from 1998 - 2008, when it changes to """"The International Conference for High Performance Computing, Networking, Storage and Analysis"". It is these two names that are used in the full citations to the conference proceedings provided in this article.


== History ==
The table below provides the location, name of the general chair, and acceptance statistics for each year of SC. Note that references for data in these tables apply to data preceding the reference to the left on the same row; for example, for SC17 the single reference substantiates all the information in that row, but for SC05 the source for the convention center and chair is different than the source for the acceptance statistics.
The following table details the keynote speakers during the history of the conference; as of SC17, 20% of the keynote speakers have been female, with a mix of speakers from corporate, academic, and national government organizations.


== See also ==
Gordon Bell Prize
Sidney Fernbach Award
Seymour Cray Award
Ken Kennedy Award
TOP500
Green500
HPC Challenge Awards
SCinet
Storcloud


== References ==


== External links ==
The SC Conference Website
SC12 - The International Conference for High Performance Computing, Networking, Storage and Analysis"
850,International Conference on Automated Reasoning with Analytic Tableaux and Related Methods,6544209,1414,"The International Conference on Automated Reasoning with Analytic Tableaux and Related Methods (TABLEAUX) is an annual international academic conference that deals with all aspects of automated reasoning with analytic tableaux. Periodically, it joins with CADE and TPHOLs into the International Joint Conference on Automated Reasoning (IJCAR).
The first table convened in 1992. Since 1995, the proceedings of this conference have been published by Springer's LNAI series.
In August 2006 TABLEAUX was part of the Federated Logic Conference in Seattle, USA. The following TABLEAUX were held in 2007 in Aix en Provence, France, as part of IJCAR 2008, in Sydney, Australia, as TABLEAUX 2009, in Oslo, Norway, as part of IJCAR 2010, Edinburgh, UK, as TABLEAUX 2011, in Bern, Switzerland, 4-8 July 2011, as part of IJCAR 2012, Manchester, United Kingdom, as TABLEAUX 2013, Nancy, France, 16-19 September 2013, and as part of IJCAR 2014, Vienna, Austria, 19-22 July 2014.


== External links ==
TABLEAUX home page"
851,Best bin first,15585323,1377,"Best bin first is a search algorithm that is designed to efficiently find an approximate solution to the nearest neighbor search problem in very-high-dimensional spaces. The algorithm is based on a variant of the kd-tree search algorithm which makes indexing higher-dimensional spaces possible. Best bin first is an approximate algorithm which returns the nearest neighbor for a large fraction of queries and a very close neighbor otherwise.


== Differences from kd tree ==
Bins are looked in order to increasing distance from the query point. The distance to a bin is defined as a minimal distance to any point of its boundary. This is implemented with priority queue.
Search a fixed number of nearest candidates and stop.
A speedup of two orders of magnitude is typical.


== References =="
852,Memory organisation,3657056,1376,"There are several ways to organise memories with respect to the way they are connected to the cache:
one-word-wide memory organisation
wide memory organisation
interleaved memory organisation
independent memory organisation


== One-Word-Wide ==
The memory is one word wide and connected via a one word wide bus to the cache.


== Wide ==
The memory is more than one word wide (usually four words wide) and connected by an equally wide bus to the low level cache (which is also wide). From the cache multiple busses of one word wide go to a MUX which selects the correct bus to connect to the high level cache.


== Interleaved ==

There are several memory banks which are one word wide, and one word wide bus. There is some logic in the memory that selects the correct bank to use when the memory gets accessed by the cache.
Memory interleaving is a way to distribute individual addresses over memory modules. Its aim is to keep the most of modules busy as computations proceed. With memory interleaving, the low-order k bits of the memory address generally specify the module on several buses."
853,John Leslie King,38276626,1370,"John Leslie King is a W.W. Bishop Professor at the University of Michigan School of Information. His main works deal with computerization in the public sector and municipalities, as well as other organizations. He has also worked on privacy issues and some of the primary computerization projects such as project SAGE. He is the author (together with Kalle Lyytinen) of Information systems : the state of the field, published by Wiley in 2006, which, according to WorldCat, is held in 126 libraries.
Prof. King has a BA in Philosophy, and graduated with a PhD from the University of California, Irvine School of Administration. Today the School of Administration in UC Irvine is the Paul Merage School of Business.


== References ==


== External links ==
Personal website at U. Mich."
854,Arithmetic IF,16902101,1369,"Arithmetic (from the Greek ἀριθμός arithmos, ""number"") is a branch of mathematics that consists of the study of numbers, especially the properties of the traditional operations on them—addition, subtraction, multiplication and division. Arithmetic is an elementary part of number theory, and number theory is considered to be one of the top-level divisions of modern mathematics, along with algebra, geometry, and analysis. The terms arithmetic and higher arithmetic were used until the beginning of the 20th century as synonyms for number theory and are sometimes still used to refer to a wider part of number theory.


== History ==
The prehistory of arithmetic is limited to a small number of artifacts which may indicate the conception of addition and subtraction, the best-known being the Ishango bone from central Africa, dating from somewhere between 20,000 and 18,000 BC, although its interpretation is disputed.
The earliest written records indicate the Egyptians and Babylonians used all the elementary arithmetic operations as early as 2000 BC. These artifacts do not always reveal the specific process used for solving problems, but the characteristics of the particular numeral system strongly influence the complexity of the methods. The hieroglyphic system for Egyptian numerals, like the later Roman numerals, descended from tally marks used for counting. In both cases, this origin resulted in values that used a decimal base but did not include positional notation. Complex calculations with Roman numerals required the assistance of a counting board or the Roman abacus to obtain the results.
Early number systems that included positional notation were not decimal, including the sexagesimal (base 60) system for Babylonian numerals and the vigesimal (base 20) system that defined Maya numerals. Because of this place-value concept, the ability to reuse the same digits for different values contributed to simpler and more efficient methods of calculation.
The continuous historical development of modern arithmetic starts with the Hellenistic civilization of ancient Greece, although it originated much later than the Babylonian and Egyptian examples. Prior to the works of Euclid around 300 BC, Greek studies in mathematics overlapped with philosophical and mystical beliefs. For example, Nicomachus summarized the viewpoint of the earlier Pythagorean approach to numbers, and their relationships to each other, in his Introduction to Arithmetic.
Greek numerals were used by Archimedes, Diophantus and others in a positional notation not very different from ours. Because the ancient Greeks lacked a symbol for zero (until the Hellenistic period), they used three separate sets of symbols. One set for the unit's place, one for the ten's place, and one for the hundred's. Then for the thousand's place they would reuse the symbols for the unit's place, and so on. Their addition algorithm was identical to ours, and their multiplication algorithm was only very slightly different. Their long division algorithm was the same, and the square root algorithm that was once taught in school was known to Archimedes, who may have invented it. He preferred it to Hero's method of successive approximation because, once computed, a digit doesn't change, and the square roots of perfect squares, such as 7485696, terminate immediately as 2736. For numbers with a fractional part, such as 546.934, they used negative powers of 60 instead of negative powers of 10 for the fractional part 0.934.
The ancient Chinese had advanced arithmetic studies dating from the Shang Dynasty and continuing through the Tang Dynasty, from basic numbers to advanced algebra. The ancient Chinese used a positional notation similar to that of the Greeks. Since they also lacked a symbol for zero, they had one set of symbols for the unit's place, and a second set for the ten's place. For the hundred's place they then reused the symbols for the unit's place, and so on. Their symbols were based on the ancient counting rods. It is a complicated question to determine exactly when the Chinese started calculating with positional representation, but it was definitely before 400 BC. The ancient Chinese were the first to meaningfully discover, understand, and apply negative numbers as explained in the Nine Chapters on the Mathematical Art (Jiuzhang Suanshu), which was written by Liu Hui.
The gradual development of Hindu–Arabic numerals independently devised the place-value concept and positional notation, which combined the simpler methods for computations with a decimal base and the use of a digit representing 0. This allowed the system to consistently represent both large and small integers. This approach eventually replaced all other systems. In the early 6th century AD, the Indian mathematician Aryabhata incorporated an existing version of this system in his work, and experimented with different notations. In the 7th century, Brahmagupta established the use of 0 as a separate number and determined the results for multiplication, division, addition and subtraction of zero and all other numbers, except for the result of division by 0. His contemporary, the Syriac bishop Severus Sebokht (650 AD) said, ""Indians possess a method of calculation that no word can praise enough. Their rational system of mathematics, or of their method of calculation. I mean the system using nine symbols."" The Arabs also learned this new method and called it hesab.

Although the Codex Vigilanus described an early form of Arabic numerals (omitting 0) by 976 AD, Leonardo of Pisa (Fibonacci) was primarily responsible for spreading their use throughout Europe after the publication of his book Liber Abaci in 1202. He wrote, ""The method of the Indians (Latin Modus Indoram) surpasses any known method to compute. It's a marvelous method. They do their computations using nine figures and symbol zero"".
In the Middle Ages, arithmetic was one of the seven liberal arts taught in universities.
The flourishing of algebra in the medieval Islamic world and in Renaissance Europe was an outgrowth of the enormous simplification of computation through decimal notation.
Various types of tools have been invented and widely used to assist in numeric calculations. Before Renaissance, they were various types of abaci. More recent examples include slide rules, nomograms and mechanical calculators, such as Pascal's calculator. At present, they have been supplanted by electronic calculators and computers.


== Arithmetic operations ==

The basic arithmetic operations are addition, subtraction, multiplication and division, although this subject also includes more advanced operations, such as manipulations of percentages, square roots, exponentiation, and logarithmic functions. Arithmetic is performed according to an order of operations. Any set of objects upon which all four arithmetic operations (except division by 0) can be performed, and where these four operations obey the usual laws, is called a field.


=== Addition (+) ===

Addition is the basic operation of arithmetic. In its simplest form, addition combines two numbers, the addends or terms, into a single number, the sum of the numbers (Such as 2 + 2 = 4 or 3 + 5 = 8).
Adding more than two numbers can be viewed as repeated addition; this procedure is known as summation and includes ways to add infinitely many numbers in an infinite series; repeated addition of the number 1 is the most basic form of counting.
Addition is commutative and associative so the order the terms are added in does not matter. The identity element of addition (the additive identity) is 0, that is, adding 0 to any number yields that same number. Also, the inverse element of addition (the additive inverse) is the opposite of any number, that is, adding the opposite of any number to the number itself yields the additive identity, 0. For example, the opposite of 7 is −7, so 7 + (−7) = 0.
Addition can be given geometrically as in the following example:
If we have two sticks of lengths 2 and 5, then if we place the sticks one after the other, the length of the stick thus formed is 2 + 5 = 7.


=== Subtraction (−) ===

Subtraction is the inverse of addition. Subtraction finds the difference between two numbers, the minuend minus the subtrahend. If the minuend is larger than the subtrahend, the difference is positive; if the minuend is smaller than the subtrahend, the difference is negative; if they are equal, the difference is 0.
Subtraction is neither commutative nor associative. For that reason, it is often helpful to look at subtraction as addition of the minuend and the opposite of the subtrahend, that is a − b = a + (−b). When written as a sum, all the properties of addition hold.
There are several methods for calculating results, some of which are particularly advantageous to machine calculation. For example, digital computers employ the method of two's complement. Of great importance is the counting up method by which change is made. Suppose an amount P is given to pay the required amount Q, with P greater than Q. Rather than performing the subtraction P − Q and counting out that amount in change, money is counted out starting at Q and continuing until reaching P. Although the amount counted out must equal the result of the subtraction P − Q, the subtraction was never really done and the value of P − Q might still be unknown to the change-maker.


=== Multiplication (× or · or *) ===

Multiplication is the second basic operation of arithmetic. Multiplication also combines two numbers into a single number, the product. The two original numbers are called the multiplier and the multiplicand, sometimes both simply called factors.
Multiplication may be viewed as a scaling operation. If the numbers are imagined as lying in a line, multiplication by a number, say x, greater than 1 is the same as stretching everything away from 0 uniformly, in such a way that the number 1 itself is stretched to where x was. Similarly, multiplying by a number less than 1 can be imagined as squeezing towards 0. (Again, in such a way that 1 goes to the multiplicand.)
Multiplication is commutative and associative; further it is distributive over addition and subtraction. The multiplicative identity is 1, that is, multiplying any number by 1 yields that same number. Also, the multiplicative inverse is the reciprocal of any number (except 0; 0 is the only number without a multiplicative inverse), that is, multiplying the reciprocal of any number by the number itself yields the multiplicative identity.
The product of a and b is written as a × b or a·b. When a or b are expressions not written simply with digits, it is also written by simple juxtaposition: ab. In computer programming languages and software packages in which one can only use characters normally found on a keyboard, it is often written with an asterisk: a * b.


=== Division (÷ or /) ===

Division is essentially the inverse of multiplication. Division finds the quotient of two numbers, the dividend divided by the divisor. Any dividend divided by 0 is undefined. For distinct positive numbers, if the dividend is larger than the divisor, the quotient is greater than 1, otherwise it is less than 1 (a similar rule applies for negative numbers). The quotient multiplied by the divisor always yields the dividend.
Division is neither commutative nor associative. As it is helpful to look at subtraction as addition, it is helpful to look at division as multiplication of the dividend times the reciprocal of the divisor, that is a ÷ b = a × 1/b. When written as a product, it obeys all the properties of multiplication.


== Decimal arithmetic ==
Decimal representation refers exclusively, in common use, to the written numeral system employing arabic numerals as the digits for a radix 10 (""decimal"") positional notation; however, any numeral system based on powers of 10, e.g., Greek, Cyrillic, Roman, or Chinese numerals may conceptually be described as ""decimal notation"" or ""decimal representation"".
Modern methods for four fundamental operations (addition, subtraction, multiplication and division) were first devised by Brahmagupta of India. This was known during medieval Europe as ""Modus Indoram"" or Method of the Indians. Positional notation (also known as ""place-value notation"") refers to the representation or encoding of numbers using the same symbol for the different orders of magnitude (e.g., the ""ones place"", ""tens place"", ""hundreds place"") and, with a radix point, using those same symbols to represent fractions (e.g., the ""tenths place"", ""hundredths place""). For example, 507.36 denotes 5 hundreds (102), plus 0 tens (101), plus 7 units (100), plus 3 tenths (10−1) plus 6 hundredths (10−2).
The concept of 0 as a number comparable to the other basic digits is essential to this notation, as is the concept of 0's use as a placeholder, and as is the definition of multiplication and addition with 0. The use of 0 as a placeholder and, therefore, the use of a positional notation is first attested to in the Jain text from India entitled the Lokavibhâga, dated 458 AD and it was only in the early 13th century that these concepts, transmitted via the scholarship of the Arabic world, were introduced into Europe by Fibonacci using the Hindu–Arabic numeral system.
Algorism comprises all of the rules for performing arithmetic computations using this type of written numeral. For example, addition produces the sum of two arbitrary numbers. The result is calculated by the repeated addition of single digits from each number that occupies the same position, proceeding from right to left. An addition table with ten rows and ten columns displays all possible values for each sum. If an individual sum exceeds the value 9, the result is represented with two digits. The rightmost digit is the value for the current position, and the result for the subsequent addition of the digits to the left increases by the value of the second (leftmost) digit, which is always one. This adjustment is termed a carry of the value 1.
The process for multiplying two arbitrary numbers is similar to the process for addition. A multiplication table with ten rows and ten columns lists the results for each pair of digits. If an individual product of a pair of digits exceeds 9, the carry adjustment increases the result of any subsequent multiplication from digits to the left by a value equal to the second (leftmost) digit, which is any value from 1 to 8 (9 × 9 = 81). Additional steps define the final result.
Similar techniques exist for subtraction and division.
The creation of a correct process for multiplication relies on the relationship between values of adjacent digits. The value for any single digit in a numeral depends on its position. Also, each position to the left represents a value ten times larger than the position to the right. In mathematical terms, the exponent for the radix (base) of 10 increases by 1 (to the left) or decreases by 1 (to the right). Therefore, the value for any arbitrary digit is multiplied by a value of the form 10n with integer n. The list of values corresponding to all possible positions for a single digit is written as {..., 102, 10, 1, 10−1, 10−2, ...}.
Repeated multiplication of any value in this list by 10 produces another value in the list. In mathematical terminology, this characteristic is defined as closure, and the previous list is described as closed under multiplication. It is the basis for correctly finding the results of multiplication using the previous technique. This outcome is one example of the uses of number theory.


== Compound unit arithmetic ==
Compound unit arithmetic is the application of arithmetic operations to mixed radix quantities such as feet and inches, gallons and pints, pounds shillings and pence, and so on. Prior to the use of decimal-based systems of money and units of measure, the use of compound unit arithmetic formed a significant part of commerce and industry.


=== Basic arithmetic operations ===
The techniques used for compound unit arithmetic were developed over many centuries and are well-documented in many textbooks in many different languages. In addition to the basic arithmetic functions encountered in decimal arithmetic, compound unit arithmetic employs three more functions:
Reduction where a compound quantity is reduced to a single quantity, for example conversion of a distance expressed in yards, feet and inches to one expressed in inches.
Expansion, the inverse function to reduction, is the conversion of a quantity that is expressed as a single unit of measure to a compound unit, such as expanding 24 oz to 1 lb, 8 oz.
Normalization is the conversion of a set of compound units to a standard form – for example rewriting ""1 ft 13 in"" as ""2 ft 1 in"".
Knowledge of the relationship between the various units of measure, their multiples and their submultiples forms an essential part of compound unit arithmetic.


=== Principles of compound unit arithmetic ===
There are two basic approaches to compound unit arithmetic:
Reduction–expansion method where all the compound unit variables are reduced to single unit variables, the calculation performed and the result expanded back to compound units. This approach is suited for automated calculations. A typical example is the handling of time by Microsoft Excel where all time intervals are processed internally as days and decimal fractions of a day.
On-going normalization method in which each unit is treated separately and the problem is continuously normalized as the solution develops. This approach, which is widely described in classical texts, is best suited for manual calculations. An example of the ongoing normalization method as applied to addition is shown below.

The addition operation is carried out from right to left; in this case, pence are processed first, then shillings followed by pounds. The numbers below the ""answer line"" are intermediate results.
The total in the pence column is 25. Since there are 12 pennies in a shilling, 25 is divided by 12 to give 2 with a remainder of 1. The value ""1"" is then written to the answer row and the value ""2"" carried forward to the shillings column. This operation is repeated using the values in the shillings column, with the additional step of adding the value that was carried forward from the pennies column. The intermediate total is divided by 20 as there are 20 shillings in a pound. The pound column is then processed, but as pounds are the largest unit that is being considered, no values are carried forward from the pounds column.
For the sake of simplicity, the example chosen did not have farthings.


=== Operations in practice ===

During the 19th and 20th centuries various aids were developed to aid the manipulation of compound units, particularly in commercial applications. The most common aids were mechanical tills which were adapted in countries such as the United Kingdom to accommodate pounds, shillings, pennies and farthings and ""Ready Reckoners"" – books aimed at traders that catalogued the results of various routine calculations such as the percentages or multiples of various sums of money. One typical booklet that ran to 150 pages tabulated multiples ""from one to ten thousand at the various prices from one farthing to one pound"".
The cumbersome nature of compound unit arithmetic has been recognized for many years – in 1586, the Flemish mathematician Simon Stevin published a small pamphlet called De Thiende (""the tenth"") in which he declared that the universal introduction of decimal coinage, measures, and weights to be merely a question of time while in the modern era, many conversion programs, such as that embedded in the calculator supplied as a standard part of the Microsoft Windows 7 operating system display compound units in a reduced decimal format rather than using an expanded format (i.e. ""2.5 ft"" is displayed rather than ""2 ft 6 in"").


== Number theory ==

Until the 19th century, number theory was a synonym of ""arithmetic"". The addressed problems were directly related to the basic operations and concerned primality, divisibility, and the solution of equations in integers, such as Fermat's last theorem. It appeared that most of these problems, although very elementary to state, are very difficult and may not be solved without very deep mathematics involving concepts and methods from many other branches of mathematics. This led to new branches of number theory such as analytic number theory, algebraic number theory, Diophantine geometry and arithmetic algebraic geometry. Wiles' proof of Fermat's Last Theorem is a typical example of the necessity of sophisticated methods, which go far beyond the classical methods of arithmetic, for solving problems that can be stated in elementary arithmetic.


== Arithmetic in education ==
Primary education in mathematics often places a strong focus on algorithms for the arithmetic of natural numbers, integers, fractions, and decimals (using the decimal place-value system). This study is sometimes known as algorism.
The difficulty and unmotivated appearance of these algorithms has long led educators to question this curriculum, advocating the early teaching of more central and intuitive mathematical ideas. One notable movement in this direction was the New Math of the 1960s and 1970s, which attempted to teach arithmetic in the spirit of axiomatic development from set theory, an echo of the prevailing trend in higher mathematics.
Also, arithmetic was used by Islamic Scholars in order to teach application of the rulings related to Zakat and Irth. This was done in a book entitled The Best of Arithmetic by Abd-al-Fattah-al-Dumyati.
The book begins with the foundations of mathematics and proceeds to its application in the later chapters.


== See also ==
Lists of mathematics topics
Mathematics
Outline of arithmetic
Slide rule


=== Related topics ===


== Notes ==


== References ==


== External links ==
MathWorld article about arithmetic
The New Student's Reference Work/Arithmetic (historical)
The Great Calculation According to the Indians, of Maximus Planudes – an early Western work on arithmetic at Convergence
 Weyde, P. H. Vander (1879). ""Arithmetic"". The American Cyclopædia."
855,Global Neighborhood Watch,932825,1367,"""Global Neighborhood Watch"" is an article by Neal Stephenson that appeared in Wired Magazine in 1998. In it he proposes a specific plan for using information technology to fight crime. According to Stephenson, he is no longer pursuing the idea.


== References ==


== External links ==
Wired article
Daily Mail article
1995 interview with Neal Stephenson regarding Global Neighborhood Watch
Alternative source for the text of the article"
856,Information Harvesting,7688277,1366,"Information Harvesting (IH) was an early data mining product from the 1990s. It was invented by Ralphe Wiggins and produced by the Ryan Corp, later Information Harvesting Inc., of Cambridge, Massachusetts. IH sought to infer rules from sets of data. It did this first by classifying various input variables into one of a number of bins, thereby putting some structure on the continuous variables in the input. IH then proceeds to generate rules, trading off generalization against memorization, that will infer the value of the prediction variable, possibly creating many levels of rules in the process. It included strategies for checking if overfitting took place and, if so, correcting for it. Because of its strategies for correcting for overfitting by considering more data, and refining the rules based on that data, IH might also be considered to be a form of machine learning.
The advantage of IH, as compared with other data mining products of its time and even later, was that it provided a mechanism for finding multiple rules that would classify the data and determining, according to set criteria, the best rules to use.


== References =="
857,Lazy deletion,13999239,1361,"In computer science, lazy deletion refers to a method of deleting elements from a hash table that uses open addressing. In this method, deletions are done by marking an element as deleted, rather than erasing it entirely. Deleted locations are treated as empty when inserting and as occupied during a search.
The problem with this scheme is that as the number of delete/insert operations increases, the cost of a successful search increases. To improve this, when an element is searched and found in the table, the element is relocated to the first location marked for deletion that was probed during the search. Instead of finding an element to relocate when the deletion occurs, the relocation occurs lazily during the next search.


== References =="
858,Transaction data,15348791,1351,"Transaction data are data describing an event (the change as a result of a transaction) and is usually described with verbs. Transaction data always has a time dimension, a numerical value and refers to one or more objects (i.e. the reference data).
Typical transactions are:
Financial: orders, invoices, payments
Work: Plans, activity records
Logistics: Deliveries, storage records, travel records, etc.
Typical transaction processing systems (systems generating transactions) are SAP and Oracle Financials.


== Records management ==

Recording and retaining transactions is called records management. The record of the transaction is stored in a place where the retention can be guaranteed and where data are archived/removed following a retention period. The format of the transaction can be data (to be stored in a database), but it can also be a document.


== Data warehousing ==
Transaction data can be summarised in a data warehouse, which helps accessibility and analysis of the data.


== See also ==
Data modeling
Data architecture
Information lifecycle management
Reference data"
859,Coalition to Diversify Computing,49862973,1350,"The Coalition to Diversify Computing (CDC) is a joint organization of the Association for Computing Machinery (ACM) and the Computing Research Association (CRA). CDC emphasizes recruiting minority undergraduates to MS/PhD programs, retaining minority graduate students enrolled in MS/PhD programs, and transitioning minority MS/PhD graduates into academia, industry, and government careers.


== See also ==

Association for Computing Machinery
Diversity in computing


== References ==


== External links ==
Official website"
860,Principal (computer security),21557464,1348,"A principal in computer security is an entity that can be authenticated by a computer system or network. It is referred to as a security principal in Java and Microsoft literature.
Principals can be individual people, computers, services, computational entities such as processes and threads, or any group of such things. They need to be identified and authenticated before they can be assigned rights and privileges over resources in the network. A principal typically has an associated identifier (such as a security identifier) that allows it to be referenced for identification or assignment of properties and permissions.


== References ==


== External links ==
RFC 2744 - Generic Security Service API Version 2.
RFC 5397 - WebDAV Current Principal Extension.
RFC 4121 - The Kerberos Version 5 Generic Security Service Application Program Interface (GSS-API) Mechanism: Version 2."
861,FINO,2005111,1343,"Fino (""refined"" in Spanish) is the driest and palest of the traditional varieties of Sherry and Montilla-Moriles fortified wine. They are consumed comparatively young and, unlike the sweeter varieties, should be consumed soon after the bottle is opened as exposure to air can cause them to lose their flavour within hours.


== Flor ==
The defining component of Fino sherries is the strain of yeast known as flor that floats in a layer on top of sherry in the wine barrel. Until the mid-19th century most sherry winemakers did not understand what this yellowish foam that randomly appeared in some of their barrels was. They would mark these barrels as ""sick"" and relegate them to their lowest bottlings of wine. It turned out that this strain of Saccharomyces yeast throve in air, and the more ""head room"" there was in the barrel the more likely it was to develop. Over time winemakers noticed that these wines were lighter and fresher than their other sherries, with the flor acting as a protective blanket over the wine that shielded it from excessive oxidation.


== Varieties ==

Jerez Fino, made from grapes grown in the vineyards around Jerez and aged in the wine cellars there, where the climate is hotter than those near the coast. The hotter summers cause Jerez Fino to develop a thinner layer of flor and thus a stronger flavour due to more exposure to the air.
Puerto Fino, made around El Puerto de Santa María. The cooler climate near the sea results in a thicker layer of flor and a more acidic and delicate flavour than Jerez Fino.
Manzanilla, made around Sanlúcar de Barrameda, where the climate is cooler than El Puerto de Santa María. Similar to the Puerto Fino, Manzanilla has a fresher and more delicate flavour than Jerez Fino.
Fino may also be produced in DO Montilla-Moriles. There the Fino along with the other sweet and fortified wines is made from the Pedro Ximénez grape as opposed to the Palomino grape used in Jerez.
Sweetened Fino is called Pale Cream Sherry .
On 12 April 2012, the rules applicable to the sweet and fortified Denominations of Origen Montilla-Moriles and Jerez-Xérès-Sherry were changed.
The classification by sweetness is:


== Production ==
In production of finos, winemakers normally will only use the free run juice-the juice that is produced by crushing the grapes under their own weight before they are sent to a wine press. The juice that comes after pressing is typically more coarse and produces heavier bodied wines. That juice is typically used to make oloroso sherry.
When first barreled, sherries made using the fino method are only partially filled to allow the action of the flor yeast to give it the distinctive fresh taste of dry sherries. If the flor is allowed to die and the wine undergoes oxidative aging, the wine darkens and the flavour becomes stronger, resulting in an amontillado sherry.
In the final classification of a fino, it is judged on such qualities as cleanness, paleness, dryness, and aroma. According to the overseer's judgment, the initial stroke mark on the cask may then be embellished with one or more 'palm leaves'--curved marks that branch off the side of the initial mark. Wines receiving these marks are designated accordingly 'una palma,' 'dos palmas,' 'tres palmas,' with each additional palm leaf indicating a higher standard of quality.


== Storing ==
Fino is the most delicate form of sherry and should be drunk within a year of bottling, although some argue that fino should not be older than six months. Once opened it will immediately begin to deteriorate and should be drunk in one sitting for the best results. If necessary it can be stored, corked and refrigerated, for up to one week after opening.
Since sherry is not vintage dated, it can be hard to tell when the Fino was bottled. However, the bottling date is printed on the label, albeit in an encoded form. On the back label will be a small dot matrix number that starts with the letter L. After the L will be either a 4 or 5 digit number. For the 4 digit number, such as 7005, the first number is the year, and the last three numbers are a number between 1 and 365 that indicates the day of the year. So this bottle was bottled on 5 January 2007. The 5 digit code is similar, such as 00507, where the Julian date precedes the year. This was also 5 January 2007.


== Serving ==
As with other particularly dry sherries, it is best served chilled at 7–10 °C before a meal, and in Spain is almost always drunk with tapas of some form, such as olives, almonds, or seafood.


== External links ==
Article on dating Fino sherry bottles


== References =="
862,ECSE (Academic Degree),20846765,1342,"ECSE is an abbreviation for Electrical Engineering and Computer Sciences and Systems Engineering. It is a designation used at some universities for the major or department that blends these three fields together.
One reason behind linking the areas of study is to provide students with a broad overview of each of software, hardware and Systems theory. However there are also reasons for not blending departments: Students who major in theoretical computer science, studying such topics as algorithm analysis and software engineering, may not have any use for extensive electrical engineering or systems theory classes.
Not every university uses the ECSE designation. Several universities, for example, have separate EE/ECE and CS departments/majors. Other schools use the similar ECE (Electrical and Computer Engineering) designation. Additionally, some schools which offer an ECSE degree also offer degrees in Electrical Engineering or Computer Science separately.


== Academic Citation ==
""ECSE"". Retrieved 2008-12-26. 


== See also ==
Academic major"
863,CADE ATP System Competition,19906973,1339,"The CADE ATP System Competition (CASC) is a yearly competition of fully automated theorem provers for classical first order logic. CASC is associated with the Conference on Automated Deduction and the International Joint Conference on Automated Reasoning organized by the Association for Automated Reasoning.
The first CASC, CASC-13, was held as part of the 13th Conference on Automated Deduction at Rutgers University, New Brunswick, NJ, in 1996.


== References ==

Sutcliffe, Geoff (2011). ""The 5th IJCAR Automated Theorem Proving System Competition - CASC-J5"". AI Communications. 24 (1): 75–89. 
Geoff Sutcliffe. ""The CADE ATP System Competition"". Retrieved 2008-10-23. 
Geoff Sutcliffe and Christian Suttner (2006). ""The State of CASC"". AI Communications. 19 (1): 35–48. 
Jeff Pelletier, Geoff Sutcliffe and Christian Suttner (2002). ""The Development of CASC"". AI Communications. 15 (2–3): 79–90. 


== External links ==
CASC Website"
865,Coset leader,19199143,1335,"In coding theory, a coset leader is a word of minimum weight in any particular coset - that is, a word with the lowest amount of non-zero entries. Sometimes there are several words of equal minimum weight in a coset, and in that case, any one of those words may be chosen to be the coset leader.
Coset leaders are used in the construction of a standard array for a linear code, which can then be used to decode received vectors. For a received vector y, the decoded message is y - e, where e is the coset leader of y. Coset leaders can also be used to construct a fast decoding strategy. For each coset leader u we calculate the syndrome uH′. When we receive v we evaluate vH′ and find the matching syndrome. The corresponding coset leader is the most likely error pattern and we assume that v+u was the codeword sent.


== References ==
Hill, Raymond (1986). A First Course in Coding Theory. Oxford Applied Mathematics and Computing Science series. Oxford University Press. ISBN 978-0-19-853803-5."
866,Recursive transition network,7970632,1324,"A recursive transition network (""RTN"") is a graph theoretical schematic used to represent the rules of a context-free grammar. RTNs have application to programming languages, natural language and lexical analysis. Any sentence that is constructed according to the rules of an RTN is said to be ""well-formed"". The structural elements of a well-formed sentence may also be well-formed sentences by themselves, or they may be simpler structures. This is why RTNs are described as recursive.


== Notes and references ==


== See also ==
Syntax diagram
Computational linguistics
Context free language
Finite state machine
Formal grammar
Parse tree
Parsing
Augmented transition network"
867,Error floor,6765815,1320,"The error floor is a phenomenon encountered in modern iterated sparse graph-based error correcting codes like LDPC codes and turbo codes. When the bit error ratio (BER) is plotted for conventional codes like Reed–Solomon codes under algebraic decoding or for convolutional codes under Viterbi decoding, the BER steadily decreases in the form of a curve as the SNR condition becomes better. For LDPC codes and turbo codes there is a point after which the curve does not fall as quickly as before, in other words, there is a region in which performance flattens. This region is called the error floor region. The region just before the sudden drop in performance is called the waterfall region.
Error floors are usually attributed to low-weight codewords (in the case of Turbo codes) and trapping sets or near-codewords (in the case of LDPC codes).


== References =="
868,Interscript,52102683,1314,"Interscript was a rich text document markup language designed by Xerox to act as a common interchange format between disparate document formats. It was part of a system that included the Xerox Character Code Standard (XCCS) and the InterPress page description representation.


== References ==


== External links ==
""Introduction to Interscript"" (PDF). Xerox. 19 September 1985. Retrieved 2016-10-26. 
""INTERSCRIPT"" (PDF). Xerox. March 1984. Retrieved 2016-10-26."
869,Kousha Etessami,53563061,1313,"Kousha Etessami is a professor of Computer Science at the University of Edinburgh, Scotland, UK. He has received his Ph.D from the University of Massachusetts Amherst in 1995. He works on theoretical computer science, in particular on computational complexity theory, game theory and probabilistic systems.


== References ==


== External links ==
Kousha Etessami publications indexed by Google Scholar"
870,Alan D. Berenbaum Distinguished Service Award,54216469,1311,"The Association for Computing Machinery SIGARCH Alan D. Berenbaum Distinguished Service Award is given for outstanding service in the field of computer architecture and design. Prior recipients include:
2016 – Michael Flynn
2014 – Doug DeGroot
2013 – Norman P. Jouppi
2011 – David A. Patterson
2010 – Janie Irwin
2009 – Mark D. Hill
2008 – Alan Berenbaum


== See also ==

ACM Special Interest Group on Computer Architecture
Computer engineering
Computer science
Computing
Service


== References ==


== External links ==
ACM SIGARCH Alan D. Berenbaum Distinguished Service Award"
871,Knowledge Systems Laboratory,17232,1309,"Knowledge Systems Laboratory (KSL) is an artificial intelligence research laboratory within the Department of Computer Science at Stanford University, located at the Gates Computer Science Building, Stanford. Current work focuses on knowledge representation for shareable engineering knowledge bases and systems, computational environments for modelling physical devices, architectures for adaptive intelligent systems, and expert systems for science and engineering. The KSL has projects with Stanford Medical Informatics (SMI), the Stanford Artificial Intelligence Lab (SAIL), the Stanford Formal Reasoning Group (SFRG), the Stanford Logic Group, and the Stanford Center for Design Research (CDR).


== Past Members ==
This is a partial list (in alphabetical order) of past members:
Edward Feigenbaum
Richard Fikes
Diana E. Forsythe
Tom Gruber
Alon Y. Halevy
Deborah L. McGuinness
Paulo Pinheiro
Derek H. Sleeman
Barbara Hayes-Roth
Ruth Duran Huard
Lee Brownstein


== References ==
This article is based on material taken from the Free On-line Dictionary of Computing prior to 1 November 2008 and incorporated under the ""relicensing"" terms of the GFDL, version 1.3 or later.
Homepage: http://www.ksl.stanford.edu/"
872,Deterministic parsing,4569165,1308,"In natural language processing, deterministic parsing refers to parsing algorithms that do not back up. LR-parsers are an example. (This meaning of the words ""deterministic"" and ""non-deterministic"" differs from that used to describe nondeterministic algorithms.)
The deterministic behavior is desired and expected in compiling programming languages. In natural language processing, it was thought for a long time that deterministic parsing is impossible due to ambiguity inherent in natural languages (many sentences have more than one plausible parse). Thus, non-deterministic approaches such as the chart parser had to be applied. However, Mitch Marcus proposed in 1978 the Parsifal parser that was able to deal with ambiguities while still keeping the deterministic behavior.


== See also ==
Deterministic context-free grammar


== References ==
Alfred V. Aho, Stephen C. Johnson, Jeffrey D. Ullman (1975): Deterministic parsing of ambiguous grammars. Comm. ACM 18:8:441-452.
Mitchell Marcus (1978): A Theory of Syntactic Recognition for Natural Language. PhD Thesis, Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology."
873,John von Neumann Computer Society,17519063,1305,"The John von Neumann Computer Society (Hungarian: Neumann János Számítógép-tudományi Társaság) is the central association for Hungarian researchers of Information communication technology and official partner of the International Federation for Information Processing founded in 1968.


== References ==


== External links ==
Official website"
874,Bruce Maggs,21585321,1299,"Bruce MacDowell Maggs is an American computer scientist whose research interests span a wide area of topics including networks measurements and mapping, distributed systems, and parallel and distributed algorithms. He is the Pelham Wilder Professor of Computer Science at Duke University, and Vice President of Research for Akamai Technologies. Prior to Duke University, he was a computer science professor at Carnegie Mellon University.


== Personal life ==
Maggs is a co-creator of Avatar, an early multiplayer online game developed at the University of Illinois in the late 1970s.


== References ==
Bruce Maggs's Homepage
Bruce Maggs's Publications on DBLP


== External links ==
An Interview with Bruce Maggs, vice-president for R&D at Akamai Tech - Mihai Budiu, Netreport, March 2001"
875,Collision avoidance (networking),26475040,1292,"In computer networking and telecommunication, collision-avoidance methods try to avoid resource contention by attempting to avoid simultaneous attempts to access the same resource.
Collision-avoidance methods include prior scheduling of timeslots, carrier-detection schemes, randomized access times, and exponential backoff after collision detection.


== See also ==
Carrier sense multiple access with collision avoidance
Collision domain


== External links ==
Lenzini, L.; Luise, M.; Reggiannini, R. (June 2001). ""CRDA: A Collision Resolution and Dynamic Allocation MAC Protocol to Integrate Date and Voice in Wireless Networks"". IEEE Journal on Selected Areas in Communications. IEEE Communications Society. 19 (6): 1153–1163. ISSN 0733-8716."
876,Sequential algorithm,23868049,1290,"In computer science, a sequential algorithm or serial algorithm is an algorithm that is executed sequentially – once through, from start to finish, without other processing executing – as opposed to concurrently or in parallel. The term is primarily used to contrast with concurrent algorithm or parallel algorithm; most standard computer algorithms are sequential algorithms, and not specifically identified as such, as sequentialness is a background assumption. Concurrency and parallelism are in general distinct concepts, but they often overlap – many distributed algorithms are both concurrent and parallel – and thus ""sequential"" is used to contrast with both, without distinguishing which one. If these need to be distinguished, the opposing pairs sequential/concurrent and serial/parallel may be used.
""Sequential algorithm"" may also refer specifically to an algorithm for decoding a convolutional code.


== See also ==
Online algorithm
Streaming algorithm


== References =="
877,Epitome (data processing),18095448,1289,"An epitome, in data processing, is a condensed digital representation of the essential statistical properties of ordered datasets such as matrices that represent images, audio signals, videos or genetic sequences. Although much smaller than the data, the epitome contains many of its smaller overlapping parts with much less repetition and with some level of generalization. As such, it can be used in tasks such as data mining, machine learning and signal processing.
The first use of epitomic analysis was with image textures for the purposes of image parsing. Epitomes have also been used in video processing to replace, remove or superresolve imagery.
Epitomes are also being investigated as tools for vaccine design.


== See also ==
Image processing


== References =="
878,Exclusive relationship (programming),20002965,1284,"In computing, an exclusive relationship is a type of Relationship in computer data base design.
In Relational Database Design, in some cases the existence of one kind of relationship type precludes the existence of another. Entities within an entity type A may be related by a relationship type R to an entity in entity type B or entity type C but not both. The relationship types are said to be mutually exclusive. Usually both relationship types will have the same name.


== Example ==
A Data (Entity A) could be Sent (Relationship Name) to a Monitor (Entity B) or a Printer(Entity C) to be shown. In this case the relationship between the Monitor and Printer at one side and Data at the other side is an Exclusive Relationship. Of course it is assumed that Data could be sent to only one of the targets at a time, not to both.

     --- Sent_To ---> Monitor
 Data 
     --- Sent_To ---> Printer


== References ==
Jan L. Harrington, Relational Database Design Clearly Explained, Morgan Kaufmann, 2002, ISBN 1-55860-820-6, pages 354-355"
879,"International Conference on Simulation and Modeling Methodologies, Technologies and Applications",42091247,1278,"SIMULTECH, the International Conference on Simulation and Modeling Methodologies, Technologies and Applications, is a research conference sponsored by the Association for Computing Machinery (ACM) held every other year. [1] It is considered an important conference in system simulation in Europe. The acceptance rate for SIMULTECH has been around 25%. [2]
Like many ACM conferences, SIMULTECH has tutorial talks, technical sessions, and poster sessions. The conference is usually spread over three days. The conference proceedings is indexed by Thomson Reuters [3] and Springer. [4]"
880,List of numerical computational geometry topics,2920916,1278,"List of numerical computational geometry topics enumerates the topics of computational geometry that deals with geometric objects as continuous entities and applies methods and algorithms of nature characteristic to numerical analysis. This area is also called ""machine geometry"", computer-aided geometric design, and geometric modelling.
See List of combinatorial computational geometry topics for another flavor of computational geometry that states problems in terms of geometric objects as discrete entities and hence the methods of their solution are mostly theories and algorithms of combinatorial character.


== Curves ==
In the list of curves topics, the following ones are fundamental to geometric modelling.
Parametric curve
Bézier curve
Spline
Hermite spline
Beta spline
B-spline

Higher-order spline
NURBS

Contour line


== Surfaces ==
Bézier surface
Isosurface
Parametric surface


== Other ==
Level set method
Computational topology"
881,Empirical Methods in Natural Language Processing,43771647,1273,"Empirical Methods in Natural Language Processing or EMNLP is a leading conference in the area of Natural Language Processing. EMNLP is organized by the ACL special interest group on linguistic data (SIGDAT).
EMNLP was started in 1996, based on an earlier conference series called Workshop on Very Large Corpora (WVLC).
As of 2014, EMNLP has a field rating of 36 within computer science and a citation count of 6937, according to Microsoft Academic Search, both within the top-300 for the field of CS.


== References =="
882,Michael S. Montalbano,32003011,1271,"Michael S. Montalbano (28 April 1918 – 13 April 1989) was a computer scientist most noted for authoring ""APL Blossom Time"", a poem about the early days of the APL programming language, performed to the tune of The Battle of New Orleans. He published this poem and a few other articles under the pseudonym ""J. C. L. Guest"".
In 1974, he wrote a book called Decision Tables published by Science Research Associates.


== References ==


== External links ==
A Personal History of APL, October 1982"
883,Bit-oriented protocol,3096721,1267,"A bit-oriented protocol is a communications protocol that sees the transmitted data as an opaque stream of bits with no semantics, or meaning. Control codes are defined in terms of bit sequences instead of characters. Bit oriented protocol can transfer data frames regardless of frame contents. It can also be stated as ""bit stuffing"" this technique allows the data frames to contain an arbitrary number of bits and allows character codes with arbitrary number of bits per character.
Synchronous framing High-Level Data Link Control is a popular bit-oriented protocol. Synchronous framing High-Level Data Link Control may work like this:
Each frame begins and ends with a special bit pattern 01111110, called a flag byte.
A bit stuffing technique is used to prevent the receiver from detecting the special flag byte in user data e.g. whenever the sender's data link layer encounters 5 consecutive ones in the data, it automatically stuffs 0 into the outgoing stream.


== See also ==
Byte-oriented protocol


== References ==
Linktionary page for bit-oriented protocol"
884,John Mathieson (computer scientist),58072,1267,"John Mathieson is a Computer Science graduate who initially worked for Sinclair Research before going on to found Flare with fellow ex-Sinclair colleagues Martin Brennan and Ben Cheese.
After working at Flare on the Flare 1 and its development into the Konix Multisystem, he went on to work for Atari developing the Atari Jaguar with Martin Brennan.
He led the development of the ill-fated NUON media processor at VM Labs. He moved to work for NVIDIA at the end of 2001. As Director of Mobile Systems Architecture at NVIDIA Corp. he led the system architecture team for three generations of the Tegra applications processor.


== References ==


== External links ==
http://www.vmlabs.de/team.htm - List of VM Labs team with picture of John"
885,Cluster-aware application,23047220,1263,"A cluster-aware application is a software application designed to call cluster APIs in order to determine its running state, in case a manual failover is triggered between cluster nodes for planned technical maintenance, or an automatic failover is required, if a computing cluster node encounters hardware or software failure, to maintain business continuity. A cluster-aware application may be capable of failing over LAN or WAN. 


== Cluster-aware application characteristics ==
Use TCP/IP to maintain heartbeat between nodes.
Capable of transaction processing.
Mirroring cluster information in realtime.


== See also ==
Cluster (computing)


== References =="
886,Ontology Inference Layer,183501,1254,"OIL (Ontology Inference Layer or Ontology Interchange Language) can be regarded as an ontology infrastructure for the Semantic Web. OIL is based on concepts developed in Description Logic (DL) and frame-based systems and is compatible with RDFS.
OIL was developed by Dieter Fensel, Frank van Harmelen (Vrije Universiteit, Amsterdam) and Ian Horrocks (University of Manchester) as part of the IST OntoKnowledge project.
Much of the work in OIL was subsequently incorporated into DAML+OIL and the Web Ontology Language (OWL).


== See also ==
DARPA Agent Markup Language (DAML)
DAML+OIL
Ontology


== References =="
887,HABU equivalent,14107662,1250,"The HABU equivalent is a unit of measurement used by United States Department of Defense's High Performance Computing Modernization Program to evaluate the performance of large computers systems.
""The [HPCMP method for measuring system performance] is as follows: the ratio of time [for a given benchmark application] at a target processor count provides a relative measure of the system's performance on that application test case compared with the DoD standard system, stated in Habu-equivalents. Habu, the first DoD standard system, is an IBM POWER3 formerly located at the US Naval Oceanographic Office (NAVO) Major Shared Resource Center. One Habu-equivalent is the performance of 1,024 system-under-study processors compared with 1,024 Habu processors.


== References =="
888,Trellis quantization,11676290,1249,"Trellis quantization is an algorithm that can improve data compression in DCT-based encoding methods. It is used to optimize residual DCT coefficients after motion estimation in lossy video compression encoders such as Xvid and x264. Trellis quantization reduces the size of some DCT coefficients while recovering others to take their place. This process can increase quality because coefficients chosen by Trellis have the lowest rate-distortion ratio. Trellis quantization effectively finds the optimal quantization for each block to maximize the PSNR relative to bitrate. It has varying effectiveness depending on the input data and compression method.


== References ==
VirtualDub/Xvid guide mentioning Trellis quantization
FFMPEGx option documentation
Trellis explanation and pseudocode by the x264-author"
889,Reverse lookup,654006,1247,"Reverse lookup is a procedure of using a value to retrieve a unique key in an associative array.
Applications of reverse lookup include reverse DNS lookup, which provides the domain name associated with a particular IP address, and a reverse telephone directory, which provides the name of the entity associated with a particular telephone number.


== See also ==
Inverse function
Reverse dictionary


== References =="
890,Formula game,2903138,1245,"A formula game is an artificial game represented by a fully quantified Boolean formula. Players' turns alternate and the space of possible moves is denoted by bound variables. If a variable is universally quantified, the formula following it has the same truth value as the formula beginning with the universal quantifier regardless of the move taken. If a variable is existentially quantified, the formula following it has the same truth value as the formula beginning with the existential quantifier for at least one move available at the turn. Turns alternate, and a player loses if he cannot move at his turn. In computational complexity theory, the language FORMULA-GAME is defined as all formulas 
  
    
      
        Φ
      
    
    {\displaystyle \Phi }
   such that Player 1 has a winning strategy in the game represented by 
  
    
      
        Φ
      
    
    {\displaystyle \Phi }
  . FORMULA-GAME is PSPACE-complete.


== References ==
Sipser, Michael. (2006). Introduction to the Theory of Computation. Boston: Thomson Course Technology."
891,Conference on Innovative Data Systems Research,21047573,1244,"The Conference on Innovative Data Systems Research (CIDR) is a biennial computer science conference focused on research into new techniques for data management. It was started in 2002 by Michael Stonebraker, Jim Gray, and David DeWitt, and is held at the Asilomar Conference Grounds in Pacific Grove, California.
CIDR focuses on presenting work that is more speculative, radical, or provocative than what is typically accepted by the traditional database research conferences (such as the International Conference on Very Large Data Bases (VLDB) and the ACM SIGMOD Conference).


== See also ==
International Conference on Very Large Data Bases (VLDB)
ACM SIGMOD Conference


== External links ==
CIDR website"
892,Durability (database systems),245944,1240,"In database systems, durability is the ACID property which guarantees that transactions that have committed will survive permanently. For example, if a flight booking reports that a seat has successfully been booked, then the seat will remain booked even if the system crashes.
Durability can be achieved by flushing the transaction's log records to non-volatile storage before acknowledging commitment.
In distributed transactions, all participating servers must coordinate before commit can be acknowledged. This is usually done by a two-phase commit protocol.
Many DBMSs implement durability by writing transactions into a transaction log that can be reprocessed to recreate the system state right before any later failure. A transaction is deemed committed only after it is entered in the log.


== See also ==
Atomicity
Consistency
Isolation
Relational database management system"
893,Property equivalence,3607459,1239,"In metadata, property equivalence is the statement that two properties have the same property extension or values. This usually (but not always) implies that the two properties have the same semantics or meaning. Technically it only implies that the data elements have the same values.
Property equivalence is one of the three ways that a metadata registry can store equivalence mappings to other metadata registries.
Note that property equivalence is not the same as property equality. Equivalent properties have the same ""values"" (i.e., the same property extension), but may have different intensional meaning (i.e., denote different concepts). Property equality should be expressed with the owl:sameAs construct. As this requires that properties are treated as individuals, such axioms are only allowed in OWL Full.


== See also ==
Metadata registry
Web ontology language
Class equivalence
Synonym Ring


== External links ==
OWL equivalent property definition
OWL same as definition"
894,DUAL (cognitive architecture),3559731,1232,"DUAL is a general cognitive architecture integrating the connectionist and symbolic approaches at the micro level. DUAL is based on decentralized representation and emergent computation. It was inspired by the Society of Mind idea proposed by Marvin Minsky but departs from the initial proposal in many ways. Computations in DUAL emerge from the interaction of many micro-agents each of which is hybrid symbolic/connectionist device. The agents exchange messages and activation via links that can be learnt and modified, they form coalitions which collectively represent concepts, episodes, and facts.
Several models have been developed on the basis of DUAL. These include: AMBR (a model of analogy-making and memory), JUDGEMAP (a model of judgment), PEAN (a model of perception), etc.
DUAL is developed by a team at the New Bulgarian University led by Boicho Kokinov. The second version was co-authored by Alexander Petrov. The third version is co-authored by Georgi Petkov and Ivan Vankov.


== External links ==
Official website
[1]"
895,First-order reduction,7908748,1231,"In mathematics and other formal sciences, first-order or first order most often means either:
""linear"" (a polynomial of degree at most one), as in first-order approximation and other calculus uses, where it is contrasted with ""polynomials of higher degree"", or
""without self-reference"", as in first-order logic and other logic uses, where it is contrasted with ""allowing some self-reference"" (higher-order logic)
In detail, it may refer to:


== Mathematics ==
First-order approximation
First-order arithmetic
First-order condition
First-order control, when a desired result is attempted by adjusting a scalar (first-order) control
First-order hold, a mathematical model of the practical reconstruction of sampled signals
First-order inclusion probability
First Order Inductive Learner, a rule-based learning algorithm
First-order reduction, a very weak type of reduction between two computational problems
First-order resolution
First-order stochastic dominance


=== Differential equations ===
Exact first-order ordinary differential equation
First-order differential equation
First-order differential operator
First-order linear differential equation
First-order non-singular perturbation theory
First-order partial differential equation, a partial differential equation that involves only first derivatives of the unknown function of n variables
Order of accuracy


=== Logic ===
First-order language
First-order logic, a formal logical system used in mathematics, philosophy, linguistics, and computer science
First-order predicate, a predicate that takes only individual(s) constants or variables as argument(s)
First-order predicate calculus
First-order theorem provers
First-order theory
Monadic first-order logic


== Chemistry ==
First-order fluid, another name for a power-law fluid with exponential dependence of viscosity on temperature
First-order reaction
First-order transition


== Computer science ==
First-order abstract syntax
First-order function
First-order query


== Other uses ==
First-order desire
First-order election, in political science, the relative importance of certain elections
First order Fresnel lens
First Order is a villainous military government in the Star Wars sequel trilogy.


== See also ==
First-order stream in Strahler stream order
First Order (Star Wars), the principal antagonists in the Star Wars sequel trilogy"
896,Viroinformatics,44854826,1230,"Viroinformatics is an amalgamation of virology with bioinformatics, involving the application of information and communication technology in various aspects of viral research. Currently there are more than 100 web servers and databases harboring knowledge regarding different viruses as well as distinct applications concerning diversity analysis, viral recombination, RNAi studies, drug design, protein–protein interaction, structural analysis etc.


== References ==


== External links ==
Viral bioinformatics
VBRC
ViPR
ViralZone
Viral bioinformatics: introduction
Viral genomics and bioinformatics"
897,Polylogarithmic function,445957,1229,"A polylogarithmic function in n is a polynomial in the logarithm of n,

  
    
      
        
          a
          
            k
          
        
        (
        log
        ⁡
        n
        
          )
          
            k
          
        
        +
        ⋯
        +
        
          a
          
            1
          
        
        (
        log
        ⁡
        n
        )
        +
        
          a
          
            0
          
        
        .
      
    
    {\displaystyle a_{k}(\log n)^{k}+\cdots +a_{1}(\log n)+a_{0}.}
  
In computer science, polylogarithmic functions occur as the order of time or memory used by some algorithms (e.g., ""it has polylogarithmic order"").
All polylogarithmic functions of 
  
    
      
        n
      
    
    {\displaystyle n}
   are 
  
    
      
        o
        (
        
          n
          
            ε
          
        
        )
      
    
    {\displaystyle o(n^{\varepsilon })}
   for every exponent ε > 0 (for the meaning of this symbol, see small o notation), that is, a polylogarithmic function grows more slowly than any positive exponent. This observation is the basis for the soft O notation Õ(n).


== References ==
Black, Paul E. (2004-12-17). ""polylogarithmic"". Dictionary of Algorithms and Data Structures. U.S. National Institute of Standards and Technology. Retrieved 2010-01-10."
898,Identity map pattern,46955,1228,"In the design of database management systems, the identity map pattern is a database access design pattern used to improve performance by providing a context-specific, in-memory cache to prevent duplicate retrieval of the same object data from the database.
If the requested data has already been loaded from the database, the identity map returns the same instance of the already instantiated object, but if it has not been loaded yet, it loads it and stores the new object in the map. In this way, it follows a similar principle to lazy loading.
There are four types of Identity Map
Explicit
Generic
Session
Class


== See also ==
Design pattern
Active record
Lazy loading


== References =="
899,Principle of deferred decision,20818154,1227,"Principle of deferred decisions is a technique used in analysis of randomized algorithms.


== Definition ==
A randomized algorithm makes a set of random choices. These random choices may be intricately related making it difficult to analyze it. In many of these cases Principle of Deferred Decisions is used. The idea behind the principle is that the entire set of random choices are not made in advance, but rather fixed only as they are revealed to the algorithm.


== Applications ==


=== The clock solitaire game ===
The principle is used to evaluate and determine the probability of ""win"" from a deck of cards. The idea is to let the random choices unfold, until the iteration ends at 52, where if the fourth card is drawn out of a group labeled ""K"", the game terminates.


== References ==
M. Mitzenmacher and E. Upfal. Probability and Computing : Randomized Algorithms and Probabilistic Analysis. Cambridge University Press, New York (NY), 2005. Section 1.3, page 9."
900,Memetic Computing Society,49862426,1224,"The Memetic Computing Society is a society focusing on research in the area of memetic algorithms and evolutionary computation. The society is located in Singapore.


== Conferences ==
The Memetic Computing Society supports the following conferences.
The IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT).


== See also ==
Association for Computing Machinery
ACM SIGAI
IEEE Computer Society
Web Intelligence Consortium


== References ==


== External links ==
Official web site"
901,International Middleware Conference,38683862,1222,"The International Middleware Conference brings together academic and industrial delegates who have an interest in the development, optimisation, evaluation and evolution of middleware.


== History ==
The first instance of the Middleware conference was held in 1998. Since 2003 the conference has been run annually. Many recent conference events have been ACM/IFIP/USENIX supported events.


== Conference structure ==
Middleware uses a single-track conference program, although it includes a growing number of submission categories. As of 2013, these include:
Research papers
Experimentation and deployment papers
Big ideas papers
The conference also includes:
Tutorials
Demonstrations and posters
A doctoral workshop
A number (six, in 2012) of workshops are typically co-located with the main conference.


== See also ==
List of computer science conferences


== References ==


== External links ==
http://www.middleware-conference.org/
http://2013.middleware-conference.org/"
902,Well-formed Petri net,6733149,1222,"A Petri net, also known as a place/transition (PT) net, is one of several mathematical modeling languages for the description of distributed systems. It is a class of discrete event dynamic system. A Petri net is a directed bipartite graph, in which the nodes represent transitions (i.e. events that may occur, represented by bars) and places (i.e. conditions, represented by circles). The directed arcs describe which places are pre- and/or postconditions for which transitions (signified by arrows). Some sources state that Petri nets were invented in August 1939 by Carl Adam Petri—at the age of 13—for the purpose of describing chemical processes.
Like industry standards such as UML activity diagrams, Business Process Model and Notation and EPCs, Petri nets offer a graphical notation for stepwise processes that include choice, iteration, and concurrent execution. Unlike these standards, Petri nets have an exact mathematical definition of their execution semantics, with a well-developed mathematical theory for process analysis.


== Petri net basics ==
A Petri net consists of places, transitions, and arcs. Arcs run from a place to a transition or vice versa, never between places or between transitions. The places from which an arc runs to a transition are called the input places of the transition; the places to which arcs run from a transition are called the output places of the transition.
Graphically, places in a Petri net may contain a discrete number of marks called tokens. Any distribution of tokens over the places will represent a configuration of the net called a marking. In an abstract sense relating to a Petri net diagram, a transition of a Petri net may fire if it is enabled, i.e. there are sufficient tokens in all of its input places; when the transition fires, it consumes the required input tokens, and creates tokens in its output places. A firing is atomic, i.e. a single non-interruptible step.
Unless an execution policy  is defined, the execution of Petri nets is nondeterministic: when multiple transitions are enabled at the same time, they will fire in a any order.
Since firing is nondeterministic, and multiple tokens may be present anywhere in the net (even in the same place), Petri nets are well suited for modeling the concurrent behavior of distributed systems.


== Formal definition and basic terminology ==
Petri nets are state-transition systems that extend a class of nets called elementary nets.
Definition 1. A net is a triple 
  
    
      
        N
        =
        (
        P
        ,
        T
        ,
        F
        )
      
    
    {\displaystyle N=(P,T,F)}
   where:

  
    
      
        P
      
    
    {\displaystyle P}
   and 
  
    
      
        T
      
    
    {\displaystyle T}
   are disjoint finite sets of places and transitions, respectively.

  
    
      
        F
        ⊂
        (
        P
        ×
        T
        )
        ∪
        (
        T
        ×
        P
        )
      
    
    {\displaystyle F\subset (P\times T)\cup (T\times P)}
   is a set of arcs (or flow relations).
Definition 2. Given a net N = (P, T, F), a configuration is a set C so that C ⊆ P.

Definition 3. An elementary net is a net of the form EN = (N, C) where:
N = (P, T, F) is a net.
C is such that C ⊆ P is a configuration.
Definition 4. A Petri net is a net of the form PN = (N, M, W), which extends the elementary net so that:
N = (P, T, F ) is a net.
M : P → Z is a place multiset, where Z is a countable set. M extends the concept of configuration and is commonly described with reference to Petri net diagrams as a marking.
W : F → Z is an arc multiset, so that the count (or weight) for each arc is a measure of the arc multiplicity.
If a Petri net is equivalent to an elementary net, then Z can be the countable set {0,1} and those elements in P that map to 1 under M form a configuration. Similarly, if a Petri net is not an elementary net, then the multiset M can be interpreted as representing a non-singleton set of configurations. In this respect, M extends the concept of configuration for elementary nets to Petri nets.
In the diagram of a Petri net (see top figure right), places are conventionally depicted with circles, transitions with long narrow rectangles and arcs as one-way arrows that show connections of places to transitions or transitions to places. If the diagram were of an elementary net, then those places in a configuration would be conventionally depicted as circles, where each circle encompasses a single dot called a token. In the given diagram of a Petri net (see right), the place circles may encompass more than one token to show the number of times a place appears in a configuration. The configuration of tokens distributed over an entire Petri net diagram is called a marking.
In the top figure (see right), the place p1 is an input place of transition t; whereas, the place p2 is an output place to the same transition. Let PN0 (Fig. top) be a Petri net with a marking configured M0 and PN1 (Fig. bottom) be a Petri net with a marking configured M1. The configuration of PN0 enable transition t through the property that all input places have sufficient number of tokens (shown in the figures as dots) ""equal to or greater"" than the multiplicities on their respective arcs to t. Once and only once a transition is enabled will the transition fire. In this example, the firing of transition t generates a map that has the marking configured M1 in the image of M0 and results in Petri net PN1, seen in the bottom figure. In the diagram, the firing rule for a transition can be characterised by subtracting a number of tokens from its input places equal to the multiplicity of the respective input arcs and accumulating a new number of tokens at the output places equal to the multiplicity of the respective output arcs.
Remark 1. The precise meaning of ""equal to or greater"" will depend on the precise algebraic properties of addition being applied on Z in the firing rule, where subtle variations on the algebraic properties can lead to other classes of Petri nets; for example, Algebraic Petri nets.
The following formal definition is loosely based on (Peterson 1981). Many alternative definitions exist.


=== Syntax ===
A Petri net graph (called Petri net by some, but see below) is a 3-tuple 
  
    
      
        (
        S
        ,
        T
        ,
        W
        )
      
    
    {\displaystyle (S,T,W)}
  , where
S is a finite set of places
T is a finite set of transitions
S and T are disjoint, i.e. no object can be both a place and a transition

  
    
      
        W
        :
        (
        S
        ×
        T
        )
        ∪
        (
        T
        ×
        S
        )
        →
        
          N
        
      
    
    {\displaystyle W:(S\times T)\cup (T\times S)\to \mathbb {N} }
   is a multiset of arcs, i.e. it assigns to each arc a non-negative integer arc multiplicity (or weight); note that no arc may connect two places or two transitions.
The flow relation is the set of arcs: 
  
    
      
        F
        =
        {
        (
        x
        ,
        y
        )
        ∣
        W
        (
        x
        ,
        y
        )
        >
        0
        }
      
    
    {\displaystyle F=\{(x,y)\mid W(x,y)>0\}}
  . In many textbooks, arcs can only have multiplicity 1. These texts often define Petri nets using F instead of W. When using this convention, a Petri net graph is a bipartite multigraph 
  
    
      
        (
        S
        ∪
        T
        ,
        F
        )
      
    
    {\displaystyle (S\cup T,F)}
   with node partitions S and T.
The preset of a transition t is the set of its input places: 
  
    
      
        
          

          
          
            ∙
          
        
        t
        =
        {
        s
        ∈
        S
        ∣
        W
        (
        s
        ,
        t
        )
        >
        0
        }
      
    
    {\displaystyle {}^{\bullet }t=\{s\in S\mid W(s,t)>0\}}
  ; its postset is the set of its output places: 
  
    
      
        
          t
          
            ∙
          
        
        =
        {
        s
        ∈
        S
        ∣
        W
        (
        t
        ,
        s
        )
        >
        0
        }
      
    
    {\displaystyle t^{\bullet }=\{s\in S\mid W(t,s)>0\}}
  . Definitions of pre- and postsets of places are analogous.
A marking of a Petri net (graph) is a multiset of its places, i.e., a mapping 
  
    
      
        M
        :
        S
        →
        
          N
        
      
    
    {\displaystyle M:S\to \mathbb {N} }
  . We say the marking assigns to each place a number of tokens.
A Petri net (called marked Petri net by some, see above) is a 4-tuple 
  
    
      
        (
        S
        ,
        T
        ,
        W
        ,
        
          M
          
            0
          
        
        )
      
    
    {\displaystyle (S,T,W,M_{0})}
  , where

  
    
      
        (
        S
        ,
        T
        ,
        W
        )
      
    
    {\displaystyle (S,T,W)}
   is a Petri net graph;

  
    
      
        
          M
          
            0
          
        
      
    
    {\displaystyle M_{0}}
   is the initial marking, a marking of the Petri net graph.


=== Execution semantics ===
In words:
firing a transition t in a marking M consumes 
  
    
      
        W
        (
        s
        ,
        t
        )
      
    
    {\displaystyle W(s,t)}
   tokens from each of its input places s, and produces 
  
    
      
        W
        (
        t
        ,
        s
        )
      
    
    {\displaystyle W(t,s)}
   tokens in each of its output places s
a transition is enabled (it may fire) in M if there are enough tokens in its input places for the consumptions to be possible, i.e. iff 
  
    
      
        ∀
        s
        :
        M
        (
        s
        )
        ≥
        W
        (
        s
        ,
        t
        )
      
    
    {\displaystyle \forall s:M(s)\geq W(s,t)}
  .
We are generally interested in what may happen when transitions may continually fire in arbitrary order.
We say that a marking M' is reachable from a marking M in one step if 
  
    
      
        M
        
          
            ⟶
            G
          
        
        
          M
          ′
        
      
    
    {\displaystyle M{\underset {G}{\longrightarrow }}M'}
  ; we say that it is reachable from M if 
  
    
      
        M
        
          
            
              ⟶
              G
            
            ∗
          
        
        
          M
          ′
        
      
    
    {\displaystyle M{\overset {*}{\underset {G}{\longrightarrow }}}M'}
  , where 
  
    
      
        
          
            
              ⟶
              G
            
            ∗
          
        
      
    
    {\displaystyle {\overset {*}{\underset {G}{\longrightarrow }}}}
   is the reflexive transitive closure of 
  
    
      
        
          
            ⟶
            G
          
        
      
    
    {\displaystyle {\underset {G}{\longrightarrow }}}
  ; that is, if it is reachable in 0 or more steps.
For a (marked) Petri net 
  
    
      
        N
        =
        (
        S
        ,
        T
        ,
        W
        ,
        
          M
          
            0
          
        
        )
      
    
    {\displaystyle N=(S,T,W,M_{0})}
  , we are interested in the firings that can be performed starting with the initial marking 
  
    
      
        
          M
          
            0
          
        
      
    
    {\displaystyle M_{0}}
  . Its set of reachable markings is the set 
  
    
      
        R
        (
        N
        )
         
        
          
            
              
                =
              
              
                D
              
            
          
        
         
        
          {
          
            
              M
              ′
            
            
              
                |
              
            
            
              M
              
                0
              
            
            
              
                →
                
                  
                    (
                    S
                    ,
                    T
                    ,
                    W
                    )
                  
                
                
                  ∗
                
              
            
            
              M
              ′
            
          
          }
        
      
    
    {\displaystyle R(N)\ {\stackrel {D}{=}}\ \left\{M'{\Bigg |}M_{0}{\xrightarrow[{(S,T,W)}]{*}}M'\right\}}
  
The reachability graph of N is the transition relation 
  
    
      
        
          
            ⟶
            G
          
        
      
    
    {\displaystyle {\underset {G}{\longrightarrow }}}
   restricted to its reachable markings 
  
    
      
        R
        (
        N
        )
      
    
    {\displaystyle R(N)}
  . It is the state space of the net.
A firing sequence for a Petri net with graph G and initial marking 
  
    
      
        
          M
          
            0
          
        
      
    
    {\displaystyle M_{0}}
   is a sequence of transitions 
  
    
      
        
          
            
              σ
              →
            
          
        
        =
        ⟨
        
          t
          
            
              i
              
                1
              
            
          
        
        ⋯
        
          t
          
            
              i
              
                n
              
            
          
        
        ⟩
      
    
    {\displaystyle {\vec {\sigma }}=\langle t_{i_{1}}\cdots t_{i_{n}}\rangle }
   such that 
  
    
      
        
          M
          
            0
          
        
        
          
            →
            
              
                G
                ,
                
                  t
                  
                    
                      i
                      
                        1
                      
                    
                  
                
              
            
            
          
        
        
          M
          
            1
          
        
        ∧
        ⋯
        ∧
        
          M
          
            n
            −
            1
          
        
        
          
            →
            
              
                G
                ,
                
                  t
                  
                    
                      i
                      
                        n
                      
                    
                  
                
              
            
            
          
        
        
          M
          
            n
          
        
      
    
    {\displaystyle M_{0}{\xrightarrow[{G,t_{i_{1}}}]{}}M_{1}\wedge \cdots \wedge M_{n-1}{\xrightarrow[{G,t_{i_{n}}}]{}}M_{n}}
  . The set of firing sequences is denoted as 
  
    
      
        L
        (
        N
        )
      
    
    {\displaystyle L(N)}
  .


== Variations on the definition ==
As already remarked, a common variation is to disallow arc multiplicities and replace the bag of arcs W with a simple set, called the flow relation, 
  
    
      
        F
        ⊆
        (
        S
        ×
        T
        )
        ∪
        (
        T
        ×
        S
        )
      
    
    {\displaystyle F\subseteq (S\times T)\cup (T\times S)}
  . This doesn't limit expressive power as both can represent each other.
Another common variation, e.g. in, Desel and Juhás (2001), is to allow capacities to be defined on places. This is discussed under extensions below.


== Formulation in terms of vectors and matrices ==
The markings of a Petri net 
  
    
      
        (
        S
        ,
        T
        ,
        W
        ,
        
          M
          
            0
          
        
        )
      
    
    {\displaystyle (S,T,W,M_{0})}
   can be regarded as vectors of nonnegative integers of length 
  
    
      
        
          |
        
        S
        
          |
        
      
    
    {\displaystyle |S|}
  .
Its transition relation can be described as a pair of 
  
    
      
        
          |
        
        S
        
          |
        
      
    
    {\displaystyle |S|}
   by 
  
    
      
        
          |
        
        T
        
          |
        
      
    
    {\displaystyle |T|}
   matrices:

  
    
      
        
          W
          
            −
          
        
      
    
    {\displaystyle W^{-}}
  , defined by 
  
    
      
        ∀
        s
        ,
        t
        :
        
          W
          
            −
          
        
        [
        s
        ,
        t
        ]
        =
        W
        (
        s
        ,
        t
        )
      
    
    {\displaystyle \forall s,t:W^{-}[s,t]=W(s,t)}
  

  
    
      
        
          W
          
            +
          
        
      
    
    {\displaystyle W^{+}}
  , defined by 
  
    
      
        ∀
        s
        ,
        t
        :
        
          W
          
            +
          
        
        [
        s
        ,
        t
        ]
        =
        W
        (
        t
        ,
        s
        )
        .
      
    
    {\displaystyle \forall s,t:W^{+}[s,t]=W(t,s).}
  
Then their difference

  
    
      
        
          W
          
            T
          
        
        =
        
          W
          
            +
          
        
        −
        
          W
          
            −
          
        
      
    
    {\displaystyle W^{T}=W^{+}-W^{-}}
  
can be used to describe the reachable markings in terms of matrix multiplication, as follows. For any sequence of transitions w, write 
  
    
      
        o
        (
        w
        )
      
    
    {\displaystyle o(w)}
   for the vector that maps every transition to its number of occurrences in w. Then, we have

  
    
      
        R
        (
        N
        )
        =
        {
        M
        ∣
        ∃
        w
        :
        M
        =
        
          M
          
            0
          
        
        +
        
          W
          
            T
          
        
        ⋅
        o
        (
        w
        )
        ∧
        w
        
           is a firing sequence of 
        
        N
        }
      
    
    {\displaystyle R(N)=\{M\mid \exists w:M=M_{0}+W^{T}\cdot o(w)\wedge w{\text{ is a firing sequence of }}N\}}
  .
Note that it must be required that w is a firing sequence; allowing arbitrary sequences of transitions will generally produce a larger set.

  
    
      
        
          W
          
            +
          
        
        =
        
          
            [
            
              
                
                  ∗
                
                
                  t
                  1
                
                
                  t
                  2
                
              
              
                
                  p
                  1
                
                
                  0
                
                
                  1
                
              
              
                
                  p
                  2
                
                
                  1
                
                
                  0
                
              
              
                
                  p
                  3
                
                
                  1
                
                
                  0
                
              
              
                
                  p
                  4
                
                
                  0
                
                
                  1
                
              
            
            ]
          
        
        ,
         
        
          W
          
            −
          
        
        =
        
          
            [
            
              
                
                  ∗
                
                
                  t
                  1
                
                
                  t
                  2
                
              
              
                
                  p
                  1
                
                
                  1
                
                
                  0
                
              
              
                
                  p
                  2
                
                
                  0
                
                
                  1
                
              
              
                
                  p
                  3
                
                
                  0
                
                
                  1
                
              
              
                
                  p
                  4
                
                
                  0
                
                
                  0
                
              
            
            ]
          
        
        ,
         
        
          W
          
            T
          
        
        =
        
          
            [
            
              
                
                  ∗
                
                
                  t
                  1
                
                
                  t
                  2
                
              
              
                
                  p
                  1
                
                
                  −
                  1
                
                
                  1
                
              
              
                
                  p
                  2
                
                
                  1
                
                
                  −
                  1
                
              
              
                
                  p
                  3
                
                
                  1
                
                
                  −
                  1
                
              
              
                
                  p
                  4
                
                
                  0
                
                
                  1
                
              
            
            ]
          
        
      
    
    {\displaystyle W^{+}={\begin{bmatrix}*&t1&t2\\p1&0&1\\p2&1&0\\p3&1&0\\p4&0&1\end{bmatrix}},\ W^{-}={\begin{bmatrix}*&t1&t2\\p1&1&0\\p2&0&1\\p3&0&1\\p4&0&0\end{bmatrix}},\ W^{T}={\begin{bmatrix}*&t1&t2\\p1&-1&1\\p2&1&-1\\p3&1&-1\\p4&0&1\end{bmatrix}}}
  

  
    
      
        
          M
          
            0
          
        
        =
        
          
            [
            
              
                
                  1
                
                
                  0
                
                
                  2
                
                
                  1
                
              
            
            ]
          
        
      
    
    {\displaystyle M_{0}={\begin{bmatrix}1&0&2&1\end{bmatrix}}}
  


== Mathematical properties of Petri nets ==
One thing that makes Petri nets interesting is that they provide a balance between modeling power and analyzability: many things one would like to know about concurrent systems can be automatically determined for Petri nets, although some of those things are very expensive to determine in the general case. Several subclasses of Petri nets have been studied that can still model interesting classes of concurrent systems, while these problems become easier.
An overview of such decision problems, with decidability and complexity results for Petri nets and some subclasses, can be found in Esparza and Nielsen (1995).


=== Reachability ===
The reachability problem for Petri nets is to decide, given a Petri net N and a marking M, whether 
  
    
      
        M
        ∈
        R
        (
        N
        )
      
    
    {\displaystyle M\in R(N)}
  .
Clearly, this is a matter of walking the reachability graph defined above, until either we reach the requested marking or we know it can no longer be found. This is harder than it may seem at first: the reachability graph is generally infinite, and it is not easy to determine when it is safe to stop.
In fact, this problem was shown to be EXPSPACE-hard years before it was shown to be decidable at all (Mayr, 1981). Papers continue to be published on how to do it efficiently.
While reachability seems to be a good tool to find erroneous states, for practical problems the constructed graph usually has far too many states to calculate. To alleviate this problem, linear temporal logic is usually used in conjunction with the tableau method to prove that such states cannot be reached. LTL uses the semi-decision technique to find if indeed a state can be reached, by finding a set of necessary conditions for the state to be reached then proving that those conditions cannot be satisfied.


=== Liveness ===

Petri nets can be described as having different degrees of liveness 
  
    
      
        
          L
          
            1
          
        
        −
        
          L
          
            4
          
        
      
    
    {\displaystyle L_{1}-L_{4}}
  . A Petri net 
  
    
      
        (
        N
        ,
        
          M
          
            0
          
        
        )
      
    
    {\displaystyle (N,M_{0})}
   is called 
  
    
      
        
          L
          
            k
          
        
      
    
    {\displaystyle L_{k}}
  -live iff all of its transitions are 
  
    
      
        
          L
          
            k
          
        
      
    
    {\displaystyle L_{k}}
  -live, where a transition is
dead, if it can never fire, i.e. it is not in any firing sequence in 
  
    
      
        L
        (
        N
        ,
        
          M
          
            0
          
        
        )
      
    
    {\displaystyle L(N,M_{0})}
  

  
    
      
        
          L
          
            1
          
        
      
    
    {\displaystyle L_{1}}
  -live (potentially fireable), iff it may fire, i.e. it is in some firing sequence in 
  
    
      
        L
        (
        N
        ,
        
          M
          
            0
          
        
        )
      
    
    {\displaystyle L(N,M_{0})}
  

  
    
      
        
          L
          
            2
          
        
      
    
    {\displaystyle L_{2}}
  -live iff it can fire arbitrarily often, i.e. if for every positive integer k, it occurs at least k times in some firing sequence in 
  
    
      
        L
        (
        N
        ,
        
          M
          
            0
          
        
        )
      
    
    {\displaystyle L(N,M_{0})}
  

  
    
      
        
          L
          
            3
          
        
      
    
    {\displaystyle L_{3}}
  -live iff it can fire infinitely often, i.e. if for every positive integer k, it occurs at least k times in V, for some prefix-closed set of firing sequences 
  
    
      
        
          
            V
            ⊆
            L
            (
            N
            ,
            
              M
              
                0
              
            
            )
          
        
      
    
    {\textstyle \textstyle {V\subseteq L(N,M_{0})}}
  

  
    
      
        
          L
          
            4
          
        
      
    
    {\displaystyle L_{4}}
  -live (live) iff it may always fire, i.e. it is 
  
    
      
        
          L
          
            1
          
        
      
    
    {\displaystyle L_{1}}
  -live in every reachable marking in 
  
    
      
        R
        (
        N
        ,
        
          M
          
            0
          
        
        )
      
    
    {\displaystyle R(N,M_{0})}
  
Note that these are increasingly stringent requirements: 
  
    
      
        
          L
          
            j
            +
            1
          
        
      
    
    {\displaystyle L_{j+1}}
  -liveness implies 
  
    
      
        
          L
          
            j
          
        
      
    
    {\displaystyle L_{j}}
  -liveness, for 
  
    
      
        
          
            j
            ∈
            
              1
              ,
              2
              ,
              3
            
          
        
      
    
    {\textstyle \textstyle {j\in {1,2,3}}}
  .
These definitions are in accordance with Murata's overview, which additionally uses 
  
    
      
        
          L
          
            0
          
        
      
    
    {\displaystyle L_{0}}
  -live as a term for dead.


=== Boundedness ===

A place in a Petri net is called k-bounded if it does not contain more than k tokens in all reachable markings, including the initial marking; it is said to be safe if it is 1-bounded; it is bounded if it is k-bounded for some k.
A (marked) Petri net is called k-bounded, safe, or bounded when all of its places are. A Petri net (graph) is called (structurally) bounded if it is bounded for every possible initial marking.
Note that a Petri net is bounded if and only if its reachability graph is finite.
Boundedness is decidable by looking at covering, by constructing the Karp–Miller Tree.
It can be useful to explicitly impose a bound on places in a given net. This can be used to model limited system resources.
Some definitions of Petri nets explicitly allow this as a syntactic feature. Formally, Petri nets with place capacities can be defined as tuples 
  
    
      
        (
        S
        ,
        T
        ,
        W
        ,
        C
        ,
        
          M
          
            0
          
        
        )
      
    
    {\displaystyle (S,T,W,C,M_{0})}
  , where 
  
    
      
        (
        S
        ,
        T
        ,
        W
        ,
        
          M
          
            0
          
        
        )
      
    
    {\displaystyle (S,T,W,M_{0})}
   is a Petri net, 
  
    
      
        C
        :
        P
        →
        
        
        
        ∣
        I
        
        N
      
    
    {\displaystyle C:P\rightarrow \!\!\!\shortmid I\!N}
   an assignment of capacities to (some or all) places, and the transition relation is the usual one restricted to the markings in which each place with a capacity has at most that many tokens.

For example, if in the net N, both places are assigned capacity 2, we obtain a Petri net with place capacities, say N2; its reachability graph is displayed on the right.

Alternatively, places can be made bounded by extending the net. To be exact, a place can be made k-bounded by adding a ""counter-place"" with flow opposite to that of the place, and adding tokens to make the total in both places k.


== Discrete, continuous, and hybrid Petri nets ==
As well as for discrete events, there are Petri nets for continuous and hybrid discrete-continuous processes that are useful in discrete, continuous and hybrid control theory, and related to discrete, continuous and hybrid automata.


== Extensions ==
There are many extensions to Petri nets. Some of them are completely backwards-compatible (e.g. coloured Petri nets) with the original Petri net, some add properties that cannot be modelled in the original Petri net formalism (e.g. timed Petri nets). Although backwards-compatible models do not extend the computational power of Petri nets, they may have more succinct representations and may be more convenient for modeling. Extensions that cannot be transformed into Petri nets are sometimes very powerful, but usually lack the full range of mathematical tools available to analyse ordinary Petri nets.
The term high-level Petri net is used for many Petri net formalisms that extend the basic P/T net formalism; this includes coloured Petri nets, hierarchical Petri nets such as Nets within Nets, and all other extensions sketched in this section. The term is also used specifically for the type of coloured nets supported by CPN Tools.
A short list of possible extensions:
Additional types of arcs; two common types are:
a reset arc does not impose a precondition on firing, and empties the place when the transition fires; this makes reachability undecidable, while some other properties, such as termination, remain decidable;
an inhibitor arc imposes the precondition that the transition may only fire when the place is empty; this allows arbitrary computations on numbers of tokens to be expressed, which makes the formalism Turing complete and implies existence of a universal net.

In a standard Petri net, tokens are indistinguishable. In a Coloured Petri net, every token has a value. In popular tools for coloured Petri nets such as CPN Tools, the values of tokens are typed, and can be tested (using guard expressions) and manipulated with a functional programming language. A subsidiary of coloured Petri nets are the well-formed Petri nets, where the arc and guard expressions are restricted to make it easier to analyse the net.
Another popular extension of Petri nets is hierarchy; this in the form of different views supporting levels of refinement and abstraction was studied by Fehling. Another form of hierarchy is found in so-called object Petri nets or object systems where a Petri net can contain Petri nets as its tokens inducing a hierarchy of nested Petri nets that communicate by synchronisation of transitions on different levels. See for an informal introduction to object Petri nets.
A vector addition system with states (VASS) is an equivalent formalism to Petri nets. However, it can be superficially viewed as a generalisation of Petri nets. Consider a finite state automaton where each transition is labelled by a transition from the Petri net. The Petri net is then synchronised with the finite state automaton, i.e., a transition in the automaton is taken at the same time as the corresponding transition in the Petri net. It is only possible to take a transition in the automaton if the corresponding transition in the Petri net is enabled, and it is only possible to fire a transition in the Petri net if there is a transition from the current state in the automaton labelled by it. (The definition of VASS is usually formulated slightly differently.)
Prioritised Petri nets add priorities to transitions, whereby a transition cannot fire, if a higher-priority transition is enabled (i.e. can fire). Thus, transitions are in priority groups, and e.g. priority group 3 can only fire if all transitions are disabled in groups 1 and 2. Within a priority group, firing is still non-deterministic.
The non-deterministic property has been a very valuable one, as it lets the user abstract a large number of properties (depending on what the net is used for). In certain cases, however, the need arises to also model the timing, not only the structure of a model. For these cases, timed Petri nets have evolved, where there are transitions that are timed, and possibly transitions which are not timed (if there are, transitions that are not timed have a higher priority than timed ones). A subsidiary of timed Petri nets are the stochastic Petri nets that add nondeterministic time through adjustable randomness of the transitions. The exponential random distribution is usually used to 'time' these nets. In this case, the nets' reachability graph can be used as a continuous time Markov chain (CTMC).
Dualistic Petri Nets (dP-Nets) is a Petri Net extension developed by E. Dawis, et al. to better represent real-world process. dP-Nets balance the duality of change/no-change, action/passivity, (transformation) time/space, etc., between the bipartite Petri Net constructs of transformation and place resulting in the unique characteristic of transformation marking, i.e., when the transformation is ""working"" it is marked. This allows for the transformation to fire (or be marked) multiple times representing the real-world behavior of process throughput. Marking of the transformation assumes that transformation time must be greater than zero. A zero transformation time used in many typical Petri Nets may be mathematically appealing but impractical in representing real-world processes. dP-Nets also exploit the power of Petri Nets' hierarchical abstraction to depict Process architecture. Complex process systems are modeled as a series of simpler nets interconnected through various levels of hierarchical abstraction. The process architecture of a packet switch is demonstrated in, where development requirements are organized around the structure of the designed system.
There are many more extensions to Petri nets, however, it is important to keep in mind, that as the complexity of the net increases in terms of extended properties, the harder it is to use standard tools to evaluate certain properties of the net. For this reason, it is a good idea to use the most simple net type possible for a given modelling task.


== Restrictions ==

Instead of extending the Petri net formalism, we can also look at restricting it, and look at particular types of Petri nets, obtained by restricting the syntax in a particular way. Ordinary Petri nets are the nets where all arc weights are 1. Restricting further, the following types of ordinary Petri nets are commonly used and studied:
In a state machine (SM), every transition has one incoming arc, and one outgoing arc, and all markings have exactly one token. As a consequence, there can not be concurrency, but there can be conflict (i.e. nondeterminism). Mathematically: 
  
    
      
        ∀
        t
        ∈
        T
        :
        
          |
        
        
          t
          
            ∙
          
        
        
          |
        
        =
        
          |
        
        
          

          
          
            ∙
          
        
        t
        
          |
        
        =
        1
      
    
    {\displaystyle \forall t\in T:|t^{\bullet }|=|{}^{\bullet }t|=1}
  
In a marked graph (MG), every place has one incoming arc, and one outgoing arc. This means, that there can not be conflict, but there can be concurrency. Mathematically: 
  
    
      
        ∀
        s
        ∈
        S
        :
        
          |
        
        
          s
          
            ∙
          
        
        
          |
        
        =
        
          |
        
        
          

          
          
            ∙
          
        
        s
        
          |
        
        =
        1
      
    
    {\displaystyle \forall s\in S:|s^{\bullet }|=|{}^{\bullet }s|=1}
  
In a free choice net (FC), – every arc from a place to a transition is either the only arc from that place or the only arc to that transition. I.e. there can be both concurrency and conflict, but not at the same time. Mathematically: 
  
    
      
        ∀
        s
        ∈
        S
        :
        (
        
          |
        
        
          s
          
            ∙
          
        
        
          |
        
        ≤
        1
        )
        ∨
        (
        
          

          
          
            ∙
          
        
        (
        
          s
          
            ∙
          
        
        )
        =
        {
        s
        }
        )
      
    
    {\displaystyle \forall s\in S:(|s^{\bullet }|\leq 1)\vee ({}^{\bullet }(s^{\bullet })=\{s\})}
  
Extended free choice (EFC) – a Petri net that can be transformed into an FC.
In an asymmetric choice net (AC), concurrency and conflict (in sum, confusion) may occur, but not symmetrically. Mathematically: 
  
    
      
        ∀
        
          s
          
            1
          
        
        ,
        
          s
          
            2
          
        
        ∈
        S
        :
        (
        
          s
          
            1
          
        
        
          

          
          
            ∙
          
        
        ∩
        
          s
          
            2
          
        
        
          

          
          
            ∙
          
        
        ≠
        ∅
        )
        →
        [
        (
        
          s
          
            1
          
        
        
          

          
          
            ∙
          
        
        ⊆
        
          s
          
            2
          
        
        
          

          
          
            ∙
          
        
        )
        ∨
        (
        
          s
          
            2
          
        
        
          

          
          
            ∙
          
        
        ⊆
        
          s
          
            1
          
        
        
          

          
          
            ∙
          
        
        )
        ]
      
    
    {\displaystyle \forall s_{1},s_{2}\in S:(s_{1}{}^{\bullet }\cap s_{2}{}^{\bullet }\neq \emptyset )\to [(s_{1}{}^{\bullet }\subseteq s_{2}{}^{\bullet })\vee (s_{2}{}^{\bullet }\subseteq s_{1}{}^{\bullet })]}
  


== Work flow nets ==
Workflow nets (WF-nets) are a subclass of Petri nets intending to model the workflow of process activities. The WF-net transitions are assigned to tasks or activities, and places are assigned to the pre/post conditions. The WF-nets have additional structural and operational requirements, mainly the addition of a single input (source) place with no previous transitions, and output place (sink) with no following transitions. Accordingly, start and termination markings can be defined that represent the process status.
WF-nets have the soundness property, indicating that a process with a start marking of k tokens in its source place, can reach the termination state marking with k tokens in its sink place (defined as K-sound WF-net). Additionally, all the transitions in the process could fire (i.e., for each transition there is a reachable state in which the transition is enabled). A general sound (G-sound) WF-net is defined as being K-sound for every k>0.
A directed path in the Petri net is defined as the sequence of nodes (places and transitions) linked by the directed arcs. An elementary path includes every node in the sequence only once.
A Well-handled Petri net is a net in which there are no fully distinct elementary paths between a place and a transition (or transition and a place), i.e., if there are two paths between the pair of node then these paths share a node. An acyclic well-handled WF-net is sound (G-sound).
Extended WF-net is a Petri net that is composed of a WF-net with additional transition t (feedback transition). The sink place is connected as the input place of transition t and the source place as its output place. Firing of the transition causes iteration of the process (Note: the extended WF-net is not a WF-net).
WRI (Well-handled with Regular Iteration) WF-net, is an extended acyclic well-handled WF-net. WRI-WF-net can be built as composition of nets, i.e., replacing a transition within a WRI-WF-net with a subnet which is a WRI-WF-net. The result is also WRI-WF-net. WRI-WF-nets are G-sound, therefore by using only WRI-WF-net building blocks, one can get WF-nets that are G-sound by construction.
The Design structure matrix (DSM) can model process relations, and be utilized for process planning. The DSM-nets are realization of DSM-based plans into workflow processes by Petri nets, and are equivalent to WRI-WF-nets. The DSM-net construction process ensures the soundness property of the resulting net.


== Other models of concurrency ==
Other ways of modelling concurrent computation have been proposed, including process algebra, the actor model, and trace theory. Different models provide tradeoffs of concepts such as compositionality, modularity, and locality.
An approach to relating some of these models of concurrency is proposed in the chapter by Winskel and Nielsen.


== Application areas ==
Business Process Modeling
Concurrent programming
Data analysis
Diagnosis (Artificial intelligence)
Boolean differential calculus
Discrete process control
Kahn process networks
Process modeling
Reliability engineering
Simulation
Software design
Workflow management systems


== See also ==
Communicating finite-state machine
Finite-state machine
Kahn process networks
Petri Net Markup Language
Petriscript
Process architecture


== References ==


== Further reading ==
Cardoso, Janette; Camargo, Heloisa (1999). Fuzziness in Petri Nets. Physica-Verlag. ISBN 3-7908-1158-0. 
Grobelna, Iwona (2011). ""Formal verification of embedded logic controller specification with computer deduction in temporal logic"". Przeglad Elektrotechniczny. 87 (12a): 47–50. 
Jensen, Kurt (1997). Coloured Petri Nets. Springer Verlag. ISBN 3-540-62867-3. 
Котов, Вадим (1984). Сети Петри (Petri Nets, in Russian). Наука, Москва. 
Pataricza, András (2004). Formális módszerek az informatikában (Formal methods in informatics). TYPOTEX Kiadó. ISBN 963-9548-08-1. 
Peterson, James L. (1977). ""Petri Nets"". ACM Computing Surveys. 9 (3): 223–252. doi:10.1145/356698.356702. 
Peterson, James Lyle (1981). ""Petri Net Theory and the Modeling of Systems"". Prentice Hall. ISBN 0-13-661983-5. 
Petri, Carl A. (1962). Kommunikation mit Automaten (Ph. D. thesis). University of Bonn. 
Petri, Carl Adam; Reisig, Wolfgang. ""Petri net"". Scholarpedia. 3 (4): 6477. doi:10.4249/scholarpedia.6477. Retrieved 2008-07-13. 
Reisig, Wolfgang (1992). A Primer in Petri Net Design. Springer-Verlag. ISBN 3-540-52044-9. 
Riemann, Robert-Christoph (1999). Modelling of Concurrent Systems: Structural and Semantical Methods in the High Level Petri Net Calculus. Herbert Utz Verlag. ISBN 3-89675-629-X. 
Störrle, Harald (2000). Models of Software Architecture – Design and Analysis with UML and Petri-Nets. Books on Demand. ISBN 3-8311-1330-0. 
Zhou, Mengchu; Dicesare, Frank (1993). Petri Net Synthesis for Discrete Event Control of Manufacturing Systems. Kluwer Academic Publishers. ISBN 0-7923-9289-2. 
Zhou, Mengchu; Venkatesh, Kurapati (1998). Modeling, Simulation, & Control of Flexible Manufacturing Systems: A Petri Net Approach. World Scientific Publishing. ISBN 981-02-3029-X. 
Zaitsev, Dmitry (2013). Clans of Petri Nets: Verification of protocols and performance evaluation of networks. LAP LAMBERT Academic Publishing. ISBN 978-3-659-42228-7. 


== External links ==
Petri Nets World
Petri Net Markup Language
Java implementation of Petri nets in the jBPT library (see jbpt-petri module)
Java Petri net simulator
Petia Wohed's Flash-based tutorial introduction to Workflow Technology with Petri Nets
List of Petri net tools"
903,Epigrams on Programming,323141,1220,"""Epigrams on Programming"" is an article by Alan Perlis published in 1982, for ACM's SIGPLAN journal. The epigrams are a series of short, programming-language-neutral, humorous statements about computers and programming, which are widely quoted.
It first appeared in SIGPLAN Notices 17(9), September 1982.


== References ==
Perlis, A. J. (September 1982). ""Epigrams on programming"". ACM SIGPLAN Notices. New York, NY, USA: Association for Computing Machinery. 17 (9): 7–13. doi:10.1145/947955.1083808. Archived from the original on January 17, 1999. 


== External links ==
List of quotes (Yale)
Full article text -- (including so-called ""meta epigrams"", numbers 122-130)"
904,Boolean,212335,1218,"In most computer programming languages, a Boolean data type is a data type with only two possible values: true or false.
Related to this, Boolean (named after George Boole) may also refer to:
Boolean algebra, a logical calculus of truth values or set membership
Boolean algebra (structure), a set with operations resembling logical ones
Boolean domain, a set consisting of exactly two elements whose interpretations include false and true
Boolean circuit, a mathematical model for digital logical circuits.
Boolean expression, an expression in a programming language that produces a Boolean value when evaluated
Boolean function, a function that determines Boolean values or operators
Boolean model (probability theory), a model in stochastic geometry
Boolean network, a certain network consisting of a set of Boolean variables whose state is determined by other variables in the network
Boolean processor, a 1-bit variable computing unit
Boolean satisfiability problem"
905,ClearTalk,2907487,1217,"ClearTalk is a controlled natural language—a kind of a formal language for expressing information that is designed to be both human-readable (being based on English) and easily processed by a computer.
Anyone who can read English can immediately read ClearTalk, and the people who write ClearTalk learn to write it while using it. The ClearTalk system itself does most of the training through use: the restrictions are shown by menus and templates and are enforced by immediate syntactic checks. By consistently using ClearTalk for its output, a system reinforces the acceptable syntactic forms.
It is used by the experimental knowledge management software Ikarus and by a knowledge base management system Fact Guru.


== See also ==
Newspeak
Attempto Controlled English


== References =="
906,Constrained Delaunay triangulation,29638267,1216,"In computational geometry, a constrained Delaunay triangulation is a generalization of the Delaunay triangulation that forces certain required segments into the triangulation. Because a Delaunay triangulation is almost always unique, often a constrained Delaunay triangulation contains edges that do not satisfy the Delaunay condition. Thus a constrained Delaunay triangulation often is not a Delaunay triangulation itself.


== See also ==
Chew's second algorithm


== References ==


== External links ==
Daedalus Lib Open Source. Daedalus Lib manages fully dynamic constrained Delaunay triangulations."
907,Symposium on Operating Systems Principles,27006114,1206,"The Symposium on Operating Systems Principles (SOSP), organized by the Association for Computing Machinery (ACM), is one of the most prestigious single-track academic conferences on operating systems.
SOSP is held every other year, alternating with the conference on Operating Systems Design and Implementation (OSDI). The first SOSP was held in 1967. It is sponsored by the ACM's Special Interest Group on Operating Systems (SIGOPS).


== See also ==
List of computer science conferences


== References ==


== External links ==
http://sosp.org/"
908,Open Architecture Computing Environment,10871573,1205,"Open Architecture Computing Environment (OACE) was a specification that aimed to provide a standards-based computing environment in order to decouple computing environment from software applications. It was proposed for the United States Department of Defense in 2004.


== See also ==

Open architecture
[[Mission Data Interface)]


== References =="
909,Tin Kam Ho,56442210,1204,"Tin Kam Ho is a computer scientist at IBM research with contributions to machine learning, data mining, and classification. Ho is noted for introducing random decision forests in 1995 and is an IEEE fellow and IAPR fellow.


== References =="
911,Stochastic probe,38573054,1202,"In process calculus a stochastic probe is a measurement device that measures the time between arbitrary start and end events over a stochastic process algebra model.


== References =="
912,Local ternary patterns,37412518,1202,"Local ternary patterns (LTP) are an extension of Local binary patterns (LBP). Unlike LBP, it does not threshold the pixels into 0 and 1, rather it uses a threshold constant to threshold pixels into three values. Considering k as the threshold constant, c as the value of the center pixel, a neighboring pixel p, the result of threshold is:
  
    
      
        
          
            {
            
              
                
                  1
                  ,
                
                
                  
                    if 
                  
                  p
                  >
                  c
                  +
                  k
                
              
              
                
                  0
                  ,
                
                
                  
                    if 
                  
                  p
                  >
                  c
                  −
                  k
                  
                     and 
                  
                  p
                  <
                  c
                  +
                  k
                
              
              
                
                  −
                  1
                
                
                  
                    if 
                  
                  p
                  <
                  c
                  −
                  k
                
              
            
            
          
        
      
    
    {\displaystyle {\begin{cases}1,&{\text{if }}p>c+k\\0,&{\text{if }}p>c-k{\text{ and }}p<c+k\\-1&{\text{if }}p<c-k\\\end{cases}}}
  
In this way, each thresholded pixel has one of the three values. Neighboring pixels are combined after thresholding into a ternary pattern. Computing a histogram of these ternary values will result in a large range, so the ternary pattern is split into two binary patterns. Histograms are concatenated to generate a descriptor double the size of LBP.


== See also ==
Local binary patterns


== References =="
913,Throttling process (computing),10026229,1199,"In software, a throttling process, or a throttling controller as it is sometimes called, is a process responsible for regulating the rate at which application processing is conducted, either statically or dynamically.
For example, in high throughput processing scenarios, as may be common in online transactional processing (OLTP) architectures, a throttling controller may be embedded in the application hosting platform to balance the application's outbound publishing rates with its inbound consumption rates, optimize available system resources for the processing profile, and prevent eventually unsustainable consumption. In something like an enterprise application integration (EAI) architecture, a throttling process may be built into the application logic to prevent an expectedly slow end-system from becoming overloaded as a result of overly aggressive publishing from the middleware tier.


== See also ==
Bandwidth throttling"
914,Software safety classification,54309575,1197,"
== Safety Classes ==
Software classification is based on potential for hazard(s) that could cause injury to the user or patient.
Per IEC 62304:2006, software can be divided into three separate classes:
Class A: No injury or damage to health is possible
Class B: Nonserious injury is possible
Class C: Death or serious injury is possible


=== Serious injury ===
Injury or illness that directly or indirectly
is life threatening
results in permanent impairment of a body function or permanent damage to a body structure, or
necessitates medical or surgical intervention to prevent permanent impairment of a body function or permanent damage to a body structure.

^ ""Developing Medical Device Software to IEC 62304 | MDDI Medical Device and Diagnostic Industry News Products and Suppliers"". www.mddionline.com. Retrieved 2017-06-15."
915,N-jet,6838895,1197,"An N-jet is the set of (partial) derivatives of a function 
  
    
      
        f
        (
        x
        )
      
    
    {\displaystyle f(x)}
   up to order N.
Specifically, in the area of computer vision, the N-jet is usually computed from a scale space representation 
  
    
      
        L
      
    
    {\displaystyle L}
   of the input image 
  
    
      
        f
        (
        x
        ,
        y
        )
      
    
    {\displaystyle f(x,y)}
  , and the partial derivatives of 
  
    
      
        L
      
    
    {\displaystyle L}
   are used as a basis for expressing various types of visual modules. For example, algorithms for tasks such as feature detection, feature classification, stereo matching, tracking and object recognition can be expressed in terms of N-jets computed at one or several scales in scale space.


== See also ==
Scale space implementation
Jet (mathematics)


== References =="
916,Program Dependence Graph,37360516,1195,"A Program Dependence Graph (PDG) in computer science is a representation, using graph notation that makes data dependencies and control dependencies explicit. These dependencies are used during dependence analysis in optimizing compilers to make transformations so that multiple cores are used, and parallelism is improved.
see: dependency graph


== References =="
917,Computer-Aided Design Technical Committee,10886612,1191,"Computer-aided design (CAD) is the use of computer systems (or workstations) to aid in the creation, modification, analysis, or optimization of a design. CAD software is used to increase the productivity of the designer, improve the quality of design, improve communications through documentation, and to create a database for manufacturing. CAD output is often in the form of electronic files for print, machining, or other manufacturing operations. The term CADD (for Computer Aided Design and Drafting) is also used.
Its use in designing electronic systems is known as electronic design automation, or EDA. In mechanical design it is known as mechanical design automation (MDA) or computer-aided drafting (CAD), which includes the process of creating a technical drawing with the use of computer software.
CAD software for mechanical design uses either vector-based graphics to depict the objects of traditional drafting, or may also produce raster graphics showing the overall appearance of designed objects. However, it involves more than just shapes. As in the manual drafting of technical and engineering drawings, the output of CAD must convey information, such as materials, processes, dimensions, and tolerances, according to application-specific conventions.
CAD may be used to design curves and figures in two-dimensional (2D) space; or curves, surfaces, and solids in three-dimensional (3D) space.
CAD is an important industrial art extensively used in many applications, including automotive, shipbuilding, and aerospace industries, industrial and architectural design, prosthetics, and many more. CAD is also widely used to produce computer animation for special effects in movies, advertising and technical manuals, often called DCC digital content creation. The modern ubiquity and power of computers means that even perfume bottles and shampoo dispensers are designed using techniques unheard of by engineers of the 1960s. Because of its enormous economic importance, CAD has been a major driving force for research in computational geometry, computer graphics (both hardware and software), and discrete differential geometry.
The design of geometric models for object shapes, in particular, is occasionally called computer-aided geometric design (CAGD).


== Overview of CAD software ==
Starting around the mid 1960s, with the IBM Drafting System, computer-aided design systems began to provide more capability than just an ability to reproduce manual drafting with electronic drafting, the cost-benefit for companies to switch to CAD became apparent. The benefits of CAD systems over manual drafting are the capabilities one often takes for granted from computer systems today; automated generation of Bill of Material, auto layout in integrated circuits, interference checking, and many others. Eventually, CAD provided the designer with the ability to perform engineering calculations. During this transition, calculations were still performed either by hand or by those individuals who could run computer programs. CAD was a revolutionary change in the engineering industry, where draftsmen, designers and engineering roles begin to merge. It did not eliminate departments, as much as it merged departments and empowered draftsman, designers and engineers. CAD is just another example of the pervasive effect computers were beginning to have on industry. Current computer-aided design software packages range from 2D vector-based drafting systems to 3D solid and surface modelers. Modern CAD packages can also frequently allow rotations in three dimensions, allowing viewing of a designed object from any desired angle, even from the inside looking out. Some CAD software is capable of dynamic mathematical modeling.
CAD technology is used in the design of tools and machinery and in the drafting and design of all types of buildings, from small residential types (houses) to the largest commercial and industrial structures (hospitals and factories).
CAD is mainly used for detailed engineering of 3D models or 2D drawings of physical components, but it is also used throughout the engineering process from conceptual design and layout of products, through strength and dynamic analysis of assemblies to definition of manufacturing methods of components. It can also be used to design objects such as jewelry, furniture, appliances, etc. Furthermore, many CAD applications now offer advanced rendering and animation capabilities so engineers can better visualize their product designs. 4D BIM is a type of virtual construction engineering simulation incorporating time or schedule related information for project management.
CAD has become an especially important technology within the scope of computer-aided technologies, with benefits such as lower product development costs and a greatly shortened design cycle. CAD enables designers to layout and develop work on screen, print it out and save it for future editing, saving time on their drawings.


== Uses ==
Computer-aided design is one of the many tools used by engineers and designers and is used in many ways depending on the profession of the user and the type of software in question.
CAD is one part of the whole Digital Product Development (DPD) activity within the Product Lifecycle Management (PLM) processes, and as such is used together with other tools, which are either integrated modules or stand-alone products, such as:
Computer-aided engineering (CAE) and Finite element analysis (FEA)
Computer-aided manufacturing (CAM) including instructions to Computer Numerical Control (CNC) machines
Photorealistic rendering and Motion Simulation.
Document management and revision control using Product Data Management (PDM).
CAD is also used for the accurate creation of photo simulations that are often required in the preparation of Environmental Impact Reports, in which computer-aided designs of intended buildings are superimposed into photographs of existing environments to represent what that locale will be like, where the proposed facilities are allowed to be built. Potential blockage of view corridors and shadow studies are also frequently analyzed through the use of CAD.
CAD has been proven to be useful to engineers as well. Using four properties which are history, features, parameterization, and high-level constraints. The construction history can be used to look back into the model's personal features and work on the single area rather than the whole model. Parameters and constraints can be used to determine the size, shape, and other properties of the different modeling elements. The features in the CAD system can be used for the variety of tools for measurement such as tensile strength, yield strength, electrical or electromagnetic properties. Also its stress, strain, timing or how the element gets affected in certain temperatures, etc.


== Types ==

There are several different types of CAD, each requiring the operator to think differently about how to use them and design their virtual components in a different manner for each.
There are many producers of the lower-end 2D systems, including a number of free and open source programs. These provide an approach to the drawing process without all the fuss over scale and placement on the drawing sheet that accompanied hand drafting since these can be adjusted as required during the creation of the final draft.
3D wireframe is basically an extension of 2D drafting (not often used today). Each line has to be manually inserted into the drawing. The final product has no mass properties associated with it and cannot have features directly added to it, such as holes. The operator approaches these in a similar fashion to the 2D systems, although many 3D systems allow using the wireframe model to make the final engineering drawing views.
3D ""dumb"" solids are created in a way analogous to manipulations of real-world objects (not often used today). Basic three-dimensional geometric forms (prisms, cylinders, spheres, and so on) have solid volumes added or subtracted from them as if assembling or cutting real-world objects. Two-dimensional projected views can easily be generated from the models. Basic 3D solids don't usually include tools to easily allow motion of components, set limits to their motion, or identify interference between components.
There are two types of 3D Solid Modeling
Parametric modeling allows the operator to use what is referred to as ""design intent"". The objects and features created are modifiable. Any future modifications can be made by changing how the original part was created. If a feature was intended to be located from the center of the part, the operator should locate it from the center of the model. The feature could be located using any geometric object already available in the part, but this random placement would defeat the design intent. If the operator designs the part as it functions the parametric modeler is able to make changes to the part while maintaining geometric and functional relationships.
Direct or Explicit modeling provide the ability to edit geometry without a history tree. With direct modeling, once a sketch is used to create geometry the sketch is incorporated into the new geometry and the designer just modifies the geometry without needing the original sketch. As with parametric modeling, direct modeling has the ability to include relationships between selected geometry (e.g., tangency, concentricity).
Top end systems offer the capabilities to incorporate more organic, aesthetics and ergonomic features into designs. Freeform surface modeling is often combined with solids to allow the designer to create products that fit the human form and visual requirements as well as they interface with the machine.


== Technology ==

Originally software for Computer-Aided Design systems was developed with computer languages such as Fortran, ALGOL but with the advancement of object-oriented programming methods this has radically changed. Typical modern parametric feature-based modeler and freeform surface systems are built around a number of key C modules with their own APIs. A CAD system can be seen as built up from the interaction of a graphical user interface (GUI) with NURBS geometry or boundary representation (B-rep) data via a geometric modeling kernel. A geometry constraint engine may also be employed to manage the associative relationships between geometry, such as wireframe geometry in a sketch or components in an assembly.
Unexpected capabilities of these associative relationships have led to a new form of prototyping called digital prototyping. In contrast to physical prototypes, which entail manufacturing time in the design. That said, CAD models can be generated by a computer after the physical prototype has been scanned using an industrial CT scanning machine. Depending on the nature of the business, digital or physical prototypes can be initially chosen according to specific needs.
Today, CAD systems exist for all the major platforms (Windows, Linux, UNIX and Mac OS X); some packages support multiple platforms.
Right now, no special hardware is required for most CAD software. However, some CAD systems can do graphically and computationally intensive tasks, so a modern graphics card, high speed (and possibly multiple) CPUs and large amounts of RAM may be recommended.
The human-machine interface is generally via a computer mouse but can also be via a pen and digitizing graphics tablet. Manipulation of the view of the model on the screen is also sometimes done with the use of a Spacemouse/SpaceBall. Some systems also support stereoscopic glasses for viewing the 3D model.Technologies which in the past were limited to larger installations or specialist applications have become available to a wide group of users. These include the CAVE or HMDs and interactive devices like motion-sensing technology


== Software ==
CAD software enables engineers and architects to design, inspect and manage engineering projects within an integrated graphical user interface (GUI) on a personal computer system. Most applications support solid modeling with boundary representation (B-Rep) and NURBS geometry, and enable the same to be published in a variety of formats. A geometric modeling kernel is a software component that provides solid modeling and surface modeling features to CAD applications.
Based on market statistics, commercial software from Autodesk, Dassault Systems, Siemens PLM Software, and PTC dominate the CAD industry. The following is a list of major CAD applications, grouped by usage statistics.


== History ==
Designers have long used computers for their calculations. Digital computers were used in power system analysis or optimization as early as proto-""Whirlwind"" in 1949. Circuit design theory, or power network methodology would be algebraic, symbolic, and often vector-based. Examples of problems being solved in the mid-1940s to 50s include: servo motors controlled by generated pulse (1949), a digital computer with built-in computer operations to automatically co-ordinate transforms to compute radar related vectors (1951) and the essentially graphic mathematical process of forming a shape with a digital machine tool (1952). These were accomplished with the use of computer software. The man credited with coining the term CAD. Douglas T. Ross stated ""As soon as I saw the interactive display equipment,"" [being used by radar operators 1953] it would be just what his data reduction group needed. With the Lincoln Lab people, they were the only ones who used the big, complex display systems put in for the pre-SAGE, Cape Cod system. But ""we used it for our own personal workstation."".  The designers of these very early computers built utility programs so that programmers could debug programs using flowcharts on a display scope with logical switches that could be opened and closed during the debugging session. They found that they could create electronic symbols and geometric figures to be used to create simple circuit diagrams and flowcharts. They made the pleasant discovery that an object once drawn could be reproduced at will, its orientation, Linkage [ flux, mechanical, lexical scoping ] or scale changed. This suggested numerous possibilities to them. It took ten years of interdisciplinary development work before SKETCHPAD sitting on evolving math libraries emerged from MIT's labs. Additional developments were carried out in the 1960s within the aircraft, automotive, industrial control and electronics industries in the area of 3D surface construction, NC programming, and design analysis, most of it independent of one another and often not publicly published until much later. Some of the mathematical description work on curves was developed in the early 1940s by Robert Issac Newton from Pawtucket, Rhode Island. Robert A. Heinlein in his 1957 novel The Door into Summer suggested the possibility of a robotic Drafting Dan. However, probably the most important work on polynomial curves and sculptured surface was done by Pierre Bézier, Paul de Casteljau (Citroen), Steven Anson Coons (MIT, Ford), James Ferguson (Boeing), Carl de Boor (GM), Birkhoff (GM) and Garibedian (GM) in the 1960s and W. Gordon (GM) and R. Riesenfeld in the 1970s.
The invention of the 3D CAD/CAM is attributed to a French engineer, Pierre Bézier (Arts et Métiers ParisTech, Renault). After his mathematical work concerning surfaces, he developed UNISURF, between 1966 and 1968, to ease the design of parts and tools for the automotive industry. Then, UNISURF became the working base for the following generations of CAD software.
It is argued that a turning point was the development of the SKETCHPAD system at MIT by Ivan Sutherland (who later created a graphics technology company with David Evans). The distinctive feature of SKETCHPAD was that it allowed the designer to interact with his computer graphically: the design can be fed into the computer by drawing on a CRT monitor with a light pen. Effectively, it was a prototype of graphical user interface, an indispensable feature of modern CAD. Sutherland presented his paper Sketchpad: A Man-Machine Graphical Communication System in 1963 at a Joint Computer Conference having worked on it as his PhD thesis paper for a few years. Quoting, ""For drawings where motion of the drawing or analysis of a drawn problem is of value to the user, Sketchpad excels. For highly repetitive drawings or drawings where accuracy is required, Sketchpad is sufficiently faster than conventional techniques to be worthwhile. For drawings which merely communicate with shops, it is probably better to use conventional paper and pencil."" Over time efforts would be directed toward the goal of having the designers drawings communicate not just with shops but with the shop tool itself. This goal would be a long time arriving.
The first commercial applications of CAD were in large companies in the automotive and aerospace industries, as well as in electronics. Only large corporations could afford the computers capable of performing the calculations. Notable company projects were, a joint project of GM (Patrick J. Hanratty) and IBM (Sam Matsa, Doug Ross's MIT APT research assistant) to develop a prototype system for design engineers DAC-1 (Design Augmented by Computer) 1964; Lockheed projects; Bell GRAPHIC 1 and Renault.
One of the most influential events in the development of CAD was the founding of MCS (Manufacturing and Consulting Services Inc.) in 1971 by Patrick J. Hanratty, who wrote the system ADAM (Automated Drafting And Machining) but more importantly supplied code to companies such as McDonnell Douglas (Unigraphics), Computervision (CADDS), Calma, Gerber, Autotrol and Control Data.
As computers became more affordable, the application areas have gradually expanded. The development of CAD software for personal desktop computers was the impetus for almost universal application in all areas of construction.
Other key points in the 1960s and 1970s would be the foundation of CAD systems United Computing, Intergraph, IBM, Intergraph IGDS in 1974 (which led to Bentley Systems MicroStation in 1984).
CAD implementations have evolved dramatically since then. Initially, with 3D in the 1970s, it was typically limited to producing drawings similar to hand-drafted drawings. Advances in programming and computer hardware, notably solid modeling in the 1980s have allowed more versatile applications of computers in design activities.
Key products for 1981 were the solid modeling packages - Romulus (ShapeData) and Uni-Solid (Unigraphics) based on PADL-2 and the release of the surface modeler CATIA (Dassault Systemes). Autodesk was founded 1982 by John Walker, which led to the 2D system AutoCAD. The next milestone was the release of Pro/ENGINEER in 1987, which heralded greater usage of feature-based modeling methods and parametric linking of the parameters of features. Also of importance to the development of CAD was the development of the B-rep solid modeling kernels (engines for manipulating geometrically and topologically consistent 3D objects) Parasolid (ShapeData) and ACIS (Spatial Technology Inc.) at the end of the 1980s and beginning of the 1990s, both inspired by the work of Ian Braid. This led to the release of mid-range packages such as SolidWorks and TriSpective (later known as IRONCAD) in 1995, Solid Edge (then Intergraph) in 1996 and Autodesk Inventor in 1999. An independent geometric modeling kernel has been evolving in Russia since the 1990s.


== See also ==


== References ==


== External links ==
MIT 1982 CAD lab
 Learning materials related to Computer-aided Geometric Design at Wikiversity
 Learning materials related to Computer-aided design at Wikiversity
 Media related to Computer-aided design at Wikimedia Commons
 The dictionary definition of computer-aided design at Wiktionary
Beginning AutoCAD 2018, Industrial Press"
918,Reduction strategy (code optimization),4303385,1182,"In code optimization during the translation of computer programs into an executable form, and in mathematical reduction generally, a reduction strategy for a term rewriting system determines which reducible subterms (or reducible expressions, redexes) should be reduced (contracted) within a term; it may be the case that a term may contain multiple redexes which are disjoint from one another and that choosing to contract one redex before another may have no influence on the resulting reduced form of the term, or that the redexes in a term do overlap and that choosing to contract one of the overlapping redexes over the other may result in a different reduced form of the term. It is the choice of which redex at each step in the reduction to contract that determines the strategy chosen. This can be seen as a practical application of the theoretical notion of reduction strategy in lambda calculus.


== See also ==
Reduction system
Thunk"
919,AF-heap,1857196,1178,"In computer science, the AF-heap is a type of priority queue for integer data, an extension of the fusion tree using an atomic heap proposed by M. L. Fredman and D. E. Willard.
Using an AF-heap, it is possible to perform m insert or decrease-key operations and n delete-min operations on machine-integer keys in time O(m + n log n / log log n). This allows Dijkstra's algorithm to be performed in the same O(m + n log n / log log n) time bound on graphs with n edges and m vertices, and leads to a linear time algorithm for minimum spanning trees, with the assumption for both problems that the edge weights of the input graph are machine integers in the transdichotomous model.


== See also ==
Fusion tree


== References =="
920,Given-When-Then,43562339,1177,"Given-When-Then or GWT is a semi-structured way to write down test cases. They can either be tested manually or automated as browser tests with Selenium.
It derives its name from the three clauses used, which start with the words GIVEN, WHEN and THEN.
The Given-When-Then was invented by Dan North as part of behavior-driven development.


== See also ==
Acceptance test-driven development
Acceptance testing
Behavior-driven development
Hoare triple


== References ==


== External links ==
http://guide.agilealliance.org/guide/gwt.html"
921,Defining length,1514171,1166,"In genetic algorithms and genetic programming defining length L(H) is the maximum distance between two defining symbols (that is symbols that have a fixed value as opposed to symbols that can take any value, commonly denoted as # or *) in schema H. In tree GP schemata, L(H) is the number of links in the minimum tree fragment including all the non-= symbols within a schema H.


== Example ==
Schemata ""00##0"", ""1###1"", ""01###"", and ""##0##"" have defining lengths of 4, 4, 1, and 0, respectively. Lengths are computed by determining the last fixed position and subtracting from it the first fixed position.
In genetic algorithms as the defining length of a solution increases so does the susceptibility of the solution to disruption due to mutation or cross-over.


== References =="
922,Link distance,50761879,1162,"In computational geometry, the link distance between two points in a polygon is the minimum number of line segments of any polygonal chain within the polygon that has the two points as its endpoints. The link diameter of the polygon is the maximum link distance of any two of its points.
A polygon is a convex polygon if and only if its link diameter is one. Every star-shaped polygon has link diameter at most two: every two points may be connected by a polygonal chain that bends once, inside the kernel of the polygon. However, this property does not characterize star-shaped polygons, as there also exist polygons with holes in which the link diameter is two.


== References ==
Maheshwari, Anil; Sack, Jörg-Rüdiger; Djidjev, Hristo N. (2000), ""Link distance problems"", Handbook of Computational Geometry, North-Holland, Amsterdam, pp. 519–558, doi:10.1016/B978-044482537-7/50013-9, MR 1746684 ."
923,Anytime A*,52609984,1156,"Aadhaar (English: Foundation) is a 12-digit unique identity number issued to all Indian residents based on their biometric and demographic data. The data is collected by the Unique Identification Authority of India (UIDAI), a statutory authority established in January 2009 by the government of India, under the jurisdiction of the Ministry of Electronics and Information Technology, following the provisions of the Aadhaar (Targeted Delivery of Financial and other Subsidies, benefits and services) Act, 2016.
Aadhaar is the world's largest biometric ID system, with over 1.19 billion enrolled members as of 30 November 2017, representing over 99% of Indians. World Bank Chief Economist Paul Romer described Aadhaar as ""the most sophisticated ID programme in the world"". Considered a proof of residence and not a proof of citizenship, Aadhaar does not itself grant any rights to domicile in India. In June 2017 the Home Ministry clarified that Aadhaar is not a valid identification document for Indians travelling to Nepal and Bhutan.
Prior to the enactment of the Act, the UIDAI functioned, since 28 January 2009, as an attached office of the Planning Commission (now NITI Aayog). On 3 March 2016 a money bill was introduced in the Parliament to give legislative backing to Aadhaar. On 11 March 2016 the Aadhaar (Targeted Delivery of Financial and other Subsidies, benefits and services) Act, 2016, was passed in the Lok Sabha.
Aadhaar is the subject of several rulings by the Supreme Court of India. On 23 September 2013 the Supreme Court issued an interim order saying that ""no person should suffer for not getting Aadhaar"", adding that the government cannot deny a service to a resident who does not possess Aadhaar, as it is voluntary and not mandatory. The court also limited the scope of the program and reaffirmed the voluntary nature of the identity number in other rulings. On 24 August 2017 the Indian Supreme Court delivered a landmark verdict affirming the right to privacy as a fundamental right, overruling previous judgments on the issue. A five-judge constitutional bench of the Supreme Court is hearing various cases relating to the validity of Aadhaar on various grounds including privacy, surveillance, and exclusion from welfare benefits. On 9 January 2017 the five-judge Constitution bench of the Supreme Court of India reserved its judgement on the interim relief sought by petitions to extend the deadline making Aadhaar mandatory for everything from bank accounts to mobile services. The court said that the final hearing for the extension of Aadhaar Linking Deadlines will start on 17 January 2018. Some civil liberty groups such as the Citizens Forum for Civil Liberties and the Indian Social Action Forum (INSAF) have also opposed the project over privacy concerns.
Despite the validity of Aadhaar being challenged in the court, the central government has pushed citizens to link their Aadhaar numbers with a host of services, including mobile sim cards, bank accounts, the Employee Provident Fund, and a large number of welfare schemes including but not limited to the Mahatma Gandhi National Rural Employment Guarantee Act, the Public Distribution System, and old age pensions. Recent reports suggest that HIV patients have been forced to discontinue treatment for fear of identity breach as access to the treatment has become contingent on producing Aadhaar.


== Overview ==
The Unique Identification Authority of India (UIDAI) is a statutory authority established on 12 July 2016 by the Government of India under the jurisdiction of the Ministry of Electronics and Information Technology, following the provisions of the Aadhaar Act 2016.
The UIDAI is mandated to assign a 12-digit unique identification (UID) number (termed ""Aadhaar"") to all the residents of India. The implementation of the UID scheme entails generation and assignment of UIDs to residents; defining mechanisms and processes for interlinking UIDs with partner databases; operation and management of all stages of the UID life cycle; framing policies and procedures for updating mechanism and defining usage and applicability of UIDs for delivery of various services, among others. The number is linked to the resident's basic demographic and biometric information such as a photograph, ten fingerprints and two iris scans, which are stored in a centralised database.
The UIDAI was initially set up by the Government of India in January 2009, as an attached office under the aegis of the Planning Commission via a notification in the Gazette of India. According to the notification, the UIDAI was given the responsibility to lay down plans and policies to implement the UID scheme, to own and operate the UID database, and to be responsible for its updating and maintenance on an ongoing basis.
The UIDAI data centre is located at Industrial Model Township (IMT), Manesar, which was inaugurated by the then Chief Minister of Haryana Mr. Bhupendra Singh Hooda on 7 January 2013.

Starting with issuing of first UID in September 2010, the UIDAI has been aiming to issue an Aadhaar number to all the residents ensuring that it is robust enough to eliminate duplicate and fake identities, and that the number can be verified and authenticated in an easy and cost-effective way online anywhere, anytime. In a notification dated 16 December 2010 the Government of India indicated that it would recognise a letter issued by the UIDAI containing details of name, address, and Aadhaar number, as an official, valid document. Aadhaar is not intended to replace any existing identity cards, nor does it constitute proof of citizenship. Aadhaar neither confers citizenship nor guarantees rights, benefits, or entitlements. Aadhaar is a random number that never starts with a 0 or 1, and is not loaded with profiling or intelligence that would make it insusceptible to fraud or theft, and thus provides a measure of privacy in this regard. The unique ID also qualifies as a valid ID while availing various government services such as a LPG connection, a subsidised ration, kerosene from the PDS, or benefits under NSAP or pension schemes, e-sign, a digital locker, a Universal Account Number (UAN) under EPFO, and some other services such as a SIM card or opening a bank account. According to the UIDAI website, any Aadhaar holder or service provider can verify the genuineness of an Aadhaar number through a user-friendly service of UIDAI called the Aadhaar Verification Service (AVS), which is available on its website. Also, a resident already enrolled under the National Population Register is not required to enrol again for Aadhaar.


== History ==


=== Previous identity card programs ===
In 1998 after the Kargil war, the Kargil Review Committee, headed by security analyst K. Subrahmanyam, was formed to study the state of national security. It submitted its report to the then Prime Minister, Atal Bihari Vajpayee, on 7 January 2000. Among its various recommendations was the proposal that citizens in villages in border regions be issued identity cards on a priority basis, with such ID cards issued later to all people living in border states.
A Group of Ministers (GoM), headed by L. K. Advani, was formed to study the recommendations and examine possible implementation. The GoM submitted its report in May 2001 in which it accepted the recommendation for an ID card and stated that a ""multi-purpose National Identity Card"" project would be started soon, with the card to be issued first in border villages and then elsewhere. In late September 2001 the Ministry of External Affairs proposed that a mandatory national identity card be issued. This announcement followed reports that some people had obtained multiple Indian passports with different details. This was attributed to the lack of computerisation between the passport centres. In December 2003 the Citizenship (Amendment) Bill 2003 was introduced in the Lok Sabha by L. K. Advani. It primarily aimed to provide various rights to persons of Indian origin, but the bill also introduced Clause 14 (a) that said: ""The Central Government may compulsorily register every citizen of India and issue national identity card to him.""


=== 2009–2013 ===
The UIDAI was established on 28 January 2009 after the Planning Commission issued a notification. On 23 June Nandan Nilekani, the co-founder of Infosys, was appointed by the then-government, UPA, to head the project. He was given the newly created position of Chairman of the UIDAI, which was equivalent in rank to a Cabinet minister. In April 2010 the logo and the brand name Aadhaar was launched by Nilekani. In May 2010 Nilekani said he would support legislation to protect the data held by the UIDAI.
In July 2010 UIDAI published a list 15 of agencies which were qualified to provide training to personnel to be involved in the enrolment process. It also published a list of 220 agencies that were qualified to take part in the enrolment process. Before this, the project had been only 20 states and with the LIC of India and the State Bank of India as qualified registrars. This announcement introduced several private firms. It was estimated that to achieve the target of enrolling 40% of the population in two years, 31,019 personnel and 155 training centres would be needed. It was also estimated that 4,431 enrolment centres and 22,157 enrolment stations would have to be established.
On 7 February 2012 the UIDAI launched an online verification system for Aadhaar numbers. Using the system, banks, telecom companies and government departments could enter an Aadhaar number and verify if the person was a resident of India.
On 26 November 2012 Prime Minister Manmohan Singh launched an Aadhaar-linked direct benefit transfer scheme. The project aimed to eliminate leakages in the system by directly transferring the money to the bank account of the recipient. The project was to be introduced in 51 districts on 1 January 2013 and then slowly expanded to cover all of India.
In late November 2012 a former Karnataka High Court judge, K. S. Puttaswamy, and a lawyer, Parvesh Khanna, filed a Public Interest Litigation (PIL) against the government in the Supreme Court of India. They contended that the government was implementing the project without any legislative backing. They pointed out that the National Identification Authority of India Bill 2010, which had been introduced in the Rajya Sabha, was still pending. They further said that since the UIDAI was proceeding only on the basis of an executive order issued on 28 January 2009, it could not collect biometric data of citizens as it would be a violation of privacy under Article 21 of the Constitution. In December 2011 the Parliamentary Standing Committee on Finance, led by Yashwant Sinha, rejected the National Identification Authority of India Bill 2010 and suggested modifications. It termed the project ""unethical and violative of Parliament's prerogatives"". On 23 September 2013 the Supreme Court issued an interim order saying that the government could not deny a service to anyone who did not possess Aadhaar, as the identity number was voluntary.
In late September 2013, following the Supreme Court verdict, Union Minister of State for Parliamentary Affairs and Planning, Rajeev Shukla, said that it would attempt to pass the National Identification Authority of India Bill 2010 in the winter session of the Parliament. On 9 October 2013 the National Payments Corporation of India launched an Aadhaar-based remittance system. Using the system, funds could be transferred to any Aadhaar-linked bank accounts if the Aadhaar number was known. It was announced that an SMS could be used for amounts up to ₹5,000 (US$77) and for amounts over that a mobile bank app could be used. By this time around 440 million Aadhaar numbers had been issued.


=== 2014–2015 ===
In March 2014 Nilekani resigned as the Chairman to contest in the general election on an Indian National Congress nomination from Bangalore South. His responsibilities were taken over by 1981-batch IAS officer Vijay Madan, who was given an extension of his term as the director-general and mission director by the government. Nilekani lost to Ananth Kumar.
On 10 June 2014 the new government disbanded four cabinet committees to streamline the decision-making process; among them was the cabinet committee on Aadhaar. Also in June 2014 the IT Department held a meeting with the secretaries of the states to receive feedback on the project.
On 1 July 2014 Nilekani met with the Prime Minister Modi and Finance Minister Arun Jaitley to convince them of the project's merits. On 5 July 2014 Modi announced that his government would retain the project and asked an official to look into the possibility of linking the project with passports. The 2014 budget allotted ₹20.3964 billion (US$310 million) to the project for the fiscal year 2014–2015. It was a substantial increase from the previous year's allotment of ₹15.50 billion (US$240 million). Also in July, it was reported that UIDAI would hire an advertising agency and spend about ₹300 million (US$4.6 million) on an advertising campaign.
On 10 September 2014 the Cabinet Committee on Economic Affairs gave approval to Phase V of the UIDAI project, starting the enrolment process in Uttar Pradesh, Bihar, Chhattisgarh, and Uttarakhand. The Union Cabinet allocated ₹12 billion (US$180 million) to the project in order to reach the target of 1 billion enrolments by the end of 2015.
On 5 July 2015 finding the experience with DBT scheme in LPG ""very encouraging"", with a reported savings of ₹127 billion (US$1.9 billion) to the public exchequer this year, Jaitley said, ""If we can realize the government's JAM—Jan Dhan, Aadhaar, Mobile—vision we can ensure that money goes directly and more quickly into the pockets of the poor and from the savings we achieve, we can put even more money for the poor. If we can be careful in our design and implementation, we can extend DBT to other commodities, so that the poor get more money to spend for their upliftment.""
In March 2015 the Aadhaar-linked DigiLocker service was launched, using which Aadhaar-holders can scan and save their documents on the cloud, and can share them with the government officials whenever required without any need to carry them.
On 18 June 2015 in a high-level review meeting on the progress of the UID project and DBT scheme, Prime Minister Modi asked officials to accelerate the delivery of benefits and expand the applications of the Aadhaar (UID) platform. He also asked them to examine the possibility of offering incentives to the states to increase participation in the project, through a one-time sharing of a portion of the savings. It was reported that the government was saving up to 14–15% in the direct benefit transfers of subsidies on LPG to the beneficiaries through Aadhaar.


=== 2016–present ===
During the budget presentation on 29 February 2016, Jaitley announced that a bill would be introduced within a week to provide legislative support to the Aadhaar project. On 3 March 2016 the Aadhaar (Targeted Delivery of Financial and Other Subsidies, Benefits and Services) Bill, 2016, was introduced in the Parliament as a money bill by Jaitley. The decision to introduce it as a money bill was criticised by the opposition parties. Ghulam Nabi Azad, an INC leader, wrote in a letter to the Jaitley that the ruling party, the BJP, was attempting to bypass the Rajya Sabha, as they did not have the majority in the upper house. A money bill is only required to pass in the lower house Lok Sabha. Tathagata Satpathy of Biju Janata Dal (BJD) raised concerns that the project could be used for mass surveillance or ethnic cleansing in the future.
On 11 March 2016 the Aadhaar (Targeted Delivery of Financial and other Subsidies, benefits and services) Act, 2016, was passed in the Lok Sabha. During the Rajya Sabha debate on 16 March, Sitaram Yechury of the CPI-M said that bill should not have been passed when the issue of the right to privacy was still in the Supreme Court. On 16 March 2016 the bill was returned to the Lok Sabha by the Rajya Sabha with some suggested amendments, which the Lok Sabha promptly rejected.


== Direct Benefit Transfer (DBT) ==

The Aadhaar project has been linked to some public subsidy and unemployment benefit schemes such as the domestic LPG scheme and MGNREGA. In these Direct Benefit Transfer schemes, the subsidy money is directly transferred to a bank account which is Aadhaar-linked. Previously, however, the direct-benefit transfer had been carried out quite successfully via the National Electronic Funds Transfer (NEFT) system, which did not depend on Aadhaar.
On 29 July 2011 the Ministry of Petroleum and Natural Gas signed a memorandum of understanding with UIDAI. The Ministry had hoped the ID system would help eliminate loss of the subsidised kerosene and LPG. In May 2012 the government announced that it would begin issuing Aadhaar-linked MGNREGS cards. On 26 November 2012 a pilot programme was launched in 51 districts.
Under the original policy for liquefied petroleum gas subsidies, the customers bought gas cylinders from retailers at subsidised prices, and the government compensated companies for their losses. Under the current Direct Benefit Transfer of LPG (DBTL), introduced in 2013, customers had to buy at full price, and the subsidy would be then directly credited to their Aadhaar-linked bank accounts. This scheme, however, did not take off, and in September 2013 a Supreme Court order put a halt on it. Subsequently, the GOI constituted a committee to review the ""Direct Benefits Transfer for LPG Scheme"" to study the shortcomings in the scheme and recommend changes. The DBTL scheme was modified later as PAHAL by the new government in November 2014. Under PAHAL, subsidies could be credited to a purchaser's bank account even if he or she did not have an Aadhaar number. Official data show that cooking gas consumption during the January–June period grew at a slower 7.82%, which is nearly four percentage points less than the 11.4% growth in the same period last year.
The PAHAL scheme has covered 118.9 million of the 145.4 million active LPG consumers until March, as stated by the Petroleum Minister in the Parliament. The DBT has thereby become a ""game changer"" for India, claimed the Chief Economic Adviser to the Finance Ministry, Government of India, Arvind Subramanian, for in case of LPG subsidy, DBT had resulted in a 24% reduction in the sale of subsidized LPG, as ""ghost beneficiaries"" had been excluded. The savings to the government were to the tune of ₹127 billion (US$1.9 billion) in 2014–2015. The success of the modified scheme helped fuel marketing companies save almost ₹80 billion (US$1.2 billion) from November 2014 to June 2015, said oil company officials. The DBT for the public distribution system (PDS) will be rolled out in September 2015.
The government's own data, however, suggest that the cost of implementing the DBT for LPG was over a million dollars, a figure quite at odds with the savings figures that the government cites.
Prime Minister Modi has asked for integration of all land records with Aadhaar at the earliest, emphasising at his monthly PRAGATI (Pro-Active Governance And Timely Implementation) meeting on 23 March 2016 that this was extremely important to enable monitoring of the successful implementation of the Pradhan Mantri Fasal Bima Yojana or crop insurance scheme.


=== Aadhaar-enabled biometric attendance systems ===
In July 2014 Aadhaar-enabled biometric attendance systems were introduced in government offices. The system was introduced to check late arrival and absenteeism of government employees. The public could see the daily in and out of employees on the website attendance.gov.in. In October 2014 the website was closed to the public but as of 24 March 2016 is again active and open to public access. The employees use the last four digits (last eight digits for government employee registering as of August 2016) of their Aadhaar number and their fingerprints, for authentication. Technological glitches with the system have, however, meant that employees have to often spend


=== Other uses by central government agencies ===
In November 2014 it was reported that the Ministry for External Affairs was considering making Aadhaar a mandatory requirement for passport holders. In February 2015 it was reported that people with an Aadhaar number would get their passports issued within 10 days, as it sped up the verification process by making it easier to check if an applicant had any criminal records in the National Crime Records Bureau database. In May 2015 it was announced that the Ministry of External Affairs was testing the linking of passports to the Aadhaar database.
In October 2014 the Department of Electronics and Information Technology said that they were considering linking Aadhaar to SIM cards. In November 2014 the Department of Telecom asked all telecom operators to collect Aadhaar from all new applicants of SIM cards. On 4 March 2015 a pilot project was launched allowing Aadhaar-linked SIM cards to be sold in some cities. The purchaser could activate the SIM at the time of purchase by submitting his Aadhaar number and pressing his fingerprints on a machine. It is part of the Digital India plan. The Digital India project aims to provide all government services to citizens electronically and is expected to be completed by 2018.
In July 2014 the Employees' Provident Fund Organisation of India (EPFO) began linking provident fund accounts with Aadhaar numbers. In November 2014 the EPFO became a UIDAI registrar and began issuing Aadhaar number to provident fund subscribers. In December 2014 Labour Minister Bandaru Dattatreya clarified that an Aadhaar number was not necessary for any provident fund transaction.
In August 2014 Prime Minister Modi directed the Planning Commission of India to enrol all prisoners in India under the UIDAI.
In December 2014 it was proposed by the Minister for Women and Child Development, Maneka Gandhi, that Aadhaar should be made mandatory for men to create a profile on matrimonial websites, to prevent fake profiles. In July 2015 the Department of Electronics and Information Technology (DeitY) called a meeting of various matrimonial sites and other stakeholders discuss the use of Aadhaar to prevent fake profiles and protect women from exploitation.
On 3 March 2015 the National Electoral Roll Purification and Authentication Programme (NERPAP) of the Election Commission was started. It aims to link the Elector's Photo Identity Card (EPIC) with the Aadhaar number of the registered voter. It aims to create an error-free voter identification system in India, especially by removing duplications.


=== Other uses by states ===
In the Hyderabad region of Telangana state, Aadhaar numbers were linked to ration cards to remove duplicate ration cards. The project was started in July 2012 and was carried out despite the 2013 Supreme Court order. More than 63,932 ration cards in the white category and 229,757 names were removed from its database in the drive between July 2012 and September 2014. In August 2012 the government of the state of Andhra Pradesh asked citizens to surrender illegal ration cards before it began to link them with Aadhaar numbers. By September 2014 15 lakh illegal ration cards had been surrendered. In April 2015 the state of Maharashtra began enrolling all school students in the state in the Aadhaar project to implement the Right to Education Act properly.
Electronic-Know Your Customer (e-KYC) using Aadhaar card is also being introduced to activate mobile connections instantly to check Aadhaar Card Status.


== Aadhaar as digital identity ==
A number of features make the Aadhaar card a digital identity and facilitate digital identity: the document of the card itself is electronic in PDF format; a QR Code provides digital XML representation of some core details of the card; the number and some limited details can be validated online (with the notable exclusion of the name); updating details can be done electronically using a mobile phone number and/or email as the second factor of authentication; the system collects a photo, all 10 finger scans, and eye scan; however, there is no known common usage of this data to date to electronically validate a holder.


== Impediments and other concerns ==


=== Feasibility concerns ===
In October 2010 R. Ramakumar, an economist at the Tata Institute of Social Sciences, wrote in an editorial for The Hindu that the project was being implemented without any cost-benefit or feasibility studies to ensure whether the project would meet its stipulated goals. He also pointed out that the government was obscuring the security aspects of Aadhaar and focusing on the social benefit schemes. He quoted a former chief of the Intelligence Bureau Ajit Doval, who had said that originally Aadhaar aimed to weed out illegal aliens.
In March 2011 Rajanish Dass of IIM Ahmedabad's Computer and Information Systems Group published a paper titled ""Unique Identity Project in India: A divine dream or a miscalculated heroism"". Dass claimed that even if enrolment was voluntary, it was being made mandatory by indirect means. He pointed out that essential schemes like the National Food Security Act, 2013, was being linked to the UIDAI. He also stated that the feasibility of a project of this size had not been studied and raised concerns about the quality of the biometric data being collected. He cited statements of another researcher, Usha Ramanathan, that the UIDAI would ultimately have to become profit-making to sustain itself.
The debate on the feasibility of sustaining a project of the size of population of India is settled as over 1.19 billion Indians are enrolled in Aadhaar as of 30 November 2017, representing over 99% of the total population. The quality of the data collected is also the most advanced in the world as of now. The scheme compliments other initiatives taken by the government, for example Digital India, to benefit people by giving easier access to public services.
On 9 November 2012 the National Institute of Public Finance and Policy (NIPFP) published a paper titled A cost-benefit analysis of Aadhaar. The paper claimed that by 2015–2016 the benefits of the project would surpass the costs, and by 2020–2021 the total benefit would be ₹251 billion (US$3.8 billion) against a total expenditure of ₹48.35 billion (US$740 million). The benefits would come from plugging leakages in various subsidy and social benefit schemes.
On 2 February 2013 Reetika Khera, a development economist at IIT Delhi, published a paper in the Economic and Political Weekly titled A 'Cost-Benefit' Analysis of UID, in response to the cost-benefit analysis published by NIPFP. She argued that the seemingly large benefits were based 'almost entirely on unrealistic assumptions' and outdated data. The paper pointed to how the relative cost-effectiveness of Aadhaar in comparison with alternative technologies – the basic premise of any cost-benefit analysis – was entirely ignored. Further, concerns regarding a possible conflict of interest were also raised.
In March 2016 the International Institute for Sustainable Development released a report that the benefit from Aadhaar-linked LPG subsidy scheme for 2014–2015 was ₹140 million (US$2.1 million) and for 2015–2016 was ₹1.209 billion (US$19 million). These sums were much lower than the number stated by Finance Minister Jaitley in the Lok Sabha. He had said in March 2016 that the government had saved ₹150 billion (US$2.3 billion) from the scheme. The paper said that the government was also including the savings from the efforts of oil marketing companies (OMCs) prior to the introduction of Aadhaar. The method used by the OMCs to weed out duplicates and ghost customers was 15–20 times more effective than the Aadhaar-based method. It has to be noted that the savings of ₹150 billion (US$2.3 billion) from the scheme was not claimed by the government to be from LPG subsidy alone, but by plugging leaks and checking corruption with the help of Aadhaar in all the schemes administered by the government of India. Though Aadhaar records were not available for removing the duplicates and ghost customers previously, they are effective in future to prevent or remove duplicates and ghost accounts, not just for LPG subsidy, but for numerous Direct Benefit Schemes operated by government of India. Being a socialist country with large government subsidies along with widespread corruption and poverty, a biometric system if used properly can prove to be a boon.


=== Lack of legislation and privacy concerns ===
On 2 February 2015 the Supreme Court asked the new government to clarify its stance on the project. This was in response to a new PIL filed by Mathew Thomas, a former army officer. Thomas had claimed that the government was ignoring previous orders while pushing ahead with the project and that the project was unconstitutional as it allowed profiling of citizens. In a reply on 12 February the government said that it would continue the project. On 16 July 2015 the government requested the Supreme Court to revoke its order, saying that it intended to use Aadhaar for various services. On 21 July 2015 the Court noted that some states were insisting on Aadhaar for benefits despite its order.
On 11 August 2015 the Supreme Court directed the government to widely publicise in print and electronic media that Aadhaar was not mandatory for any welfare scheme. The Court also referred the petitions claiming Aadhaar was unconstitutional to a Constitutional Bench.
On 19 July 2017 a nine judge bench of the Supreme Court began hearing the arguments on whether there is a fundamental right to privacy. On 24 August 2017 the nine judge bench unanimously upheld the right to privacy as a fundamental right under the Constitution.
A five-judge constitutional bench of the Supreme Court is currently hearing various cases relating to the validity of Aadhaar on various grounds including privacy, surveillance, and exclusion from welfare benefits. As of 27 February 2018, senior counsels Shyam Divan, Kapil Sibal, and Gopal Subramanium, have argued over a span of 13 days in this matter.


=== Legality of sharing data with law enforcement ===
In 2013 in Goa the CBI was trying to solve the case of a rape of a schoolgirl. It approached a Goa local court saying that they had acquired some fingerprints from the scene that could be matched with the UIDAI database. The court asked the UIDAI to hand over all data of all persons in Goa to the CBI.
The UIDAI appealed in the Bombay High Court saying that accepting such a request would set precedent for more such requests. The High Court rejected the argument and on 26 February 2014 in an interim order directed Central Forensic Science Laboratory (CFSL) to study the technological capability of the database to see if it could solve such a crime. The UIDAI then appealed in the Supreme Court. It argued that the chance of a false positive was 0.057% and with 600 million people in its database it would result in hundreds of thousands of false results.
On 24 March 2014, the Supreme Court restrained the central government and the UIDAI from sharing data with any third party or agency, whether government or private, without the consent of the Aadhaar-holder in writing. Vide another interim order dated 16 March 2015, the Supreme Court of India has directed that the Union of India and States and all their functionaries should adhere to the order passed by this court on 23 September 2013. It observed that some government agencies were still treating Aadhaar as mandatory and asked all agencies to issue notifications clarifying that it was not.


=== Land Allotment Dispute ===
In September 2013 the Delhi Development Authority accepted a complaint from the activist group India Against Corruption and cancelled a land allotment to the UIDAI. The land was previously owned by BSNL, and MTNL had also laid claims on it. It had an estimated ₹9 billion (US$140 million) value but had been allotted to the UIDAI at a very cheap rate.
The issue of constructing the UIDAI HQs and UIDAI Regional Office building in Delhi was resolved with Department of Telecom (DoT), following which the Ministry of Urban Development issued a notification on 21 May 2015 clearing the titles of the land in favour of the UIDAI, including projected land use.


=== Security concerns ===
In an August 2009 interview with the Tehelka, former chief of the Intelligence Bureau (IB), Ajit Doval, said that Aadhaar was originally intended to flush out illegal immigrants, but social security benefits were later added to avoid privacy concerns. In December 2011 the Parliamentary Standing Committee on Finance, led by Yashwant Sinha, rejected the National Identification Authority of India Bill, 2010, and suggested modifications. It expressed objections to the issuing of Aadhaar numbers to illegal immigrants. The Committee said that the project was being implemented in an unplanned manner and bypassing the Parliament.
In May 2013, deputy director general of the UIDAI, Ashok Dalwai, admitted that there had been some errors in the registration process. Some people had received Aadhaar cards with wrong photographs or fingerprints. According to Aloke Tikku of the Hindustan Times, some officials of the Intelligence Bureau (IB) had criticised the UIDAI project in September 2013, with the officials saying that the Aadhaar number cannot be considered a credible proof of residence. As under the liberal pilot phase, where a person claimed to live was accepted as the address and recorded.


=== Overlaps with National Population Register ===

The Aadhaar and the similar National Population Register (NPR) projects have been reported to be having conflicts. In January 2012 it was reported that the UIDAI would share its data with NPR and the NPR would continue to collect its own data. In January 2013 then-Home Minister Sushilkumar Shinde said that Aadhaar was not an identity card but a number, while the NPR was necessary for national security purposes. The 2013 Supreme Court order did not affect the NPR project as it was not linked to any subsidy.
In July 2014 a meeting was held to discuss the possibility of merging the two projects, Aadhaar and NPR, or making them complementary. The meeting was attended by Home Minister Rajnath Singh, Law and Justice and Telecom Minister Ravi Shankar Prasad, and Minister of State for Planning Rao Inderjit Singh. Later in the same month, Rao Inderjit Singh told the Lok Sabha that no plan to merge the two projects has been made.


=== Fraud ===
In order to make Aadhaar accessible to often undocumented poorer citizens, obtaining an Aadhaar card does not require significant documentation, with multiple options available. In theory, the use of biometric facilities should reduce or eliminate duplication. So, in theory, while it may be possible to obtain the card under a false name, it is less likely that a person would be able to obtain another Aadhaar card under a different (or real) name.
The Aadhaar card itself is not a secure document (being printed on paper) and according to the agency should not be treated as an identity card though it is often treated as such. However, with currently no practical way to validate the card (e.g. by police at airport entry locations) it is of questionable utility as an identity card. ""There are five main components in an Aadhaar app transaction – the customer, the vendor, the app, the back-end validation software, and the Aadhaar system itself. There are also two main external concerns – the security of the data at rest on the phone and the security of the data in transit. At all seven points, the customer's data is vulnerable to attack ... The app and validation software are insecure, the Aadhaar system itself is insecure, the network infrastructure is insecure, and the laws are inadequate,"" claims Bhairav Acharya, Program Fellow, New America.
The Aadhaar card is usually printed on glossy paper, and the government has stated black and white copies are valid. Some agencies charge extra to laminate the document. Other agencies have been reported charging ₹50 to 200 to produce a PVC version of the card, and it is marketed by them as a smart card, despite having no official validity and no chip.
Certain mobile apps claim to verify an Aadhaar card using a QR code scanner. However, the QR code is not a secure representation of an Aadhaar card either and can be copied and edited. The only way to validate an Aadhaar card is to perform an online validation, which will confirm that the card number is valid, confirm the postal code and gender of the holder (but not their name or photo). In theory, this means that is possible to create a false Aadhaar card using the number of a genuine holder from the same postal code with the same gender, with the card subject to a number of cases of counterfeiting.
The digital document itself is self-signed by a non-internationally recognised certificate authority (n)Code Solutions, a division of Gujarat Narmada Valley Fertilizers Company Ltd (GNFC) and needs to be manually installed on the PC. This is despite Entrust assisting in the development of the solution.


=== Application issues ===
While the service is free for citizens, some agents have been charging fees. Despite the modern processes, there are cases where enrolments are lost in the system without explanation. mAadhaar is an official mobile application developed by the UIDAI to provide an interface to Aadhaar number holders to carry their demographic information including name, date of birth, gender, and address along with photograph as linked with their Aadhaar number in smartphones. In one case, every resident in a village in Haridwar was assigned a birthday of 1 January.


=== Exclusion ===
Documentary proof may be difficult to obtain, with the system requiring documents such as bank accounts, insurance policies, and driving licences that themselves increasingly require an Aadhaar card or similar documentary evidence to originate. This may lead to a significant minority underclass of undocumented citizens who will find it harder to obtain necessary services. Introducers and Heads of family may also assist in documentation; however, for many agencies and legitimate applications, this facility may not be practical.
Non-resident Indians, overseas citizens of India, and other resident foreigners may also find it difficult to avail themselves of services they could previously freely obtain, such as local SIM cards, despite assurances to the contrary.


=== Data leaks ===
The detailed personal information being collected is of extremely high importance to an individual. However, once collected, it is not being treated with the required sensitivity for privacy concerns. Major financial transactions are linked with information collected in Aadhaar. Data leaks are a gold mine for criminals who now use sophisticated hackers. Government departments and various other agencies that collect this information such as banks cannot be trusted to maintain the secrecy of all this collected information. Another case occurred wherein Aadhaar data collected by Reliance Jio was leaked online, and the data may now be widely available to hackers. The UIDAI confirms more than 200 government websites were publicly displaying confidential Aadhaar data; though removed now, the data leaked cannot be scrubbed from hackers' databases. On 2017 July privacy issues with regard to the Aadhaar card were discussed in the Supreme Court.  A report from the Center for Internet and Society suggests that the records of about 135 million Indians may have been leaked. A loophole was identified that allows all records to be accessed by anyone though hackers can find other routes.
Wikileaks tweeted on 25 August 2017 that the same American supplier of fingerprint and Iris scanning equipment that collaborated with the CIA to identify Osama Bin Laden was also supplying equipment to India. The complex structure of ownership is detailed in an article in Fountainink.in Concerns were raised as early as 2011 in the Sunday Guardian regarding not following due process and handing over contracts to entities with links to the FBI and having a past history of leaking data across countries. How the CIA can hack and access the Aadhaar database using a secret Expresslane project is documented in a report on the GGInews website and saved in an archive lest it be removed. Further communications have also identified the clauses under which data may have freely flowed to foreign agencies due to the nature and wordings in the Aadhaar contracts and archived here.


== See also ==

Indian ration card
Indian passport
Permanent account number


== References ==


== Further reading ==


=== Supportive views ===
""A cost-benefit analysis of Aadhaar"" (PDF). National Institute of Public Finance and Policy. 9 November 2012. 
Nilekani, Nandan; Shah, Viral (2015). Rebooting India: Realizing a Billion Aspirations. India: Penguin Group. p. 340. ISBN 9780670087891. ASIN 0670087890. 
Aiyar, Shankkar (21 July 2017). Aadhaar: A Biometric History of India's 12-Digit Revolution. Westland Publications. ASIN 938622495X. 
Shekhar Gupta (9 January 2018). ""God, please save India from our upper class Aadhaarophobics"". ThePrint. 


=== Critical views ===
Rajanish Dass (March 2011). ""Unique Identification for Indians: A Divine Dream or a Miscalculated Heroism?"". IIM Ahmedabad. 
Kieran Clarke; Shruti Sharma; Damon Vis-Dunbar (30 September 2015). ""Ghost savings: Understanding the fiscal impacts of India's LPG subsidy"". International Institute for Sustainable Development. 
Deciphering Aadhaar bill, its benefits and privacy issues
A rant on Aadhaar
Aadhar sitting duck


=== Other ===
Elonnai Hickok (21 July 2015). ""Aadhaar Number vs the Social Security Number"". Centre for Internet and Society (India). 


== External links ==
 Media related to Unique Identification Authority of India at Wikimedia Commons
Official website"
924,Fxgrep,12348533,1150,"fxgrep (The Functional XML Querying Tool) is an XML querying tool written in the Standard ML programming language. The Fxgrep addresses the problem of dealing with 
  
    
      
        k
      
    
    {\displaystyle k}
  -ary queries, that is, the task of simultaneously locating 
  
    
      
        k
      
    
    {\displaystyle k}
   nodes of an input XML tree that satisfy a given relation.


== Notes and references ==


== External links ==
fxgrep web page"
925,System on TPTP,29466441,1149,"System on TPTP is an online interface to several automated theorem proving systems and other automated reasoning tools. It allows users to run the systems either on problems from the latest releases from the TPTP problem library or on user-supplied problems in the TPTP syntax.
The system is maintained by Geoff Sutcliffe at the University of Miami. In November 2010, it featured more than 50 systems, including both theorem provers and model finders. System on TPTP can either run user-selected systems, or pick systems automatically based on problem features, and run them in parallel.


== References =="
926,Knuth's Simpath algorithm,43278241,1146,"Simpath is an algorithm introduced by Donald Knuth that constructs a zero-suppressed decision diagram (ZDD) representing all simple paths between two vertices in a given graph.


== References ==


== External links ==
Graphillion library which implements the algorithm for manipulating large sets of paths and other structures.
[1], A CWEB implementation by Donald Knuth."
927,Datatjej,2484381,1146,"Datatjej (translated: ""computer girl"" or ""data girl"") is an annual conference for women studying Computer science in Sweden. (The Computer Science education must be at least 3 years long). The conference lasts for 4 days and has about 100 visitors each year. Every year there is a different theme, examples of previous themes are mentorship, back to the future, personal development.
The first conference was held at Umeå University in 1998 with 20 participants. It has since then grown and been hosted at several other Swedish universities.


== External links ==
Official homepage for Datatjej
Datatjej 2006
Datatjej 2005
Datatjej 2004
Datatjej 2003
Datatjej 1998"
928,Strafunski,12263700,1136,"Strafunski is a functional programming-based toolset for implementing program analysis and transformation components. It is implemented in Haskell, and provides an API for operating on parse trees such as those generated by SDF.
Features:
Generic traversal over typed representations of parse trees
integration with external components


== See also ==
ASF+SDF Meta Environment
Stratego/XT


== External links ==
ASF+SDF Meta Environment


== Related Material =="
929,Subgroup method,22497597,1131,"The subgroup method is an algorithm used in the mathematical field of group theory. It is used to find the word of an element. It doesn't always return the minimal word, but it can return optimal words based on the series of subgroups that is used. The code looks like this:

  function operate(element, generator)
       <returns generator operated on element>
  
  function subgroup(g)
       sequence := (set of subgroups that will be used, depending on the method.)
       word := []
       for subgroup in sequence
            coset_representatives := []
            <fill coset_representatives with coset representatives of (next subgroup)/subgroup>
            for operation in coset_representatives
                 if operate(g, operation) is in the next subgroup then
                      append operation onto word
                      g = operate(g, operation)
                      break
       return word"
930,International Conference on Interactive Computer Aided Learning,14778585,1130,"International Conference on Interactive Computer Aided Learning (ICL) is an annual International Association of Online Engineering (IAOE) conference.
ICL is a conference covering topics on interactive computer aided learning. ICL is an interdisciplinary conference focusing on the exchange of relevant trends and research results as well as the presentation of practical experiences gained while developing and testing elements of interactive computer aided learning. This conference is annually organized by the Carinthia University of Applied Sciences in Villach Austria in cooperation with a number of organizations and groups.
ICL was started in 1998 by Michael E. Auer. It operates under the auspices of the International Association of Online Engineering (IAOE).


== External links ==
Official website"
931,Greedy Perimeter Stateless Routing in Wireless Networks,42346684,1129,"The Greedy Perimeter Stateless Routing in Wireless Networks is a routing protocol for mobile ad-hoc networks. It was developed by B. Karp. It uses a greedy algorithm to do the routing and orbits around a perimeter.


== Coordinates instead of receiver names ==
GPSR is a geo routing method, which means that data packages are not sent to a special receiver but to coordinates. The packages should be relayed to the node that's geographically closest to the coordinates. This assumes that every node knows its own position.


== Literature ==
B.Karp: Challenges in Geographic Routing: Sparse Networks, Obstacles, and Traffic Provisioning. In DIMACS Workshop on Pervasive Networking, Piscataway, NJ, May 2001
B.Karp: Geographic Routing for Wireless Networks. Ph.D. Dissertation, Harvard University, Cambridge, MA, October 2000
B.Karp, H.T.Kung: Greedy Perimeter Stateless Routing for Wireless Networks. In Proceedings of the Sixth Annual ACM/IEEE International Conference on Mobile Computing and Networking (MobiCom 2000), Boston, MA, August 2000, pp. 243-254"
932,Algorithmic paradigm,51411922,1127,"An algorithmic paradigm, algorithm design paradigm, algorithmic technique, or algorithmic strategy is a generic method or approach which underlies the design of a class of algorithms. It is an abstraction higher than the notion of an algorithm, just as an algorithm is an abstraction higher than a computer program.  Examples of algorithmic paradigms include the greedy algorithm in optimization problems, dynamic programming, prune and search, and divide and conquer algorithms. More specialized algorithmic paradigms used in parameterized complexity include kernelization and iterative compression. In computational geometry, additional algorithmic paradigms include sweep line algorithms, rotating calipers, and randomized incremental construction.


== References =="
933,WINEPI,43357456,1123,"In data mining, the WINEPI algorithm is an influential algorithm for episode mining, which helps discover the knowledge hidden in an event sequence.
WINEPI derives part of its name from the fact that it uses a sliding window to go through the event sequence.
The outcome of the algorithm are episode rules describe temporal relationships between events and form an extension of association rules.


== References ==
Heikki Mannila; Hannu Toivonen; A. Inkeri Verkamo (1997). ""Discovery of Frequent Episodes in Event Sequences"". Data Min. Knowl. Discov. 1 (3): 259–289. doi:10.1023/A:1009748302351. 
Klemettinen & Moen. ""Data mining course"" (Powerpoint presentation). p. 12."
934,Lossy Count Algorithm,42326508,1122,"The lossy count algorithm is an algorithm to identify elements in a data stream whose frequency count exceed a user-given threshold. The frequency computed by this algorithm is not always accurate, but has an error threshold that can be specified by the user. The run time space required by the algorithm is inversely proportional to the specified error threshold, hence larger the error, the smaller the footprint. It was created by eminent computer scientists Rajeev Motwani and Gurmeet Singh Manku. This algorithm finds huge application in computations where data takes the form of a continuous data stream instead of a finite data set, for e.g. network traffic measurements, web server logs, clickstreams.


== References =="
935,VICAR file format,39259185,1120,"VICAR is an image file format developed by the NASA's Jet Propulsion Laboratory. It has been used to transport images from a variety of space missions including Cassini–Huygens and the Viking Orbiter.


== References ==


== External links ==
Collection of images from the Cassini orbiter
VICAR2PNG a tool that converts VICAR images."
936,General-purpose language,40406527,1118,"In computer software, a general-purpose programming language is a programming language designed to be used for writing software in a wide variety of application domains (a general-purpose language). In many ways a general-purpose language only has this status because it does not include language constructs designed to be used within a specific application domain (e.g., a page description language contains constructs intended to make it easier to write programs that control the layout of text and graphics on a page).
Conversely, a domain-specific programming language is one designed to be used within a specific application domain.
The following are some general-purpose languages:


== References =="
937,Cost efficiency,3505721,1116,"Cost efficiency (or cost optimality), in the context of parallel computer algorithms, refers to a measure of how effectively parallel computing can be used to solve a particular problem. A parallel algorithm is considered cost efficient if its asymptotic running time multiplied by the number of processing units involved in the computation is comparable to the running time of the best sequential algorithm.
For example, an algorithm that can be solved in 
  
    
      
        O
        (
        n
        )
      
    
    {\displaystyle O(n)}
   time using the best known sequential algorithm and 
  
    
      
        O
        
          (
          
            
              n
              p
            
          
          )
        
      
    
    {\displaystyle O\left({\frac {n}{p}}\right)}
   in a parallel computer with 
  
    
      
        p
      
    
    {\displaystyle p}
   processors will be considered cost efficient.
Cost efficiency also has applications to human services.


== References ==
Advanced Computer Architectures: A Design Space Approach, D. Sima, T. Fountain and P. Kacsuk, Addison-Wesley, 1997."
938,Education informatics,18239398,1107,"Education Informatics is a sub-field of informatics. The primary focus is on computer applications, systems and networks that support research in and delivery of education. Education informatics is based upon information science, computer science and education but particularly addresses the intersection of these broad areas. Note that it is distinct from Informatics Education, a term that relates more to the practice of teaching/learning about informatics, rather than the use of information science and technology in the support of teaching and learning.
The term has been in widespread use since at latest 2004.


== References =="
939,Coalescing (computer science),16296155,1094,"In computer science, coalescing is the act of merging two adjacent free blocks of memory. When an application frees memory, gaps can fall in the memory segment that the application uses. Among other techniques, coalescing is used to reduce external fragmentation, but is not totally effective. Coalescing can be done as soon as blocks are freed, or it can be deferred until some time later (known as deferred coalescing), or it might not be done at all.
Coalescence and related techniques like heap compaction, can be used in garbage collection.


== See also ==

Timer coalescing


== References ==


== External links ==
The Memory Management Reference, Beginner's Guide Allocation"
940,Uniform information representation,1610862,1090,"Uniform information representation allows information from several realms or disciplines to be displayed and worked with as if it came from the same realm or discipline. It takes information from a number of sources, which may have used different methodologies and metrics in their data collection, and builds a single large collection of information, where some records may be more complete than others across all fields of data
Uniform information representation is particularly important in the fields of Enterprise Information Integration (EII) and Electronic Data Interchange (EDI), where different departments of a large organization may have collected information for different purposes, with different labels and units, until one department realized that data already collected by those other departments could be re-purposed for their own needs—saving the enterprise the effort and cost of re-collecting the same information."
941,International Symposium on Mixed and Augmented Reality,34581851,1089,"Augmented reality (AR) is a direct or indirect live view of a physical, real-world environment whose elements are ""augmented"" by computer-generated perceptual information, ideally across multiple sensory modalities, including visual, auditory, haptic, somatosensory, and olfactory. The overlaid sensory information can be constructive (i.e. additive to the natural environment) or destructive (i.e. masking of the natural environment) and is spatially registered with the physical world such that it is perceived as an immersive aspect of the real environment. In this way, Augmented reality alters one’s current perception of a real world environment, whereas virtual reality replaces the real world environment with a simulated one. Augmented Reality is related to two largely synonymous terms: mixed reality and computer-mediated reality.
The primary value of augmented reality is that it brings components of the digital world into a person's perception of the real world, and does so not as a simple display of data, but through the integration of immersive sensations that are perceived as natural parts of an environment. The first functional AR systems that provided immersive mixed reality experiences for users were invented in the early 1990s, starting with the Virtual Fixtures system developed at the U.S. Air Force's Armstrong Laboratory in 1992. The first commercial augmented reality experiences were used largely in the entertainment and gaming businesses, but now other industries are also getting interested about AR's possibilities for example in knowledge sharing, educating, managing the information flood and organizing distant meetings. Augmented reality is also transforming the world of education, where content may be accessed by scanning or viewing an image with a mobile device. Another example is an AR helmet for construction workers which display information about the construction sites.
Augmented reality is used to enhance the natural environments or situations and offer perceptually enriched experiences. With the help of advanced AR technologies (e.g. adding computer vision and object recognition) the information about the surrounding real world of the user becomes interactive and digitally manipulable. Information about the environment and its objects is overlaid on the real world. This information can be virtual or real, e.g. seeing other real sensed or measured information such as electromagnetic radio waves overlaid in exact alignment with where they actually are in space.Augmented reality also has a lot of potential in gathering and sharing tacit knowledge. Augmentation techniques are typically performed in real time and in semantic context with environmental elements. Immersive perceptual information is sometimes combined with supplemental information like scores over a live video feed of a sporting event. This combines the benefits of augmented reality technology and heads up display technology (HUD).


== Technology ==


=== Hardware ===
Hardware components for augmented reality are: processor, display, sensors and input devices. Modern mobile computing devices like smartphones and tablet computers contain these elements which often include a camera and MEMS sensors such as accelerometer, GPS, and solid state compass, making them suitable AR platforms.


==== Display ====
Various technologies are used in augmented reality rendering, including optical projection systems, monitors, handheld devices, and display systems worn on the human body.
A head-mounted display (HMD) is a display device worn on the forehead, such as a harness or helmet. HMDs place images of both the physical world and virtual objects over the user's field of view. Modern HMDs often employ sensors for six degrees of freedom monitoring that allow the system to align virtual information to the physical world and adjust accordingly with the user's head movements. HMDs can provide VR users with mobile and collaborative experiences. Specific providers, such as uSens and Gestigon, include gesture controls for full virtual immersion.
In January 2015, Meta launched a project led by Horizons Ventures, Tim Draper, Alexis Ohanian, BOE Optoelectronics and Garry Tan. On February 17, 2016, Meta announced their second-generation product at TED, Meta 2. The Meta 2 head-mounted display headset uses a sensory array for hand interactions and positional tracking, visual field view of 90 degrees (diagonal), and resolution display of 2560 x 1440 (20 pixels per degree), which is considered the largest field of view (FOV) currently available.


===== Eyeglasses =====

AR displays can be rendered on devices resembling eyeglasses. Versions include eyewear that employs cameras to intercept the real world view and re-display its augmented view through the eyepieces and devices in which the AR imagery is projected through or reflected off the surfaces of the eyewear's lenspieces.


====== HUD ======

A head-up display (HUD) is a transparent display that presents data without requiring users to look away from their usual viewpoints. A precursor technology to augmented reality, heads-up displays were first developed for pilots in the 1950s, projecting simple flight data into their line of sight, thereby enabling them to keep their ""heads up"" and not look down at the instruments. Near-eye augmented reality devices can be used as portable head-up displays as they can show data, information, and images while the user views the real world. Many definitions of augmented reality only define it as overlaying the information. This is basically what a head-up display does; however, practically speaking, augmented reality is expected to include registration and tracking between the superimposed perceptions, sensations, information, data, and images and some portion of the real world.
CrowdOptic, an existing app for smartphones, applies algorithms and triangulation techniques to photo metadata including GPS position, compass heading, and a time stamp to arrive at a relative significance value for photo objects. CrowdOptic technology can be used by Google Glass users to learn where to look at a given point in time.
A number of smartglasses have been launched for augmented reality. Due to encumbered control, smartglasses are primarily designed for micro-interaction like reading a text message but still far from more well-rounded applications of augmented reality. In January 2015, Microsoft introduced HoloLens, an independent smartglasses unit. Brian Blau, Research Director of Consumer Technology and Markets at Gartner, said that ""Out of all the head-mounted displays that I've tried in the past couple of decades, the HoloLens was the best in its class."" First impressions and opinions were generally that HoloLens is a superior device to the Google Glass, and manages to do several things ""right"" in which Glass failed.


===== Contact lenses =====
Contact lenses that display AR imaging are in development. These bionic contact lenses might contain the elements for display embedded into the lens including integrated circuitry, LEDs and an antenna for wireless communication. The first contact lens display was reported in 1999, then 11 years later in 2010-2011. Another version of contact lenses, in development for the U.S. military, is designed to function with AR spectacles, allowing soldiers to focus on close-to-the-eye AR images on the spectacles and distant real world objects at the same time.
The futuristic short film Sight features contact lens-like augmented reality devices.
Many scientists have been working on contact lenses capable of many different technological feats. The company Samsung has been working on a contact lens as well. This lens, when finished, is meant to have a built-in camera on the lens itself. The design is intended to have you blink to control its interface for recording purposed. It is also intended to be linked with your smartphone to review footage, and control it separately. When successful, the lens would feature a camera, or sensor inside of it. It is said that it could be anything from a light sensor, to a temperature sensor.
In Augmented Reality, the distinction is made between two distinct modes of tracking, known as ''marker'' and ''markerless''. Marker are visual cues which trigger the display of the virtual information. A piece of paper with some distinct geometries can be used. The camera recognizes the geometries by identifying specific pointsin the drawing. Markerless also called instant tracking does not use marker. Instead the user positions the object in the camera view preferably in an horizontal plane.It uses sensors in mobile devices to accurately detect the real-world environment, such as the locations of walls and points of intersection.


===== Virtual retinal display =====
A virtual retinal display (VRD) is a personal display device under development at the University of Washington's Human Interface Technology Laboratory under Dr. Thomas A. Furness III. With this technology, a display is scanned directly onto the retina of a viewer's eye. This results in bright images with high revolution and high contrast. The viewer sees what appears to be a conventional display floating in space.
Several of tests were done in order to analyze the safety of the VRD. In one test, patients with partial loss of vision were selected to view images using the technology having either macular degeneration (a disease that degenerates the retina) or keratoconus. In the macular degeneration group, 5 out of 8 subjects preferred the VRD images to the CRT or paper images and thought they were better and brighter and were able to see equal or better resolution levels. The Kerocunus patients could all resolve smaller lines in several line tests using the VDR as opposed to their own correction. They also found the VDR images to be easier to view and sharper. As a result of these several tests, virtual retinal display is considered safe technology.
Virtual retinal display creates images that can be seen in ambient daylight and ambient roomlight. The VRD is considered a preferred candidate to use in a surgical display due to its combination of high resolution and high contrast and brightness. Additional tests show high potential for VRD to be used as a display technology for patients that have low vision.


===== EyeTap =====
The EyeTap (also known as Generation-2 Glass) captures rays of light that would otherwise pass through the center of the lens of the eye of the wearer, and substitutes synthetic computer-controlled light for each ray of real light.
The Generation-4 Glass (Laser EyeTap) is similar to the VRD (i.e. it uses a computer-controlled laser light source) except that it also has infinite depth of focus and causes the eye itself to, in effect, function as both a camera and a display by way of exact alignment with the eye and resynthesis (in laser light) of rays of light entering the eye.


===== Handheld =====
A Handheld display employs a small display that fits in a user's hand. All handheld AR solutions to date opt for video see-through. Initially handheld AR employed fiducial markers, and later GPS units and MEMS sensors such as digital compasses and six degrees of freedom accelerometer–gyroscope. Today SLAM markerless trackers such as PTAM are starting to come into use. Handheld display AR promises to be the first commercial success for AR technologies. The two main advantages of handheld AR are the portable nature of handheld devices and the ubiquitous nature of camera phones. The disadvantages are the physical constraints of the user having to hold the handheld device out in front of them at all times, as well as the distorting effect of classically wide-angled mobile phone cameras when compared to the real world as viewed through the eye. The issues arising from the user having to hold the handheld device (manipulability) and perceiving the visualisation correctly (comprehensibility) have been summarised into the HARUS usability questionnaire.
Games such as Pokémon Go and Ingress utilize an Image Linked Map (ILM) interface, where approved geotagged locations appear on a stylized map for the user to interact with.


===== Spatial =====
Spatial augmented reality (SAR) augments real-world objects and scenes without the use of special displays such as monitors, head-mounted displays or hand-held devices. SAR makes use of digital projectors to display graphical information onto physical objects. The key difference in SAR is that the display is separated from the users of the system. Because the displays are not associated with each user, SAR scales naturally up to groups of users, thus allowing for collocated collaboration between users.
Examples include shader lamps, mobile projectors, virtual tables, and smart projectors. Shader lamps mimic and augment reality by projecting imagery onto neutral objects, providing the opportunity to enhance the object's appearance with materials of a simple unit - a projector, camera, and sensor.
Other applications include table and wall projections. One innovation, the Extended Virtual Table, separates the virtual from the real by including beam-splitter mirrors attached to the ceiling at an adjustable angle. Virtual showcases, which employ beam-splitter mirrors together with multiple graphics displays, provide an interactive means of simultaneously engaging with the virtual and the real. Many more implementations and configurations make spatial augmented reality display an increasingly attractive interactive alternative.
An SAR system can display on any number of surfaces of an indoor setting at once. SAR supports both a graphical visualization and passive haptic sensation for the end users. Users are able to touch physical objects in a process that provides passive haptic sensation.


==== Tracking ====
Modern mobile augmented-reality systems use one or more of the following tracking technologies: digital cameras and/or other optical sensors, accelerometers, GPS, gyroscopes, solid state compasses, RFID. These technologies offer varying levels of accuracy and precision. The most important is the position and orientation of the user's head. Tracking the user's hand(s) or a handheld input device can provide a 6DOF interaction technique.


==== Networking ====
Mobile augmented reality applications are gaining popularity due to the wide adoption of mobile and especially wearable devices. However, they often rely on computationally intensive computer vision algorithms with extreme latency requirements. To compensate for the lack of computing power, offloading data processing to a distant machine is often desired. Computation offloading introduces new constraints in applications, especially in terms of latency and bandwidth. Although there are a plethora of real-time multimedia transport protocols, there is a need for support from network infrastructure as well.


==== Input devices ====
Techniques include speech recognition systems that translate a user's spoken words into computer instructions, and gesture recognition systems that interpret a user's body movements by visual detection or from sensors embedded in a peripheral device such as a wand, stylus, pointer, glove or other body wear. Products which are trying to serve as a controller of AR headsets include Wave by Seebright Inc. and Nimble by Intugine Technologies.


==== Computer ====
The computer analyzes the sensed visual and other data to synthesize and position augmentations. Computers are responsible for the graphics that go with augmented reality. Augmented reality uses a computer-generated image and it has an striking effect on the way the real world is shown. With the improvement of technology and computers, augmented reality is going to have a drastic change on our perspective of the real world. According to Time Magazine, in about 15–20 years it is predicted that Augmented reality and virtual reality are going to become the primary use for computer interactions. Computers are improving at a very fast rate, which means that we are figuring out new ways to improve other technology. The more that computers progress, augmented reality will become more flexible and more common in our society. Computers are the core of augmented reality.
 The Computer receives data from the sensors which determine the relative position of objects surface. This translates to an input to the computer which then outputs to the users by adding something that would otherwise not be there. The computer comprises memory and a processor. The computer takes the scanned environment then generates images or a video and puts it on the receiver for the observer to see. The fixed marks on an objects surface are stored in the memory of a computer. The computer also withdrawals from its memory to present images realistically to the onlooker. The best example of this is of the Pepsi Max AR Bus Shelter.


=== Software and algorithms ===
A key measure of AR systems is how realistically they integrate augmentations with the real world. The software must derive real world coordinates, independent from the camera, from camera images. That process is called image registration, and uses different methods of computer vision, mostly related to video tracking. Many computer vision methods of augmented reality are inherited from visual odometry.
Usually those methods consist of two parts. The first stage is to detect interest points, fiducial markers or optical flow in the camera images. This step can use feature detection methods like corner detection, blob detection, edge detection or thresholding, and other image processing methods. The second stage restores a real world coordinate system from the data obtained in the first stage. Some methods assume objects with known geometry (or fiducial markers) are present in the scene. In some of those cases the scene 3D structure should be precalculated beforehand. If part of the scene is unknown simultaneous localization and mapping (SLAM) can map relative positions. If no information about scene geometry is available, structure from motion methods like bundle adjustment are used. Mathematical methods used in the second stage include projective (epipolar) geometry, geometric algebra, rotation representation with exponential map, kalman and particle filters, nonlinear optimization, robust statistics.
Augmented Reality Markup Language (ARML) is a data standard developed within the Open Geospatial Consortium (OGC), which consists of XML grammar to describe the location and appearance of virtual objects in the scene, as well as ECMAScript bindings to allow dynamic access to properties of virtual objects.
To enable rapid development of augmented reality applications, some software development kits (SDKs) have emerged. A few SDKs such as CloudRidAR leverage cloud computing for performance improvement. AR SDKs are offered by Vuforia, ARToolKit, Catchoom CraftAR Mobinett AR, Wikitude, Blippar Layar, Meta. and ARLab.


== Possible applications ==
Augmented reality has been explored for many applications. Since the 1970s and early 1980s, Steve Mann has developed technologies meant for everyday use i.e. ""horizontal"" across all applications rather than a specific ""vertical"" market. Examples include Mann's ""EyeTap Digital Eye Glass"", a general-purpose seeing aid that does dynamic-range management (HDR vision) and overlays, underlays, simultaneous augmentation and diminishment (e.g. diminishing the electric arc while looking at a welding torch).


=== Literature ===

The first description of AR as it is known today was in Virtual Light, the 1994 novel by William Gibson. In 2011, AR was blended with poetry by ni ka from Sekai Camera in Tokyo, Japan. The prose of these AR poems come from Paul Celan, ""Die Niemandsrose"", expressing the aftermath of the 2011 Tōhoku earthquake and tsunami.


=== Archaeology ===
AR has been used to aid archaeological research. By augmenting archaeological features onto the modern landscape, AR allows archaeologists to formulate possible site configurations from extant structures. Computer generated models of ruins, buildings, landscapes or even ancient people have been recycled into early archaeological AR applications. For example, implementing a system like, ""VITA (Visual Interaction Tool for Archaeology)"" will allow users to imagine and investigate instant excavation results without leaving their home. Each user can collaborate by mutually ""navigating, searching, and viewing data."" Hrvjone Benko, a researcher for the computer science department at Colombia University, points out that these particular systems and others like it can provide ""3D panoramic images and 3D models of the site itself at different excavation stages."" All a while, it organizes much of the data in a collaborative way that is easy to use. Collaborative AR systems supply multimodal interactions that combine the real world with virtual images of both environments.


=== Architecture ===
AR can aid in visualizing building projects. Computer-generated images of a structure can be superimposed into a real life local view of a property before the physical building is constructed there; this was demonstrated publicly by Trimble Navigation in 2004. AR can also be employed within an architect's workspace, rendering animated 3D visualizations of their 2D drawings. Architecture sight-seeing can be enhanced with AR applications, allowing users viewing a building's exterior to virtually see through its walls, viewing its interior objects and layout.
With the continual improvements to GPS accuracy, businesses are able to use augmented reality to visualize georeferenced models of construction sites, underground structures, cables and pipes using mobile devices. Augmented reality is applied to present new projects, to solve on-site construction challenges, and to enhance promotional materials. Examples include the Daqri Smart Helmet, an Android-powered hard hat used to create augmented reality for the industrial worker, including visual instructions, real-time alerts, and 3D mapping.
Following the Christchurch earthquake, the University of Canterbury released CityViewAR, which enabled city planners and engineers to visualize buildings that had been destroyed. Not only did this provide planners with tools to reference the previous cityscape, but it also served as a reminder to the magnitude of the devastation caused, as entire buildings had been demolished.


=== Visual art ===
AR applied in the visual arts allows objects or places to trigger artistic multidimensional experiences and interpretations of reality.
AR technology aided the development of eye tracking technology to translate a disabled person's eye movements into drawings on a screen.


=== Commerce ===

AR is used to integrate print and video marketing. Printed marketing material can be designed with certain ""trigger"" images that, when scanned by an AR-enabled device using image recognition, activate a video version of the promotional material. A major difference between augmented reality and straightforward image recognition is that one can overlay multiple media at the same time in the view screen, such as social media share buttons, the in-page video even audio and 3D objects. Traditional print-only publications are using augmented reality to connect many different types of media.
AR can enhance product previews such as allowing a customer to view what's inside a product's packaging without opening it. AR can also be used as an aid in selecting products from a catalog or through a kiosk. Scanned images of products can activate views of additional content such as customization options and additional images of the product in its use.
By 2010, virtual dressing rooms had been developed for e-commerce.

In 2012, a mint used AR techniques to market a commemorative coin for Aruba. The coin itself was used as an AR trigger, and when held in front of an AR-enabled device it revealed additional objects and layers of information that were not visible without the device.
In 2013, L'Oreal Paris used CrowdOptic technology to create an augmented reality experience at the seventh annual Luminato Festival in Toronto, Canada.
In 2014, L'Oreal brought the AR experience to a personal level with their ""Makeup Genius"" app. It allowed users to try out make-up and beauty styles via a mobile device.
In 2015, the Bulgarian startup iGreet developed its own AR technology and used it to make the first premade ""live"" greeting card. A traditional paper card was augmented with digital content which was revealed by using the iGreet app.
In 2015, the Luxembourg startup itondo.com launched an AR app for the art market that lets art buyers accurately visualize 2D artworks to scale on their own walls before they buy.


=== Education ===
In educational settings, AR has been used to complement a standard curriculum. Text, graphics, video, and audio may be superimposed into a student's real-time environment. Textbooks, flashcards and other educational reading material may contain embedded ""markers"" or triggers that, when scanned by an AR device, produced supplementary information to the student rendered in a multimedia format. This makes AR a good alternative method for presenting information and Multimedia Learning Theory can be applied.
As AR evolved, students can participate interactively and interact with knowledge more authentically. Instead of remaining passive recipients, students can become active learners, able to interact with their learning environment. Computer-generated simulations of historical events allow students to explore and learning details of each significant area of the event site.
In higher education, Construct3D, a Studierstube system, allows students to learn mechanical engineering concepts, math or geometry. Chemistry AR apps allow students to visualize and interact with the spatial structure of a molecule using a marker object held in the hand. Anatomy students can visualize different systems of the human body in three dimensions.
Augmented reality technology enhances remote collaboration, allowing students and instructors in different locales to interact by sharing a common virtual learning environment populated by virtual objects and learning materials.
Primary school children learn easily from interactive experiences. Astronomical constellations and the movements of objects in the solar system were oriented in 3D and overlaid in the direction the device was held, and expanded with supplemental video information. Paper-based science book illustrations could seem to come alive as video without requiring the child to navigate to web-based materials.
While some educational apps were available for AR in 2016, it was not broadly used. Apps that leverage augmented reality to aid learning included SkyView for studying astronomy, AR Circuits for building simple electric circuits, and SketchAr for drawing.
AR would also be a way for parents and teachers to achieve their goals for modern education, which might include providing a more individualized and flexible learning, making closer connections between what is taught at school and the real world, and helping students to become more engaged in their own learning.
A recent research compared the functionalities of augmented reality tools with potential for education 


=== Emergency management/search and rescue ===
Augmented reality systems are used in public safety situations, from super storms to suspects at large.
As early as 2009, two articles from Emergency Management magazine discussed the power of this technology for emergency management. The first was ""Augmented Reality--Emerging Technology for Emergency Management"" by Gerald Baron. Per Adam Crowe: ""Technologies like augmented reality (ex: Google Glass) and the growing expectation of the public will continue to force professional emergency managers to radically shift when, where, and how technology is deployed before, during, and after disasters.""
Another early example was a search aircraft looking for a lost hiker in rugged mountain terrain. Augmented reality systems provided aerial camera operators with a geographic awareness of forest road names and locations blended with the camera video. The camera operator was better able to search for the hiker knowing the geographic context of the camera image. Once located, the operator could more efficiently direct rescuers to the hiker's location because the geographic position and reference landmarks were clearly labeled.


=== Video games ===

The gaming industry embraced AR technology. A number of games were developed for prepared indoor environments, such as AR air hockey, Titans of Space, collaborative combat against virtual enemies, and AR-enhanced pool table games.
Augmented reality allowed video game players to experience digital game play in a real world environment. Companies and platforms like Niantic and Proxy42 emerged as major augmented reality gaming creators. Niantic is notable for releasing the record-breaking game Pokémon Go. Disney has partnered with Lenovo to create the augmented reality game Star Wars: Jedi Challenges that works with a Lenovo Mirage AR headset, a tracking sensor and a Lightsaber controller, scheduled to launch in December 2017.


=== Industrial design ===

AR allows industrial designers to experience a product's design and operation before completion. Volkswagen has used AR for comparing calculated and actual crash test imagery. AR has been used to visualize and modify car body structure and engine layout. It has also been used to compare digital mock-ups with physical mock-ups for finding discrepancies between them.


=== Medical ===
Since 2005, a device called a near-infrared vein finder that films subcutaneous veins, processes and projects the image of the veins onto the skin has been used to locate veins.
AR provides surgeons with patient monitoring data in the style of a fighter pilot's heads-up display, and allows patient imaging records, including functional videos, to be accessed and overlaid. Examples include a virtual X-ray view based on prior tomography or on real-time images from ultrasound and confocal microscopy probes, visualizing the position of a tumor in the video of an endoscope, or radiation exposure risks from X-ray imaging devices. AR can enhance viewing a fetus inside a mother's womb. Siemens, Karl Storz and IRCAD have developed a system for laparoscopic liver surgery that uses AR to view sub-surface tumors and vessels. AR has been used for cockroach phobia treatment. Patients wearing augmented reality glasses can be reminded to take medications. Virtual reality has been seen promising in the medical field since the 90's. Augmented reality can be very helpful in the medical field. It could be used to provide crucial information to a doctor or surgeon with having them take their eyes off the patient. On the 30th of April, 2015 Microsoft announced the Microsoft HoloLens, their first shot at augmented reality. The HoloLens has advanced through the years and it has gotten so advanced that it has been used to project holograms for near infrared fluorescence based image guided surgery. As augment reality advances, the more it is implemented into medical use. Augmented reality and other computer based-utility is being used today to help train medical professionals. With the creation of Google Glass and Microsoft HoloLens, has helped pushed Augmented Reality into medical education.


=== Spatial immersion and interaction ===
Augmented reality applications, running on handheld devices utilized as virtual reality headsets, can also digitalize human presence in space and provide a computer generated model of them, in a virtual space where they can interact and perform various actions. Such capabilities are demonstrated by ""Project Anywhere"", developed by a postgraduate student at ETH Zurich, which was dubbed as an ""out-of-body experience"".


=== Flight training ===
Building on decades of perceptual-motor research in experimental psychology, researchers at the Aviation Research Laboratory of the University of Illinois at Urbana-Champaign used augmented reality in the form of a flight path in the sky to teach flight students how to land a flight simulator. An adaptive augmented schedule in which students were shown the augmentation only when they departed from the flight path proved to be a more effective training intervention than a constant schedule. Flight students taught to land in the simulator with the adaptive augmentation learned to land a light aircraft more quickly than students with the same amount of landing training in the simulator but with constant augmentation or without any augmentation.


=== Military ===
An interesting early application of AR occurred when Rockwell International created video map overlays of satellite and orbital debris tracks to aid in space observations at Air Force Maui Optical System. In their 1993 paper ""Debris Correlation Using the Rockwell WorldView System"" the authors describe the use of map overlays applied to video from space surveillance telescopes. The map overlays indicated the trajectories of various objects in geographic coordinates. This allowed telescope operators to identify satellites, and also to identify and catalog potentially dangerous space debris.
Starting in 2003 the US Army integrated the SmartCam3D augmented reality system into the Shadow Unmanned Aerial System to aid sensor operators using telescopic cameras to locate people or points of interest. The system combined both fixed geographic information including street names, points of interest, airports, and railroads with live video from the camera system. The system offered a ""picture in picture"" mode that allows the system to show a synthetic view of the area surrounding the camera's field of view. This helps solve a problem in which the field of view is so narrow that it excludes important context, as if ""looking through a soda straw"". The system displays real-time friend/foe/neutral location markers blended with live video, providing the operator with improved situational awareness.
Researchers at USAF Research Lab (Calhoun, Draper et al.) found an approximately two-fold increase in the speed at which UAV sensor operators found points of interest using this technology. This ability to maintain geographic awareness quantitatively enhances mission efficiency. The system is in use on the US Army RQ-7 Shadow and the MQ-1C Gray Eagle Unmanned Aerial Systems.
In combat, AR can serve as a networked communication system that renders useful battlefield data onto a soldier's goggles in real time. From the soldier's viewpoint, people and various objects can be marked with special indicators to warn of potential dangers. Virtual maps and 360° view camera imaging can also be rendered to aid a soldier's navigation and battlefield perspective, and this can be transmitted to military leaders at a remote command center.


=== Navigation ===

The NASA X-38 was flown using a Hybrid Synthetic Vision system that overlaid map data on video to provide enhanced navigation for the spacecraft during flight tests from 1998 to 2002. It used the LandForm software and was useful for times of limited visibility, including an instance when the video camera window frosted over leaving astronauts to rely on the map overlays. The LandForm software was also test flown at the Army Yuma Proving Ground in 1999. In the photo at right one can see the map markers indicating runways, air traffic control tower, taxiways, and hangars overlaid on the video.
AR can augment the effectiveness of navigation devices. Information can be displayed on an automobile's windshield indicating destination directions and meter, weather, terrain, road conditions and traffic information as well as alerts to potential hazards in their path. Aboard maritime vessels, AR can allow bridge watch-standers to continuously monitor important information such as a ship's heading and speed while moving throughout the bridge or performing other tasks.


=== Workplace ===
Augmented reality may have a good impact on work collaboration as people may be inclined to interact more actively with their learning environment. It may also encourage tacit knowledge renewal which makes firms more competitive. AR was used to facilitate collaboration among distributed team members via conferences with local and virtual participants. AR tasks included brainstorming and discussion meetings utilizing common visualization via touch screen tables, interactive digital whiteboards, shared design spaces, and distributed control rooms.
Complex tasks such as assembly, maintenance, and surgery were simplified by inserting additional information into the field of view. For example, labels were displayed on parts of a system to clarify operating instructions for a mechanic performing maintenance on a system. Assembly lines benefited from the usage of AR. In addition to Boeing, BMW and Volkswagen were known for incorporating this technology into assembly lines for monitoring process improvements. Big machines are difficult to maintain because of their multiple layers or structures. AR permits people to look through the machine as if with an x-ray, pointing them to the problem right away.
The new wave of professionals, the Millennial workforce, demands more efficient knowledge sharing solutions and easier access to rapidly growing knowledge bases. Augmented reality offers a solution to that.


=== Broadcast and live events ===
Weather visualizations were the first application of augmented reality to television. It has now become common in weathercasting to display full motion video of images captured in real-time from multiple cameras and other imaging devices. Coupled with 3D graphics symbols and mapped to a common virtual geospace model, these animated visualizations constitute the first true application of AR to TV.
AR has become common in sports telecasting. Sports and entertainment venues are provided with see-through and overlay augmentation through tracked camera feeds for enhanced viewing by the audience. Examples include the yellow ""first down"" line seen in television broadcasts of American football games showing the line the offensive team must cross to receive a first down. AR is also used in association with football and other sporting events to show commercial advertisements overlaid onto the view of the playing area. Sections of rugby fields and cricket pitches also display sponsored images. Swimming telecasts often add a line across the lanes to indicate the position of the current record holder as a race proceeds to allow viewers to compare the current race to the best performance. Other examples include hockey puck tracking and annotations of racing car performance and snooker ball trajectories.
Augmented reality for Next Generation TV allows viewers to interact with the programs they were watching. They can place objects into an existing program and interact with them, such as moving them around. Objects include avatars of real persons in real time who are also watching the same program.
AR has been used to enhance concert and theater performances. For example, artists allow listeners to augment their listening experience by adding their performance to that of other bands/groups of users.


=== Tourism and sightseeing ===
Travelers may use AR to access real-time informational displays regarding a location, its features, and comments or content provided by previous visitors. Advanced AR applications include simulations of historical events, places, and objects rendered into the landscape.
AR applications linked to geographic locations present location information by audio, announcing features of interest at a particular site as they become visible to the user.
Companies can use AR to attract tourists to particular areas that they may not be familiar with by name. Tourists will be able to experience beautiful landscapes in first person with the use of AR devices. Companies like Phocuswright plan to use such technology in order to expose the lesser known but beautiful areas of the planet, and in turn, increase tourism. Other companies such as Matoke Tours have already developed an application where the user can see 360 degress from several different places in Uganda. Matoke Tours and Phocuswright have the ability to display their apps on virtual reality headsets like the Samsung VR and Oculus Rift.


=== Translation ===
AR systems such as Word Lens can interpret the foreign text on signs and menus and, in a user's augmented view, re-display the text in the user's language. Spoken words of a foreign language can be translated and displayed in a user's view as printed subtitles.


=== Music ===
It has been suggested that augmented reality may be used in new methods of music production, mixing, control and visualization.
A tool for 3D music creation in clubs that, in addition to regular sound mixing features, allows the DJ to play dozens of sound samples, placed anywhere in 3D space, has been conceptualized.
Leeds College of Music teams have developed an AR app that can be used with Audient desks and allow students to use their smartphone or tablet to put layers of information or interactivity on top of an Audient mixing desk.
ARmony is a software package that makes use of augmented reality to help people to learn an instrument.
In a proof-of-concept project Ian Sterling, interaction design student at California College of the Arts, and software engineer Swaroop Pal demonstrated a HoloLens app whose primary purpose is to provide a 3D spatial UI for cross-platform devices — the Android Music Player app and Arduino-controlled Fan and Light — and also allow interaction using gaze and gesture control.
AR Mixer is an app that allows one to select and mix between songs by manipulating objects – such as changing the orientation of a bottle or can.
In a video Uriel Yehezkel, demonstrates using the Leap Motion controller and GECO MIDI to control Ableton Live with hand gestures and states that by this method he was able to control more than 10 parameters simultaneously with both hands and take full control over the construction of the song, emotion and energy.
A novel musical instrument that allows novices to play electronic musical compositions, interactively remixing and modulating their elements, by manipulating simple physical objects has been proposed.
A system using explicit gestures and implicit dance moves to control the visual augmentations of a live music performance that enable more dynamic and spontaneous performances and—in combination with indirect augmented reality—leading to a more intense interaction between artist and audience has been suggested.
Research by members of the CRIStAL at the University of Lille makes use of augmented reality in order to enrich musical performance. The ControllAR project allows musicians to augment their MIDI control surfaces with the remixed graphical user interfaces of music software. The Rouages project proposes to augment digital musical instruments in order to reveal their mechanisms to the audience and thus improve the perceived liveness. Reflets is a novel augmented reality display dedicated to musical performances where the audience acts as a 3D display by revealing virtual content on stage, which can also be used for 3D musical interaction and collaboration.


=== Retail ===
Augmented reality is becoming more frequently used for online advertising. Retailers offer the ability to upload a picture on their website and ""try on"" various clothes which is overlaid on the picture. Even further, companies such as Bodymetrics install dressing booths in department stores that offer full-body scanning. These booths render a 3-D model of the user, allowing the consumers to view different outfits on themselves without the need of physically changing clothes.


=== Snapchat ===
Snapchat users have access to augmented reality in the company's instant messaging app through use of camera filters. In September 2017, Snapchat updated its app to include a camera filter that allowed users to render an animated, cartoon version of themselves called ""Bitmoji"". These animated avatars would be projected in the real world through the camera, and can be photographed or video recorded. In the same month, Snapchat also announced a new feature called ""Sky Filters"" that will be available on its app. This new feature makes use of augmented reality to alter the look of a picture taken of the sky, much like how users can apply the app's filters to other pictures. Users can choose from sky filters such as starry night, stormy clouds, beautiful sunsets, and rainbow.


== Privacy concerns ==
The concept of modern augmented reality depends on the ability of the device to record and analyze the environment in real time. Because of this, there are potential legal concerns over privacy. While the First Amendment to the United States Constitution allows for such recording in the name of public interest, the constant recording of an AR device makes it difficult to do so without also recording outside of the public domain. Legal complications would be found in areas where a right to a certain amount of privacy is expected or where copyrighted media are displayed.
In terms of individual privacy, there exists the ease of access to information that one should not readily possess about a given person. This is accomplished through facial recognition technology. Assuming that AR automatically passes information about persons that the user sees, there could be anything seen from social media, criminal record, and marital status.
Privacy-compliant image capture solutions can be deployed to temper the impact of constant filming on individual privacy.


== Notable researchers ==
Ivan Sutherland invented the first VR head-mounted display at Harvard University.
Steve Mann formulated an earlier concept of mediated reality in the 1970s and 1980s, using cameras, processors, and display systems to modify visual reality to help people see better (dynamic range management), building computerized welding helmets, as well as ""augmediated reality"" vision systems for use in everyday life. He is also an adviser to Meta.
Louis Rosenberg developed one of the first known AR systems, called Virtual Fixtures, while working at the U.S. Air Force Armstrong Labs in 1991, and published the first study of how an AR system can enhance human performance. Rosenberg's subsequent work at Stanford University in the early 90's, was the first proof that virtual overlays when registered and presented over a user's direct view of the real physical world, could significantly enhance human performance.
Mike Abernathy pioneered one of the first successful augmented reality applications of video overlay using map data for space debris in 1993, while at Rockwell International. He co-founded Rapid Imaging Software, Inc. and was the primary author of the LandForm system in 1995, and the SmartCam3D system. LandForm augmented reality was successfully flight tested in 1999 aboard a helicopter and SmartCam3D was used to fly the NASA X-38 from 1999–2002. He and NASA colleague Francisco Delgado received the National Defense Industries Association Top5 awards in 2004.
Steven Feiner, Professor at Columbia University, is the author of a 1993 paper on an AR system prototype, KARMA (the Knowledge-based Augmented Reality Maintenance Assistant), along with Blair MacIntyre and Doree Seligmann. He is also an advisor to Meta.
Tracy McSheery, of Phasespace, developer of wide field of view AR lenses as used in Meta 2 and others.
S. Ravela, B. Draper, J. Lim and A. Hanson develop marker/fixture-less augmented reality system with computer vision in 1994. They augmented an engine block observed from a single video camera with annotations for repair. They use model-based pose estimation, aspect graphs and visual feature tracking to dynamically register model with the observed video.
Francisco ""Frank"" Delgado is a NASA engineer and project manager specializing in human interface research and development. Starting 1998 he conducted research into displays that combined video with synthetic vision systems (called hybrid synthetic vision at the time) that we recognize today as augmented reality systems for the control of aircraft and spacecraft. In 1999 he and colleague Mike Abernathy flight-tested the LandForm system aboard a US Army helicopter. Delgado oversaw integration of the LandForm and SmartCam3D systems into the X-38 Crew Return Vehicle. In 2001, Aviation Week reported NASA astronaut's successful use of hybrid synthetic vision (augmented reality) to fly the X-38 during a flight test at Dryden Flight Research Center. The technology was used in all subsequent flights of the X-38. Delgado was co-recipient of the National Defense Industries Association 2004 Top 5 software of the year award for SmartCam3D.
Bruce H. Thomas and Wayne Piekarski develop the Tinmith system in 1998. They along with Steve Feiner with his MARS system pioneer outdoor augmented reality.
Mark Billinghurst is one of the world's leading augmented reality researchers. Director of the HIT Lab New Zealand (HIT Lab NZ) at the University of Canterbury in New Zealand, he has produced over 250 technical publications and presented demonstrations and courses at a wide variety of conferences.
Reinhold Behringer performed important early work in image registration for augmented reality, and prototype wearable testbeds for augmented reality. He also co-organized the First IEEE International Symposium on Augmented Reality in 1998 (IWAR'98), and co-edited one of the first books on augmented reality.
Dieter Schmalstieg and Daniel Wagner developed a marker tracking systems for mobile phones and PDAs in 2009.


== History ==
1901: L. Frank Baum, an author, first mentions the idea of an electronic display/spectacles that overlays data onto real life (in this case 'people'), it is named a 'character marker'.
1957–62: Morton Heilig, a cinematographer, creates and patents a simulator called Sensorama with visuals, sound, vibration, and smell.
1968: Ivan Sutherland invents the head-mounted display and positions it as a window into a virtual world.
1975: Myron Krueger creates Videoplace to allow users to interact with virtual objects.
1980: The research by Gavan Lintern of the University of Illinois is the first published work to show the value of a heads up display for teaching real-world flight skills.
1980: Steve Mann creates the first wearable computer, a computer vision system with text and graphical overlays on a photographically mediated scene. See EyeTap. See Heads Up Display.
1981: Dan Reitan geospatially maps multiple weather radar images and space-based and studio cameras to earth maps and abstract symbols for television weather broadcasts, bringing a precursor concept to augmented reality (mixed real/graphical images) to TV.
1984: In the film The Terminator, the Terminator uses a heads-up display in several parts of the film. In one part, he accesses a diagram of the gear system of the truck he gets into towards the end of the film.
1987: Douglas George and Robert Morris create a working prototype of an astronomical telescope-based ""heads-up display"" system (a precursor concept to augmented reality) which superimposed in the telescope eyepiece, over the actual sky images, multi-intensity star, and celestial body images, and other relevant information.
1989: Jaron Lanier creates VPL Research, an early commercial business around virtual worlds.
1990: The term 'Augmented Reality' is attributed to Thomas P. Caudell, a former Boeing researcher.
1992: Louis Rosenberg develops one of the first functioning AR systems, called Virtual Fixtures, at the U.S. Air Force Research Laboratory—Armstrong, and demonstrates benefits to human performance.
1993: Steven Feiner, Blair MacIntyre and Doree Seligmann present an early paper on an AR system prototype, KARMA, at the Graphics Interface conference.
1993: Mike Abernathy, et al., report the first use of augmented reality in identifying space debris using Rockwell WorldView by overlaying satellite geographic trajectories on live telescope video.
1993 A widely cited version of the paper above is published in Communications of the ACM – Special issue on computer augmented environments, edited by Pierre Wellner, Wendy Mackay, and Rich Gold.
1993: Loral WDL, with sponsorship from STRICOM, performed the first demonstration combining live AR-equipped vehicles and manned simulators. Unpublished paper, J. Barrilleaux, ""Experiences and Observations in Applying Augmented Reality to Live Training"", 1999.
1994: Julie Martin creates first 'Augmented Reality Theater production', Dancing In Cyberspace, funded by the Australia Council for the Arts, features dancers and acrobats manipulating body–sized virtual object in real time, projected into the same physical space and performance plane. The acrobats appeared immersed within the virtual object and environments. The installation used Silicon Graphics computers and Polhemus sensing system.
1995: S. Ravela et al. at University of Massachusetts introduce a vision-based system using monocular cameras to track objects (engine blocks) across views for augmented reality.
1998: Spatial Augmented Reality introduced at University of North Carolina at Chapel Hill by Ramesh Raskar, Welch, Henry Fuchs.
1999: Frank Delgado, Mike Abernathy et al. report successful flight test of LandForm software video map overlay from a helicopter at Army Yuma Proving Ground overlaying video with runways, taxiways, roads and road names.
1999: The US Naval Research Laboratory engages on a decade-long research program called the Battlefield Augmented Reality System (BARS) to prototype some of the early wearable systems for dismounted soldier operating in urban environment for situation awareness and training.
1999: Hirokazu Kato (加藤 博一) created ARToolKit at HITLab, where AR later was further developed by other HITLab scientists, demonstrating it at SIGGRAPH.
2000: Bruce H. Thomas develops ARQuake, the first outdoor mobile AR game, demonstrating it in the International Symposium on Wearable Computers.
2001: NASA X-38 flown using LandForm software video map overlays at Dryden Flight Research Center.
2004: Outdoor helmet-mounted AR system demonstrated by Trimble Navigation and the Human Interface Technology Laboratory.
2008: Wikitude AR Travel Guide launches on 20 Oct 2008 with the G1 Android phone.
2009: ARToolkit was ported to Adobe Flash (FLARToolkit) by Saqoosha, bringing augmented reality to the web browser.
2012: Launch of Lyteshot, an interactive AR gaming platform that utilizes smart glasses for game data
2013: Meta announces the Meta 1 developer kit, the first to market AR see-through display
2013: Google announces an open beta test of its Google Glass augmented reality glasses. The glasses reach the Internet through Bluetooth, which connects to the wireless service on a user’s cellphone. The glasses respond when a user speaks, touches the frame or moves the head.
2014: Mahei creates the first generation of augmented reality enhanced educational toys.
2015: Microsoft announces Windows Holographic and the HoloLens augmented reality headset. The headset utilizes various sensors and a processing unit to blend high definition ""holograms"" with the real world.
2016: Niantic released Pokémon Go for iOS and Android in July 2016. The game quickly became one of the most popular smartphone applications and in turn spikes the popularity of augmented reality games.
2017: Magic Leap announces the use of Digital Lightfield technology embedded into the Magic Leap One headset. The creators edition headset includes the glasses and a computing pack worn on your belt.


== See also ==


== References ==


== External links =="
942,Partial word,17374931,1088,"A partial word is a string that may contain a number of ""do not know"" or ""do not care"" symbols i.e. placeholders in the string where the symbol value is not known or not specified. More formally, a partial word is a partial function 
  
    
      
        u
        :
        {
        0
        ,
        …
        ,
        n
        −
        1
        }
        →
        A
      
    
    {\displaystyle u:\{0,\ldots ,n-1\}\rightarrow A}
   where 
  
    
      
        A
      
    
    {\displaystyle A}
   is some finite alphabet. If u(k) is not defined for some 
  
    
      
        k
        ∈
        {
        0
        ,
        …
        ,
        n
        −
        1
        }
      
    
    {\displaystyle k\in \{0,\ldots ,n-1\}}
   then the unknown element at place k in the string is called a ""hole"". In regular expressions (following the POSIX standard) a hole is represented by the metacharacter ""."". For example, aab.ab.b is a partial word of length 8 over the alphabet A ={a,b} in which the fourth and seventh characters are holes.


== See also ==
String


== References =="
943,Vowel–consonant synthesis,2148549,1088,"Vowel–consonant synthesis is a type of hybrid digital–analogue synthesis developed and employed by the early Casiotone keyboards. It employs two digital waveforms, which are mixed and filtered by a static lowpass filter, with different filter positions selected for use according to presets. The filters are modeled on the frequencies present in the human vocal tract, hence the name given by Casio technicians during the research and development process.
The waveforms are stored and unalterable without considerable modification, such as the addition of a computer or microcontroller, to deliver alternative control data to the sound synthesis chip.


== References =="
944,PlanetLab,3727977,1088,"PlanetLab is a group of computers available as a testbed for computer networking and distributed systems research. It was established in 2002 by Prof. Larry L. Peterson and Prof. David Culler, and as of June 2010, it was composed of 1090 nodes at 507 sites worldwide. Each research project has a ""slice"", or virtual machine access to a subset of the nodes.
Accounts are limited to persons affiliated with corporations and universities that host PlanetLab nodes. However, a number of free, public services have been deployed on PlanetLab, including CoDeeN, the Coral Content Distribution Network, and Open DHT. Open DHT was taken down on 1 July 2009.


== References ==


== External links ==
PlanetLab"
945,International Conference on Database Theory,50665213,1086,"The International Conference on Database Theory (ICDT) is an international research conference on foundations of database theory, and has been held since 1982. It is frequently also called as the PODS of Europe. Since 2009, ICDT has been held jointly with the EDBT, a research conference on systems aspects of data management. Until 2009, the conference used to happen biennially. The conference now happens annually at a location typically within Europe. ICDT has been instrumental in developing a flourishing research community within Europe, working on problems in database theory.


== References ==


== External links ==
ICDT Official Page — ICDT Official Website"
946,Fileset,2228872,1084,"Fileset has several meanings, depending on the context:
In the AIX operating system it is the smallest individually installable unit (a collection of files that provides a specific function).
DCE/DFS uses the term fileset to define a tree containing directories, files, and mount points (links to other DFS filesets). A DFS fileset is also a unit of administrative control. Properties such as data location, storage quota, and replication are controlled at this level of granularity. The concept of a fileset in DFS is essentially identical to the concept of a volume in AFS. The glamor filesystem uses the same concept of filesets. Filesets are lightweight components compared to file systems, so management of a file set is easier.
In IBM GPFS represents a set of files within a file system which have an independent inode space.


== See also =="
947,International Conference on Logic Programming,6555463,1081,"The International Conference on Logic Programming (ICLP) is an annual academic conference on the topic of logic programming. It is sponsored by the Association for Logic Programming (ALP). The conference consists of peer-reviewed papers with the proceedings published by Springer's LNCS series.
The first ICLP was held in September 1982 in Marseille, France. The 31st and most recent ICLP was part of the Boole Conferences in Cork, Ireland in September 2015.


== References ==


== External links ==
Previous ICLP conferences on the ALP web-site."
948,Expectation propagation,14923880,1077,"Expectation propagation (EP) is a technique in Bayesian machine learning.
EP finds approximations to a probability distribution. It uses an iterative approach that leverages the factorization structure of the target distribution. It differs from other Bayesian approximation approaches such as Variational Bayesian methods.


== References ==
Thomas Minka (August 2–5, 2001). ""Expectation Propagation for Approximate Bayesian Inference"". In Jack S. Breese, Daphne Koller. UAI '01: Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence (PDF). University of Washington, Seattle, Washington, USA. pp. 362–369. 


== External links ==
Minka's EP papers
List of papers using EP."
949,Out-of-kilter algorithm,40772392,1071,"The out-of-kilter algorithm is an algorithm that computes the solution to the minimum-cost flow problem in a flow network. It was published in 1961 by D. R. Fulkerson  and is described here.


== References ==


== External links ==
Algoritmo Out-of-Kilter on YouTube (in Spanish)"
950,System Fault Tolerance,7098644,1071,"Fault tolerance is the property that enables a system to continue operating properly in the event of the failure of (or one or more faults within) some of its components. If its operating quality decreases at all, the decrease is proportional to the severity of the failure, as compared to a naively designed system in which even a small failure can cause total breakdown. Fault tolerance is particularly sought after in high-availability or life-critical systems. The ability of maintaining functionality when portions of a system break down is referred to as graceful degradation.
A fault-tolerant design enables a system to continue its intended operation, possibly at a reduced level, rather than failing completely, when some part of the system fails. The term is most commonly used to describe computer systems designed to continue more or less fully operational with, perhaps, a reduction in throughput or an increase in response time in the event of some partial failure. That is, the system as a whole is not stopped due to problems either in the hardware or the software. An example in another field is a motor vehicle designed so it will continue to be drivable if one of the tires is punctured, or a structure that is able to retain its integrity in the presence of damage due to causes such as fatigue, corrosion, manufacturing flaws, or impact.
Within the scope of an individual system, fault tolerance can be achieved by anticipating exceptional conditions and building the system to cope with them, and, in general, aiming for self-stabilization so that the system converges towards an error-free state. However, if the consequences of a system failure are catastrophic, or the cost of making it sufficiently reliable is very high, a better solution may be to use some form of duplication. In any case, if the consequence of a system failure is so catastrophic, the system must be able to use reversion to fall back to a safe mode. This is similar to roll-back recovery but can be a human action if humans are present in the loop.


== Terminology ==

A highly fault-tolerant system might continue at the same level of performance even though one or more components have failed. For example, a building with a backup electrical generator will provide the same voltage to wall outlets even if the grid power fails.
A system that is designed to fail safe, or fail-secure, or fail gracefully, whether it functions at a reduced level or fails completely, does so in a way that protects people, property, or data from injury, damage, intrusion, or disclosure. In computers, a program might fail-safe by executing a graceful exit (as opposed to an uncontrolled crash) in order to prevent data corruption after experiencing an error. A similar distinction is made between ""failing well"" and ""failing badly"".
Fail-deadly is the opposite strategy, which can be used in weapon systems that are designed to kill or injure targets even if part of the system is damaged or destroyed.
A system that is designed to experience graceful degradation, or to fail soft (used in computing, similar to ""fail safe"") operates at a reduced level of performance after some component failures. For example, a building may operate lighting at reduced levels and elevators at reduced speeds if grid power fails, rather than either trapping people in the dark completely or continuing to operate at full power. In computing an example of graceful degradation is that if insufficient network bandwidth is available to stream an online video, a lower-resolution version might be streamed in place of the high-resolution version. Progressive enhancement is an example in computing, where web pages are available in a basic functional format for older, small-screen, or limited-capability web browsers, but in an enhanced version for browsers capable of handling additional technologies or that have a larger display available.
In fault-tolerant computer systems, programs that are considered robust are designed to continue operation despite an error, exception, or invalid input, instead of crashing completely. Software brittleness is the opposite of robustness. Resilient networks continue to transmit data despite the failure of some links or nodes; resilient buildings and infrastructure are likewise expected to prevent complete failure in situations like earthquakes, floods, or collisions.
A system with high failure transparency will alert users that a component failure has occurred, even if it continues to operate with full performance, so that failure can be repaired or imminent complete failure anticipated. Likewise, a fail-fast component is designed to report at the first point of failure, rather than allow downstream components to fail and generate reports then. This allows easier diagnosis of the underlying problem, and may prevent improper operation in a broken state.


== Redundancy ==

Redundancy is the provision of functional capabilities that would be unnecessary in a fault-free environment. This can consist of backup components that automatically ""kick in"" should one component fail. For example, large cargo trucks can lose a tire without any major consequences. They have many tires, and no one tire is critical (with the exception of the front tires, which are used to steer, but generally carry less load, each and in total, than the other four to 16, so are less likely to fail). The idea of incorporating redundancy in order to improve the reliability of a system was pioneered by John von Neumann in the 1950s.
Two kinds of redundancy are possible: space redundancy and time redundancy. Space redundancy provides additional components, functions, or data items that are unnecessary for fault-free operation. Space redundancy is further classified into hardware, software and information redundancy, depending on the type of redundant resources added to the system. In time redundancy the computation or data transmission is repeated and the result is compared to a stored copy of the previous result. The current terminology for this kind of testing is referred to as 'In Service Fault Tolerance Testing or ISFTT for short.


== Criteria ==
Providing fault-tolerant design for every component is normally not an option. Associated redundancy brings a number of penalties: increase in weight, size, power consumption, cost, as well as time to design, verify, and test. Therefore, a number of choices have to be examined to determine which components should be fault tolerant:
How critical is the component? In a car, the radio is not critical, so this component has less need for fault tolerance.
How likely is the component to fail? Some components, like the drive shaft in a car, are not likely to fail, so no fault tolerance is needed.
How expensive is it to make the component fault tolerant? Requiring a redundant car engine, for example, would likely be too expensive both economically and in terms of weight and space, to be considered.
An example of a component that passes all the tests is a car's occupant restraint system. While we do not normally think of the primary occupant restraint system, it is gravity. If the vehicle rolls over or undergoes severe g-forces, then this primary method of occupant restraint may fail. Restraining the occupants during such an accident is absolutely critical to safety, so we pass the first test. Accidents causing occupant ejection were quite common before seat belts, so we pass the second test. The cost of a redundant restraint method like seat belts is quite low, both economically and in terms of weight and space, so we pass the third test. Therefore, adding seat belts to all vehicles is an excellent idea. Other ""supplemental restraint systems"", such as airbags, are more expensive and so pass that test by a smaller margin.
Another excellent and long-term example of this principle being put into practice is the braking system: whilst the actual brake mechanisms are critical, they are not particularly prone to sudden (rather than progressive) failure, and are in any case necessarily duplicated to allow even and balanced application of brake force to all wheels. It would also be prohibitively costly to further double-up the main components and they would add considerable weight. However, the similarly critical systems for actuating the brakes under driver control are inherently less robust, generally using a cable (can rust, stretch, jam, snap) or hydraulic fluid (can leak, boil and develop bubbles, absorb water and thus lose effectiveness). Thus in most modern cars the footbrake hydraulic brake circuit is diagonally divided to give two smaller points of failure, the loss of either only reducing brake power by 50% and not causing as much dangerous brakeforce imbalance as a straight front-back or left-right split, and should the hydraulic circuit fail completely (a relatively very rare occurrence), there is a failsafe in the form of the cable-actuated parking brake that operates the otherwise relatively weak rear brakes, but can still bring the vehicle to a safe halt in conjunction with transmission/engine braking so long as the demands on it are in line with normal traffic flow. The culmulatively unlikely combination of total footbrake failure with the need for harsh braking in an emergency will likely result in a collision, but still one at lower speed than would otherwise have been the case.
In comparison with the footpedal activated service brake, the parking brake itself is a less critical item, and unless it is being used as a one-time backup for the footbrake, will not cause immediate danger if it is found to be nonfunctional at the moment of application. Therefore, no redundancy is built into it per se (and it typically uses a cheaper, lighter, but less hardwearing cable actuation system), and it can suffice, if this happens on a hill, to use the footbrake to momentarily hold the vehicle still, before driving off to find a flat piece of road on which to stop. Alternatively, on shallow gradients, the transmission can be shifted into Park, Reverse or First gear, and the transmission lock / engine compression used to hold it stationary, as there is no need for them to include the sophistication to first bring it to a halt.
On motorcycles, a similar level of fail-safety is provided by simpler methods; firstly the front and rear brake systems being entirely separate, regardless of their method of activation (that can be cable, rod or hydraulic), allowing one to fail entirely whilst leaving the other unaffected. Secondly, the rear brake is relatively strong compared to its automotive cousin, even being a powerful disc on sports models, even though the usual intent is for the front system to provide the vast majority of braking force; as the overall vehicle weight is more central, the rear tyre is generally larger and grippier, and the rider can lean back to put more weight on it, therefore allowing more brake force to be applied before the wheel locks up. On cheaper, slower utility-class machines, even if the front wheel should use a hydraulic disc for extra brake force and easier packaging, the rear will usually be a primitive, somewhat inefficient, but exceptionally robust rod-actuated drum, thanks to the ease of connecting the footpedal to the wheel in this way and, more importantly, the near impossibility of catastrophic failure even if the rest of the machine, like a lot of low-priced bikes after their first few years of use, is on the point of collapse from neglected maintenance.


== Requirements ==
The basic characteristics of fault tolerance require:
No single point of failure – If a system experiences a failure, it must continue to operate without interruption during the repair process.
Fault isolation to the failing component – When a failure occurs, the system must be able to isolate the failure to the offending component. This requires the addition of dedicated failure detection mechanisms that exist only for the purpose of fault isolation. Recovery from a fault condition requires classifying the fault or failing component. The National Institute of Standards and Technology (NIST) categorizes faults based on locality, cause, duration, and effect.
Fault containment to prevent propagation of the failure – Some failure mechanisms can cause a system to fail by propagating the failure to the rest of the system. An example of this kind of failure is the ""rogue transmitter"" that can swamp legitimate communication in a system and cause overall system failure. Firewalls or other mechanisms that isolate a rogue transmitter or failing component to protect the system are required.
Availability of reversion modes
In addition, fault-tolerant systems are characterized in terms of both planned service outages and unplanned service outages. These are usually measured at the application level and not just at a hardware level. The figure of merit is called availability and is expressed as a percentage. For example, a five nines system would statistically provide 99.999% availability.
Fault-tolerant systems are typically based on the concept of redundancy.


== Replication ==
Spare components address the first fundamental characteristic of fault tolerance in three ways:
Replication: Providing multiple identical instances of the same system or subsystem, directing tasks or requests to all of them in parallel, and choosing the correct result on the basis of a quorum;
Redundancy: Providing multiple identical instances of the same system and switching to one of the remaining instances in case of a failure (failover);
Diversity: Providing multiple different implementations of the same specification, and using them like replicated systems to cope with errors in a specific implementation.
All implementations of RAID, redundant array of independent disks, except RAID 0, are examples of a fault-tolerant storage device that uses data redundancy.
A lockstep fault-tolerant machine uses replicated elements operating in parallel. At any time, all the replications of each element should be in the same state. The same inputs are provided to each replication, and the same outputs are expected. The outputs of the replications are compared using a voting circuit. A machine with two replications of each element is termed dual modular redundant (DMR). The voting circuit can then only detect a mismatch and recovery relies on other methods. A machine with three replications of each element is termed triple modular redundant (TMR). The voting circuit can determine which replication is in error when a two-to-one vote is observed. In this case, the voting circuit can output the correct result, and discard the erroneous version. After this, the internal state of the erroneous replication is assumed to be different from that of the other two, and the voting circuit can switch to a DMR mode. This model can be applied to any larger number of replications.
Lockstep fault-tolerant machines are most easily made fully synchronous, with each gate of each replication making the same state transition on the same edge of the clock, and the clocks to the replications being exactly in phase. However, it is possible to build lockstep systems without this requirement.
Bringing the replications into synchrony requires making their internal stored states the same. They can be started from a fixed initial state, such as the reset state. Alternatively, the internal state of one replica can be copied to another replica.
One variant of DMR is pair-and-spare. Two replicated elements operate in lockstep as a pair, with a voting circuit that detects any mismatch between their operations and outputs a signal indicating that there is an error. Another pair operates exactly the same way. A final circuit selects the output of the pair that does not proclaim that it is in error. Pair-and-spare requires four replicas rather than the three of TMR, but has been used commercially.


== Disadvantages ==
Fault-tolerant design's advantages are obvious, while many of its disadvantages are not:
Interference with fault detection in the same component. To continue the above passenger vehicle example, with either of the fault-tolerant systems it may not be obvious to the driver when a tire has been punctured. This is usually handled with a separate ""automated fault-detection system"". In the case of the tire, an air pressure monitor detects the loss of pressure and notifies the driver. The alternative is a ""manual fault-detection system"", such as manually inspecting all tires at each stop.
Interference with fault detection in another component. Another variation of this problem is when fault tolerance in one component prevents fault detection in a different component. For example, if component B performs some operation based on the output from component A, then fault tolerance in B can hide a problem with A. If component B is later changed (to a less fault-tolerant design) the system may fail suddenly, making it appear that the new component B is the problem. Only after the system has been carefully scrutinized will it become clear that the root problem is actually with component A.
Reduction of priority of fault correction. Even if the operator is aware of the fault, having a fault-tolerant system is likely to reduce the importance of repairing the fault. If the faults are not corrected, this will eventually lead to system failure, when the fault-tolerant component fails completely or when all redundant components have also failed.
Test difficulty. For certain critical fault-tolerant systems, such as a nuclear reactor, there is no easy way to verify that the backup components are functional. The most infamous example of this is Chernobyl, where operators tested the emergency backup cooling by disabling primary and secondary cooling. The backup failed, resulting in a core meltdown and massive release of radiation.
Cost. Both fault-tolerant components and redundant components tend to increase cost. This can be a purely economic cost or can include other measures, such as weight. Manned spaceships, for example, have so many redundant and fault-tolerant components that their weight is increased dramatically over unmanned systems, which don't require the same level of safety.
Inferior components. A fault-tolerant design may allow for the use of inferior components, which would have otherwise made the system inoperable. While this practice has the potential to mitigate the cost increase, use of multiple inferior components may lower the reliability of the system to a level equal to, or even worse than, a comparable non-fault-tolerant system.


== Examples ==
Hardware fault tolerance sometimes requires that broken parts be taken out and replaced with new parts while the system is still operational (in computing known as hot swapping). Such a system implemented with a single backup is known as single point tolerant, and represents the vast majority of fault-tolerant systems. In such systems the mean time between failures should be long enough for the operators to have time to fix the broken devices (mean time to repair) before the backup also fails. It helps if the time between failures is as long as possible, but this is not specifically required in a fault-tolerant system.
Fault tolerance is notably successful in computer applications. Tandem Computers built their entire business on such machines, which used single-point tolerance to create their NonStop systems with uptimes measured in years.
Fail-safe architectures may encompass also the computer software, for example by process replication (computer science).
Data formats may also be designed to degrade gracefully. HTML for example, is designed to be forward compatible, allowing new HTML entities to be ignored by Web browsers that do not understand them without causing the document to be unusable.


== Related terms ==
There is a difference between fault tolerance and systems that rarely have problems. For instance, the Western Electric crossbar systems had failure rates of two hours per forty years, and therefore were highly fault resistant. But when a fault did occur they still stopped operating completely, and therefore were not fault tolerant.


== See also ==
Control reconfiguration
Damage tolerance
Defence in depth
Elegant degradation
Error-tolerant design (human-error-tolerant design)
Failure semantics
Fault-tolerant computer system
List of system quality attributes
Resilience (ecology)
Resilience (network)
Safe-life design


== References ==


== Further reading =="
951,HOOD method,623281,1070,"HOOD (Hierarchic Object-Oriented Design) is a detailed software design method. It is based on hierarchical decomposition of a software problem. It comprises textual and graphical representations of the design.
HOOD was initially created for the European Space Agency and is used in such varied domains as aerospace (Eurofighter Typhoon, Helios 2 Earth Observation ground control, Ariane 5 on-board computer), ground transportation, and nuclear plants.
HOOD main target languages are Ada, Fortran and C.


== External links ==
Introduction to HOOD HOOD page at ESA
ESA's HOOD user manual, gzipped postscript"
952,Genetic memory (computer science),23107207,1069,"In computer science, genetic memory refers to an artificial neural network combination of genetic algorithm and the mathematical model of sparse distributed memory. It can be used to predict weather patterns. Genetic memory and genetic algorithms have also gained an interest in the creation of artificial life.


== References =="
953,Prefix hash tree,11576493,1068,"A prefix hash tree (PHT) is a distributed data structure that enables more sophisticated queries over a distributed hash table (DHT). The prefix hash tree uses the lookup interface of a DHT to construct a trie-based data structure that is both efficient (updates are doubly logarithmic in the size of the domain being indexed), and resilient (the failure of any given node in a prefix hash tree does not affect the availability of data stored at other nodes).


== External links ==
https://www.eecs.berkeley.edu/~sylvia/papers/pht.pdf - Prefix Hash Tree: An Indexing Data Structure over Distributed Hash Tables
http://pier.cs.berkeley.edu - PHT was developed as part of work on the PIER project.
http://www.tecnohobby.net/ppal/index.php/programacion/java/35-solucionador-de-puzzles-usando-arboles-trie-trie-trees-o-prefix-trees - Java implementation for searching words in a puzzle using a prefix tree.


== See also ==
Prefix tree
P-Grid"
954,Madcap (project),10516537,1057,"Madcap is a prototype system for using computers to automatically segment and highlight a video stream that was designed by Dan Russell's group at PARC. The digital video of weekly forums was stored on a media server, any events such as clapping were marked and time-stamped. Member notes from attendees with laptops were stored. The audio stream was transcribed. All these were used as annotations and were cross indexed.
The development of this system is mentioned in John Seely Brown's work Growing up Digital and his book The Social Life of Information.


== External links ==
https://web.archive.org/web/20070305070159/http://www.usdla.org/html/journal/FEB02_Issue/article01.html
http://www.rice.edu/projects/code/broadband.html
http://www.cs.berkeley.edu/~jfc/hcc/F00/abstracts/russell.html


== References =="
955,Computational human modeling,54690679,1055,"Computational human modeling is an interdisciplinary computational science that links the diverse fields of artificial intelligence, cognitive science, and computer vision with machine learning, mathematics, and cognitive psychology.
Computational human modeling emphasizes descriptions of human for A.I. research and applications.


== Major topics ==
Research in computational human modeling can include computer vision studies on identify (face recognition), attributes (gender, age, skin color), expressions, geometry (3D face modeling, 3D body modeling), and activity (pose, gaze, actions, and social interactions).


== See Also ==
Activity recognition
Computational theory of mind
Emotion recognition
Facial recognition system
Three-dimensional face recognition"
956,Precompressor,11450765,1052,"A precompressor is a computer program, which alters file content so that a real lossless compression program will achieve better results than without precompressing. This is possible by creating such patterns that compression programs will recognize.


== Compressing already compressed files ==
Usually, the compression rate on files that are already compressed (e.g. using ZIP) is poor because the data seems random to the compressor. However, compression can usually be improved by decompressing the ZIP file and recompressing it with a better compressor (e.g. one of the PAQ family). But to achieve lossless compression, the original ZIP file has to be restored using exactly the same version of the ZIP compressor and the same compression settings.
Using this strategy, certain filetypes that so far achieved poor compression rates can be processed.


== External links ==
Precomp program"
957,Flat neighborhood network,11632712,1046,"Flat Neighborhood Network (FNN) is a topology for distributed computing and other computer networks. Each node connects to two or more switches which, ideally, entirely cover the node collection, so that each node can connect to any other node in two ""hops"" (jump up to one switch and down to the other node). This contrasts to toplogies with fewer cables per node which communicate with remote nodes via intermediate nodes, as in Hypercube (see The Connection Machine).


== See also ==
Thinking Machines Corporation built the Connection Machine employing hypercube topology for its compute nodes.
Kentucky's Linux/Athlon Testbed KLAT2 is an archetypal implementation.


== External links ==
The Aggregate (at the University of Kentucky) defines FNN and includes a bibliography."
958,Distributed design patterns,5474056,1045,"In software engineering, a distributed design pattern is a design pattern focused on distributed computing problems.


== Classification ==
Distributed design patterns can be divided into several groups:
Distributed communication patterns
Security and reliability patterns
Event driven patterns


== Examples ==
MapReduce
Bulk synchronous parallel
Remote Session


== See also ==
Software engineering
List of software engineering topics


== References =="
959,Decentralized object location and routing,10847346,1041,"In computer science, Decentralized Object Location and Routing (DOLR) is a scalable, location-independent routing technology. It uses location-independent names, or aliases, for each node in the network, and it is an example of peer-to-peer networking that uses a structured-overlay system called Tapestry. It was designed to facilitate large internet applications with millions of users physically distributed around the globe and using a variety of wireless and wired interfaces, specifically in situations where a traditional unstructured network of popular Domain name system servers would fail to perform well.


== References =="
960,Gale–Church alignment algorithm,12223583,1038,"In computational linguistics, the Gale–Church algorithm is a method for aligning corresponding sentences in a parallel corpus. It works on the principle that equivalent sentences should roughly correspond in length—that is, longer sentences in one language should correspond to longer sentences in the other language. The algorithm was described in a 1993 paper by William A. Gale and Kenneth W. Church of AT&T Bell Laboratories.


== References ==


== External links ==
Gale, William A.; Church, Kenneth W. (1993), ""A Program for Aligning Sentences in Bilingual Corpora"" (PDF), Computational Linguistics, 19 (1): 75–102"
961,Existence detection,26320219,1036,"Existence checking or existence detection is an important aspect of many computer programs. An existence check before reading a file can catch and/or prevent a fatal error, for instance. For that reason, most programming language libraries contain a means of checking whether a file exists.
An existence check can sometimes involve a ""brute force"" approach of checking all records for a given identifier, as in this Microsoft Excel Visual Basic for Applications code for detecting whether a worksheet exists:


== References =="
962,Matrix clock,19505063,1036,"A matrix clock is a mechanism for capturing chronological and causal relationships in a distributed system.
Matrix clocks are a generalization of the notion of vector clocks. A matrix clock maintains a vector of the vector clocks for each communicating host.
Every time a message is exchanged, the sending host sends not only what it knows about the global state of time, but also the state of time that it received from other hosts.
This allows establishing a lower bound on what other hosts know, and is useful in applications such as checkpointing and garbage collection.


== References =="
963,Technical informatics,23590353,1027,"Technical informatics is a subclass of computer engineering that combines general informatics with technology. Particularly in the field of electrical engineering, it has numerous correlations with the engineering sciences. However, the perspective of technical informatics is not marked by the technical disciplines alone but the focus is rather on giving special emphasis to typical aspects of informatics in order to find more general and universal solutions.
This discipline is usually taught at vocational universities up to a master's degree level.


== References =="
964,Orthogonality (term rewriting),5099917,1027,"Orthogonality as a property of term rewriting systems describes where the reduction rules of the system are all left-linear, that is each variable occurs only once on the left hand side of each reduction rule, and there is no overlap between them.
Orthogonal term rewriting systems have the consequent property that all reducible expressions (redexes) within a term are completely disjoint -- that is, the redexes share no common function symbol.
For example, the term rewriting system with reduction rules

  
    
      
        
          ρ
          
            1
          
        
         
        :
         
        f
        (
        x
        ,
        y
        )
        →
        g
        (
        y
        )
      
    
    {\displaystyle \rho _{1}\ :\ f(x,y)\rightarrow g(y)}
  

  
    
      
        
          ρ
          
            2
          
        
         
        :
         
        h
        (
        y
        )
        →
        f
        (
        g
        (
        y
        )
        ,
        y
        )
      
    
    {\displaystyle \rho _{2}\ :\ h(y)\rightarrow f(g(y),y)}
  
is orthogonal -- it is easy to observe that each reduction rule is left-linear, and the left hand side of each reduction rule shares no function symbol in common, so there is no overlap.
Orthogonal term rewriting systems are confluent."
965,Mortality (computability theory),10646392,1022,"In computability theory, the mortality problem is a decision problem which can be stated as follows:
Given a Turing machine, decide whether it halts when run on any configuration (not necessarily a starting one)
In the statement above, the configuration is a pair <q, w>, where q is one of the machine's states (not necessarily its initial state) and w is an infinite sequence of symbols representing the initial content of the tape. Note that while we usually assume that in the starting configuration all but finitely many cells on the tape are blanks, in the mortality problem the tape can have arbitrary content, including infinitely many non-blank symbols written on it.
Philip K. Hooper proved in 1966 that the mortality problem is undecidable. However, it can be shown that the set of Turing machines which are mortal (i.e. halt on every starting configuration) is recursively enumerable."
966,VerbNet,5750862,1020,"The VerbNet project maps PropBank verb types to their corresponding Levin classes. It is a lexical resource that incorporates both semantic and syntactic information about its contents.
VerbNet is part of the SemLink project in development at the University of Colorado.


== Related projects ==
UBY a database of 10 resources including VerbNet.


== External links ==
Karin Kipper's dissertation — VerbNet: a broad-coverage, comprehensive verb lexicon
Martha Palmer's Verbnet page — contains download of the VerbNet XML files and web interface to the database
Unified Verb Index — unified index to three lexical semantic resources, VerbNet, PropBank, and FrameNet"
967,International Joint Conference on Web Intelligence and Intelligent Agent Technology,49862247,1018,"The IEEE/WIC/ACM 'International Joint Conference on Web Intelligence and Intelligent Agent Technology' (WI-IAT) is a colocated conference focusing on Web intelligence and Intelligent Agent Technology. The conference is a joint undertaking of the Institute of Electrical and Electronics Engineers (IEEE) Computer Society Technical Committee on Intelligent Informatics (TCII), the Web Intelligence Consortium (WIC), SIGAI, and the Memetic Computing Society.


== See also ==
ACM
ACM SIGAI
IEEE
IEEE Computer Society
Memetic Computing Society
Web Intelligence Consortium


== References =="
968,Symposium on Applied Computing,18586502,1013,"The ACM Symposium on Applied Computing is an annual conference sponsored by the ACM Special Interest Group on Applied Computing.
The first Symposium on Applied Computing was held in 1985. Since the 1990s, the acceptance ratio for paper submissions has dropped from 54%-67% to below 30% in the 2008 SAC held in Fortaleza, Brazil. The 2009 conference was held in Honolulu, Hawaii.


== See also ==
List of computer science conferences


== References ==


== External links ==
Main web page of the Symposium on Applied Computing, with links to the yearly conferences"
969,MIT Media Lab Object-Based Media,4322917,1004,"The MIT Media Lab is an interdisciplinary research laboratory at the Massachusetts Institute of Technology, growing out of MIT's Architecture Machine Group in the School of Architecture. Its research draws from technology, media, science, art and design. As of 2014, research groups included neurobiology, biologically inspired fabrication, socially engaging robots, emotive computing, bionics, and hyperinstruments.
The Lab has been written about in the popular press since 1988, when Stewart Brand published ""The Media Lab: Inventing the Future at M.I.T."", and its work was a regular feature of technology journals in the 1990s.
The Media Lab was founded in 1985 by Nicholas Negroponte and former MIT President Jerome Wiesner and opened its doors in the Wiesner Building (designed by I. M. Pei), also known as Building E15. In 2009, it expanded into a second building.


== Administration ==
The founding director of the lab was Nicholas Negroponte, who directed it until 2000. Later directors were Walter Bender (2000–2006), Frank Moss (2006–2011), and Joi Ito (2011-present).
As of 2014, the Media Lab had roughly 70 administrative and support staff members. Associate Directors of the Lab were Hiroshi Ishii and Andrew Lippman. Pattie Maes and Mitchel Resnick were co-heads of the Program in Media Arts and Sciences, and the Lab's Chief Knowledge Officer was Henry Holtzman.
The Media Lab has at times had regional branches in other parts of the world, such as Media Lab Europe and Media Lab Asia, each with their own staff and governing bodies.


=== Funding model ===
The Lab's primary funding comes from corporate sponsorship. Rather than accepting funding on a per-project or per-group basis, the Lab asks sponsors to fund general themes; sponsors can then connect with Media Lab research. Specific projects and researchers are also funded more traditionally through government institutions including the NIH, NSF, and DARPA. Also, consortia with other schools or other departments at MIT are often able to have money that does not enter into the common pool.


=== Intellectual property ===
Companies sponsoring the Lab can share in the Lab’s intellectual property without paying license fees or royalties. Non-sponsors can't making use of Media Lab developments for two years after technical disclosure is made to MIT and Media Lab sponsors. The Media Lab generates approximately 20 new patents every year.


== Research at the Lab ==
Some recurring themes of work at the Media Lab include human adaptability, human computer interaction, education and communication, artistic creation and visualization, and designing technology for the developing world. Other research foci include machines with common sense, sociable robots, prosthetics, sensor networks, musical devices, city design, and public health. Research programs all include iterative development of prototypes which are tested and displayed for visitors.
Each of these areas of research may incorporate others. Interaction design research includes designing intelligent objects and environments. Educational research has also included integrating more computation into learning activities - including software for learning, programmable toys, and artistic or musical instruments. Examples include Lego Mindstorms, the PicoCricket, and One Laptop per Child.


=== Research groups ===
As of 2017, the MIT Media Lab has the following research groups:
Affective Computing -""helping people better communicate, understand, and respond to affective information.""
Biomechatronics-""enhancing human physical capability.""
Camera Culture-""capturing and sharing visual information.""
City Science-""designing dynamic urban environments.""
Civic Media-""fostering civic participation and information flow""
Conformable Decoders
Design Fictions-""discussing implications of new technologies through design and storytelling.""
Fluid Interfaces-""integrating information and services into our daily physical lives.""
Human Dynamics-""how social networks influence our lives""
Lifelong Kindergarten-""creative learning experiences.""
Collective Learning-""transforming data into knowledge.""
Mediated Matter-""transforming the design and construction of objects and systems.""
Molecular Machines-""engineering at the limits of complexity with molecular-scale parts.""
Object-Based Media-""new sensing and interface technologies""
Opera of the Future-""musical composition, performance, and instrumentation.""
Personal Robots-""socially engaging robots""
Playful Systems-""sensor networks augmenting human experience.""
Responsive Environments-""designing large-scale social systems.""
Scalable Cooperation-""reimagining the way society organizes, cooperates, and governs.""
Signal Kinetics-""extend human and computer abilities through signals and networks""
Social Machines-""building machines that learn to use language in human-like ways, modeling children and adults""
Social Computing-""assisting users by learning from interaction and anticipating needs.""
Speech + Mobility-""engineering intelligent neurotechnologies.""
Synthetic Neurobiology-""interfaces between humans, information, and the environment.""
Tangible Media
Viral Spaces-""scalable systems that enhance how we learn from and experience real spaces.""


== Academic program ==
The Media Arts and Sciences program is a part of MIT's School of Architecture and Planning, and includes three levels of study: a doctoral program, a master's of science program, and a program that offers an alternative to the standard MIT freshman year as well as a set of undergraduate subjects that may form the basis for a future joint major. All graduate students are fully supported (tuition plus a stipend) from the outset, normally by appointments as research assistants at the Media Laboratory, where they work on research programs and faculty projects, including assisting with courses. These research activities typically take up about half of a student’s time in the degree program.
The Media Arts and Sciences academic program has a close relationship with the Media Lab. Most Media Lab faculty are professors of Media Arts and Sciences. Students who earn a degree in Media Arts and Sciences have been predominantly in residence at the Media Lab, taking classes and doing research. Some students from other programs at MIT, such as Mechanical Engineering, or Electrical Engineering and Computer Science, do their research at the Media Lab, working with a Media Lab/Media Arts and Sciences faculty advisor, but earn their degrees (such as MEng or an MS in EECS) from other departments.


== Buildings ==

In addition to the Media Lab, the combined original Wiesner building (E15) and new (E14) buildings also host the List Visual Arts Center, the School of Architecture and Planning's Program in Art, Culture and Technology (ACT), and MIT's Program in Comparative Media Studies.
In 2009, the Media Lab expanded into a new building designed by Pritzker Prize-winning Japanese architect Fumihiko Maki. The local architect of record was Leers Weinzapfel Associates, of Boston. The Maki building has predominantly glass walls, with long lines of sight through the building, making ongoing research visible and encouraging connections and collaboration.


== Faculty and academic research staff ==
Media Arts and Sciences faculty and academic research staff are principal investigators/heads of the Media Lab's various research groups. They also advise Media Arts and Sciences graduate students, and mentor MIT undergraduates. ""Most departments accept grad students based on their prospects for academic success; the Media Lab attempts to select ones that will best be able to help with some of the ongoing projects.""
As of 2014, there are more than 25 faculty and academic research staff members, including a dozen named professorships. A full list of Media Lab faculty and academic research staff, with bios and other information, is available via the Media Lab Website.


== Selected publications ==
Books
Cesar A. Hidalgo: Why Information Grows (Basic Books, 2015)
Cynthia L. Breazeal: Designing Sociable Robots, Biologically Inspired Intelligent Robots (co-editor with Yoseph Bar-Cohen)
Dan Ariely: Predictably Irrational (HarperCollins 2008)
Frank Moss: The Sorcerers and Their Apprentices: How the Digital Magicians of the MIT Media Lab Are Creating the Innovative Technologies That Will Transform Our Lives
Idit Harel Caperton: Children Designers
John Maeda: The Laws of Simplicity, Design by Numbers
Joichi Ito and Jeffrey Howe: ""Whiplash: How to Survive Our Faster Future"" (Hachette, 2016)
Marvin Minsky, Seymour Papert: Perceptrons: An Introduction to Computational Geometry
Marvin Minsky: The Emotion Machine: Commonsense Thinking, Artificial Intelligence, and the Future of the Human Mind, Society of Mind
Mitchel Resnick: Turtles, Termites, and Traffic Jams: Explorations in Massively Parallel Microworlds
Neil Gershenfeld: When Things Start to Think
Nicholas Negroponte: Being Digital
Rosalind W. Picard: Affective Computing
Seymour Papert: The Children's Machine: Rethinking School in the Age of the Computer, Mindstorms: Children, Computers, and Powerful Ideas
Stephen A. Benton and V. Michael Bove, Jr.: Holographic Imaging (Wiley 2008)
Vanessa Stevens Colella, Eric Klopfer, Mitchel Resnick: Adventures in Modeling: Exploring Complex, Dynamic Systems with StarLogo
William J. Mitchell: Imagining MIT: Designing a Campus for the Twenty-First Century, Me++: The Cyborg Self and the Networked City


== Outputs and spin-offs ==
Some Media Lab-developed technologies made it into products or public software packages, such as the Lego Mindstorms, LEGO WeDo and the pointing stick in IBM laptop keyboards, the Benton hologram used in most credit cards, the Fisher-Price's Symphony Painter, the Nortel Wireless Mesh Network, the NTT Comware Sensetable, the Taito’s Karaoke-on-Demand Machine. A 1994 device called the Sensor Chair used to control a musical orchestra was adapted by several car manufacturers into capacitive sensors to prevent dangerous airbag deployments.
The MPEG-4 SA project developed at the Media Lab made structured audio a practical reality and the Aspen Movie Map was the precursor to the ideas in Google Street View.
In 2001, two research centers were spun off: Media Lab Asia and Media Lab Europe. Media Lab Asia, based in India, was a result of cooperation with the Government of India but eventually broke off in 2003 after disagreement. Media Lab Europe, based in Dublin, Ireland, was founded with a similar concept in association with Irish universities and government, and closed in January 2005.
Created collaboratively by the Computer Museum and the Media Lab, the Computer Clubhouse, a worldwide network of after-school learning centers, focuses on youth from underserved communities who would not otherwise have access to technological tools and activities.
Launched in 2003, Scratch is a block-based programming language and community developed for children 8-16, and used by people of all ages to learn programming. Millions of people have created Scratch projects in a wide variety of settings, including homes, schools, museums, libraries, and community centers.
In January 2005, the Lab's chairman emeritus Nicholas Negroponte announced at the World Economic Forum a new research initiative to develop a $100 laptop computer. A non-profit organization, One Laptop per Child, was created to oversee the actual deployment, MIT did not manufacture or distribute the device.
The Synthetic Neurobiology group created reagents and devices for the analysis of brain circuits are in use by hundreds of biology labs around the world.
In 2011, Ramesh Raskar's group published their femto-photography technique, that is able to image the movement of individual light pulses.


=== Spin-offs ===
Media Lab industry spin-offs include:
Affectiva, Inc., commercializing software that detects emotions in pictures of faces
Ambient Devices, which produces glanceable information displays
Dimagi, a company that develops software for healthcare in the developing world.
E Ink, which makes electronic paper displays that power the Amazon Kindle and Barnes & Noble Nook.
Elance
EyeNetra, which makes eye tests as $2 clip-ons for mobile phones, including potential use to correct vision for virtual reality displays.
Formlabs makes high-resolution, desktop 3D printers (spin out from Center for Bits and Atoms)
Groundhog Technologies, global leader in mobility intelligence and its applications on geo-analytics, geo-marketing, and network optimization.
Harmonix, game company creator of Rock Band and Guitar Hero.
Holosonics selling ""audio spotlight"" speakers using sound from ultrasound technology
Oblong Industries, creators of the digital screen used by Tom Cruise in Minority Report
One Laptop per Child's XO laptop
Potion Design, an interactive design firm
RadioSherpa, an online guide for HD Radio stations. acquired by Tune-in.
reQall, a memory aid company.
Salient Stills, a video resolution enhancement and video forensics company founded in 1996, acquired by DAC in 2013. The combined entity has been rebranded Salient Sciences.
Sifteo, a company that has developed a tabletop gaming platform that grew out of Siftables.
Squid Labs, engineering consulting company
The Echo Nest, a music intelligence platform
Zebra Imaging, a digital holographic display company
First Mile Solutions, bringing communications infrastructure to rural communities
Nanda, a company that markets the Clocky alarm clock
Physiio International, merged with Empatica; manufacturer of wearable medical sensors
Supermechanical, manufacturer of Twine, a wifi interface for various environmental sensors; and Range, a smartphone-connected thermometer
Wireless 5th Dimensional Networking, Inc. (acquired in 2006), which developed the first hybrid search engine


== See also ==


== References ==


== External links ==
Official website
MIT Media Lab on Twitter
LabCAST
LabCAST Media Lab episode"
970,Jump threading,369445,1004,"In computing, jump threading is a compiler optimization of one jump directly to a second jump. If the second condition is a subset or inverse of the first, it can be eliminated, or threaded through the first jump. This is easily done in a single pass through the program, following acyclic chained jumps until the compiler arrives at a fixed point.


== Example ==
The following pseudocode demonstrates when a jump may be threaded.

   10. a = SomeNumber();
   20. IF a > 10 GOTO 50
   ...
   50. IF a > 0 GOTO 100
   ...

The jump on line 50 will always be taken if the jump on line 20 is taken. Therefore the jump on line 20 may safely be modified to jump directly to line 100.


== References =="
971,FMLLR,43867843,1000,"Feature space Maximum Likelihood Linear Regression (fMLLR) is a widely used technique for speaker adaptation in HMM-based speech recognition.


== See also ==
Feature scaling


== References =="
972,Ernesto Damiani,50900028,999,"Ernesto Damiani is a professor of computer science at the University of Milan, where he leads the SEcure Service-oriented Architectures Research (SESAR) Lab. He is the Director of the Information Security Research Center at Khalifa University, in the UAE. He holds visiting positions at Tokyo Denki University, Université de Bourgogne. Damiani received a honorary doctorate from Institut national des sciences appliquées de Lyon, France (2017). His research spans security, Big Data and knowledge processing, where he has published over 400 peer-reviewed articles and books. He is a Senior Member of the IEEE and a Distinguished Scientist of ACM.


== External links ==
Homepage at University of Milan
ACM author page"
973,GLOH,12341751,998,"GLOH (Gradient Location and Orientation Histogram) is a robust image descriptor that can be used in computer vision tasks. It is a SIFT-like descriptor that considers more spatial regions for the histograms. The higher dimensionality of the descriptor is reduced to 64 through principal components analysis (PCA).


== See also ==
Scale-invariant feature transform
Speeded Up Robust Features
LESH - Local Energy based Shape Histogram
Feature detection (computer vision)


== References ==
Krystian Mikolajczyk and Cordelia Schmid ""A performance evaluation of local descriptors"", IEEE Transactions on Pattern Analysis and Machine Intelligence, 10, 27, pp 1615--1630, 2005."
974,Multiple asynchronous periodic polling,17927149,994,"Multiple Asynchronous Periodic Polling (MAPP) is a synchronization method used primarily in Computer Science. This method is similar to periodic polling.
In MAPP, two or more periodic polling handlers (such as using multiple timers) are applied to form a single aggregate polling scheme. By changing the combination of periods and phase shifts, MAPP can optimize corner cases which are problematic to conventional periodic polling. MAPP is mainly used when the temporal resolution of conventional periodic polling is not high enough for the particular application.
For example, 5 timers with identical polling period but 1/5 cycle phase shift from the previous one may be used together to increase the polling rate to 5 times of that which could be achieved with a single timer."
975,Functional compiler,7885634,990,"A functional compiler is a compiler for a functional programming language. Functional compilers perform such transformation on source code which transform it to continuation-passing style or administrative normal form and need to handle tail calls correctly.


== Further reading ==
Simon Peyton Jones (1987). The Implementation of Functional Programming Languages
R. Douence and P. Fradet. ""A systematic study of functional language implementations"". ACM Transactions on Programming Languages and Systems, 20(2):344–387, March 1998.
Several proceedings on Implementation of functional languages in the Lecture Notes in Computer Science series.


== External links ==
An Incremental Approach to Compiler Construction"
976,ERCIM Cor Baayen Award,47339689,990,"The Cor Baayen Award is an annual award given to a researcher in computer science and applied mathematics. In 1995, the award was created to honor the first ERCIM (European Research Consortium for Informatics and Mathematics) president.
The award is presented as a check for 5000 Euro and a certificate. The awardee is then invited to ERCIM meetings the following autumn. An article is then published in ERCIM news with the name of the winner, and all nominees.


== References =="
977,Inverse parser,952277,982,"An inverse parser, as its name suggests, is a parser that works in reverse. Rather than the user typing into the computer, the computer presents a list of words fitting the context, and excludes words that would be unreasonable. This ensures the user knows all of his or her options. The concept and an implementation were originally developed and patented by Texas Instruments. A few years later, it was independently developed by Chris Crawford, a game designer, for his game, Trust & Betrayal: The Legacy of Siboot, but the implementation was different enough not to infringe on the patent.


== External links ==
How to Build an Inverse Parser, an essay by Chris Crawford originally published in the Journal of Computer Game Design"
978,Structural risk minimization,10704974,982,"Structural risk minimization (SRM) is an inductive principle of use in machine learning. Commonly in machine learning, a generalized model must be selected from a finite data set, with the consequent problem of overfitting – the model becoming too strongly tailored to the particularities of the training set and generalizing poorly to new data. The SRM principle addresses this problem by balancing the model's complexity against its success at fitting the training data.
The SRM principle was first set out in a 1974 paper by Vladimir Vapnik and Alexey Chervonenkis and uses the VC dimension.


== See also ==
Vapnik–Chervonenkis theory
Support vector machines
Model selection
Occam Learning


== External links ==
Structural risk minimization at the support vector machines website."
979,Yobibyte,2718218,979,"The yobibyte is a multiple of the unit byte for digital information. It is a member of the set of units with binary prefixes defined by the International Electrotechnical Commission (IEC). Its unit symbol is YiB
The prefix yobi (symbol Yi) represents multiplication by 10248, therefore:
1 yobibyte = 280 bytes = 1208925819614629174706176bytes = 1024 zebibytes
The prefixes zebi and yobi were added to the system of binary prefixes in August 2005.
One yobibyte (1 YiB) is equal to eight yobibits (8 Yibit).


== See also ==
IEC 80000-13
IEEE 1541
Orders of magnitude (data)
SI prefix


== References =="
980,Automatic mutual exclusion,36675168,978,"Automatic mutual exclusion is a parallel computing programming paradigm in which threads are divided into atomic chunks, and the atomic execution of the chunks automatically parallelized using transactional memory.


== References ==


== See also ==
Bulk synchronous parallel"
981,Room synchronization,2511663,972,"The room synchronization technique is a form of concurrency control in computer science.
The room synchronization problem involves supporting a set of m mutually exclusive ""rooms"" where any number of users can execute code simultaneously in a shared room (any one of them), but no two users can simultaneously execute code in separate rooms.
Room synchronization can be used to implement asynchronous parallel queues and stacks with constant time access (assuming a fetch-and-add operation).


== References ==
G.E. Blelloch, P. Cheng, P.B. Gibbons, Room synchronizations, Annual ACM Symposium on Parallel Algorithms and Architectures 2001, 122–133 [1]


== See also ==
Monitor (synchronization).
The Single Threaded Apartment Model in Microsoft's Component Object Model#Threading in COM, as used by Visual Basic."
982,Left corner,7955869,967,"The left corner of a production rule in a context-free grammar is the left-most symbol on the right side of the rule.
For example, in the rule A→Xα, X is the left corner.
The left corner table associates a symbol with all possible left corners for that symbol, and the left corners of those symbols, etc.
Given the grammar
S→VP
S→NP VP
VP→V NP
NP→DET N
Left corners are used to add bottom-up filtering of a top-down parser.
You can use the left corners to do top-down filtering of a bottom-up parser.


== References =="
983,Computer Science Tripos,14051342,967,"The Computer Science Tripos is the undergraduate course in computer science offered by the University of Cambridge Computer Laboratory. It evolved out of the Diploma in Computer Science, the world’s first taught course in computer science, which started in 1953. Successful candidates are awarded a BA (Bachelor of Arts) honours degree after three years or, optionally, a combined BA + MEng (Master of Engineering) honours degree after four years of study.


== Notable alumni ==
Andrew Gower
Aubrey de Grey
Demis Hassabis
Simon Tatham


== See also ==
Tripos


== External links ==
Computer Laboratory undergraduate admissions"
984,Data truncation,25653002,964,"In databases and computer networking data truncation occurs when data or a data stream (such as a file) is stored in a location too short to hold its entire length. Data truncation may occur automatically, such as when a long string is written to a smaller buffer, or deliberately, when only a portion of the data is wanted.
Depending on what type of data validation a program or operating system has, the data may be truncated silently (i.e., without informing the user), or the user may be given an error message.
For example, sometimes instead of rounding off a numerical value obtained from a calculation, some of the digits might just be removed i.e. truncated


== See also ==
Fixed-point arithmetic -> Precision loss and overflow
Data loss"
985,Surfel,1791366,963,"Surfel is an abbreviation of ""surface element"". In 3D computer graphics, the use of surfels is an alternative to polygonal modeling. An object is represented by a dense set of points or viewer-facing discs holding lighting information. Surfels are well suited to modeling dynamic geometry, because there is no need to compute topology information such as adjacency lists. Common applications are medical scanner data representation, real time rendering of particle systems, and more generally, rendering surfaces of volumetric data by first extracting the isosurface.


== Notes ==


== See also ==
Volume rendering
Isosurface
Point splatting"
986,(1+ε)-approximate nearest neighbor search,32003319,961,"Nearest neighbor search (NNS), as a form of proximity search, is the optimization problem of finding the point in a given set that is closest (or most similar) to a given point. Closeness is typically expressed in terms of a dissimilarity function: the less similar the objects, the larger the function values. Formally, the nearest-neighbor (NN) search problem is defined as follows: given a set S of points in a space M and a query point q ∈ M, find the closest point in S to q. Donald Knuth in vol. 3 of The Art of Computer Programming (1973) called it the post-office problem, referring to an application of assigning to a residence the nearest post office. A direct generalization of this problem is a k-NN search, where we need to find the k closest points.
Most commonly M is a metric space and dissimilarity is expressed as a distance metric, which is symmetric and satisfies the triangle inequality. Even more common, M is taken to be the d-dimensional vector space where dissimilarity is measured using the Euclidean distance, Manhattan distance or other distance metric. However, the dissimilarity function can be arbitrary. One example are asymmetric Bregman divergences, for which the triangle inequality does not hold.


== Applications ==
The nearest neighbor search problem arises in numerous fields of application, including:
Pattern recognition – in particular for optical character recognition
Statistical classification – see k-nearest neighbor algorithm
Computer vision
Computational geometry – see Closest pair of points problem
Databases – e.g. content-based image retrieval
Coding theory – see maximum likelihood decoding
Data compression – see MPEG-2 standard
Robotic sensing
Recommendation systems, e.g. see Collaborative filtering
Internet marketing – see contextual advertising and behavioral targeting
DNA sequencing
Spell checking – suggesting correct spelling
Plagiarism detection
Contact searching algorithms in FEA
Similarity scores for predicting career paths of professional athletes.
Cluster analysis – assignment of a set of observations into subsets (called clusters) so that observations in the same cluster are similar in some sense, usually based on Euclidean distance
Chemical similarity
Sampling-based motion planning


== Methods ==
Various solutions to the NNS problem have been proposed. The quality and usefulness of the algorithms are determined by the time complexity of queries as well as the space complexity of any search data structures that must be maintained. The informal observation usually referred to as the curse of dimensionality states that there is no general-purpose exact solution for NNS in high-dimensional Euclidean space using polynomial preprocessing and polylogarithmic search time.


=== Exact methods ===


==== Linear search ====
The simplest solution to the NNS problem is to compute the distance from the query point to every other point in the database, keeping track of the ""best so far"". This algorithm, sometimes referred to as the naive approach, has a running time of O(dN) where N is the cardinality of S and d is the dimensionality of M. There are no search data structures to maintain, so linear search has no space complexity beyond the storage of the database. Naive search can, on average, outperform space partitioning approaches on higher dimensional spaces.


==== Space partitioning ====
Since the 1970s, branch and bound methodology has been applied to the problem. In the case of Euclidean space this approach is known as spatial index or spatial access methods. Several space-partitioning methods have been developed for solving the NNS problem. Perhaps the simplest is the k-d tree, which iteratively bisects the search space into two regions containing half of the points of the parent region. Queries are performed via traversal of the tree from the root to a leaf by evaluating the query point at each split. Depending on the distance specified in the query, neighboring branches that might contain hits may also need to be evaluated. For constant dimension query time, average complexity is O(log N)  in the case of randomly distributed points, worst case complexity is O(kN^(1-1/k)) Alternatively the R-tree data structure was designed to support nearest neighbor search in dynamic context, as it has efficient algorithms for insertions and deletions such as the R* tree. R-trees can yield nearest neighbors not only for Euclidean distance, but can also be used with other distances.
In case of general metric space branch and bound approach is known under the name of metric trees. Particular examples include vp-tree and BK-tree.
Using a set of points taken from a 3-dimensional space and put into a BSP tree, and given a query point taken from the same space, a possible solution to the problem of finding the nearest point-cloud point to the query point is given in the following description of an algorithm. (Strictly speaking, no such point may exist, because it may not be unique. But in practice, usually we only care about finding any one of the subset of all point-cloud points that exist at the shortest distance to a given query point.) The idea is, for each branching of the tree, guess that the closest point in the cloud resides in the half-space containing the query point. This may not be the case, but it is a good heuristic. After having recursively gone through all the trouble of solving the problem for the guessed half-space, now compare the distance returned by this result with the shortest distance from the query point to the partitioning plane. This latter distance is that between the query point and the closest possible point that could exist in the half-space not searched. If this distance is greater than that returned in the earlier result, then clearly there is no need to search the other half-space. If there is such a need, then you must go through the trouble of solving the problem for the other half space, and then compare its result to the former result, and then return the proper result. The performance of this algorithm is nearer to logarithmic time than linear time when the query point is near the cloud, because as the distance between the query point and the closest point-cloud point nears zero, the algorithm needs only perform a look-up using the query point as a key to get the correct result.


=== Approximation methods ===
An approximation algorithm is allowed to return a point, whose distance from the query is at most 
  
    
      
        c
      
    
    {\displaystyle c}
   times the distance from the query to its nearest points. The appeal of this approach is that, in many cases, an approximate nearest neighbor is almost as good as the exact one. In particular, if the distance measure accurately captures the notion of user quality, then small differences in the distance should not matter. 


==== Locality sensitive hashing ====
Locality sensitive hashing (LSH) is a technique for grouping points in space into 'buckets' based on some distance metric operating on the points. Points that are close to each other under the chosen metric are mapped to the same bucket with high probability.


==== Nearest neighbor search in spaces with small intrinsic dimension ====
The cover tree has a theoretical bound that is based on the dataset's doubling constant. The bound on search time is O(c12 log n) where c is the expansion constant of the dataset.


==== Projected radial search ====
In the special case where the data is a dense 3D map of geometric points, the projection geometry of the sensing technique can be used to dramatically simplify the search problem. This approach requires that the 3D data is organized by a projection to a two dimensional grid and assumes that the data is spatially smooth across neighboring grid cells with the exception of object boundaries. These assumptions are valid when dealing with 3D sensor data in applications such as surveying, robotics and stereo vision but may not hold for unorganized data in general. In practice this technique has an average search time of O(1) or O(K) for the k-nearest neighbor problem when applied to real world stereo vision data. 


==== Vector approximation files ====
In high dimensional spaces, tree indexing structures become useless because an increasing percentage of the nodes need to be examined anyway. To speed up linear search, a compressed version of the feature vectors stored in RAM is used to prefilter the datasets in a first run. The final candidates are determined in a second stage using the uncompressed data from the disk for distance calculation.


==== Compression/clustering based search ====
The VA-file approach is a special case of a compression based search, where each feature component is compressed uniformly and independently. The optimal compression technique in multidimensional spaces is Vector Quantization (VQ), implemented through clustering. The database is clustered and the most ""promising"" clusters are retrieved. Huge gains over VA-File, tree-based indexes and sequential scan have been observed. Also note the parallels between clustering and LSH.


==== Greedy walk search in small-world graphs ====
One possible way to solve NNS is to construct a graph 
  
    
      
        G
        (
        V
        ,
        E
        )
      
    
    {\displaystyle G(V,E)}
  , where every point 
  
    
      
        
          x
          
            i
          
        
        ∈
        S
      
    
    {\displaystyle x_{i}\in S}
   is uniquely associated with vertex 
  
    
      
        
          v
          
            i
          
        
        ∈
        V
      
    
    {\displaystyle v_{i}\in V}
  . The search of the point in the set S closest to the query q takes the form of the search of vertex in the graph 
  
    
      
        G
        (
        V
        ,
        E
        )
      
    
    {\displaystyle G(V,E)}
  . One of the basic vertex search algorithms in graphs with metric objects is the greedy search algorithm. It starts from the random vertex 
  
    
      
        
          v
          
            i
          
        
        ∈
        V
      
    
    {\displaystyle v_{i}\in V}
  . The algorithm computes a distance value from the query q to each vertex from the neighborhood 
  
    
      
        {
        
          v
          
            j
          
        
        :
        (
        
          v
          
            i
          
        
        ,
        
          v
          
            j
          
        
        )
        ∈
        E
        }
      
    
    {\displaystyle \{v_{j}:(v_{i},v_{j})\in E\}}
   of the current vertex 
  
    
      
        
          v
          
            i
          
        
      
    
    {\displaystyle v_{i}}
  , and then selects a vertex with the minimal distance value. If the distance value between the query and the selected vertex is smaller than the one between the query and the current element, then the algorithm moves to the selected vertex, and it becomes new current vertex. The algorithm stops when it reaches a local minimum: a vertex whose neighborhood does not contain a vertex that is closer to the query than the vertex itself. This idea was exploited in the VoroNet system for the plane, in the RayNet system for the 
  
    
      
        
          
            E
          
          
            n
          
        
      
    
    {\displaystyle \mathbb {E} ^{n}}
  , and for the general metric space in the Metrized Small World algorithm.


== Variants ==
There are numerous variants of the NNS problem and the two most well-known are the k-nearest neighbor search and the ε-approximate nearest neighbor search.


=== k-nearest neighbor ===
k-nearest neighbor search identifies the top k nearest neighbors to the query. This technique is commonly used in predictive analytics to estimate or classify a point based on the consensus of its neighbors. k-nearest neighbor graphs are graphs in which every point is connected to its k nearest neighbors.


=== Approximate nearest neighbor ===
In some applications it may be acceptable to retrieve a ""good guess"" of the nearest neighbor. In those cases, we can use an algorithm which doesn't guarantee to return the actual nearest neighbor in every case, in return for improved speed or memory savings. Often such an algorithm will find the nearest neighbor in a majority of cases, but this depends strongly on the dataset being queried.
Algorithms that support the approximate nearest neighbor search include locality-sensitive hashing, best bin first and balanced box-decomposition tree based search.


=== Nearest neighbor distance ratio ===
Nearest neighbor distance ratio do not apply the threshold on the direct distance from the original point to the challenger neighbor but on a ratio of it depending on the distance to the previous neighbor. It is used in CBIR to retrieve pictures through a ""query by example"" using the similarity between local features. More generally it is involved in several matching problems.


=== Fixed-radius near neighbors ===
Fixed-radius near neighbors is the problem where one wants to efficiently find all points given in Euclidean space within a given fixed distance from a specified point. The data structure should work on a distance which is fixed however the query point is arbitrary.


=== All nearest neighbors ===
For some applications (e.g. entropy estimation), we may have N data-points and wish to know which is the nearest neighbor for every one of those N points. This could of course be achieved by running a nearest-neighbor search once for every point, but an improved strategy would be an algorithm that exploits the information redundancy between these N queries to produce a more efficient search. As a simple example: when we find the distance from point X to point Y, that also tells us the distance from point Y to point X, so the same calculation can be reused in two different queries.
Given a fixed dimension, a semi-definite positive norm (thereby including every Lp norm), and n points in this space, the nearest neighbour of every point can be found in O(n log n) time and the m nearest neighbours of every point can be found in O(mn log n) time.


== See also ==


== Open source implementations ==
Accord.NET has C# implementation of KNN classifier
ALGLIB has C# and C++ implementations of nearest neighbor and approximate nearest neighbor algorithms
ANN library (LGPL license) implements exact and approximate NN search in C++
Annoy (Apache license) is an implementation in C++ with Python binding from Spotify
FLANN library (BSD license) implements special fast approximate NN algorithms for high-dimensional problems
dD Spatial Searching in CGAL – the Computational Geometry Algorithms Library
Non-Metric Space Library – An open source similarity search library containing realisations of various Nearest neighbor search methods.


== References ==


=== Citations ===


=== Sources ===
Andrews, L. (November 2001). ""A template for the nearest neighbor problem"". C/C++ Users Journal. 19 (11): 40–49. ISSN 1075-2838. 
Arya, S.; Mount, D.M.; Netanyahu, N. S.; Silverman, R.; Wu, A. Y. ""An Optimal Algorithm for Approximate Nearest Neighbor Searching in Fixed Dimensions"". Journal of the ACM. 45 (6): 891–923. doi:10.1145/293347.293348. 
Beyer, K.; Goldstein, J.; Ramakrishnan, R.; Shaft, U. (1999). ""When is nearest neighbor meaningful?"". Proceedings of the 7th ICDT. Jerusalem, Israel. 
Chen, Chung-Min; Ling, Yibei (2002). ""A Sampling-Based Estimator for Top-k Query"". ICDE: 617–627. 
Samet, H. (2006). Foundations of Multidimensional and Metric Data Structures. Morgan Kaufmann. ISBN 0-12-369446-9. 
Zezula, P.; Amato, G.; Dohnal, V.; Batko, M. (2006). Similarity Search – The Metric Space Approach. Springer. ISBN 0-387-29146-6. 


== Further reading ==
Shasha, Dennis (2004). High Performance Discovery in Time Series. Berlin: Springer. ISBN 0-387-00857-8. 


== External links ==
Nearest Neighbors and Similarity Search – a website dedicated to educational materials, software, literature, researchers, open problems and events related to NN searching. Maintained by Yury Lifshits
Similarity Search Wiki – a collection of links, people, ideas, keywords, papers, slides, code and data sets on nearest neighbours"
987,LZRW,11244014,956,"Lempel–Ziv Ross Williams (LZRW) refers to variants of the LZ77 lossless data compression algorithms with an emphasis on improving compression speed through the use of hash tables and other techniques. This family was explored by Ross Williams, who published a series of algorithms beginning with LZRW1 in 1991.
The variants are:
LZRW1
LZRW1-A
LZRW2
LZRW3
LZRW3-A
LZRW4
LZRW5
The LZJB algorithm used in ZFS is derived from LZRW1.


== Notes =="
988,Zero address arithmetic,13055219,955,"Zero address arithmetic is a feature of a few innovative computer architectures, whereby the assignment to a physical address space is deferred until programming statement execution time. It eliminates the link step of conventional compile and link architectures, and more generally relocation.
All Burroughs large systems and medium systems had this property, as do their modern day successors that preserve the original physical architecture.
The 1960 announcement of the English Electric KDF9 is the first announcement of a zero-address instruction format computer, rapidly followed by the Burroughs B5000.


== References =="
989,Pattern Oriented Rule Implementation,13339949,952,"The Pattern Oriented Rule Implementation (PORI) table is a data structure invented by Amdocs for representation of algorithms for determining a price or set of prices in a financial transaction. The PORI table is used to describe the pricing logic associated with the input event. The PORI table enables business analysts to build a language to describe the business logic associated with financial pricing. The pricing logic is composed without the need for programming, and transformed into executable code using the PORI Core Processor execution engine. The unique PORI representation, and the inference engine for executing the PORI rules, is used to compute a variety of pricing problems on the input event in real-time fashion."
990,Run to completion scheduling,37606787,951,"Run-to-completion scheduling is a scheduling model in which each task runs until it either finishes, or explicitly yields control back to the scheduler. Run to completion systems typically have an event queue which is serviced either in strict order of admission by an event loop, or by an admission scheduler which is capable of scheduling events out of order, based on other constraints such as deadlines.
Some preemptive multitasking scheduling systems behave as run-to-completion schedulers in regard to scheduling tasks at one particular process priority level, at the same time as those processes still preempt other lower priority tasks and are themselves preempted by higher priority tasks.


== See also ==
Deadline scheduling
Preemptive multitasking
Cooperative multitasking"
991,Control variable (programming),47790980,948,"A control variable in computer programming is a program variable that is used to regulate the flow of control of the program .


== Usage ==
A loop control variable is used to regulate the number of times the body of a program loop is executed; it is incremented (or decremented when counting down) each time the loop body is executed. A single control variable can identify the present state of a computer program.


== References =="
992,Comma code,49348672,935,"A comma code is a type of prefix-free code in which a comma, a particular symbol or sequence of symbols, occurs at the end of a code word and never occurs otherwise.
For example, Fibonacci coding is a comma code in which the comma is 11. 11 and 1011 are valid Fibonacci code words, but 101, 0111, and 11011 are not.


== Examples ==
Unary coding, in which the comma is 0.
Fibonacci coding, in which the comma is 11.


== See also ==
Self-synchronizing code


== References =="
993,Convergence (logic),32106812,935,"In mathematics, computer science and logic, convergence refers to the idea that different sequences of transformations come to a conclusion in a finite amount of time (the transformations are terminating), and that the conclusion reached is independent of the path taken to get to it (they are confluent).
More formally, a preordered set of term rewriting transformations are said to be convergent if they are confluent and terminating.


== See also ==
Logical equality
Logical equivalence
Rule of replacement


== References =="
994,Pulse computation,319106,935,"Pulse computation is a hybrid of digital and analog computation that uses aperiodic electrical spikes, as opposed to the periodic voltages in a digital computer or the continuously varying voltages in an analog computer. Pulse streams are unlocked, so they can arrive at arbitrary times and can be generated by analog processes, although each spike is allocated a binary value, as it would be in a digital computer.
Pulse computation is primarily studied as part of the field of neural networks. The processing unit in such a network is called a ""neuron"".


== References =="
995,Minger Email Address Verification Protocol,34820939,932,"Minger Email Address Verification Protocol is an Internet Engineering Task Force draft for lightweight verification of an e-mail address between trusted servers. It was created by Arvel Hathcock and Jonathan Merkel as a practical alternative to the Finger protocol or SMTP call-forward. The MDaemon e-mail server uses Minger to realize domain sharing over multiple servers with distributed mailboxes.
On February 3, 2010, draft 6 expired. On March 9, 2016, draft 7 was released, but it is available only within Alt-N MDaemon's documentation.


== References =="
996,Sstream,24784894,929,"In the C++ programming language, <sstream> is a part of the C++ Standard Library. It is a header file that provides templates and types that enable interoperation between stream buffers and string objects.


== References ==


== External links ==
C++ reference for std::stringstream"
997,Internet services technology,4320862,928,"Internet services technology is broad field of study usually resulting in receiving an Associate of Applied Science Degree. This two-year degree, often awarded at community colleges, is a gateway to more specialized studies but can also be applied to immediate workforce demands. Students learn languages such as HTML, C++, ActionScript, and JavaScript. This program of study also encompasses business courses with an emphasis on e-commerce and macroeconomics.
Internet services technology covers a broad range of technologies used for web development, web production, design, networking, and e-commerce. The field also covers internet programming, website maintenance, internet architect, and web master.It make our work easy."
998,Factory Interface Network Service,4311607,925,"FINS, Factory Interface Network Service, is a network protocol used by Omron PLCs, over different physical networks like Ethernet, Controller Link, DeviceNet and RS-232C.
The FINS communications service was developed by Omron to provide a consistent way for PLCs and computers on various networks to communicate. Compatible network types include Ethernet, Host Link, Controller Link, SYSMAC LINK, SYSMAC WAY, and Toolbus. FINS allows communications between nodes up to three network levels. A direct connection between a computer and a PLC via Host Link is not considered a network level.


== References ==
Omron FINS Ethernet"
999,Algebraic semantics (computer science),35728290,921,"In computer science, algebraic semantics is a form of axiomatic semantics based on algebraic laws for describing and reasoning about program semantics in a formal manner.


== See also ==
OBJ (programming language)
Joseph Goguen


== References ==
Eric G. Wagner (1995). ""Algebraic Semantic"". In Samson Abramsky; Dov M. Gabbay; Thomas S. E. Maibaum. Handbook of Logic in Computer Science: Semantic structures. 3. Clarendon Press. ISBN 9780198537625. 
Joseph Goguen; Grant Malcolm (1996). Algebraic semantics of imperative programs. MIT Press. ISBN 9780262071727."
1000,Memory-disk synchronization,2491732,909,"Memory-disk synchronisation is a process used in computers that immediately writes to disk any data queued for writing in volatile memory. Data is often held in this way for efficiency's sake, since writing to disk is a much slower process than writing to RAM. Disk synchronization is needed when the computer is going to be shut down, or occasionally if a particularly important bit of data has just been written.
In Unix-like systems, a disk synchronization may be requested by any user with the sync command.


== See also ==
mmap, a POSIX-compliant Unix system call that maps files or devices into memory
msync, a POSIX-compliant Unix system call that forcefully flush memory to disk and synchronize"
1001,Uniform consensus,18297485,907,"In computer science, Uniform consensus is a distributed computing problem that is a similar to the consensus problem with one more condition which is no two processes (whether faulty or not) decide differently.
More specifically one should consider this problem:
Each process has an input, should on decide an output (one-shot problem)
Uniform Agreement: every two decisions are the same
Validity: every decision is an input of one of the processes
Termination: eventually all correct processes decide


== References ==
Charron-Bost, Bernadette; Schiper, André (April 2004). ""Uniform consensus is harder than consensus"". Journal of Algorithms. 51 (1): 15–37. doi:10.1016/j.jalgor.2003.11.001."
1002,Thought vector,51399698,904,"Thought vector is a term popularized by Geoffrey Hinton, the prominent deep-learning researcher now at Google, which uses vectors based on natural language to improve its search results.


== References =="
1003,Cepstral mean and variance normalization,43940687,897,"Cepstral mean and variance normalization (CMVN) is a computationally efficient normalization technique for robust speech recognition. The performance of CMVN is known to degrade for short utterances. This is due to insufficient data for parameter estimation and loss of discriminable information as all utterances are forced to have zero mean and unit variance.


== References =="
1004,Single pushout graph rewriting,51724747,894,"In computer science, single pushout graph rewriting or SPO graph rewriting refers to a mathematical framework for graph rewriting, and is used in contrast to the double-pushout approach of graph rewriting.


== References ==


== Further reading ==
Ehrig, H.; R. Heckel; M. Korff; M. Löwe; L. Ribeiro; A. Wagner; A. Corradini (1997). ""Chapter 4. Algebraic approaches to graph transformation. Part II: single pushout approach and comparison with double pushout approach"". In Grzegorz Rozenberg. Handbook of Graph Grammars and Computing by Graph Transformation. World Scientific. pp. 247–312. ISBN 978-981-238-472-0. CS1 maint: Multiple names: authors list (link)"
