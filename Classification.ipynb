{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "%matplotlib inline  \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the wikipedia pages articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "politics_data = pd.read_csv(\"politics_full.csv\")\n",
    "sports_data = pd.read_csv(\"sports_full.csv\")\n",
    "history_data = pd.read_csv(\"history_full.csv\")\n",
    "culture_data = pd.read_csv(\"culture_full.csv\")\n",
    "comp_science_data = pd.read_csv(\"comp_science_full.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping different categories to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_dict = {\n",
    "    1: 'politics',\n",
    "    2: 'sports',\n",
    "    3: 'history',\n",
    "    4: 'culture',\n",
    "    5: 'computer_science'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing newlines and punctuations from raw content of the pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = str.maketrans('', '', string.punctuation)\n",
    "politics_data['content'] = politics_data['content'].map(lambda x: re.sub('\\n', ' ', x))\\\n",
    ".map(lambda x: x.translate(translator))\n",
    "politics_data['category'] = [1]* len(politics_data)\n",
    "\n",
    "sports_data['content'] = sports_data['content'].map(lambda x: re.sub('\\n', ' ', x))\\\n",
    ".map(lambda x: x.translate(translator))\n",
    "sports_data['category'] = [2]* len(sports_data)\n",
    "\n",
    "history_data['content'] = history_data['content'].map(lambda x: re.sub('\\n', ' ', x))\\\n",
    ".map(lambda x: x.translate(translator))\n",
    "history_data['category'] = [3]* len(history_data)\n",
    "\n",
    "culture_data['content'] = culture_data['content'].map(lambda x: re.sub('\\n', ' ', x))\\\n",
    ".map(lambda x: x.translate(translator))\n",
    "culture_data['category'] = [4]* len(culture_data)\n",
    "\n",
    "comp_science_data['content'] = comp_science_data['content'].map(lambda x: re.sub('\\n', ' ', x))\\\n",
    ".map(lambda x: x.translate(translator))\n",
    "comp_science_data['category'] = [5]* len(comp_science_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>pageid</th>\n",
       "      <th>length</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Artificial intelligence</td>\n",
       "      <td>1164</td>\n",
       "      <td>231620</td>\n",
       "      <td>Artificial intelligence AI also machine intell...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Comparison of programming languages (string fu...</td>\n",
       "      <td>3681422</td>\n",
       "      <td>109570</td>\n",
       "      <td>String functions are used in computer programm...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Geographic information system</td>\n",
       "      <td>12398</td>\n",
       "      <td>77692</td>\n",
       "      <td>A geographic information system GIS is a syste...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Computational creativity</td>\n",
       "      <td>16300571</td>\n",
       "      <td>61153</td>\n",
       "      <td>Computational creativity also known as artific...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Computational phylogenetics</td>\n",
       "      <td>3986130</td>\n",
       "      <td>58742</td>\n",
       "      <td>Computational phylogenetics is the application...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title    pageid  \\\n",
       "0           0                            Artificial intelligence      1164   \n",
       "1           1  Comparison of programming languages (string fu...   3681422   \n",
       "2           2                      Geographic information system     12398   \n",
       "3           3                           Computational creativity  16300571   \n",
       "4           4                        Computational phylogenetics   3986130   \n",
       "\n",
       "   length                                            content  category  \n",
       "0  231620  Artificial intelligence AI also machine intell...         5  \n",
       "1  109570  String functions are used in computer programm...         5  \n",
       "2   77692  A geographic information system GIS is a syste...         5  \n",
       "3   61153  Computational creativity also known as artific...         5  \n",
       "4   58742  Computational phylogenetics is the application...         5  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_science_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Union the 5 dataframes and make a new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#union data frames\n",
    "df_list = [politics_data, sports_data, history_data, culture_data, comp_science_data]\n",
    "full_df = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffling the rows of dataframe\n",
    "full_df = full_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = list(full_df['content'])\n",
    "targets = np.array(full_df['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "making the TF IDF matrix of the documents. We choose to remove the words that appear in less than 5 documnets in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 31700)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', min_df=5)\n",
    "#vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(contents)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the resulting matrix is relatively sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of zeros: 1505991\n",
      "301.1982 average zero per data sample\n"
     ]
    }
   ],
   "source": [
    "print('total number of zeros:', vectors.nnz)\n",
    "print('{} average zero per data sample'.format(vectors.nnz / float(vectors.shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is desirable to have equal number of samples in both training and test corpus. So we first shuffle the indicies for each of the classes and then we pick 90% data from each of the classes for training and 10% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicies_1 = [i for i,x in enumerate(targets) if x == 1]\n",
    "np.random.shuffle(indicies_1)\n",
    "indicies_2 = [i for i,x in enumerate(targets) if x == 2]\n",
    "np.random.shuffle(indicies_2)\n",
    "indicies_3 = [i for i,x in enumerate(targets) if x == 3]\n",
    "np.random.shuffle(indicies_3)\n",
    "indicies_4 = [i for i,x in enumerate(targets) if x == 4]\n",
    "np.random.shuffle(indicies_4)\n",
    "indicies_5 = [i for i,x in enumerate(targets) if x == 5]\n",
    "np.random.shuffle(indicies_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.9\n",
    "train_indicies = indicies_1[0:int(ratio*len(indicies_1))]\\\n",
    "+indicies_2[0:int(ratio*len(indicies_2))]+indicies_3[0:int(ratio*len(indicies_3))]\\\n",
    "+indicies_4[0:int(ratio*len(indicies_4))]+indicies_5[0:int(ratio*len(indicies_5))]\n",
    "np.random.shuffle(train_indicies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indicies = indicies_1[int(ratio*len(indicies_1)):]\\\n",
    "+indicies_2[int(ratio*len(indicies_2)):]+indicies_3[int(ratio*len(indicies_3)):]\\\n",
    "+indicies_4[int(ratio*len(indicies_4)):]+indicies_5[int(ratio*len(indicies_5)):]\n",
    "np.random.shuffle(test_indicies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "final training and test vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors = vectors[train_indicies]\n",
    "test_vectors = vectors[test_indicies]\n",
    "#------------\n",
    "train_targets = targets[train_indicies]\n",
    "test_targets = targets[test_indicies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00', '000', '0000', '001', '007']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "feature_names[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fisrt we choose to train a random forest classifier. Random forset is bassically a collection of simple models (decision trees), so as we increase the number of trees the model becomes more complex and hence tends to overfit.\n",
    "\n",
    "In the first try we fit a random forest model with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RFclassifier = RandomForestClassifier(n_jobs=-1)\n",
    "RFclassifier.fit(train_vectors,train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res = RFclassifier.predict(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 74.6%\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.mean([test_res[i]==test_targets[i] for i in range(len(test_targets))])\n",
    "print('accuracy = {}%'.format(accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning for Random Forest parameters for getting better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'n_estimators': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000], 'max_depth': [50, 60, 70, 80, 90]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_estimator = [i for i in range(100,1100,100)]\n",
    "max_depth = [i for i in range(50,100,10)]\n",
    "clf = GridSearchCV(estimator=RFclassifier, param_grid={'n_estimators':n_estimator,'max_depth':max_depth},n_jobs=-1)\n",
    "clf.fit(train_vectors,train_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe below shows the best train and test score and their associated parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajalloei/.conda/envs/ahmad_virtualenv/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/ajalloei/.conda/envs/ahmad_virtualenv/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/ajalloei/.conda/envs/ahmad_virtualenv/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/ajalloei/.conda/envs/ahmad_virtualenv/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/ajalloei/.conda/envs/ahmad_virtualenv/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>163.670287</td>\n",
       "      <td>7.145987</td>\n",
       "      <td>0.821556</td>\n",
       "      <td>0.990333</td>\n",
       "      <td>70</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'max_depth': 70, 'n_estimators': 1000}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.814667</td>\n",
       "      <td>0.992333</td>\n",
       "      <td>0.823333</td>\n",
       "      <td>0.990333</td>\n",
       "      <td>0.826667</td>\n",
       "      <td>0.988333</td>\n",
       "      <td>11.686875</td>\n",
       "      <td>1.077985</td>\n",
       "      <td>0.005058</td>\n",
       "      <td>0.001633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>125.858999</td>\n",
       "      <td>6.559073</td>\n",
       "      <td>0.821111</td>\n",
       "      <td>0.989222</td>\n",
       "      <td>50</td>\n",
       "      <td>900</td>\n",
       "      <td>{'max_depth': 50, 'n_estimators': 900}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.813333</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>0.825333</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.824667</td>\n",
       "      <td>0.986000</td>\n",
       "      <td>1.134310</td>\n",
       "      <td>0.123111</td>\n",
       "      <td>0.005506</td>\n",
       "      <td>0.002378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>150.759535</td>\n",
       "      <td>7.794189</td>\n",
       "      <td>0.821111</td>\n",
       "      <td>0.989111</td>\n",
       "      <td>50</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'max_depth': 50, 'n_estimators': 1000}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.814667</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>0.823333</td>\n",
       "      <td>0.989667</td>\n",
       "      <td>0.825333</td>\n",
       "      <td>0.986000</td>\n",
       "      <td>7.780510</td>\n",
       "      <td>0.247463</td>\n",
       "      <td>0.004629</td>\n",
       "      <td>0.002347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>119.400220</td>\n",
       "      <td>5.967087</td>\n",
       "      <td>0.821111</td>\n",
       "      <td>0.989778</td>\n",
       "      <td>60</td>\n",
       "      <td>700</td>\n",
       "      <td>{'max_depth': 60, 'n_estimators': 700}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.814000</td>\n",
       "      <td>0.992000</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.990333</td>\n",
       "      <td>0.819333</td>\n",
       "      <td>0.987000</td>\n",
       "      <td>5.114454</td>\n",
       "      <td>0.159478</td>\n",
       "      <td>0.006652</td>\n",
       "      <td>0.002079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>157.372489</td>\n",
       "      <td>7.499259</td>\n",
       "      <td>0.820889</td>\n",
       "      <td>0.989778</td>\n",
       "      <td>60</td>\n",
       "      <td>900</td>\n",
       "      <td>{'max_depth': 60, 'n_estimators': 900}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.814667</td>\n",
       "      <td>0.992000</td>\n",
       "      <td>0.826667</td>\n",
       "      <td>0.990333</td>\n",
       "      <td>0.821333</td>\n",
       "      <td>0.987000</td>\n",
       "      <td>2.454800</td>\n",
       "      <td>0.460805</td>\n",
       "      <td>0.004909</td>\n",
       "      <td>0.002079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "29     163.670287         7.145987         0.821556          0.990333   \n",
       "8      125.858999         6.559073         0.821111          0.989222   \n",
       "9      150.759535         7.794189         0.821111          0.989111   \n",
       "16     119.400220         5.967087         0.821111          0.989778   \n",
       "18     157.372489         7.499259         0.820889          0.989778   \n",
       "\n",
       "   param_max_depth param_n_estimators  \\\n",
       "29              70               1000   \n",
       "8               50                900   \n",
       "9               50               1000   \n",
       "16              60                700   \n",
       "18              60                900   \n",
       "\n",
       "                                     params  rank_test_score  \\\n",
       "29  {'max_depth': 70, 'n_estimators': 1000}                1   \n",
       "8    {'max_depth': 50, 'n_estimators': 900}                2   \n",
       "9   {'max_depth': 50, 'n_estimators': 1000}                2   \n",
       "16   {'max_depth': 60, 'n_estimators': 700}                2   \n",
       "18   {'max_depth': 60, 'n_estimators': 900}                5   \n",
       "\n",
       "    split0_test_score  split0_train_score  split1_test_score  \\\n",
       "29           0.814667            0.992333           0.823333   \n",
       "8            0.813333            0.991667           0.825333   \n",
       "9            0.814667            0.991667           0.823333   \n",
       "16           0.814000            0.992000           0.830000   \n",
       "18           0.814667            0.992000           0.826667   \n",
       "\n",
       "    split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
       "29            0.990333           0.826667            0.988333     11.686875   \n",
       "8             0.990000           0.824667            0.986000      1.134310   \n",
       "9             0.989667           0.825333            0.986000      7.780510   \n",
       "16            0.990333           0.819333            0.987000      5.114454   \n",
       "18            0.990333           0.821333            0.987000      2.454800   \n",
       "\n",
       "    std_score_time  std_test_score  std_train_score  \n",
       "29        1.077985        0.005058         0.001633  \n",
       "8         0.123111        0.005506         0.002378  \n",
       "9         0.247463        0.004629         0.002347  \n",
       "16        0.159478        0.006652         0.002079  \n",
       "18        0.460805        0.004909         0.002079  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(clf.cv_results_)\n",
    "df.sort_values('rank_test_score',inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=70, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_RFclassifier = RandomForestClassifier(n_estimators=1000, max_depth=70, n_jobs=-1)\n",
    "best_RFclassifier.fit(train_vectors,train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for best RF classifier = 82.8%\n"
     ]
    }
   ],
   "source": [
    "print('accuracy for best RF classifier = {}%'.format(best_RFclassifier.score(test_vectors, test_targets)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second classifier is Multinomial Naive Classifier which is relatively a simple model and the learning time is quiet fast. It is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work. (http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB().fit(train_vectors, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for Multinomial Naive Bayes classifier = 84.0%\n"
     ]
    }
   ],
   "source": [
    "print('accuracy for Multinomial Naive Bayes classifier = {}%'.format(clf.score(test_vectors, test_targets)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning $\\alpha $ parameter for the classifier. This is basically the smoothing parameter for the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'alpha': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = [i/10 for i in range(1,20,1)]\n",
    "clf = GridSearchCV(estimator=MultinomialNB(), param_grid={'alpha':alpha},n_jobs=-1, cv=10, return_train_score=True)\n",
    "clf.fit(train_vectors,train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split7_train_score</th>\n",
       "      <th>split8_test_score</th>\n",
       "      <th>split8_train_score</th>\n",
       "      <th>split9_test_score</th>\n",
       "      <th>split9_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.337634</td>\n",
       "      <td>0.010722</td>\n",
       "      <td>0.845778</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.2</td>\n",
       "      <td>{'alpha': 0.2}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.853333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.853333</td>\n",
       "      <td>0.931605</td>\n",
       "      <td>0.831111</td>\n",
       "      <td>0.934074</td>\n",
       "      <td>0.851111</td>\n",
       "      <td>0.933086</td>\n",
       "      <td>0.023646</td>\n",
       "      <td>0.002840</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.001059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.236525</td>\n",
       "      <td>0.008792</td>\n",
       "      <td>0.843333</td>\n",
       "      <td>0.940889</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'alpha': 0.1}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.940741</td>\n",
       "      <td>0.851111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.848889</td>\n",
       "      <td>0.938272</td>\n",
       "      <td>0.831111</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.848889</td>\n",
       "      <td>0.940247</td>\n",
       "      <td>0.105323</td>\n",
       "      <td>0.004253</td>\n",
       "      <td>0.010669</td>\n",
       "      <td>0.001646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.307943</td>\n",
       "      <td>0.011466</td>\n",
       "      <td>0.842889</td>\n",
       "      <td>0.927333</td>\n",
       "      <td>0.3</td>\n",
       "      <td>{'alpha': 0.3}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.927654</td>\n",
       "      <td>0.842222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.855556</td>\n",
       "      <td>0.925679</td>\n",
       "      <td>0.824444</td>\n",
       "      <td>0.927901</td>\n",
       "      <td>0.842222</td>\n",
       "      <td>0.927654</td>\n",
       "      <td>0.019358</td>\n",
       "      <td>0.002061</td>\n",
       "      <td>0.012998</td>\n",
       "      <td>0.001018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.295297</td>\n",
       "      <td>0.012531</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.922123</td>\n",
       "      <td>0.4</td>\n",
       "      <td>{'alpha': 0.4}</td>\n",
       "      <td>4</td>\n",
       "      <td>0.862222</td>\n",
       "      <td>0.923951</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.853333</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.817778</td>\n",
       "      <td>0.922222</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.923210</td>\n",
       "      <td>0.023494</td>\n",
       "      <td>0.001517</td>\n",
       "      <td>0.013481</td>\n",
       "      <td>0.001750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.229445</td>\n",
       "      <td>0.008891</td>\n",
       "      <td>0.838222</td>\n",
       "      <td>0.917654</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'alpha': 0.5}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.851111</td>\n",
       "      <td>0.918519</td>\n",
       "      <td>0.835556</td>\n",
       "      <td>...</td>\n",
       "      <td>0.851111</td>\n",
       "      <td>0.915802</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.918272</td>\n",
       "      <td>0.831111</td>\n",
       "      <td>0.919012</td>\n",
       "      <td>0.091586</td>\n",
       "      <td>0.003984</td>\n",
       "      <td>0.010968</td>\n",
       "      <td>0.001185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "1       0.337634         0.010722         0.845778          0.933333   \n",
       "0       0.236525         0.008792         0.843333          0.940889   \n",
       "2       0.307943         0.011466         0.842889          0.927333   \n",
       "3       0.295297         0.012531         0.840000          0.922123   \n",
       "4       0.229445         0.008891         0.838222          0.917654   \n",
       "\n",
       "  param_alpha          params  rank_test_score  split0_test_score  \\\n",
       "1         0.2  {'alpha': 0.2}                1           0.866667   \n",
       "0         0.1  {'alpha': 0.1}                2           0.860000   \n",
       "2         0.3  {'alpha': 0.3}                3           0.866667   \n",
       "3         0.4  {'alpha': 0.4}                4           0.862222   \n",
       "4         0.5  {'alpha': 0.5}                5           0.851111   \n",
       "\n",
       "   split0_train_score  split1_test_score       ...         split7_test_score  \\\n",
       "1            0.933333           0.853333       ...                  0.853333   \n",
       "0            0.940741           0.851111       ...                  0.848889   \n",
       "2            0.927654           0.842222       ...                  0.855556   \n",
       "3            0.923951           0.833333       ...                  0.853333   \n",
       "4            0.918519           0.835556       ...                  0.851111   \n",
       "\n",
       "   split7_train_score  split8_test_score  split8_train_score  \\\n",
       "1            0.931605           0.831111            0.934074   \n",
       "0            0.938272           0.831111            0.940000   \n",
       "2            0.925679           0.824444            0.927901   \n",
       "3            0.920000           0.817778            0.922222   \n",
       "4            0.915802           0.820000            0.918272   \n",
       "\n",
       "   split9_test_score  split9_train_score  std_fit_time  std_score_time  \\\n",
       "1           0.851111            0.933086      0.023646        0.002840   \n",
       "0           0.848889            0.940247      0.105323        0.004253   \n",
       "2           0.842222            0.927654      0.019358        0.002061   \n",
       "3           0.833333            0.923210      0.023494        0.001517   \n",
       "4           0.831111            0.919012      0.091586        0.003984   \n",
       "\n",
       "   std_test_score  std_train_score  \n",
       "1        0.012500         0.001059  \n",
       "0        0.010669         0.001646  \n",
       "2        0.012998         0.001018  \n",
       "3        0.013481         0.001750  \n",
       "4        0.010968         0.001185  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(clf.cv_results_)\n",
    "df.sort_values('rank_test_score',inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for best Multinomial Naive Bayes classifier = 84.8%\n"
     ]
    }
   ],
   "source": [
    "best_NB = MultinomialNB(alpha=0.2).fit(train_vectors, train_targets)\n",
    "print('accuracy for best Multinomial Naive Bayes classifier = {}%'.format(best_NB.score(test_vectors, test_targets)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a multiclass Logestic Regression for the task. We use the Logistic Regression cross validation function. The rgularization parameter will vary between $10^{-4}$ and $10^{4}$. We choose to have 20 different values in this interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=20, class_weight=None, cv=None, dual=False,\n",
       "           fit_intercept=True, intercept_scaling=1.0, max_iter=100,\n",
       "           multi_class='multinomial', n_jobs=-1, penalty='l2',\n",
       "           random_state=None, refit=True, scoring=None, solver='lbfgs',\n",
       "           tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegressionCV(Cs=20, multi_class='multinomial', n_jobs=-1)\n",
    "clf.fit(train_vectors, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for best Logistic Regression classifier = 87.8%\n"
     ]
    }
   ],
   "source": [
    "print('accuracy for best Logistic Regression classifier = {}%'.format(clf.score(test_vectors, test_targets)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "scors = np.concatenate((np.mean(clf.scores_[1], axis = 0), np.mean(clf.scores_[2], axis = 0)\n",
    "                        , np.mean(clf.scores_[3], axis = 0), np.mean(clf.scores_[4], axis = 0)\n",
    "                        , np.mean(clf.scores_[5], axis = 0)), axis=0)\n",
    "scors = scors.reshape((5,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph below shows the best regularizer value that gives the best accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f365c1506a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(clf.Cs_, np.mean(scors, axis=0))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res = clf.predict(test_vectors)\n",
    "cnf_matrix = confusion_matrix(test_targets, test_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f35dc091f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(category_dict))\n",
    "    plt.xticks(tick_marks, category_dict.values(), rotation=90,fontsize=12)\n",
    "    plt.yticks(tick_marks, category_dict.values(), fontsize=12)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label',fontsize = 15)\n",
    "    plt.xlabel('Predicted label',fontsize = 15)\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_confusion_matrix(cnf_matrix, classes=newsgroups.target_names,\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix show wich classes where misclassified with each other. As expected similar categories (culture, politics and history) were misclassified mostly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP classifier\n",
    "The next classifier is a multi layer perceptron neural network. MLP is a feedforward artificial neural network and consists of at least three layers of nodes. Except for the input nodes, each node is a neuron that uses a nonlinear activation function. MLP utilizes a supervised learning technique called backpropagation for training. hrefIt can distinguish data that is not linearly separable. (<a href=\"https://en.wikipedia.org/wiki/Multilayer_perceptron\">ref</a>)\n",
    "\n",
    "Training MLP could be time consuming and therefore i choosed to tune only the tuning parameter ($\\alpha$) for this classifier. All other parameters are the default ones from sklearn. (i.e. network has only one hidden layer with 100 neurons) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.52194577\n",
      "Iteration 1, loss = 1.52083370\n",
      "Iteration 1, loss = 3.22521775\n",
      "Iteration 1, loss = 163.68572051\n",
      "Iteration 1, loss = 1.51941495\n",
      "Iteration 1, loss = 3.22488165\n",
      "Iteration 1, loss = 1.55507851\n",
      "Iteration 1, loss = 1.55658394\n",
      "Iteration 1, loss = 163.68573255\n",
      "Iteration 1, loss = 1.55362878\n",
      "Iteration 1, loss = 1.52120972\n",
      "Iteration 1, loss = 1.51978456\n",
      "Iteration 1, loss = 1.52235540\n",
      "Iteration 1, loss = 163.68556286\n",
      "Iteration 1, loss = 3.22502684\n",
      "Iteration 2, loss = 38.62316338\n",
      "Iteration 2, loss = 1.22633366\n",
      "Iteration 2, loss = 1.22408381\n",
      "Iteration 2, loss = 38.62312719\n",
      "Iteration 2, loss = 1.95633527\n",
      "Iteration 3, loss = 19.83151175\n",
      "Iteration 2, loss = 1.95590119\n",
      "Iteration 2, loss = 1.28546733\n",
      "Iteration 2, loss = 1.22149688\n",
      "Iteration 2, loss = 1.28220302\n",
      "Iteration 2, loss = 38.62307878\n",
      "Iteration 2, loss = 1.28945893\n",
      "Iteration 2, loss = 1.22482482\n",
      "Iteration 2, loss = 1.22231785\n",
      "Iteration 2, loss = 1.22716796\n",
      "Iteration 3, loss = 0.93246572\n",
      "Iteration 2, loss = 1.95516890\n",
      "Iteration 4, loss = 14.77340628\n",
      "Iteration 3, loss = 0.93041753\n",
      "Iteration 3, loss = 1.77200934\n",
      "Iteration 3, loss = 19.83143819\n",
      "Iteration 3, loss = 1.05199069\n",
      "Iteration 3, loss = 1.77221964\n",
      "Iteration 5, loss = 11.67399779\n",
      "Iteration 3, loss = 0.92856452\n",
      "Iteration 3, loss = 1.05685158\n",
      "Iteration 3, loss = 1.04954173\n",
      "Iteration 4, loss = 1.72574906\n",
      "Iteration 3, loss = 19.83156716\n",
      "Iteration 3, loss = 0.93437258\n",
      "Iteration 3, loss = 0.93222538\n",
      "Iteration 3, loss = 0.93046152\n",
      "Iteration 4, loss = 0.67930570\n",
      "Iteration 3, loss = 1.77095939\n",
      "Iteration 6, loss = 9.58875718\n",
      "Iteration 4, loss = 0.68206744\n",
      "Iteration 5, loss = 1.69890307\n",
      "Iteration 4, loss = 0.86469905\n",
      "Iteration 4, loss = 14.77334575\n",
      "Iteration 4, loss = 1.72585866\n",
      "Iteration 7, loss = 7.97606778\n",
      "Iteration 4, loss = 0.67842977\n",
      "Iteration 4, loss = 0.87036502\n",
      "Iteration 4, loss = 14.77335779\n",
      "Iteration 4, loss = 0.68532328\n",
      "Iteration 6, loss = 1.68145652\n",
      "Iteration 4, loss = 0.86447897\n",
      "Iteration 4, loss = 0.68248175\n",
      "Iteration 5, loss = 0.48696183\n",
      "Iteration 4, loss = 0.68165825\n",
      "Iteration 4, loss = 1.72497828\n",
      "Iteration 8, loss = 6.67648671\n",
      "Iteration 5, loss = 0.49219323\n",
      "Iteration 5, loss = 11.67397221\n",
      "Iteration 5, loss = 0.72447196\n",
      "Iteration 7, loss = 1.66785855\n",
      "Iteration 5, loss = 1.69889174\n",
      "Iteration 9, loss = 5.62359515\n",
      "Iteration 5, loss = 11.67394231\n",
      "Iteration 5, loss = 0.48854204\n",
      "Iteration 5, loss = 0.73120234\n",
      "Iteration 5, loss = 0.72737180\n",
      "Iteration 5, loss = 0.49681571\n",
      "Iteration 5, loss = 0.49153320\n",
      "Iteration 5, loss = 0.49336571\n",
      "Iteration 6, loss = 0.35255590\n",
      "Iteration 6, loss = 9.58875581\n",
      "Iteration 8, loss = 1.65683346\n",
      "Iteration 5, loss = 1.69834358\n",
      "Iteration 10, loss = 4.77250272\n",
      "Iteration 6, loss = 0.62543315\n",
      "Iteration 6, loss = 0.35961784\n",
      "Iteration 11, loss = 4.08764811\n",
      "Iteration 6, loss = 0.63272544\n",
      "Iteration 9, loss = 1.64792201\n",
      "Iteration 7, loss = 7.97606414\n",
      "Iteration 6, loss = 9.58876301\n",
      "Iteration 6, loss = 1.68161439\n",
      "Iteration 6, loss = 0.35473228\n",
      "Iteration 6, loss = 0.35841424\n",
      "Iteration 6, loss = 0.36550676\n",
      "Iteration 6, loss = 0.63001455\n",
      "Iteration 12, loss = 3.53956634\n",
      "Iteration 10, loss = 1.64057752\n",
      "Iteration 7, loss = 0.26015971\n",
      "Iteration 6, loss = 0.36146020\n",
      "Iteration 6, loss = 1.68124892\n",
      "Iteration 8, loss = 6.67648713\n",
      "Iteration 7, loss = 0.55436280\n",
      "Iteration 7, loss = 0.26851874\n",
      "Iteration 11, loss = 1.63457786\n",
      "Iteration 13, loss = 3.10353278\n",
      "Iteration 7, loss = 0.56267653\n",
      "Iteration 7, loss = 7.97608188\n",
      "Iteration 7, loss = 1.66816119\n",
      "Iteration 7, loss = 0.27554700\n",
      "Iteration 7, loss = 0.26336238\n",
      "Iteration 12, loss = 1.62974728\n",
      "Iteration 7, loss = 0.56019604\n",
      "Iteration 7, loss = 0.26714927\n",
      "Iteration 7, loss = 0.27105928\n",
      "Iteration 14, loss = 2.75884337\n",
      "Iteration 8, loss = 0.19716401\n",
      "Iteration 9, loss = 5.62359805\n",
      "Iteration 7, loss = 1.66774761\n",
      "Iteration 8, loss = 0.50273447\n",
      "Iteration 8, loss = 0.20586401\n",
      "Iteration 13, loss = 1.62569045\n",
      "Iteration 8, loss = 0.51171578\n",
      "Iteration 15, loss = 2.48807481\n",
      "Iteration 8, loss = 6.67650296\n",
      "Iteration 8, loss = 0.21388915\n",
      "Iteration 8, loss = 1.65713782\n",
      "Iteration 8, loss = 0.20138466\n",
      "Iteration 14, loss = 1.62245653\n",
      "Iteration 8, loss = 0.20512528\n",
      "Iteration 16, loss = 2.27678636\n",
      "Iteration 8, loss = 0.20980401\n",
      "Iteration 8, loss = 1.65693969\n",
      "Iteration 9, loss = 0.15334176\n",
      "Iteration 8, loss = 0.50962503\n",
      "Iteration 10, loss = 4.77248700\n",
      "Iteration 15, loss = 1.61986036\n",
      "Iteration 9, loss = 0.46332554\n",
      "Iteration 9, loss = 0.16219214\n",
      "Iteration 17, loss = 2.11301237\n",
      "Iteration 9, loss = 0.47267766\n",
      "Iteration 9, loss = 0.17105584\n",
      "Iteration 9, loss = 5.62362447\n",
      "Iteration 9, loss = 1.64821727\n",
      "Iteration 16, loss = 1.61777858\n",
      "Iteration 18, loss = 1.98695361\n",
      "Iteration 9, loss = 0.15763242\n",
      "Iteration 10, loss = 0.12190294\n",
      "Iteration 9, loss = 1.64808024\n",
      "Iteration 9, loss = 0.16213854\n",
      "Iteration 9, loss = 0.16672303\n",
      "Iteration 9, loss = 0.47086808\n",
      "Iteration 17, loss = 1.61610974\n",
      "Iteration 11, loss = 4.08764822\n",
      "Iteration 19, loss = 1.89054452\n",
      "Iteration 10, loss = 0.43202050\n",
      "Iteration 10, loss = 0.13025518\n",
      "Iteration 10, loss = 0.12645378\n",
      "Iteration 10, loss = 0.44157909\n",
      "Iteration 10, loss = 0.13985176\n",
      "Iteration 10, loss = 1.64071182\n",
      "Iteration 18, loss = 1.61477929\n",
      "Iteration 10, loss = 4.77250881\n",
      "Iteration 20, loss = 1.81738823\n",
      "Iteration 11, loss = 0.10335417\n",
      "Iteration 11, loss = 0.09918802\n",
      "Iteration 10, loss = 0.13613482\n",
      "Iteration 10, loss = 1.64067754\n",
      "Iteration 11, loss = 0.10713756\n",
      "Iteration 10, loss = 0.13140872\n",
      "Iteration 10, loss = 0.44010248\n",
      "Iteration 19, loss = 1.61363228\n",
      "Iteration 12, loss = 3.53955951\n",
      "Iteration 21, loss = 1.76221996\n",
      "Iteration 11, loss = 0.40714474\n",
      "Iteration 12, loss = 0.08643753\n",
      "Iteration 11, loss = 0.41712805\n",
      "Iteration 20, loss = 1.61282275\n",
      "Iteration 11, loss = 0.11735512\n",
      "Iteration 11, loss = 1.63462469\n",
      "Iteration 11, loss = 4.08762142\n",
      "Iteration 22, loss = 1.72093886\n",
      "Iteration 12, loss = 0.08218991\n",
      "Iteration 12, loss = 0.08961791\n",
      "Iteration 11, loss = 0.11357375\n",
      "Iteration 13, loss = 0.07338963\n",
      "Iteration 11, loss = 1.63468993\n",
      "Iteration 21, loss = 1.61211307\n",
      "Iteration 11, loss = 0.41492202\n",
      "Iteration 11, loss = 0.10930108\n",
      "Iteration 13, loss = 3.10357709\n",
      "Iteration 23, loss = 1.69027024\n",
      "Iteration 12, loss = 0.38601789\n",
      "Iteration 12, loss = 0.39629847\n",
      "Iteration 14, loss = 0.06351275\n",
      "Iteration 12, loss = 0.10038014\n",
      "Iteration 12, loss = 1.62980119\n",
      "Iteration 24, loss = 1.66764250\n",
      "Iteration 12, loss = 3.53956515\n",
      "Iteration 22, loss = 1.61156236\n",
      "Iteration 13, loss = 0.06961431\n",
      "Iteration 12, loss = 0.09714040\n",
      "Iteration 13, loss = 0.07617989\n",
      "Iteration 15, loss = 0.05558696\n",
      "Iteration 12, loss = 0.39378094\n",
      "Iteration 12, loss = 1.62980349\n",
      "Iteration 25, loss = 1.65106127\n",
      "Iteration 12, loss = 0.09282287\n",
      "Iteration 14, loss = 2.75885999\n",
      "Iteration 23, loss = 1.61114360\n",
      "Iteration 13, loss = 0.36853658\n",
      "Iteration 13, loss = 0.37886645\n",
      "Iteration 13, loss = 0.08739562\n",
      "Iteration 26, loss = 1.63900500\n",
      "Iteration 13, loss = 1.62576008\n",
      "Iteration 24, loss = 1.61082515\n",
      "Iteration 16, loss = 0.04931160\n",
      "Iteration 14, loss = 0.06003641\n",
      "Iteration 13, loss = 3.10355412\n",
      "Iteration 13, loss = 0.08452301\n",
      "Iteration 13, loss = 1.62590054\n",
      "Iteration 27, loss = 1.63029878\n",
      "Iteration 14, loss = 0.06616604\n",
      "Iteration 13, loss = 0.37630818\n",
      "Iteration 13, loss = 0.08069169\n",
      "Iteration 25, loss = 1.61056372\n",
      "Iteration 14, loss = 0.35375084\n",
      "Iteration 17, loss = 0.04444654\n",
      "Iteration 14, loss = 0.36383685\n",
      "Iteration 14, loss = 0.07777237\n",
      "Iteration 15, loss = 2.48808379\n",
      "Iteration 28, loss = 1.62405364\n",
      "Iteration 26, loss = 1.61034199\n",
      "Iteration 14, loss = 1.62246201\n",
      "Iteration 15, loss = 0.05200392\n",
      "Iteration 14, loss = 2.75883742\n",
      "Iteration 18, loss = 0.04039837\n",
      "Iteration 14, loss = 0.07502030\n",
      "Iteration 14, loss = 1.62260449\n",
      "Iteration 14, loss = 0.36149310\n",
      "Iteration 29, loss = 1.61961831\n",
      "Iteration 27, loss = 1.61017961\n",
      "Iteration 15, loss = 0.05790179\n",
      "Iteration 15, loss = 0.35052572\n",
      "Iteration 14, loss = 0.07150146\n",
      "Iteration 15, loss = 0.34027200\n",
      "Iteration 16, loss = 2.27680466\n",
      "Iteration 15, loss = 0.06985164\n",
      "Iteration 30, loss = 1.61647820\n",
      "Iteration 28, loss = 1.61003493\n",
      "Iteration 19, loss = 0.03666774\n",
      "Iteration 16, loss = 0.04638112\n",
      "Iteration 15, loss = 1.61988193\n",
      "Iteration 15, loss = 2.48806722\n",
      "Iteration 15, loss = 0.06743100\n",
      "Iteration 15, loss = 1.61998182\n",
      "Iteration 29, loss = 1.60993697\n",
      "Iteration 31, loss = 1.61430401\n",
      "Iteration 15, loss = 0.34830490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20, loss = 0.03480100\n",
      "Iteration 16, loss = 0.33885180\n",
      "Iteration 15, loss = 0.06379230\n",
      "Iteration 16, loss = 0.05163800\n",
      "Iteration 16, loss = 0.32886283\n",
      "Iteration 17, loss = 2.11304466\n",
      "Iteration 16, loss = 0.06388094\n",
      "Iteration 32, loss = 1.61275868\n",
      "Iteration 16, loss = 1.61781876\n",
      "Iteration 30, loss = 1.60984856\n",
      "Iteration 21, loss = 0.03212924\n",
      "Iteration 17, loss = 0.04152171\n",
      "Iteration 16, loss = 0.06143611\n",
      "Iteration 16, loss = 2.27679250\n",
      "Iteration 33, loss = 1.61170737\n",
      "Iteration 16, loss = 1.61788525\n",
      "Iteration 31, loss = 1.60986008\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 16, loss = 0.33640896\n",
      "Iteration 16, loss = 0.05844873\n",
      "Iteration 22, loss = 0.02969623\n",
      "Iteration 17, loss = 0.32863919\n",
      "Iteration 17, loss = 0.04655920\n",
      "Iteration 17, loss = 0.31855838\n",
      "Iteration 17, loss = 0.05905761\n",
      "Iteration 18, loss = 1.98699213\n",
      "Iteration 34, loss = 1.61096207\n",
      "Iteration 17, loss = 1.61611172\n",
      "Iteration 23, loss = 0.02851777\n",
      "Iteration 18, loss = 0.03750011\n",
      "Iteration 17, loss = 2.11302461\n",
      "Iteration 17, loss = 0.05681853\n",
      "Iteration 17, loss = 1.61620978\n",
      "Iteration 17, loss = 0.32607921\n",
      "Iteration 35, loss = 1.61048544\n",
      "Iteration 18, loss = 0.31937234\n",
      "Iteration 24, loss = 0.02702854\n",
      "Iteration 17, loss = 0.05383347\n",
      "Iteration 18, loss = 0.30955764\n",
      "Iteration 19, loss = 1.89059219\n",
      "Iteration 18, loss = 0.05485829\n",
      "Iteration 18, loss = 0.04214375\n",
      "Iteration 36, loss = 1.61014281\n",
      "Iteration 19, loss = 0.03419476\n",
      "Iteration 25, loss = 0.02548056\n",
      "Iteration 18, loss = 1.61474453\n",
      "Iteration 18, loss = 1.98694059\n",
      "Iteration 37, loss = 1.60989567\n",
      "Iteration 18, loss = 0.05298942\n",
      "Iteration 18, loss = 0.31696629\n",
      "Iteration 18, loss = 1.61490208\n",
      "Iteration 19, loss = 0.31027556\n",
      "Iteration 18, loss = 0.05002503\n",
      "Iteration 26, loss = 0.02500659\n",
      "Iteration 19, loss = 0.30096730\n",
      "Iteration 19, loss = 0.05115728\n",
      "Iteration 20, loss = 1.81742573\n",
      "Iteration 19, loss = 0.03825820\n",
      "Iteration 38, loss = 1.60977442\n",
      "Iteration 20, loss = 0.03170062\n",
      "Iteration 27, loss = 0.02377220\n",
      "Iteration 19, loss = 1.61367212\n",
      "Iteration 19, loss = 1.89056672\n",
      "Iteration 19, loss = 0.04943872\n",
      "Iteration 39, loss = 1.60966799\n",
      "Iteration 19, loss = 0.30819752\n",
      "Iteration 20, loss = 0.30363236\n",
      "Iteration 19, loss = 1.61376477\n",
      "Iteration 28, loss = 0.02284338\n",
      "Iteration 19, loss = 0.04689965\n",
      "Iteration 21, loss = 1.76226507\n",
      "Iteration 20, loss = 0.29337160\n",
      "Iteration 20, loss = 0.04937089\n",
      "Iteration 20, loss = 0.03630817\n",
      "Iteration 40, loss = 1.60959205\n",
      "Iteration 21, loss = 0.02955450\n",
      "Iteration 29, loss = 0.02219356\n",
      "Iteration 20, loss = 1.61277796\n",
      "Iteration 20, loss = 0.04775128\n",
      "Iteration 41, loss = 1.60953674\n",
      "Iteration 20, loss = 1.81739070\n",
      "Iteration 20, loss = 1.61287437\n",
      "Iteration 21, loss = 0.29658288\n",
      "Iteration 20, loss = 0.30170217\n",
      "Iteration 30, loss = 0.02142480\n",
      "Iteration 22, loss = 1.72096881\n",
      "Iteration 21, loss = 0.03356988\n",
      "Iteration 20, loss = 0.04455917\n",
      "Iteration 21, loss = 0.28706178\n",
      "Iteration 21, loss = 0.04676350\n",
      "Iteration 42, loss = 1.60950225\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 22, loss = 0.02760896\n",
      "Iteration 31, loss = 0.02065290\n",
      "Iteration 21, loss = 1.61210484\n",
      "Iteration 21, loss = 1.76222950\n",
      "Iteration 21, loss = 0.04520185\n",
      "Iteration 32, loss = 0.02033777\n",
      "Iteration 21, loss = 0.29463278\n",
      "Iteration 22, loss = 0.29055401\n",
      "Iteration 21, loss = 1.61218790\n",
      "Iteration 21, loss = 0.04254009\n",
      "Iteration 22, loss = 0.04492023\n",
      "Iteration 23, loss = 1.69031935\n",
      "Iteration 22, loss = 0.28070995\n",
      "Iteration 22, loss = 0.03160299\n",
      "Iteration 33, loss = 0.01959085\n",
      "Iteration 23, loss = 0.02628452\n",
      "Iteration 22, loss = 1.61157203\n",
      "Iteration 34, loss = 0.01976041\n",
      "Iteration 22, loss = 1.72094391\n",
      "Iteration 22, loss = 0.04288739\n",
      "Iteration 23, loss = 0.28480492\n",
      "Iteration 23, loss = 0.04337500\n",
      "Iteration 22, loss = 0.28780766\n",
      "Iteration 22, loss = 1.61162160\n",
      "Iteration 35, loss = 0.01896464\n",
      "Iteration 22, loss = 0.04070552\n",
      "Iteration 23, loss = 0.27531626\n",
      "Iteration 24, loss = 0.02511848\n",
      "Iteration 24, loss = 1.66766563\n",
      "Iteration 23, loss = 0.02996015\n",
      "Iteration 23, loss = 1.61112491\n",
      "Iteration 36, loss = 0.01902778\n",
      "Iteration 23, loss = 1.69027029\n",
      "Iteration 23, loss = 0.04180114\n",
      "Iteration 23, loss = 0.28211320\n",
      "Iteration 37, loss = 0.01838654\n",
      "Iteration 24, loss = 0.04243413\n",
      "Iteration 23, loss = 0.03947169\n",
      "Iteration 24, loss = 0.27954494\n",
      "Iteration 23, loss = 1.61124709\n",
      "Iteration 24, loss = 0.27005481\n",
      "Iteration 24, loss = 0.02894641\n",
      "Iteration 25, loss = 0.02317037\n",
      "Iteration 25, loss = 1.65110615\n",
      "Iteration 38, loss = 0.01819137\n",
      "Iteration 24, loss = 1.61087294\n",
      "Iteration 24, loss = 1.66766620\n",
      "Iteration 24, loss = 0.04040041\n",
      "Iteration 24, loss = 0.27718017\n",
      "Iteration 39, loss = 0.01815586\n",
      "Iteration 24, loss = 0.03838700\n",
      "Iteration 25, loss = 0.04116270\n",
      "Iteration 25, loss = 0.27431871\n",
      "Iteration 24, loss = 1.61084937\n",
      "Iteration 25, loss = 0.26451280\n",
      "Iteration 25, loss = 0.02760704\n",
      "Iteration 26, loss = 1.63904140\n",
      "Iteration 26, loss = 0.02270704\n",
      "Iteration 25, loss = 1.65109595\n",
      "Iteration 25, loss = 1.61063645\n",
      "Iteration 40, loss = 0.01767481\n",
      "Iteration 25, loss = 0.03890004\n",
      "Iteration 25, loss = 0.03649213\n",
      "Iteration 25, loss = 0.27200399\n",
      "Iteration 26, loss = 0.26955333\n",
      "Iteration 26, loss = 0.03950948\n",
      "Iteration 41, loss = 0.01701967\n",
      "Iteration 26, loss = 0.26069255\n",
      "Iteration 25, loss = 1.61064259\n",
      "Iteration 27, loss = 0.02145306\n",
      "Iteration 27, loss = 1.63033929\n",
      "Iteration 26, loss = 0.02589403\n",
      "Iteration 26, loss = 1.63905479\n",
      "Iteration 26, loss = 1.61048740\n",
      "Iteration 42, loss = 0.01717210\n",
      "Iteration 26, loss = 0.03849814\n",
      "Iteration 26, loss = 0.03609021\n",
      "Iteration 26, loss = 0.26791049\n",
      "Iteration 27, loss = 0.03835520\n",
      "Iteration 27, loss = 0.26487534\n",
      "Iteration 28, loss = 1.62410470\n",
      "Iteration 43, loss = 0.01660578\n",
      "Iteration 26, loss = 1.61041004\n",
      "Iteration 27, loss = 0.25641637\n",
      "Iteration 28, loss = 0.02051434\n",
      "Iteration 27, loss = 0.02469287\n",
      "Iteration 27, loss = 1.63033835\n",
      "Iteration 27, loss = 1.61026607\n",
      "Iteration 44, loss = 0.01688775\n",
      "Iteration 27, loss = 0.03730798\n",
      "Iteration 27, loss = 0.03487171\n",
      "Iteration 27, loss = 0.26386993\n",
      "Iteration 28, loss = 0.03843619\n",
      "Iteration 45, loss = 0.01619209\n",
      "Iteration 28, loss = 0.26187709\n",
      "Iteration 29, loss = 1.61964372\n",
      "Iteration 28, loss = 0.25216847\n",
      "Iteration 29, loss = 0.01949659\n",
      "Iteration 27, loss = 1.61025199\n",
      "Iteration 28, loss = 1.62407234\n",
      "Iteration 46, loss = 0.01624135\n",
      "Iteration 28, loss = 0.02473668\n",
      "Iteration 28, loss = 0.03639884\n",
      "Iteration 28, loss = 1.61007226\n",
      "Iteration 30, loss = 1.61652997\n",
      "Iteration 28, loss = 0.25918222\n",
      "Iteration 28, loss = 0.03395012\n",
      "Iteration 29, loss = 0.03701143\n",
      "Iteration 29, loss = 0.24806086\n",
      "Iteration 47, loss = 0.01587879\n",
      "Iteration 29, loss = 0.25779242\n",
      "Iteration 30, loss = 0.01939423\n",
      "Iteration 28, loss = 1.61013968\n",
      "Iteration 29, loss = 0.02329524\n",
      "Iteration 48, loss = 0.01606227\n",
      "Iteration 29, loss = 1.61963904\n",
      "Iteration 29, loss = 0.03576185\n",
      "Iteration 29, loss = 0.25565519\n",
      "Iteration 31, loss = 1.61429764\n",
      "Iteration 29, loss = 1.60999793\n",
      "Iteration 29, loss = 0.03293907\n",
      "Iteration 30, loss = 0.24543741\n",
      "Iteration 49, loss = 0.01616188\n",
      "Iteration 30, loss = 0.03709004\n",
      "Iteration 29, loss = 1.61000284\n",
      "Iteration 30, loss = 0.25514728\n",
      "Iteration 30, loss = 0.02335637\n",
      "Iteration 50, loss = 0.01582534\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 31, loss = 0.01873146\n",
      "Iteration 30, loss = 1.61653499\n",
      "Iteration 32, loss = 1.61276502\n",
      "Iteration 30, loss = 0.03501262\n",
      "Iteration 31, loss = 0.03637806\n",
      "Iteration 30, loss = 0.25213011\n",
      "Iteration 31, loss = 0.24207098\n",
      "Iteration 31, loss = 0.02264532\n",
      "Iteration 30, loss = 0.03286328\n",
      "Iteration 30, loss = 1.60999058\n",
      "Iteration 30, loss = 1.60996370\n",
      "Iteration 31, loss = 0.25155704\n",
      "Iteration 32, loss = 0.01816802\n",
      "Iteration 31, loss = 1.61428445\n",
      "Iteration 32, loss = 0.23895286\n",
      "Iteration 33, loss = 1.61170868\n",
      "Iteration 31, loss = 0.03423908\n",
      "Iteration 31, loss = 0.03219989\n",
      "Iteration 31, loss = 0.24847710\n",
      "Iteration 32, loss = 0.03491154\n",
      "Iteration 32, loss = 0.02117730\n",
      "Iteration 32, loss = 0.24749258\n",
      "Iteration 31, loss = 1.60982280\n",
      "Iteration 31, loss = 1.60984084\n",
      "Iteration 33, loss = 0.23566492\n",
      "Iteration 33, loss = 0.01747510\n",
      "Iteration 32, loss = 1.61275715\n",
      "Iteration 32, loss = 0.03393307\n",
      "Iteration 34, loss = 1.61097168\n",
      "Iteration 33, loss = 0.03473682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 32, loss = 0.24534730\n",
      "Iteration 33, loss = 0.02102374\n",
      "Iteration 32, loss = 0.03163539\n",
      "Iteration 33, loss = 0.24494435\n",
      "Iteration 32, loss = 1.60978141\n",
      "Iteration 32, loss = 1.60978907\n",
      "Iteration 34, loss = 0.23279638\n",
      "Iteration 34, loss = 0.03407033\n",
      "Iteration 35, loss = 1.61050295\n",
      "Iteration 34, loss = 0.01696436\n",
      "Iteration 33, loss = 1.61169339\n",
      "Iteration 33, loss = 0.03316212\n",
      "Iteration 33, loss = 0.24232405\n",
      "Iteration 33, loss = 0.03091643\n",
      "Iteration 34, loss = 0.02036540\n",
      "Iteration 34, loss = 0.24167613\n",
      "Iteration 33, loss = 1.60971355\n",
      "Iteration 35, loss = 0.23036049\n",
      "Iteration 33, loss = 1.60975641\n",
      "Iteration 35, loss = 0.01673779\n",
      "Iteration 34, loss = 1.61098205\n",
      "Iteration 35, loss = 0.03363256\n",
      "Iteration 36, loss = 1.61013087\n",
      "Iteration 34, loss = 0.24010689\n",
      "Iteration 35, loss = 0.01996274\n",
      "Iteration 34, loss = 0.03331293\n",
      "Iteration 34, loss = 0.03039063\n",
      "Iteration 35, loss = 0.23869238\n",
      "Iteration 34, loss = 1.60970005\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 36, loss = 0.22908671\n",
      "Iteration 34, loss = 1.60973541\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 35, loss = 0.23707347\n",
      "Iteration 35, loss = 1.61048162\n",
      "Iteration 36, loss = 0.01683136\n",
      "Iteration 37, loss = 1.60993159\n",
      "Iteration 36, loss = 0.03410381\n",
      "Iteration 35, loss = 0.03251918\n",
      "Iteration 36, loss = 0.02042494\n",
      "Iteration 35, loss = 0.03015028\n",
      "Iteration 37, loss = 0.22633765\n",
      "Iteration 36, loss = 0.23806757\n",
      "Iteration 36, loss = 0.23507784\n",
      "Iteration 37, loss = 0.01614474\n",
      "Iteration 38, loss = 1.60976396\n",
      "Iteration 36, loss = 1.61012413\n",
      "Iteration 37, loss = 0.03344292\n",
      "Iteration 37, loss = 0.01979388\n",
      "Iteration 36, loss = 0.03255015\n",
      "Iteration 37, loss = 0.23540868\n",
      "Iteration 37, loss = 0.23295921\n",
      "Iteration 36, loss = 0.03022070\n",
      "Iteration 38, loss = 0.01595824\n",
      "Iteration 38, loss = 0.22393422\n",
      "Iteration 37, loss = 1.60991896\n",
      "Iteration 39, loss = 1.60967036\n",
      "Iteration 38, loss = 0.01976426\n",
      "Iteration 38, loss = 0.03337030\n",
      "Iteration 37, loss = 0.03188648\n",
      "Iteration 37, loss = 0.02949987\n",
      "Iteration 38, loss = 0.23017887\n",
      "Iteration 38, loss = 0.23308525\n",
      "Iteration 39, loss = 0.22150977\n",
      "Iteration 38, loss = 1.60976223\n",
      "Iteration 39, loss = 0.01569689\n",
      "Iteration 40, loss = 1.60959933\n",
      "Iteration 39, loss = 0.01867258\n",
      "Iteration 39, loss = 0.03222124\n",
      "Iteration 39, loss = 0.23016561\n",
      "Iteration 38, loss = 0.03162602\n",
      "Iteration 39, loss = 0.22836146\n",
      "Iteration 38, loss = 0.02928446\n",
      "Iteration 40, loss = 0.01499676\n",
      "Iteration 40, loss = 0.21908385\n",
      "Iteration 41, loss = 1.60954460\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 39, loss = 1.60968984\n",
      "Iteration 40, loss = 0.01858673\n",
      "Iteration 40, loss = 0.03211328\n",
      "Iteration 40, loss = 0.22593166\n",
      "Iteration 40, loss = 0.22804488\n",
      "Iteration 39, loss = 0.03159887\n",
      "Iteration 41, loss = 0.21802744\n",
      "Iteration 41, loss = 0.01530364\n",
      "Iteration 39, loss = 0.02897354\n",
      "Iteration 40, loss = 1.60957727\n",
      "Iteration 41, loss = 0.01895702\n",
      "Iteration 41, loss = 0.03243928\n",
      "Iteration 41, loss = 0.22366960\n",
      "Iteration 41, loss = 0.22708825\n",
      "Iteration 40, loss = 0.03107064\n",
      "Iteration 42, loss = 0.21635496\n",
      "Iteration 40, loss = 0.02822660\n",
      "Iteration 42, loss = 0.01514474\n",
      "Iteration 41, loss = 1.60953993\n",
      "Iteration 42, loss = 0.01904351\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 42, loss = 0.03248571\n",
      "Iteration 42, loss = 0.22232854\n",
      "Iteration 42, loss = 0.22495985\n",
      "Iteration 43, loss = 0.21431915\n",
      "Iteration 41, loss = 0.03034908\n",
      "Iteration 43, loss = 0.01472360\n",
      "Iteration 41, loss = 0.02851343\n",
      "Iteration 42, loss = 1.60951424\n",
      "Iteration 43, loss = 0.22305191\n",
      "Iteration 43, loss = 0.03161342\n",
      "Iteration 43, loss = 0.21988561\n",
      "Iteration 44, loss = 0.21301859\n",
      "Iteration 42, loss = 0.03046511\n",
      "Iteration 43, loss = 1.60950181\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 44, loss = 0.01464829\n",
      "Iteration 42, loss = 0.02830978\n",
      "Iteration 44, loss = 0.22188817\n",
      "Iteration 45, loss = 0.21087349\n",
      "Iteration 44, loss = 0.03217787\n",
      "Iteration 44, loss = 0.21888215\n",
      "Iteration 43, loss = 0.02983742\n",
      "Iteration 43, loss = 0.02781669\n",
      "Iteration 45, loss = 0.21999126\n",
      "Iteration 45, loss = 0.01405927\n",
      "Iteration 46, loss = 0.20995059\n",
      "Iteration 45, loss = 0.21663669\n",
      "Iteration 45, loss = 0.03128956\n",
      "Iteration 44, loss = 0.03007288\n",
      "Iteration 44, loss = 0.02772759\n",
      "Iteration 46, loss = 0.21917780\n",
      "Iteration 46, loss = 0.01436781\n",
      "Iteration 47, loss = 0.20887184\n",
      "Iteration 46, loss = 0.03131789\n",
      "Iteration 46, loss = 0.21504786\n",
      "Iteration 45, loss = 0.02931405\n",
      "Iteration 45, loss = 0.02703221\n",
      "Iteration 47, loss = 0.01440710\n",
      "Iteration 47, loss = 0.21820620\n",
      "Iteration 48, loss = 0.20747353\n",
      "Iteration 47, loss = 0.03165019\n",
      "Iteration 47, loss = 0.21341751\n",
      "Iteration 46, loss = 0.02931919\n",
      "Iteration 46, loss = 0.02734566\n",
      "Iteration 48, loss = 0.01424905\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 48, loss = 0.21660761\n",
      "Iteration 49, loss = 0.20618740\n",
      "Iteration 48, loss = 0.21275050\n",
      "Iteration 48, loss = 0.03117669\n",
      "Iteration 47, loss = 0.02888427\n",
      "Iteration 47, loss = 0.02733648\n",
      "Iteration 49, loss = 0.03063396\n",
      "Iteration 49, loss = 0.21391840\n",
      "Iteration 48, loss = 0.02903517\n",
      "Iteration 50, loss = 0.20470314\n",
      "Iteration 49, loss = 0.21181025\n",
      "Iteration 50, loss = 0.03031726\n",
      "Iteration 48, loss = 0.02711830\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 49, loss = 0.02908640\n",
      "Iteration 50, loss = 0.21011539\n",
      "Iteration 50, loss = 0.21323418\n",
      "Iteration 51, loss = 0.20332274\n",
      "Iteration 51, loss = 0.03012626\n",
      "Iteration 50, loss = 0.02869070\n",
      "Iteration 52, loss = 0.20183180\n",
      "Iteration 51, loss = 0.20911009\n",
      "Iteration 51, loss = 0.21149872\n",
      "Iteration 51, loss = 0.02880134\n",
      "Iteration 53, loss = 0.20064155\n",
      "Iteration 52, loss = 0.03036680\n",
      "Iteration 52, loss = 0.20728156\n",
      "Iteration 52, loss = 0.02839728\n",
      "Iteration 54, loss = 0.20041913\n",
      "Iteration 53, loss = 0.02801800\n",
      "Iteration 52, loss = 0.21066792\n",
      "Iteration 53, loss = 0.03029449\n",
      "Iteration 53, loss = 0.20594700\n",
      "Iteration 55, loss = 0.19900609\n",
      "Iteration 54, loss = 0.02919405\n",
      "Iteration 54, loss = 0.20499282\n",
      "Iteration 54, loss = 0.02777603\n",
      "Iteration 53, loss = 0.21069224\n",
      "Iteration 55, loss = 0.20427281\n",
      "Iteration 56, loss = 0.19846503\n",
      "Iteration 55, loss = 0.02988316\n",
      "Iteration 55, loss = 0.02795581\n",
      "Iteration 54, loss = 0.20858249\n",
      "Iteration 56, loss = 0.20387983\n",
      "Iteration 56, loss = 0.02920579\n",
      "Iteration 57, loss = 0.19734558\n",
      "Iteration 56, loss = 0.02832924\n",
      "Iteration 55, loss = 0.20814677\n",
      "Iteration 57, loss = 0.20324514\n",
      "Iteration 57, loss = 0.02889481\n",
      "Iteration 58, loss = 0.19695957\n",
      "Iteration 56, loss = 0.20624032\n",
      "Iteration 58, loss = 0.20212551\n",
      "Iteration 57, loss = 0.02782897\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 59, loss = 0.19538854\n",
      "Iteration 58, loss = 0.02975046\n",
      "Iteration 59, loss = 0.20048875\n",
      "Iteration 57, loss = 0.20525822\n",
      "Iteration 59, loss = 0.02912731\n",
      "Iteration 60, loss = 0.19410361\n",
      "Iteration 60, loss = 0.02845850\n",
      "Iteration 60, loss = 0.19996128\n",
      "Iteration 58, loss = 0.20549468\n",
      "Iteration 61, loss = 0.19393227\n",
      "Iteration 61, loss = 0.19963789\n",
      "Iteration 61, loss = 0.02896029\n",
      "Iteration 59, loss = 0.20404962\n",
      "Iteration 62, loss = 0.19274752\n",
      "Iteration 62, loss = 0.19743262\n",
      "Iteration 60, loss = 0.20217004\n",
      "Iteration 62, loss = 0.02885941\n",
      "Iteration 63, loss = 0.19103841\n",
      "Iteration 63, loss = 0.19745802\n",
      "Iteration 61, loss = 0.20176288\n",
      "Iteration 63, loss = 0.02759681\n",
      "Iteration 64, loss = 0.19202512\n",
      "Iteration 64, loss = 0.19745237\n",
      "Iteration 64, loss = 0.02846247\n",
      "Iteration 62, loss = 0.20115671\n",
      "Iteration 65, loss = 0.18977394\n",
      "Iteration 65, loss = 0.19542762\n",
      "Iteration 65, loss = 0.02803152\n",
      "Iteration 63, loss = 0.19951690\n",
      "Iteration 66, loss = 0.19052502\n",
      "Iteration 66, loss = 0.19505243\n",
      "Iteration 66, loss = 0.02916975\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 64, loss = 0.19948987\n",
      "Iteration 67, loss = 0.19525617\n",
      "Iteration 67, loss = 0.18990707\n",
      "Iteration 65, loss = 0.19852009\n",
      "Iteration 68, loss = 0.18854594\n",
      "Iteration 68, loss = 0.19346417\n",
      "Iteration 66, loss = 0.19902571\n",
      "Iteration 69, loss = 0.19226031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 69, loss = 0.18777680\n",
      "Iteration 67, loss = 0.19807091\n",
      "Iteration 70, loss = 0.19311155\n",
      "Iteration 70, loss = 0.18712956\n",
      "Iteration 68, loss = 0.19747137\n",
      "Iteration 71, loss = 0.19136456\n",
      "Iteration 72, loss = 0.19114448\n",
      "Iteration 71, loss = 0.18748286\n",
      "Iteration 69, loss = 0.19705513\n",
      "Iteration 73, loss = 0.19140888\n",
      "Iteration 72, loss = 0.18583742\n",
      "Iteration 70, loss = 0.19535809\n",
      "Iteration 74, loss = 0.18993656\n",
      "Iteration 73, loss = 0.18617644\n",
      "Iteration 71, loss = 0.19491412\n",
      "Iteration 75, loss = 0.18928863\n",
      "Iteration 74, loss = 0.18443694\n",
      "Iteration 76, loss = 0.18979529\n",
      "Iteration 72, loss = 0.19450848\n",
      "Iteration 75, loss = 0.18415037\n",
      "Iteration 77, loss = 0.18793221\n",
      "Iteration 76, loss = 0.18452990\n",
      "Iteration 73, loss = 0.19353584\n",
      "Iteration 78, loss = 0.18822474\n",
      "Iteration 74, loss = 0.19288297\n",
      "Iteration 77, loss = 0.18339926\n",
      "Iteration 79, loss = 0.18832107\n",
      "Iteration 78, loss = 0.18507405\n",
      "Iteration 75, loss = 0.19143151\n",
      "Iteration 80, loss = 0.18915008\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 79, loss = 0.18287744\n",
      "Iteration 76, loss = 0.19139172\n",
      "Iteration 80, loss = 0.18212389\n",
      "Iteration 77, loss = 0.19117569\n",
      "Iteration 81, loss = 0.18167252\n",
      "Iteration 78, loss = 0.19211008\n",
      "Iteration 82, loss = 0.18182650\n",
      "Iteration 79, loss = 0.19109581\n",
      "Iteration 83, loss = 0.18257870\n",
      "Iteration 80, loss = 0.19022669\n",
      "Iteration 84, loss = 0.18139557\n",
      "Iteration 81, loss = 0.18935172\n",
      "Iteration 82, loss = 0.18981090\n",
      "Iteration 85, loss = 0.18021192\n",
      "Iteration 83, loss = 0.18917793\n",
      "Iteration 86, loss = 0.18043242\n",
      "Iteration 84, loss = 0.18884075\n",
      "Iteration 87, loss = 0.17942629\n",
      "Iteration 85, loss = 0.18896810\n",
      "Iteration 88, loss = 0.17977190\n",
      "Iteration 89, loss = 0.17911013\n",
      "Iteration 86, loss = 0.18867622\n",
      "Iteration 90, loss = 0.18090875\n",
      "Iteration 87, loss = 0.18801168\n",
      "Iteration 91, loss = 0.17850207\n",
      "Iteration 88, loss = 0.18917264\n",
      "Iteration 92, loss = 0.17797231\n",
      "Iteration 89, loss = 0.18780057\n",
      "Iteration 93, loss = 0.17870301\n",
      "Iteration 90, loss = 0.18813428\n",
      "Iteration 94, loss = 0.17710705\n",
      "Iteration 91, loss = 0.18544745\n",
      "Iteration 95, loss = 0.17719361\n",
      "Iteration 92, loss = 0.18614652\n",
      "Iteration 93, loss = 0.18794126\n",
      "Iteration 96, loss = 0.17741056\n",
      "Iteration 94, loss = 0.18655777\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 97, loss = 0.17729728\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.50438316\n",
      "Iteration 2, loss = 1.14716750\n",
      "Iteration 3, loss = 0.87712814\n",
      "Iteration 4, loss = 0.70282607\n",
      "Iteration 5, loss = 0.59666994\n",
      "Iteration 6, loss = 0.52867470\n",
      "Iteration 7, loss = 0.48168526\n",
      "Iteration 8, loss = 0.44596345\n",
      "Iteration 9, loss = 0.41860735\n",
      "Iteration 10, loss = 0.39632619\n",
      "Iteration 11, loss = 0.37732229\n",
      "Iteration 12, loss = 0.36205923\n",
      "Iteration 13, loss = 0.34861641\n",
      "Iteration 14, loss = 0.33735309\n",
      "Iteration 15, loss = 0.32682198\n",
      "Iteration 16, loss = 0.31859053\n",
      "Iteration 17, loss = 0.30951880\n",
      "Iteration 18, loss = 0.30294852\n",
      "Iteration 19, loss = 0.29671978\n",
      "Iteration 20, loss = 0.29111293\n",
      "Iteration 21, loss = 0.28494594\n",
      "Iteration 22, loss = 0.28054560\n",
      "Iteration 23, loss = 0.27722879\n",
      "Iteration 24, loss = 0.27149012\n",
      "Iteration 25, loss = 0.26719928\n",
      "Iteration 26, loss = 0.26335985\n",
      "Iteration 27, loss = 0.26060045\n",
      "Iteration 28, loss = 0.25845788\n",
      "Iteration 29, loss = 0.25490040\n",
      "Iteration 30, loss = 0.25357400\n",
      "Iteration 31, loss = 0.25083762\n",
      "Iteration 32, loss = 0.24826164\n",
      "Iteration 33, loss = 0.24499385\n",
      "Iteration 34, loss = 0.24420904\n",
      "Iteration 35, loss = 0.24091266\n",
      "Iteration 36, loss = 0.23916613\n",
      "Iteration 37, loss = 0.23754141\n",
      "Iteration 38, loss = 0.23505303\n",
      "Iteration 39, loss = 0.23533029\n",
      "Iteration 40, loss = 0.23273995\n",
      "Iteration 41, loss = 0.23257292\n",
      "Iteration 42, loss = 0.22947249\n",
      "Iteration 43, loss = 0.22893484\n",
      "Iteration 44, loss = 0.22810259\n",
      "Iteration 45, loss = 0.22740714\n",
      "Iteration 46, loss = 0.22490499\n",
      "Iteration 47, loss = 0.22478790\n",
      "Iteration 48, loss = 0.22440723\n",
      "Iteration 49, loss = 0.22265679\n",
      "Iteration 50, loss = 0.22183913\n",
      "Iteration 51, loss = 0.22006941\n",
      "Iteration 52, loss = 0.22013673\n",
      "Iteration 53, loss = 0.21942159\n",
      "Iteration 54, loss = 0.21880915\n",
      "Iteration 55, loss = 0.21793978\n",
      "Iteration 56, loss = 0.21692758\n",
      "Iteration 57, loss = 0.21754001\n",
      "Iteration 58, loss = 0.21928198\n",
      "Iteration 59, loss = 0.21702787\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=True, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'alpha': array([1.e-05, 1.e-03, 1.e-01, 1.e+01, 1.e+03])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = np.logspace(-5, 3, 5)\n",
    "clf = GridSearchCV(estimator=MLPClassifier(verbose=True), param_grid={'alpha':alpha},n_jobs=-1, return_train_score=True)\n",
    "clf.fit(train_vectors,train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>694.772488</td>\n",
       "      <td>0.101714</td>\n",
       "      <td>0.872222</td>\n",
       "      <td>0.992556</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'alpha': 0.1}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.876000</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>0.874667</td>\n",
       "      <td>0.993667</td>\n",
       "      <td>0.866000</td>\n",
       "      <td>0.992333</td>\n",
       "      <td>17.238931</td>\n",
       "      <td>0.012881</td>\n",
       "      <td>0.004433</td>\n",
       "      <td>8.314794e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>604.430612</td>\n",
       "      <td>0.265347</td>\n",
       "      <td>0.863556</td>\n",
       "      <td>0.992556</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'alpha': 0.001}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.868000</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>0.873333</td>\n",
       "      <td>0.993667</td>\n",
       "      <td>0.849333</td>\n",
       "      <td>0.992333</td>\n",
       "      <td>29.034021</td>\n",
       "      <td>0.095347</td>\n",
       "      <td>0.010290</td>\n",
       "      <td>8.314794e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>477.282670</td>\n",
       "      <td>0.208044</td>\n",
       "      <td>0.859778</td>\n",
       "      <td>0.992556</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>{'alpha': 1e-05}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.864667</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>0.866000</td>\n",
       "      <td>0.993667</td>\n",
       "      <td>0.848667</td>\n",
       "      <td>0.992333</td>\n",
       "      <td>73.283200</td>\n",
       "      <td>0.100955</td>\n",
       "      <td>0.007876</td>\n",
       "      <td>8.314794e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>360.375565</td>\n",
       "      <td>0.321544</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>10</td>\n",
       "      <td>{'alpha': 10.0}</td>\n",
       "      <td>4</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>103.453431</td>\n",
       "      <td>0.033781</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.775558e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>419.598647</td>\n",
       "      <td>0.359369</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'alpha': 1000.0}</td>\n",
       "      <td>4</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>106.843909</td>\n",
       "      <td>0.102752</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.775558e-17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "2     694.772488         0.101714         0.872222          0.992556   \n",
       "1     604.430612         0.265347         0.863556          0.992556   \n",
       "0     477.282670         0.208044         0.859778          0.992556   \n",
       "3     360.375565         0.321544         0.200000          0.200000   \n",
       "4     419.598647         0.359369         0.200000          0.200000   \n",
       "\n",
       "  param_alpha             params  rank_test_score  split0_test_score  \\\n",
       "2         0.1     {'alpha': 0.1}                1           0.876000   \n",
       "1       0.001   {'alpha': 0.001}                2           0.868000   \n",
       "0       1e-05   {'alpha': 1e-05}                3           0.864667   \n",
       "3          10    {'alpha': 10.0}                4           0.200000   \n",
       "4        1000  {'alpha': 1000.0}                4           0.200000   \n",
       "\n",
       "   split0_train_score  split1_test_score  split1_train_score  \\\n",
       "2            0.991667           0.874667            0.993667   \n",
       "1            0.991667           0.873333            0.993667   \n",
       "0            0.991667           0.866000            0.993667   \n",
       "3            0.200000           0.200000            0.200000   \n",
       "4            0.200000           0.200000            0.200000   \n",
       "\n",
       "   split2_test_score  split2_train_score  std_fit_time  std_score_time  \\\n",
       "2           0.866000            0.992333     17.238931        0.012881   \n",
       "1           0.849333            0.992333     29.034021        0.095347   \n",
       "0           0.848667            0.992333     73.283200        0.100955   \n",
       "3           0.200000            0.200000    103.453431        0.033781   \n",
       "4           0.200000            0.200000    106.843909        0.102752   \n",
       "\n",
       "   std_test_score  std_train_score  \n",
       "2        0.004433     8.314794e-04  \n",
       "1        0.010290     8.314794e-04  \n",
       "0        0.007876     8.314794e-04  \n",
       "3        0.000000     2.775558e-17  \n",
       "4        0.000000     2.775558e-17  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(clf.cv_results_)\n",
    "df.sort_values('rank_test_score',inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for best MLP classifier = 85.2%\n"
     ]
    }
   ],
   "source": [
    "best_MLP = MLPClassifier(alpha=0.1).fit(train_vectors, train_targets)\n",
    "print('accuracy for best MLP classifier = {}%'.format(best_MLP.score(test_vectors, test_targets)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start the deep learning part here. We will use Keras library on Tensorflow backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajalloei/.conda/envs/ahmad_virtualenv/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the deep learning part we will use word embeddings as they are proved to give better results compared to one hot word vectors. So we start to tokenize the text again with nltk library and remove the stop words. We also lower case all of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "tokenized = [nltk.word_tokenize(text) for text in contents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# punctuation = string.punctuation+'â€œâ€™â€”.â€â€™â€œ--,â€' # pimp the list of punctuation to remove\n",
    "def rem_stop(txt,stop_words=stopwords.words(\"english\"),lower=True):\n",
    "    \"\"\"\n",
    "    Removes stopwords and other things from a text, inc. numbers\n",
    "    :param list txt: text tokens (list of str)\n",
    "    :param list stop_words: stopwords to remove (list of str)\n",
    "    :param bol lower: if to lowercase\n",
    "    \"\"\"\n",
    "    if lower:\n",
    "        return [t.lower() for t in txt if t.lower() not in stop_words and not t.isdigit()]\n",
    "    else:\n",
    "        return [t for t in txt if t.lower() not in stop_words and not t.isdigit()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [rem_stop(tokens) for tokens in tokenized]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we try to learn embedding vectors by applying word2vec algorithm on our corpus. We choose the dimension of the word vectors to be 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = Word2Vec(corpus, size=100, window=5, min_count=1, workers=-1, iter=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However it seems that the learned word vectors couldn't catch the semantic meaning of the words. Ideally similar words should be also closed to each other in the vector space but the result below doesn't show that.\n",
    "Also in practice by injecting these word vectors to the embedding layer of Neural Networks I saw that the results are not really interesting. So I decided to use pretrained word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('post1970s', 0.45103001594543457),\n",
       " ('lacierda', 0.4451066255569458),\n",
       " ('forgacs', 0.4363461434841156),\n",
       " ('repealing', 0.4331750273704529),\n",
       " ('interregnums', 0.4275933504104614),\n",
       " ('u818', 0.421070396900177),\n",
       " ('abcdef', 0.4119409918785095),\n",
       " ('doi10100735404755597', 0.39712589979171753),\n",
       " ('welsch', 0.396402508020401),\n",
       " ('commandinchief', 0.391961932182312)]"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model.wv.most_similar(positive=['france', 'paris'], negative=['germany'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Embedding, LSTM, Conv1D, Flatten, Dropout\n",
    "from keras.layers.merge import Concatenate, concatenate\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all the corpus should be tokenized and a word index dictionary which maps each word to a unique integer id should be made. Furthermore I decide for the maximum length of the documents to be 5000 words. So the documents which have more words than this would be truncated. Also the Sequentail model needs all of the samples to have the same length, therefore the documents which are shorter than 5000 words will be padded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 175282 unique tokens.\n",
      "Shape of data tensor: (5000, 5000)\n",
      "Shape of label tensor: (5000, 5)\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 5000\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "# 0 is a reserved index that won't be assigned to any word\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(targets-1))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the training and test corpus\n",
    "x_train = data[train_indicies]\n",
    "y_train = labels[train_indicies]\n",
    "x_test = data[test_indicies]\n",
    "y_test = labels[test_indicies]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I said earlier, I decided to use pretrained word embeddings. I used the 100-dimensional GloVe embeddings of 400k words computed on a 2014 dump of English Wikipedia. The embeddings can be downloaded from <a href=\"https://nlp.stanford.edu/projects/glove/\">here</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the embedding matrix\n",
    "EMBEDDING_DIM = 100\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#callbacks will be used to save the model after each epoch if it improved the accuracy\n",
    "ckpt_callback = ModelCheckpoint('keras_model', \n",
    "                                 monitor='val_acc', \n",
    "                                 verbose=1, \n",
    "                                 save_best_only=True, \n",
    "                                 mode='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network\n",
    "An approach for text classification is to use word embedding for representing words and a Convolutional Neural Network (CNN) for learning how to discriminate documents in classification problems. Especially when used with pre-trained word embeddings, neural networks can give better accuracies compared to linear classifiers.\n",
    "\n",
    "Convolutional neural networks are effective at document classification, because they are able to pick out important features (e.g. tokens or sequences of tokens) in a way that is invariant to their position within the input sequences. (<a href=\"https://machinelearningmastery.com/best-practices-document-classification-deep-learning/\">ref</a>)\n",
    "\n",
    "The architecture consists of different parts:\n",
    "    -  Embedding layer: this layer gets the vector representation of words. Ideally vector representations are such that similar words are also close to each other in vector space.\n",
    "\n",
    "    -  Convolutional layer :A feature extraction model that learns to extract important features from documents represented using a word embedding. The choice of kernel size and number of filters is important for this layer. For the activation function usually `relu`is the preferred choice.\n",
    "    \n",
    "    -  Maxpooling layer: usually a convolutional layer is followed by a maxpooling layer. It partitions the input data into a set of non-overlapping parts and, for each part, outputs the maximum. The pooling layer serves to progressively reduce the spatial size of the representation, to reduce the number of parameters and amount of computation in the network, and hence to also control overfitting. (<a href=\"https://en.wikipedia.org/wiki/Convolutional_neural_network\">ref</a>)\n",
    "    \n",
    "    -  Dropout: It basically drops out some units in the network and it is a regularization technique to prevent overfitting\n",
    "    \n",
    "    -  Dense layer: the high-level reasoning in the neural network is done via dense layer. Neurons in a dense layer have connections to all activations in the previous layer, as seen in regular neural networks. The output of the final dense layer should be equal to the number of categories in order to generate the final predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4050 samples, validate on 450 samples\n",
      "Epoch 1/10\n",
      "4050/4050 [==============================] - 85s 21ms/step - loss: 5.1799 - acc: 0.3195 - val_loss: 1.9039 - val_acc: 0.4200\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.42000, saving model to keras_model\n",
      "Epoch 2/10\n",
      "4050/4050 [==============================] - 85s 21ms/step - loss: 1.1859 - acc: 0.6195 - val_loss: 1.1436 - val_acc: 0.6644\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.42000 to 0.66444, saving model to keras_model\n",
      "Epoch 3/10\n",
      "4050/4050 [==============================] - 82s 20ms/step - loss: 0.7193 - acc: 0.7800 - val_loss: 0.9892 - val_acc: 0.6978\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.66444 to 0.69778, saving model to keras_model\n",
      "Epoch 4/10\n",
      "4050/4050 [==============================] - 83s 21ms/step - loss: 0.5551 - acc: 0.8128 - val_loss: 0.6565 - val_acc: 0.7933\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.69778 to 0.79333, saving model to keras_model\n",
      "Epoch 5/10\n",
      "4050/4050 [==============================] - 82s 20ms/step - loss: 0.3145 - acc: 0.8995 - val_loss: 0.7433 - val_acc: 0.7422\n",
      "\n",
      "Epoch 00005: val_acc did not improve\n",
      "Epoch 6/10\n",
      "4050/4050 [==============================] - 81s 20ms/step - loss: 0.3419 - acc: 0.8822 - val_loss: 1.0513 - val_acc: 0.7467\n",
      "\n",
      "Epoch 00006: val_acc did not improve\n",
      "Epoch 7/10\n",
      "4050/4050 [==============================] - 84s 21ms/step - loss: 0.2440 - acc: 0.9259 - val_loss: 0.6105 - val_acc: 0.8044\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.79333 to 0.80444, saving model to keras_model\n",
      "Epoch 8/10\n",
      "4050/4050 [==============================] - 84s 21ms/step - loss: 0.1354 - acc: 0.9622 - val_loss: 0.6888 - val_acc: 0.7956\n",
      "\n",
      "Epoch 00008: val_acc did not improve\n",
      "Epoch 9/10\n",
      "4050/4050 [==============================] - 83s 20ms/step - loss: 0.2481 - acc: 0.9227 - val_loss: 0.6987 - val_acc: 0.8222\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.80444 to 0.82222, saving model to keras_model\n",
      "Epoch 10/10\n",
      "4050/4050 [==============================] - 85s 21ms/step - loss: 0.1099 - acc: 0.9733 - val_loss: 0.6883 - val_acc: 0.8444\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.82222 to 0.84444, saving model to keras_model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4cd0674550>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(100, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(len(category_dict), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "# happy learning!\n",
    "model.fit(x_train, y_train, validation_split=0.1, \n",
    "          epochs=10, batch_size=512, callbacks=[ckpt_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the best model on the test set. The resulting test accuracy is 81.5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 2s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7410323133468628, 0.8159999995231628]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_model = load_model('keras_model')\n",
    "keras_model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long-Short Term Memory\n",
    "Recurrent neural networks are networks with loops in them, allowing information to persist. So they are able to connect previous information to the present task. However regular RNNs have problems in learning long-term dependencies in practice.\n",
    "\n",
    "Long-Short Term Memory networks are a special kind of RNN, capable of learning long-term dependencies. They are explicitly designed to avoid the long-term dependency problem. (<a href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\">ref</a>)\n",
    "\n",
    "Our LSTM architecture consists of only a LSTM layer followed by a dense layer. We have dropout for both the transformation of the inputs and the recurrent states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 5000, 100)         17528300  \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 196)               232848    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 5)                 985       \n",
      "=================================================================\n",
      "Total params: 17,762,133\n",
      "Trainable params: 17,762,133\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# embed_dim = 100\n",
    "lstm_out = 196\n",
    "\n",
    "# Model saving callback\n",
    "lstm_callback = ModelCheckpoint('lstm_model', \n",
    "                                 monitor='val_acc', \n",
    "                                 verbose=1, \n",
    "                                 save_best_only=True, \n",
    "                                 mode='auto')\n",
    "\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(embedding_layer)\n",
    "lstm_model.add(LSTM(lstm_out, recurrent_dropout=0.2, dropout=0.2))\n",
    "lstm_model.add(Dense(5,activation='softmax'))\n",
    "lstm_model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['acc'])\n",
    "print(lstm_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4050 samples, validate on 450 samples\n",
      "Epoch 1/20\n",
      "4050/4050 [==============================] - 430s 106ms/step - loss: 1.3970 - acc: 0.4874 - val_loss: 1.0749 - val_acc: 0.6200\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.62000, saving model to lstm_model\n",
      "Epoch 2/20\n",
      "4050/4050 [==============================] - 313s 77ms/step - loss: 0.8549 - acc: 0.6891 - val_loss: 0.7985 - val_acc: 0.6844\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.62000 to 0.68444, saving model to lstm_model\n",
      "Epoch 3/20\n",
      "4050/4050 [==============================] - 291s 72ms/step - loss: 0.6284 - acc: 0.7667 - val_loss: 0.6887 - val_acc: 0.7622\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.68444 to 0.76222, saving model to lstm_model\n",
      "Epoch 4/20\n",
      "4050/4050 [==============================] - 297s 73ms/step - loss: 0.5334 - acc: 0.8067 - val_loss: 0.6423 - val_acc: 0.7733\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.76222 to 0.77333, saving model to lstm_model\n",
      "Epoch 5/20\n",
      "4050/4050 [==============================] - 295s 73ms/step - loss: 0.4705 - acc: 0.8373 - val_loss: 0.6496 - val_acc: 0.7889\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.77333 to 0.78889, saving model to lstm_model\n",
      "Epoch 6/20\n",
      "4050/4050 [==============================] - 290s 72ms/step - loss: 0.4587 - acc: 0.8395 - val_loss: 0.6731 - val_acc: 0.7711\n",
      "\n",
      "Epoch 00006: val_acc did not improve\n",
      "Epoch 7/20\n",
      "4050/4050 [==============================] - 350s 86ms/step - loss: 0.4135 - acc: 0.8600 - val_loss: 0.6136 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.78889 to 0.79778, saving model to lstm_model\n",
      "Epoch 8/20\n",
      "4050/4050 [==============================] - 396s 98ms/step - loss: 0.3805 - acc: 0.8701 - val_loss: 0.6435 - val_acc: 0.7867\n",
      "\n",
      "Epoch 00008: val_acc did not improve\n",
      "Epoch 9/20\n",
      "4050/4050 [==============================] - 402s 99ms/step - loss: 0.3472 - acc: 0.8859 - val_loss: 0.6073 - val_acc: 0.8089\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.79778 to 0.80889, saving model to lstm_model\n",
      "Epoch 10/20\n",
      "4050/4050 [==============================] - 398s 98ms/step - loss: 0.3237 - acc: 0.8916 - val_loss: 0.6468 - val_acc: 0.8022\n",
      "\n",
      "Epoch 00010: val_acc did not improve\n",
      "Epoch 11/20\n",
      "4050/4050 [==============================] - 400s 99ms/step - loss: 0.3156 - acc: 0.8919 - val_loss: 0.5902 - val_acc: 0.8178\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.80889 to 0.81778, saving model to lstm_model\n",
      "Epoch 12/20\n",
      "4050/4050 [==============================] - 417s 103ms/step - loss: 0.3173 - acc: 0.8995 - val_loss: 0.6410 - val_acc: 0.7889\n",
      "\n",
      "Epoch 00012: val_acc did not improve\n",
      "Epoch 13/20\n",
      "4050/4050 [==============================] - 422s 104ms/step - loss: 0.2737 - acc: 0.9104 - val_loss: 0.6347 - val_acc: 0.8022\n",
      "\n",
      "Epoch 00013: val_acc did not improve\n",
      "Epoch 14/20\n",
      "4050/4050 [==============================] - 411s 101ms/step - loss: 0.2526 - acc: 0.9156 - val_loss: 0.6625 - val_acc: 0.8111\n",
      "\n",
      "Epoch 00014: val_acc did not improve\n",
      "Epoch 15/20\n",
      "4050/4050 [==============================] - 415s 103ms/step - loss: 0.2034 - acc: 0.9351 - val_loss: 0.6748 - val_acc: 0.7889\n",
      "\n",
      "Epoch 00017: val_acc did not improve\n",
      "Epoch 18/20\n",
      "4050/4050 [==============================] - 411s 101ms/step - loss: 0.1950 - acc: 0.9383 - val_loss: 0.6645 - val_acc: 0.8111\n",
      "\n",
      "Epoch 00018: val_acc did not improve\n",
      "Epoch 19/20\n",
      "4050/4050 [==============================] - 387s 95ms/step - loss: 0.1686 - acc: 0.9469 - val_loss: 0.6877 - val_acc: 0.8022\n",
      "\n",
      "Epoch 00020: val_acc did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f25fc192b38>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.fit(x_train, y_train, validation_split=0.1, verbose=1,\n",
    "          epochs=20, batch_size=512, callbacks=[lstm_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the best lstm model on the test set. The resulting test accuracy is 80.8%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 56s 112ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5992494649887085, 0.8080000009536743]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_lstm_model = load_model('lstm_model')\n",
    "best_lstm_model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Midxed Model of a convulotional layer followed by a lstm layer\n",
    "A convolutional layer followed by a LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Dropout(0.25)(x)\n",
    "# x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = LSTM(lstm_out, recurrent_dropout=0.25, dropout=0.25)(x)\n",
    "preds = Dense(len(category_dict), activation='softmax')(x)\n",
    "\n",
    "mixed_model = Model(sequence_input, preds)\n",
    "mixed_model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['acc'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4050 samples, validate on 450 samples\n",
      "Epoch 1/15\n",
      "4050/4050 [==============================] - 86s 21ms/step - loss: 1.3689 - acc: 0.4881 - val_loss: 0.8911 - val_acc: 0.7267\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.89108, saving model to mixed_model1\n",
      "Epoch 2/15\n",
      "4050/4050 [==============================] - 84s 21ms/step - loss: 0.5549 - acc: 0.8341 - val_loss: 0.5255 - val_acc: 0.8400\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.89108 to 0.52550, saving model to mixed_model1\n",
      "Epoch 3/15\n",
      "4050/4050 [==============================] - 83s 21ms/step - loss: 0.2463 - acc: 0.9175 - val_loss: 0.6100 - val_acc: 0.8178\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/15\n",
      "4050/4050 [==============================] - 84s 21ms/step - loss: 0.1408 - acc: 0.9600 - val_loss: 0.6401 - val_acc: 0.8156\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/15\n",
      "4050/4050 [==============================] - 84s 21ms/step - loss: 0.0865 - acc: 0.9709 - val_loss: 0.6233 - val_acc: 0.8067\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/15\n",
      "4050/4050 [==============================] - 84s 21ms/step - loss: 0.0685 - acc: 0.9760 - val_loss: 0.7155 - val_acc: 0.8156\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/15\n",
      "4050/4050 [==============================] - 85s 21ms/step - loss: 0.0535 - acc: 0.9837 - val_loss: 0.7042 - val_acc: 0.8156\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/15\n",
      "4050/4050 [==============================] - 84s 21ms/step - loss: 0.0453 - acc: 0.9852 - val_loss: 0.7632 - val_acc: 0.8156\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/15\n"
     ]
    }
   ],
   "source": [
    "mixed_callback = ModelCheckpoint('mixed_model1', \n",
    "                                 monitor='val_loss', \n",
    "                                 verbose=1, \n",
    "                                 save_best_only=True, \n",
    "                                 mode='auto')\n",
    "# happy learning!\n",
    "mixed_model.fit(x_train, y_train, validation_split=0.1, \n",
    "          epochs=15, batch_size=512, callbacks=[mixed_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the results for this model are not really interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 8s 16ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5140607929229737, 0.7899999995231628]"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_mixed_model = load_model('mixed_model1')\n",
    "best_mixed_model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning CNN parameters\n",
    "As mentioned earlier tuning kernel size and number of filters is important for convolutional layer. Tuning the activation function could be also effective. However, due to the large number of parameters that need to be learned (this is specially because of word embeddings being trainable) each epoch takes a considerable amount of time. So I didn't used lots of different values for tuning and gave up tuning the dropout parameter. The tuning has been done in a seperate script (deep_learning_parameter_tuning.py).\n",
    "\n",
    "The parameter values for tuning:\n",
    "- kernel size: [3,5,7,9]\n",
    "- number of filters: [100,300,500]\n",
    "- activation function: ['tanh', 'relu']\n",
    "\n",
    "these values are selected according to <a href=\"https://machinelearningmastery.com/best-practices-document-classification-deep-learning/\">this</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_14 (InputLayer)        (None, 5000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 5000, 100)         17528300  \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 4994, 100)         70100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 998, 100)          0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 998, 100)          0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 99800)             0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 128)               12774528  \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 30,373,573\n",
      "Trainable params: 30,373,573\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_cnn_model = load_model('best_cnn_model')\n",
    "best_cnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best accuracy with CNN is 83%. This is achieved by number of filters equal to 100, kernel size equal to 7 and `relu` activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 4s 8ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.761256763458252, 0.83]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_cnn_model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_layers': [['input_14', 0, 0]],\n",
       " 'layers': [{'class_name': 'InputLayer',\n",
       "   'config': {'batch_input_shape': (None, 5000),\n",
       "    'dtype': 'int32',\n",
       "    'name': 'input_14',\n",
       "    'sparse': False},\n",
       "   'inbound_nodes': [],\n",
       "   'name': 'input_14'},\n",
       "  {'class_name': 'Embedding',\n",
       "   'config': {'activity_regularizer': None,\n",
       "    'batch_input_shape': (None, 5000),\n",
       "    'dtype': 'float32',\n",
       "    'embeddings_constraint': None,\n",
       "    'embeddings_initializer': {'class_name': 'RandomUniform',\n",
       "     'config': {'maxval': 0.05, 'minval': -0.05, 'seed': None}},\n",
       "    'embeddings_regularizer': None,\n",
       "    'input_dim': 175283,\n",
       "    'input_length': 5000,\n",
       "    'mask_zero': False,\n",
       "    'name': 'embedding_1',\n",
       "    'output_dim': 100,\n",
       "    'trainable': True},\n",
       "   'inbound_nodes': [[['input_14', 0, 0, {}]]],\n",
       "   'name': 'embedding_1'},\n",
       "  {'class_name': 'Conv1D',\n",
       "   'config': {'activation': 'relu',\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'dilation_rate': (1,),\n",
       "    'filters': 100,\n",
       "    'kernel_constraint': None,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'distribution': 'uniform',\n",
       "      'mode': 'fan_avg',\n",
       "      'scale': 1.0,\n",
       "      'seed': None}},\n",
       "    'kernel_regularizer': None,\n",
       "    'kernel_size': (7,),\n",
       "    'name': 'conv1d_14',\n",
       "    'padding': 'valid',\n",
       "    'strides': (1,),\n",
       "    'trainable': True,\n",
       "    'use_bias': True},\n",
       "   'inbound_nodes': [[['embedding_1', 0, 0, {}]]],\n",
       "   'name': 'conv1d_14'},\n",
       "  {'class_name': 'MaxPooling1D',\n",
       "   'config': {'name': 'max_pooling1d_14',\n",
       "    'padding': 'valid',\n",
       "    'pool_size': (5,),\n",
       "    'strides': (5,),\n",
       "    'trainable': True},\n",
       "   'inbound_nodes': [[['conv1d_14', 0, 0, {}]]],\n",
       "   'name': 'max_pooling1d_14'},\n",
       "  {'class_name': 'Dropout',\n",
       "   'config': {'name': 'dropout_14',\n",
       "    'noise_shape': None,\n",
       "    'rate': 0.25,\n",
       "    'seed': None,\n",
       "    'trainable': True},\n",
       "   'inbound_nodes': [[['max_pooling1d_14', 0, 0, {}]]],\n",
       "   'name': 'dropout_14'},\n",
       "  {'class_name': 'Flatten',\n",
       "   'config': {'name': 'flatten_14', 'trainable': True},\n",
       "   'inbound_nodes': [[['dropout_14', 0, 0, {}]]],\n",
       "   'name': 'flatten_14'},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'activation': 'relu',\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'distribution': 'uniform',\n",
       "      'mode': 'fan_avg',\n",
       "      'scale': 1.0,\n",
       "      'seed': None}},\n",
       "    'kernel_regularizer': None,\n",
       "    'name': 'dense_27',\n",
       "    'trainable': True,\n",
       "    'units': 128,\n",
       "    'use_bias': True},\n",
       "   'inbound_nodes': [[['flatten_14', 0, 0, {}]]],\n",
       "   'name': 'dense_27'},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'activation': 'softmax',\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'distribution': 'uniform',\n",
       "      'mode': 'fan_avg',\n",
       "      'scale': 1.0,\n",
       "      'seed': None}},\n",
       "    'kernel_regularizer': None,\n",
       "    'name': 'dense_28',\n",
       "    'trainable': True,\n",
       "    'units': 5,\n",
       "    'use_bias': True},\n",
       "   'inbound_nodes': [[['dense_27', 0, 0, {}]]],\n",
       "   'name': 'dense_28'}],\n",
       " 'name': 'model_14',\n",
       " 'output_layers': [['dense_28', 0, 0]]}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_cnn_model.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, we see that deep learning methods do not give the best result. This results could improve further if more range of values could have been explored in parameter tuning. Apart from that deep neural network are generally more powerfull compared to classic linear models, when there are lots of data points. However, our dataset in this problem is rather small."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
