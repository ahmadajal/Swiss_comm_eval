,title,pageid,length,content
0,Artificial intelligence,1164,231620,"Artificial intelligence (AI, also machine intelligence, MI) is intelligence demonstrated by machines, in contrast to the natural intelligence (NI) displayed by humans and other animals. In computer science AI research is defined as the study of ""intelligent agents"": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals. Colloquially, the term ""artificial intelligence"" is applied when a machine mimics ""cognitive"" functions that humans associate with other human minds, such as ""learning"" and ""problem solving"". See glossary of artificial intelligence.
The scope of AI is disputed: as machines become increasingly capable, tasks considered as requiring ""intelligence"" are often removed from the definition, a phenomenon known as the AI effect, leading to the quip ""AI is whatever hasn't been done yet."" For instance, optical character recognition is frequently excluded from ""artificial intelligence"", having become a routine technology. Capabilities generally classified as AI as of 2017 include successfully understanding human speech, competing at the highest level in strategic game systems (such as chess and Go), autonomous cars, intelligent routing in content delivery network and military simulations.
Artificial intelligence was founded as an academic discipline in 1956, and in the years since has experienced several waves of optimism, followed by disappointment and the loss of funding (known as an ""AI winter""), followed by new approaches, success and renewed funding. For most of its history, AI research has been divided into subfields that often fail to communicate with each other. These sub-fields are based on technical considerations, such as particular goals (e.g. ""robotics"" or ""machine learning""), the use of particular tools (""logic"" or ""neural networks""), or deep philosophical differences. Subfields have also been based on social factors (particular institutions or the work of particular researchers).
The traditional problems (or goals) of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception and the ability to move and manipulate objects. General intelligence is among the field's long-term goals. Approaches include statistical methods, computational intelligence, and traditional symbolic AI. Many tools are used in AI, including versions of search and mathematical optimization, neural networks and methods based on statistics, probability and economics. The AI field draws upon computer science, mathematics, psychology, linguistics, philosophy and many others.
The field was founded on the claim that human intelligence ""can be so precisely described that a machine can be made to simulate it"". This raises philosophical arguments about the nature of the mind and the ethics of creating artificial beings endowed with human-like intelligence, issues which have been explored by myth, fiction and philosophy since antiquity. Some people also consider AI to be a danger to humanity if it progresses unabatedly. Others believe that AI, unlike previous technological revolutions, will create a risk of mass unemployment.
In the twenty-first century, AI techniques have experienced a resurgence following concurrent advances in computer power, large amounts of data, and theoretical understanding; and AI techniques have become an essential part of the technology industry, helping to solve many challenging problems in computer science.


== History ==

While thought-capable artificial beings appeared as storytelling devices in antiquity, the idea of actually trying to build a machine to perform useful reasoning may have begun with Ramon Llull (c. 1300 CE). With his Calculus ratiocinator, Gottfried Leibniz extended the concept of the calculating machine (Wilhelm Schickard engineered the first one around 1623), intending to perform operations on concepts rather than numbers. Since the 19th century, artificial beings are common in fiction, as in Mary Shelley's Frankenstein or Karel Čapek's R.U.R. (Rossum's Universal Robots).
The study of mechanical or ""formal"" reasoning began with philosophers and mathematicians in antiquity. The study of mathematical logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as ""0"" and ""1"", could simulate any conceivable act of mathematical deduction. This insight, that digital computers can simulate any process of formal reasoning, is known as the Church–Turing thesis. Along with concurrent discoveries in neurobiology, information theory and cybernetics, this led researchers to consider the possibility of building an electronic brain. The first work that is now generally recognized as AI was McCullouch and Pitts' 1943 formal design for Turing-complete ""artificial neurons"".
The field of AI research was born at a workshop at Dartmouth College in 1956. Attendees Allen Newell (CMU), Herbert Simon (CMU), John McCarthy (MIT), Marvin Minsky (MIT) and Arthur Samuel (IBM) became the founders and leaders of AI research. They and their students produced programs that the press described as ""astonishing"": computers were learning checkers strategies (c. 1954) (and by 1959 were reportedly playing better than the average human), solving word problems in algebra, proving logical theorems (Logic Theorist, first run c. 1956) and speaking English. By the middle of the 1960s, research in the U.S. was heavily funded by the Department of Defense and laboratories had been established around the world. AI's founders were optimistic about the future: Herbert Simon predicted, ""machines will be capable, within twenty years, of doing any work a man can do"". Marvin Minsky agreed, writing, ""within a generation ... the problem of creating 'artificial intelligence' will substantially be solved"".
They failed to recognize the difficulty of some of the remaining tasks. Progress slowed and in 1974, in response to the criticism of Sir James Lighthill and ongoing pressure from the US Congress to fund more productive projects, both the U.S. and British governments cut off exploratory research in AI. The next few years would later be called an ""AI winter"", a period when obtaining funding for AI projects was difficult.
In the early 1980s, AI research was revived by the commercial success of expert systems, a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985 the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S and British governments to restore funding for academic research. However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting hiatus began.
In the late 1990s and early 21st century, AI began to be used for logistics, data mining, medical diagnosis and other areas. The success was due to increasing computational power (see Moore's law), greater emphasis on solving specific problems, new ties between AI and other fields (such as statistics, economics and mathematics), and a commitment by researchers to mathematical methods and scientific standards. Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov on 11 May 1997.
Advanced statistical techniques (loosely known as deep learning), access to large amounts of data and faster computers enabled advances in machine learning and perception. By the mid 2010s, machine learning applications were used throughout the world. In a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy champions, Brad Rutter and Ken Jennings, by a significant margin. The Kinect, which provides a 3D body–motion interface for the Xbox 360 and the Xbox One use algorithms that emerged from lengthy AI research as do intelligent personal assistants in smartphones. In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps. In the 2017 Future of Go Summit, AlphaGo won a three-game match with Ke Jie, who at the time continuously held the world No. 1 ranking for two years. This marked the completion of a significant milestone in the development of Artificial Intelligence as Go is an extremely complex game, more so than Chess.
According to Bloomberg's Jack Clark, 2015 was a landmark year for artificial intelligence, with the number of software projects that use AI within Google increased from a ""sporadic usage"" in 2012 to more than 2,700 projects. Clark also presents factual data indicating that error rates in image processing tasks have fallen significantly since 2011. He attributes this to an increase in affordable neural networks, due to a rise in cloud computing infrastructure and to an increase in research tools and datasets. Other cited examples include Microsoft's development of a Skype system that can automatically translate from one language to another and Facebook's system that can describe images to blind people.


== Basics ==
A typical AI perceives its environment and takes actions that maximize its chance of successfully achieving its goals. An AI's intended goal function can be simple (""1 if the AI wins a game of Go, 0 otherwise"") or complex (""Do actions mathematically similar to the actions that got you rewards in the past""). Goals can be explicitly defined, or can be induced. If the AI is programmed for ""reinforcement learning"", goals can be implicitly induced by rewarding some types of behavior and punishing others. Alternatively, an evolutionary system can induce goals by using a ""fitness function"" to mutate and preferentially replicate high-scoring AI systems; this is similar to how animals evolved to innately desire certain goals such as finding food, or how dogs can be bred via artificial selection to possess desired traits. Some AI systems, such as nearest-neighbor, instead reason by analogy; these systems are not generally given goals, except to the degree that goals are somehow implicit in their training data. Such systems can still be benchmarked if the non-goal system is framed as a system whose ""goal"" is to successfully accomplish its narrow classification task.
AI often revolves around the use of algorithms. An algorithm is a set of unambiguous instructions that a mechanical computer can execute. A complex algorithm is often built on top of other, simpler, algorithms. A simple example of an algorithm is the following recipe for optimal play at tic-tac-toe:
If someone has a ""threat"" (that is, two in a row), take the remaining square. Otherwise,
if a move ""forks"" to create two threats at once, play that move. Otherwise,
take the center square if it is free. Otherwise,
if your opponent has played in a corner, take the opposite corner. Otherwise,
take an empty corner if one exists. Otherwise,
take any empty square.
Many AI algorithms are capable of learning from data; they can enhance themselves by learning new heuristics (strategies, or ""rules of thumb"", that have worked well in the past), or can themselves write other algorithms. Some of the ""learners"" described below, including Bayesian networks, decision trees, and nearest-neighbor, could theoretically, if given infinite data, time, and memory, learn to approximate any function, including whatever combination of mathematical functions would best describe the entire world. These learners could therefore, in theory, derive all possible knowledge, by considering every possible hypothesis and matching it against the data. In practice, it is almost never possible to consider every possibility, because of the phenomenon of ""combinatorial explosion"", where the amount of time needed to solve a problem grows exponentially. Much of AI research involves figuring out how to identify and avoid considering broad swaths of possibililities that are unlikely to be fruitful. For example, when viewing a map and looking for the shortest driving route from Denver to New York in the East, one can in most cases skip looking at any path through San Francisco or other areas far to the West; thus, an AI wielding an pathfinding algorithm like A* can avoid the combinatorial explosion that would ensue if every possible route had to be ponderously considered in turn.
The earliest (and easiest to understand) approach to AI was symbolism (such as formal logic): ""If an otherwise healthy adult has a fever, then they may have influenza"". A second, more general, approach is Bayesian inference: ""If the current patient has a fever, adjust the probability they have influenza in such-and-such way"". The third major approach, extremely popular in routine business AI applications, is analogizers such as SVM and nearest-neighbor: ""After examining the records of known past patients whose temperature, symptoms, age, and other factors mostly match the current patient, X% of those patients turned out to have influenza"". A fourth approach is harder to intuitively understand, but is inspired by how the brain's machinery works: the neural network approach uses artificial ""neurons"" that can learn by comparing itself to the desired output and altering the strengths of the connections between its internal neurons to ""reinforce"" connections that seemed to be useful. These four main approaches can overlap with each other and with evolutionary systems; for example, neural nets can learn to make inferences, to generalize, and to make analogies. Some systems implicitly or explicitly use multiple of these approaches, alongside many other AI and non-AI algorithms; the best approach is often different depending on the problem.
Learners work on the basis that strategies, algorithms, and inferences that worked well in the past are likely to continue working well in the future. These inferences can be obvious, such as ""since the sun rose every morning for the last 10,000 days, it will probably rise tomorrow morning as well"". They can be nuanced, such as ""X% of families have geographically separate species with color variants, so there is an Y% chance that undiscovered black swans exist"". Learners also work on the basis of ""Occam's razor"": The simplest theory that explains the data is the likeliest. Therefore, to be successful, a learner must be designed such that it prefers simpler theories to complex theories, except in cases where the complex theory is proven substantially better. Settling on a bad, overly complex theory gerrymandered to fit all the past training data is known as overfitting. Many systems attempt to reduce overfitting by rewarding a theory in accordance with how well it fits the data, but penalizing the theory in accordance with how complex the theory is. Besides classic overfitting, learners can also disappoint by ""learning the wrong lesson"". A toy example is that an image classifier trained only on pictures of brown horses and black cats might conclude that all brown patches are likely to be horses. A real-world example is that, unlike humans, current image classifiers don't determine the spatial relationship between components of the picture; instead, they learn abstract patterns of pixels that humans are oblivious to, but that linearly correlate with images of certain types of real objects. Faintly superimposing such a pattern on a legitimate image results in an ""adversarial"" image that the system misclassifies.
Compared with humans, existing AI lacks several features of human ""commonsense reasoning""; most notably, humans appear to have powerful mechanisms for reasoning about ""naïve physics"" such as space, time, and physical interactions. This enables even young children to easily make inferences like ""If I roll this object off a table, it will fall on the floor"". Humans also appear to have a powerful mechanism of ""folk psychology"" that helps them to interpret natural-language sentences such as ""The city councilmen refused the demonstrators a permit because they advocated violence"". (A generic AI has difficulty inferring whether the councilmen or the demonstrators are the ones alleged to be advocating violence.) This lack of ""common knowledge"" means that AI often makes different mistakes than humans make, in ways that can seem incomprehensible. For example, existing self-driving cars cannot reason about the location nor the intentions of pedestrians in the exact way that humans do, and instead must use non-human modes of reasoning to avoid accidents.


== Problems ==
The overall research goal of artificial intelligence is to create technology that allows computers and machines to function in an intelligent manner. The general problem of simulating (or creating) intelligence has been broken down into sub-problems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention.


=== Reasoning, problem solving ===
Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, AI research had developed methods for dealing with uncertain or incomplete information, employing concepts from probability and economics.
These algorithms proved to be insufficient for solving large reasoning problems, because they experienced a ""combinatorial explosion"": they became exponentially slower as the problems grew larger. In fact, even humans rarely use the step-by-step deduction that early AI research was able to model. They solve most of their problems using fast, intuitive judgements. Modern statistical approaches to AI (e.g. neural networks) mimic this human ability to make a quick guess based on experience, solving many problems as people do. However, they are not capable of step-by-step deduction.


=== Knowledge representation ===

Knowledge representation and knowledge engineering are central to AI research. Many of the problems machines are expected to solve will require extensive knowledge about the world. Among the things that AI needs to represent are: objects, properties, categories and relations between objects; situations, events, states and time; causes and effects; knowledge about knowledge (what we know about what other people know); and many other, less well researched domains. A representation of ""what exists"" is an ontology: the set of objects, relations, concepts, and properties formally described so that software agents can interpret them. The semantics of these are captured as description logic concepts, roles, and individuals, and typically implemented as classes, properties, and individuals in the Web Ontology Language. The most general ontologies are called upper ontologies, which attempt to provide a foundation for all other knowledge by acting as mediators between domain ontologies that cover specific knowledge about a particular knowledge domain (field of interest or area of concern). Such formal knowledge representations are suitable for content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery via automated reasoning (inferring new statements based on explicitly stated knowledge), etc. Video events are often represented as SWRL rules, which can be used, among others, to automatically generate subtitles for constrained videos.
Among the most difficult problems in knowledge representation are:
Default reasoning and the qualification problem
Many of the things people know take the form of ""working assumptions"". For example, if a bird comes up in conversation, people typically picture an animal that is fist sized, sings, and flies. None of these things are true about all birds. John McCarthy identified this problem in 1969 as the qualification problem: for any commonsense rule that AI researchers care to represent, there tend to be a huge number of exceptions. Almost nothing is simply true or false in the way that abstract logic requires. AI research has explored a number of solutions to this problem.
The breadth of commonsense knowledge
The number of atomic facts that the average person knows is very large. Research projects that attempt to build a complete knowledge base of commonsense knowledge (e.g., Cyc) require enormous amounts of laborious ontological engineering—they must be built, by hand, one complicated concept at a time.
The subsymbolic form of some commonsense knowledge
Much of what people know is not represented as ""facts"" or ""statements"" that they could express verbally. For example, a chess master will avoid a particular chess position because it ""feels too exposed"" or an art critic can take one look at a statue and realize that it is a fake. These are non-conscious and sub-symbolic intuitions or tendencies in the human brain. Knowledge like this informs, supports and provides a context for symbolic, conscious knowledge. As with the related problem of sub-symbolic reasoning, it is hoped that situated AI, computational intelligence, or statistical AI will provide ways to represent this kind of knowledge.


=== Planning ===

Intelligent agents must be able to set goals and achieve them. They need a way to visualize the future—a representation of the state of the world and be able to make predictions about how their actions will change it—and be able to make choices that maximize the utility (or ""value"") of available choices.
In classical planning problems, the agent can assume that it is the only system acting in the world, allowing the agent to be certain of the consequences of its actions. However, if the agent is not the only actor, then it requires that the agent can reason under uncertainty. This calls for an agent that can not only assess its environment and make predictions, but also evaluate its predictions and adapt based on its assessment.
Multi-agent planning uses the cooperation and competition of many agents to achieve a given goal. Emergent behavior such as this is used by evolutionary algorithms and swarm intelligence.


=== Learning ===

Machine learning, a fundamental concept of AI research since the field's inception, is the study of computer algorithms that improve automatically through experience.
Unsupervised learning is the ability to find patterns in a stream of input. Supervised learning includes both classification and numerical regression. Classification is used to determine what category something belongs in, after seeing a number of examples of things from several categories. Regression is the attempt to produce a function that describes the relationship between inputs and outputs and predicts how the outputs should change as the inputs change. In reinforcement learning the agent is rewarded for good responses and punished for bad ones. The agent uses this sequence of rewards and punishments to form a strategy for operating in its problem space. These three types of learning can be analyzed in terms of decision theory, using concepts like utility. The mathematical analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory.
Within developmental robotics, developmental learning approaches are elaborated upon to allow robots to accumulate repertoires of novel skills through autonomous self-exploration, social interaction with human teachers, and the use of guidance mechanisms (active learning, maturation, motor synergies, etc.).


=== Natural language processing ===

Natural language processing gives machines the ability to read and understand human language. A sufficiently powerful natural language processing system would enable natural language user interfaces and the acquisition of knowledge directly from human-written sources, such as newswire texts. Some straightforward applications of natural language processing include information retrieval, text mining, question answering and machine translation.
A common method of processing and extracting meaning from natural language is through semantic indexing. Although these indexes require a large volume of user input, it is expected that increases in processor speeds and decreases in data storage costs will result in greater efficiency.


=== Perception ===

Machine perception is the ability to use input from sensors (such as cameras, microphones, tactile sensors, sonar and others) to deduce aspects of the world. Computer vision is the ability to analyze visual input. A few selected subproblems are speech recognition, facial recognition and object recognition.


=== Motion and manipulation ===

The field of robotics is closely related to AI. Intelligence is required for robots to handle tasks such as object manipulation and navigation, with sub-problems such as localization, mapping, and motion planning. These systems require that an agent is able to: Be spatially cognizant of its surroundings, learn from and build a map of its environment, figure out how to get from one point in space to another, and execute that movement (which often involves compliant motion, a process where movement requires maintaining physical contact with an object).


=== Social intelligence ===

Affective computing is the study and development of systems that can recognize, interpret, process, and simulate human affects. It is an interdisciplinary field spanning computer sciences, psychology, and cognitive science. While the origins of the field may be traced as far back as the early philosophical inquiries into emotion, the more modern branch of computer science originated with Rosalind Picard's 1995 paper on ""affective computing"". A motivation for the research is the ability to simulate empathy, where the machine would be able to interpret human emotions and adapts its behavior to give an appropriate response to those emotions.
Emotion and social skills are important to an intelligent agent for two reasons. First, being able to predict the actions of others by understanding their motives and emotional states allow an agent to make better decisions. Concepts such as game theory, decision theory, necessitate that an agent be able to detect and model human emotions. Second, in an effort to facilitate human–computer interaction, an intelligent machine may want to display emotions (even if it does not experience those emotions itself) to appear more sensitive to the emotional dynamics of human interaction.


=== Creativity ===

A sub-field of AI addresses creativity both theoretically (the philosophical psychological perspective) and practically (the specific implementation of systems that generate novel and useful outputs).


=== General intelligence ===

Many researchers think that their work will eventually be incorporated into a machine with artificial general intelligence, combining all the skills mentioned above and even exceeding human ability in most or all these areas. A few believe that anthropomorphic features like artificial consciousness or an artificial brain may be required for such a project.
Many of the problems above may also require general intelligence, if machines are to solve the problems as well as people do. For example, even specific straightforward tasks, like machine translation, require that a machine read and write in both languages (NLP), follow the author's argument (reason), know what is being talked about (knowledge), and faithfully reproduce the author's original intent (social intelligence). A problem like machine translation is considered ""AI-complete"", because all of these problems need to be solved simultaneously in order to reach human-level machine performance.


== Approaches ==
There is no established unifying theory or paradigm that guides AI research. Researchers disagree about many issues. A few of the most long standing questions that have remained unanswered are these: should artificial intelligence simulate natural intelligence by studying psychology or neurobiology? Or is human biology as irrelevant to AI research as bird biology is to aeronautical engineering? Can intelligent behavior be described using simple, elegant principles (such as logic or optimization)? Or does it necessarily require solving a large number of completely unrelated problems? Can intelligence be reproduced using high-level symbols, similar to words and ideas? Or does it require ""sub-symbolic"" processing? John Haugeland, who coined the term GOFAI (Good Old-Fashioned Artificial Intelligence), also proposed that AI should more properly be referred to as synthetic intelligence, a term which has since been adopted by some non-GOFAI researchers.
Stuart Shapiro divides AI research into three approaches, which he calls computational psychology, computational philosophy, and computer science. Computational psychology is used to make computer programs that mimic human behavior. Computational philosophy, is used to develop an adaptive, free-flowing computer mind. Implementing computer science serves the goal of creating computers that can perform tasks that only people could previously accomplish. Together, the humanesque behavior, mind, and actions make up artificial intelligence.


=== Cybernetics and brain simulation ===

In the 1940s and 1950s, a number of researchers explored the connection between neurobiology, information theory, and cybernetics. Some of them built machines that used electronic networks to exhibit rudimentary intelligence, such as W. Grey Walter's turtles and the Johns Hopkins Beast. Many of these researchers gathered for meetings of the Teleological Society at Princeton University and the Ratio Club in England. By 1960, this approach was largely abandoned, although elements of it would be revived in the 1980s.


=== Symbolic ===

When access to digital computers became possible in the middle 1950s, AI research began to explore the possibility that human intelligence could be reduced to symbol manipulation. The research was centered in three institutions: Carnegie Mellon University, Stanford and MIT, and each one developed its own style of research. John Haugeland named these approaches to AI ""good old fashioned AI"" or ""GOFAI"". During the 1960s, symbolic approaches had achieved great success at simulating high-level thinking in small demonstration programs. Approaches based on cybernetics or neural networks were abandoned or pushed into the background. Researchers in the 1960s and the 1970s were convinced that symbolic approaches would eventually succeed in creating a machine with artificial general intelligence and considered this the goal of their field.


==== Cognitive simulation ====
Economist Herbert Simon and Allen Newell studied human problem-solving skills and attempted to formalize them, and their work laid the foundations of the field of artificial intelligence, as well as cognitive science, operations research and management science. Their research team used the results of psychological experiments to develop programs that simulated the techniques that people used to solve problems. This tradition, centered at Carnegie Mellon University would eventually culminate in the development of the Soar architecture in the middle 1980s.


==== Logic-based ====
Unlike Newell and Simon, John McCarthy felt that machines did not need to simulate human thought, but should instead try to find the essence of abstract reasoning and problem solving, regardless of whether people used the same algorithms. His laboratory at Stanford (SAIL) focused on using formal logic to solve a wide variety of problems, including knowledge representation, planning and learning. Logic was also the focus of the work at the University of Edinburgh and elsewhere in Europe which led to the development of the programming language Prolog and the science of logic programming.


==== Anti-logic or scruffy ====
Researchers at MIT (such as Marvin Minsky and Seymour Papert) found that solving difficult problems in vision and natural language processing required ad-hoc solutions – they argued that there was no simple and general principle (like logic) that would capture all the aspects of intelligent behavior. Roger Schank described their ""anti-logic"" approaches as ""scruffy"" (as opposed to the ""neat"" paradigms at CMU and Stanford). Commonsense knowledge bases (such as Doug Lenat's Cyc) are an example of ""scruffy"" AI, since they must be built by hand, one complicated concept at a time.


==== Knowledge-based ====
When computers with large memories became available around 1970, researchers from all three traditions began to build knowledge into AI applications. This ""knowledge revolution"" led to the development and deployment of expert systems (introduced by Edward Feigenbaum), the first truly successful form of AI software. The knowledge revolution was also driven by the realization that enormous amounts of knowledge would be required by many simple AI applications.


=== Sub-symbolic ===
By the 1980s progress in symbolic AI seemed to stall and many believed that symbolic systems would never be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition. A number of researchers began to look into ""sub-symbolic"" approaches to specific AI problems. Sub-symbolic methods manage to approach intelligence without specific representations of knowledge.


==== Embodied intelligence ====
This includes embodied, situated, behavior-based, and nouvelle AI. Researchers from the related field of robotics, such as Rodney Brooks, rejected symbolic AI and focused on the basic engineering problems that would allow robots to move and survive. Their work revived the non-symbolic viewpoint of the early cybernetics researchers of the 1950s and reintroduced the use of control theory in AI. This coincided with the development of the embodied mind thesis in the related field of cognitive science: the idea that aspects of the body (such as movement, perception and visualization) are required for higher intelligence.


==== Computational intelligence and soft computing ====
Interest in neural networks and ""connectionism"" was revived by David Rumelhart and others in the middle of the 1980s. Neural networks are an example of soft computing --- they are solutions to problems which cannot be solved with complete logical certainty, and where an approximate solution is often sufficient. Other soft computing approaches to AI include fuzzy systems, evolutionary computation and many statistical tools. The application of soft computing to AI is studied collectively by the emerging discipline of computational intelligence.


=== Statistical ===
In the 1990s, AI researchers developed sophisticated mathematical tools to solve specific subproblems. These tools are truly scientific, in the sense that their results are both measurable and verifiable, and they have been responsible for many of AI's recent successes. The shared mathematical language has also permitted a high level of collaboration with more established fields (like mathematics, economics or operations research). Stuart Russell and Peter Norvig describe this movement as nothing less than a ""revolution"" and ""the victory of the neats"". Critics argue that these techniques (with few exceptions) are too focused on particular problems and have failed to address the long-term goal of general intelligence. There is an ongoing debate about the relevance and validity of statistical approaches in AI, exemplified in part by exchanges between Peter Norvig and Noam Chomsky.


=== Integrating the approaches ===
Intelligent agent paradigm
An intelligent agent is a system that perceives its environment and takes actions which maximize its chances of success. The simplest intelligent agents are programs that solve specific problems. More complicated agents include human beings and organizations of human beings (such as firms). The paradigm gives researchers license to study isolated problems and find solutions that are both verifiable and useful, without agreeing on one single approach. An agent that solves a specific problem can use any approach that works – some agents are symbolic and logical, some are sub-symbolic neural networks and others may use new approaches. The paradigm also gives researchers a common language to communicate with other fields—such as decision theory and economics—that also use concepts of abstract agents. The intelligent agent paradigm became widely accepted during the 1990s.
Agent architectures and cognitive architectures
Researchers have designed systems to build intelligent systems out of interacting intelligent agents in a multi-agent system. A system with both symbolic and sub-symbolic components is a hybrid intelligent system, and the study of such systems is artificial intelligence systems integration. A hierarchical control system provides a bridge between sub-symbolic AI at its lowest, reactive levels and traditional symbolic AI at its highest levels, where relaxed time constraints permit planning and world modelling. Rodney Brooks' subsumption architecture was an early proposal for such a hierarchical system.


== Tools ==
In the course of 60 or so years of research, AI has developed a large number of tools to solve the most difficult problems in computer science. A few of the most general of these methods are discussed below.


=== Search and optimization ===

Many problems in AI can be solved in theory by intelligently searching through many possible solutions: Reasoning can be reduced to performing a search. For example, logical proof can be viewed as searching for a path that leads from premises to conclusions, where each step is the application of an inference rule. Planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis. Robotics algorithms for moving limbs and grasping objects use local searches in configuration space. Many learning algorithms use search algorithms based on optimization.
Simple exhaustive searches are rarely sufficient for most real world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes. The solution, for many problems, is to use ""heuristics"" or ""rules of thumb"" that prioritize choices in favor of those that are more likely to reach a goal, and to do so in a shorter number of steps. In some search methodologies heuristics can also serve to entirely eliminate some choices that are unlikely to lead to a goal (called ""pruning the search tree""). Heuristics supply the program with a ""best guess"" for the path on which the solution lies. Heuristics limit the search for solutions into a smaller sample size.
A very different kind of search came to prominence in the 1990s, based on the mathematical theory of optimization. For many problems, it is possible to begin the search with some form of a guess and then refine the guess incrementally until no more refinements can be made. These algorithms can be visualized as blind hill climbing: we begin the search at a random point on the landscape, and then, by jumps or steps, we keep moving our guess uphill, until we reach the top. Other optimization algorithms are simulated annealing, beam search and random optimization.
Evolutionary computation uses a form of optimization search. For example, they may begin with a population of organisms (the guesses) and then allow them to mutate and recombine, selecting only the fittest to survive each generation (refining the guesses). Forms of evolutionary computation include swarm intelligence algorithms (such as ant colony or particle swarm optimization) and evolutionary algorithms (such as genetic algorithms, gene expression programming, and genetic programming).


=== Logic ===

Logic is used for knowledge representation and problem solving, but it can be applied to other problems as well. For example, the satplan algorithm uses logic for planning and inductive logic programming is a method for learning.
Several different forms of logic are used in AI research. Propositional or sentential logic is the logic of statements which can be true or false. First-order logic also allows the use of quantifiers and predicates, and can express facts about objects, their properties, and their relations with each other. Fuzzy logic, is a version of first-order logic which allows the truth of a statement to be represented as a value between 0 and 1, rather than simply True (1) or False (0). Fuzzy systems can be used for uncertain reasoning and have been widely used in modern industrial and consumer product control systems. Subjective logic models uncertainty in a different and more explicit manner than fuzzy-logic: a given binomial opinion satisfies belief + disbelief + uncertainty = 1 within a Beta distribution. By this method, ignorance can be distinguished from probabilistic statements that an agent makes with high confidence.
Default logics, non-monotonic logics and circumscription are forms of logic designed to help with default reasoning and the qualification problem. Several extensions of logic have been designed to handle specific domains of knowledge, such as: description logics; situation calculus, event calculus and fluent calculus (for representing events and time); causal calculus; belief calculus; and modal logics.


=== Probabilistic methods for uncertain reasoning ===

Many problems in AI (in reasoning, planning, learning, perception and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of powerful tools to solve these problems using methods from probability theory and economics.
Bayesian networks are a very general tool that can be used for a large number of problems: reasoning (using the Bayesian inference algorithm), learning (using the expectation-maximization algorithm), planning (using decision networks) and perception (using dynamic Bayesian networks). Bayesian networks are used in AdSense to choose what ads to place and on XBox Live to rate and match players. Probabilistic algorithms can also be used for filtering, prediction, smoothing and finding explanations for streams of data, helping perception systems to analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).
A key concept from the science of economics is ""utility"": a measure of how valuable something is to an intelligent agent. Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis, and information value theory. These tools include models such as Markov decision processes, dynamic decision networks, game theory and mechanism design.


=== Classifiers and statistical learning methods ===

The simplest AI applications can be divided into two types: classifiers (""if shiny then diamond"") and controllers (""if shiny then pick up""). Controllers do, however, also classify conditions before inferring actions, and therefore classification forms a central part of many AI systems. Classifiers are functions that use pattern matching to determine a closest match. They can be tuned according to examples, making them very attractive for use in AI. These examples are known as observations or patterns. In supervised learning, each pattern belongs to a certain predefined class. A class can be seen as a decision that has to be made. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.
A classifier can be trained in various ways; there are many statistical and machine learning approaches. The decision tree is perhaps the most widely used machine learning algorithm. Other widely used classifiers are the neural network, k-nearest neighbor algorithm, kernel methods such as the support vector machine (SVM), Gaussian mixture model and the extremely popular naive Bayes classifier. The performance of these classifiers have been compared over a wide range of tasks. Classifier performance depends greatly on the characteristics of the data to be classified. There is no single classifier that works best on all given problems; this is also referred to as the ""no free lunch"" theorem. Determining a suitable classifier for a given problem is still more an art than science.


=== Artificial neural networks ===

Neural networks, or neural nets, were inspired by the architecture of neurons in the human brain. A simple ""neuron"" N accepts input from multiple other neurons, each of which, when activated (or ""fired""), cast a weighted ""vote"" for or against whether neuron N should itself activate. Learning requires an algorithm to adjust these weights based on the training data; one simple algorithm (dubbed ""fire together, wire together"") is to increase the weight between two connected neurons when the activation of one triggers the successful activation of another. The net forms ""concepts"" that are distributed among a subnetwork of shared neurons that tend to fire together; a concept meaning ""leg"" might be coupled with a subnetwork meaning ""foot"" that includes the sound for ""foot"". Neurons have a continuous spectrum of activation; in addition, neurons can process inputs in a nonlinear way rather than weighing straightforward votes. Modern neural nets can learn both continuous functions and, surprisingly, digital logical operations. Neural networks' early successes included predicting the stock market and (in 1995) a mostly self-driving car. In the 2010s, advances in neural networks using deep learning thrust AI into widespread public consciousness and contributed to an enormous upshift in corporate AI spending; for example, AI-related M&A in 2017 was over 25 times as large as in 2015.
The study of non-learning artificial neural networks began in the decade before the field of AI research was founded, in the work of Walter Pitts and Warren McCullouch. Frank Rosenblatt invented the perceptron, a learning network with a single layer, similar to the old concept of linear regression. Early pioneers also include Alexey Grigorevich Ivakhnenko, Teuvo Kohonen, Stephen Grossberg, Kunihiko Fukushima, Christoph von der Malsburg, David Willshaw, Shun-Ichi Amari, Bernard Widrow, John Hopfield, Eduardo R. Caianiello, and others.
The main categories of networks are acyclic or feedforward neural networks (where the signal passes in only one direction) and recurrent neural networks (which allow feedback and short-term memories of previous input events). Among the most popular feedforward networks are perceptrons, multi-layer perceptrons and radial basis networks. Neural networks can be applied to the problem of intelligent control (for robotics) or learning, using such techniques as Hebbian learning (""fire together, wire together""), GMDH or competitive learning.
Today, neural networks are often trained by the backpropagation algorithm, which had been around since 1970 as the reverse mode of automatic differentiation published by Seppo Linnainmaa, and was introduced to neural networks by Paul Werbos.
Hierarchical temporal memory is an approach that models some of the structural and algorithmic properties of the neocortex.
In short, most neural networks use some form of gradient descent on a hand-created neural topology. However, some research groups, such as Uber, argue that simple neuroevolution to mutate new neural network topologies and weights may be competitive with sophisticated gradient descent approaches. One advantage of neuroevolution is that it may be less prone to get caught in ""dead ends"".


==== Deep feedforward neural networks ====

Deep learning is any artificial neural network that can learn a long chain of causal links. For example, a feedforward network with six hidden layers can learn a seven-link causal chain (six hidden layers + output layer) and has a ""credit assignment path"" (CAP) depth of seven. Many deep learning systems need to be able to learn chains ten or more causal links in length. Deep learning has transformed many important subfields of artificial intelligence, including computer vision, speech recognition, natural language processing and others.
According to one overview, the expression ""Deep Learning"" was introduced to the Machine Learning community by Rina Dechter in 1986 and gained traction after Igor Aizenberg and colleagues introduced it to Artificial Neural Networks in 2000. The first functional Deep Learning networks were published by Alexey Grigorevich Ivakhnenko and V. G. Lapa in 1965. These networks are trained one layer at a time. Ivakhnenko's 1971 paper describes the learning of a deep feedforward multilayer perceptron with eight layers, already much deeper than many later networks. In 2006, a publication by Geoffrey Hinton and Ruslan Salakhutdinov introduced another way of pre-training many-layered feedforward neural networks (FNNs) one layer at a time, treating each layer in turn as an unsupervised restricted Boltzmann machine, then using supervised backpropagation for fine-tuning. Similar to shallow artificial neural networks, deep neural networks can model complex non-linear relationships. Over the last few years, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer.
Deep learning often uses convolutional neural networks (CNNs), whose origins can be traced back to the Neocognitron introduced by Kunihiko Fukushima in 1980. In 1989, Yann LeCun and colleagues applied backpropagation to such an architecture. In the early 2000s, in an industrial application CNNs already processed an estimated 10% to 20% of all the checks written in the US. Since 2011, fast implementations of CNNs on GPUs have won many visual pattern recognition competitions.
CNNs with 12 convolutional layers were used in conjunction with reinforcement learning by Deepmind's ""AlphaGo Lee"", the program that beat a top Go champion in 2016.


==== Deep recurrent neural networks ====

Early on, deep learning was also applied to sequence learning with recurrent neural networks (RNNs) which are in theory Turing complete and can run arbitrary programs to process arbitrary sequences of inputs. The depth of an RNN is unlimited and depends on the length of its input sequence; thus, an RNN is an example of deep learning. RNNs can be trained by gradient descent but suffer from the vanishing gradient problem. In 1992, it was shown that unsupervised pre-training of a stack of recurrent neural networks can speed up subsequent supervised learning of deep sequential problems.
Numerous researchers now use variants of a deep learning recurrent NN called the long short-term memory (LSTM) network published by Hochreiter & Schmidhuber in 1997. LSTM is often trained by Connectionist Temporal Classification (CTC). At Google, Microsoft and Baidu this approach has revolutionised speech recognition. For example, in 2015, Google's speech recognition experienced a dramatic performance jump of 49% through CTC-trained LSTM, which is now available through Google Voice to billions of smartphone users. Google also used LSTM to improve machine translation, Language Modeling and Multilingual Language Processing. LSTM combined with CNNs also improved automatic image captioning and a plethora of other applications.


=== Languages ===

Early symbolic AI inspired Lisp and Prolog, which dominated early AI programming. Modern AI development often uses mainstream languages such as Python or C++, or niche languages such as Wolfram Language.


=== Evaluating progress ===

In 1950, Alan Turing proposed a general procedure to test the intelligence of an agent now known as the Turing test. This procedure allows almost all the major problems of artificial intelligence to be tested. However, it is a very difficult challenge and at present all agents fail.
Artificial intelligence can also be evaluated on specific problems such as small problems in chemistry, hand-writing recognition and game-playing. Such tests have been termed subject matter expert Turing tests. Smaller problems provide more achievable goals and there are an ever-increasing number of positive results.
For example, performance at draughts (i.e. checkers) is optimal, performance at chess is high-human and nearing super-human (see computer chess: computers versus human) and performance at many everyday tasks (such as recognizing a face or crossing a room without bumping into something) is sub-human.
A quite different approach measures machine intelligence through tests which are developed from mathematical definitions of intelligence. Examples of these kinds of tests start in the late nineties devising intelligence tests using notions from Kolmogorov complexity and data compression. Two major advantages of mathematical definitions are their applicability to nonhuman intelligences and their absence of a requirement for human testers.
A derivative of the Turing test is the Completely Automated Public Turing test to tell Computers and Humans Apart (CAPTCHA). As the name implies, this helps to determine that a user is an actual person and not a computer posing as a human. In contrast to the standard Turing test, CAPTCHA is administered by a machine and targeted to a human as opposed to being administered by a human and targeted to a machine. A computer asks a user to complete a simple test then generates a grade for that test. Computers are unable to solve the problem, so correct solutions are deemed to be the result of a person taking the test. A common type of CAPTCHA is the test that requires the typing of distorted letters, numbers or symbols that appear in an image undecipherable by a computer.


== Applications ==

AI is relevant to any intellectual task. Modern artificial intelligence techniques are pervasive and are too numerous to list here. Frequently, when a technique reaches mainstream use, it is no longer considered artificial intelligence; this phenomenon is described as the AI effect.
High-profile examples of AI include autonomous vehicles (such as drones and self-driving cars), medical diagnosis, creating art (such as poetry), proving mathematical theorems, playing games (such as Chess or Go), search engines (such as Google search), online assistants (such as Siri), image recognition in photographs, spam filtering, prediction of judicial decisions and targeting online advertisements.
With social media sites overtaking TV as a source for news for young people and news organisations increasingly reliant on social media platforms for generating distribution, major publishers now use artificial intelligence (AI) technology to post stories more effectively and generate higher volumes of traffic.


=== Competitions and prizes ===

There are a number of competitions and prizes to promote research in artificial intelligence. The main areas promoted are: general machine intelligence, conversational behavior, data-mining, robotic cars, robot soccer and games.


=== Healthcare ===

Artificial intelligence is breaking into the healthcare industry by assisting doctors. According to Bloomberg Technology, Microsoft has developed AI to help doctors find the right treatments for cancer.  There is a great amount of research and drugs developed relating to cancer. In detail, there are more than 800 medicines and vaccines to treat cancer. This negatively affects the doctors, because there are too many options to choose from, making it more difficult to choose the right drugs for the patients. Microsoft is working on a project to develop a machine called ""Hanover"". Its goal is to memorize all the papers necessary to cancer and help predict which combinations of drugs will be most effective for each patient. One project that is being worked on at the moment is fighting myeloid leukemia, a fatal cancer where the treatment has not improved in decades. Another study was reported to have found that artificial intelligence was as good as trained doctors in identifying skin cancers. Another study is using artificial intelligence to try and monitor multiple high-risk patients, and this is done by asking each patient numerous questions based on data acquired from live doctor to patient interactions.
According to CNN, there was a recent study by surgeons at the Children's National Medical Center in Washington which successfully demonstrated surgery with an autonomous robot. The team supervised the robot while it performed soft-tissue surgery, stitching together a pig's bowel during open surgery, and doing so better than a human surgeon, the team claimed. IBM has created its own artificial intelligence computer, the IBM Watson, which has beaten human intelligence (at some levels). Watson not only won at the game show Jeopardy! against former champions, but, was declared a hero after successfully diagnosing a women who was suffering from leukemia.


=== Automotive ===
Advancements in AI have contributed to the growth of the automotive industry through the creation and evolution of self-driving vehicles. As of 2016, there are over 30 companies utilizing AI into the creation of driverless cars. A few companies involved with AI include Tesla, Google, and Apple.
Many components contribute to the functioning of self-driving cars. These vehicles incorporate systems such as braking, lane changing, collision prevention, navigation and mapping. Together, these systems, as well as high performance computers, are integrated into one complex vehicle.
Recent developments in autonomous automobiles have made the innovation of self-driving trucks possible, though they are still in the testing phase. The UK government has passed legislation to begin testing of self-driving truck platoons in 2018. Self-driving truck platoons are a fleet of self-driving trucks following the lead of one non-self-driving truck, so the truck platoons aren't entirely autonomous yet. Meanwhile, the Daimler, a German automobile corporation, is testing the Freightliner Inspiration which is a semi-autonomous truck that will only be used on the highway.
One main factor that influences the ability for a driver-less automobile to function is mapping. In general, the vehicle would be pre-programmed with a map of the area being driven. This map would include data on the approximations of street light and curb heights in order for the vehicle to be aware of its surroundings. However, Google has been working on an algorithm with the purpose of eliminating the need for pre-programmed maps and instead, creating a device that would be able to adjust to a variety of new surroundings. Some self-driving cars are not equipped with steering wheels or brake pedals, so there has also been research focused on creating an algorithm that is capable of maintaining a safe environment for the passengers in the vehicle through awareness of speed and driving conditions.
Another factor that is influencing the ability for a driver-less automobile is the safety of the passenger. To make a driver-less automobile, engineers must program it to handle high risk situations. These situations could include a head on collision with pedestrians. The car's main goal should be to make a decision that would avoid hitting the pedestrians and saving the passengers in the car. But there is a possibility the car would need to make a decision that would put someone in danger. In other words, the car would need to decide to save the pedestrians or the passengers. The programing of the car in these situations is crucial to a successful driver-less automobile.


=== Finance and economics ===
Financial institutions have long used artificial neural network systems to detect charges or claims outside of the norm, flagging these for human investigation. The use of AI in banking can be traced back to 1987 when Security Pacific National Bank in US set-up a Fraud Prevention Task force to counter the unauthorised use of debit cards. Programs like Kasisto and Moneystream are using AI in financial services.
Banks use artificial intelligence systems today to organize operations, maintain book-keeping, invest in stocks, and manage properties. AI can react to changes overnight or when business is not taking place. In August 2001, robots beat humans in a simulated financial trading competition. AI has also reduced fraud and financial crimes by monitoring behavioral patterns of users for any abnormal changes or anomalies.
The use of AI machines in the market in applications such as online trading and decision making has changed major economic theories. For example, AI based buying and selling platforms have changed the law of supply and demand in that it is now possible to easily estimate individualized demand and supply curves and thus individualized pricing. Furthermore, AI machines reduce information asymmetry in the market and thus making markets more efficient while reducing the volume of trades. Furthermore, AI in the markets limits the consequences of behavior in the markets again making markets more efficient. Other theories where AI has had impact include in rational choice, rational expectations, game theory, Lewis turning point, portfolio optimization and counterfactual thinking.


=== Video games ===

In video games, artificial intelligence is routinely used to generate dynamic purposeful behavior in non-player characters (NPCs). In addition, well-understood AI techniques are routinely used for pathfinding. Some researchers consider NPC AI in games to be a ""solved problem"" for most production tasks. Games with more atypical AI include the AI director of Left 4 Dead (2008) and the neuroevolutionary training of platoons in Supreme Commander 2 (2010).


== Platforms ==
A platform (or ""computing platform"") is defined as ""some sort of hardware architecture or software framework (including application frameworks), that allows software to run"". As Rodney Brooks pointed out many years ago, it is not just the artificial intelligence software that defines the AI features of the platform, but rather the actual platform itself that affects the AI that results, i.e., there needs to be work in AI problems on real-world platforms rather than in isolation.
A wide variety of platforms has allowed different aspects of AI to develop, ranging from expert systems such as Cyc to deep-learning frameworks to robot platforms such as the Roomba with open interface. Recent advances in deep artificial neural networks and distributed computing have led to a proliferation of software libraries, including Deeplearning4j, TensorFlow, Theano and Torch.
Collective AI is a platform architecture that combines individual AI into a collective entity, in order to achieve global results from individual behaviors. With its collective structure, developers can crowdsource information and extend the functionality of existing AI domains on the platform for their own use, as well as continue to create and share new domains and capabilities for the wider community and greater good. As developers continue to contribute, the overall platform grows more intelligent and is able to perform more requests, providing a scalable model for greater communal benefit. Organizations like SoundHound Inc. and the Harvard John A. Paulson School of Engineering and Applied Sciences have used this collaborative AI model.


=== Education in AI ===
A McKinsey Global Institute study found a shortage of 1.5 million highly trained data and AI professionals and managers and a number of private bootcamps have developed programs to meet that demand, including free programs like The Data Incubator or paid programs like General Assembly.


=== Partnership on AI ===
Amazon, Google, Facebook, IBM, and Microsoft have established a non-profit partnership to formulate best practices on artificial intelligence technologies, advance the public's understanding, and to serve as a platform about artificial intelligence. They stated: ""This partnership on AI will conduct research, organize discussions, provide thought leadership, consult with relevant third parties, respond to questions from the public and media, and create educational material that advance the understanding of AI technologies including machine perception, learning, and automated reasoning."" Apple joined other tech companies as a founding member of the Partnership on AI in January 2017. The corporate members will make financial and research contributions to the group, while engaging with the scientific community to bring academics onto the board.


== Philosophy and ethics ==

There are three philosophical questions related to AI:
Is artificial general intelligence possible? Can a machine solve any problem that a human being can solve using intelligence? Or are there hard limits to what a machine can accomplish?
Are intelligent machines dangerous? How can we ensure that machines behave ethically and that they are used ethically?
Can a machine have a mind, consciousness and mental states in exactly the same sense that human beings do? Can a machine be sentient, and thus deserve certain rights? Can a machine intentionally cause harm?


=== The limits of artificial general intelligence ===

Can a machine be intelligent? Can it ""think""?
Alan Turing's ""polite convention""
We need not decide if a machine can ""think""; we need only decide if a machine can act as intelligently as a human being. This approach to the philosophical problems associated with artificial intelligence forms the basis of the Turing test.
The Dartmouth proposal
""Every aspect of learning or any other feature of intelligence can be so precisely described that a machine can be made to simulate it."" This conjecture was printed in the proposal for the Dartmouth Conference of 1956, and represents the position of most working AI researchers.
Newell and Simon's physical symbol system hypothesis
""A physical symbol system has the necessary and sufficient means of general intelligent action."" Newell and Simon argue that intelligence consists of formal operations on symbols. Hubert Dreyfus argued that, on the contrary, human expertise depends on unconscious instinct rather than conscious symbol manipulation and on having a ""feel"" for the situation rather than explicit symbolic knowledge. (See Dreyfus' critique of AI.)
Gödelian arguments
Gödel himself, John Lucas (in 1961) and Roger Penrose (in a more detailed argument from 1989 onwards) made highly technical arguments that human mathematicians can consistently see the truth of their own ""Gödel statements"" and therefore have computational abilities beyond that of mechanical Turing machines. However, the modern consensus in the scientific and mathematical community is that these ""Gödelian arguments"" fail.
The artificial brain argument
The brain can be simulated by machines and because brains are intelligent, simulated brains must also be intelligent; thus machines can be intelligent. Hans Moravec, Ray Kurzweil and others have argued that it is technologically feasible to copy the brain directly into hardware and software, and that such a simulation will be essentially identical to the original.
The AI effect
Machines are already intelligent, but observers have failed to recognize it. When Deep Blue beat Garry Kasparov in chess, the machine was acting intelligently. However, onlookers commonly discount the behavior of an artificial intelligence program by arguing that it is not ""real"" intelligence after all; thus ""real"" intelligence is whatever intelligent behavior people can do that machines still cannot. This is known as the AI Effect: ""AI is whatever hasn't been done yet.""


=== Potential risks and moral reasoning ===
Widespread use of artificial intelligence could have unintended consequences that are dangerous or undesirable. Scientists from the Future of Life Institute, among others, described some short-term research goals to see how AI influences the economy, the laws and ethics that are involved with AI and how to minimize AI security risks. In the long-term, the scientists have proposed to continue optimizing function while minimizing possible security risks that come along with new technologies.
Machines with intelligence have the potential to use their intelligence to make ethical decisions. Research in this area includes ""machine ethics"", ""artificial moral agents"", and the study of ""malevolent vs. friendly AI"".


==== Existential risk ====

The development of full artificial intelligence could spell the end of the human race. Once humans develop artificial intelligence, it will take off on its own and redesign itself at an ever-increasing rate. Humans, who are limited by slow biological evolution, couldn't compete and would be superseded.

A common concern about the development of artificial intelligence is the potential threat it could pose to humanity. This concern has recently gained attention after mentions by celebrities including the late Stephen Hawking, Bill Gates, and Elon Musk. A group of prominent tech titans including Peter Thiel, Amazon Web Services and Musk have committed $1billion to OpenAI a nonprofit company aimed at championing responsible AI development. The opinion of experts within the field of artificial intelligence is mixed, with sizable fractions both concerned and unconcerned by risk from eventual superhumanly-capable AI.
In his book Superintelligence, Nick Bostrom provides an argument that artificial intelligence will pose a threat to mankind. He argues that sufficiently intelligent AI, if it chooses actions based on achieving some goal, will exhibit convergent behavior such as acquiring resources or protecting itself from being shut down. If this AI's goals do not reflect humanity's – one example is an AI told to compute as many digits of pi as possible – it might harm humanity in order to acquire more resources or prevent itself from being shut down, ultimately to better achieve its goal.
For this danger to be realized, the hypothetical AI would have to overpower or out-think all of humanity, which a minority of experts argue is a possibility far enough in the future to not be worth researching. Other counterarguments revolve around humans being either intrinsically or convergently valuable from the perspective of an artificial intelligence.
Concern over risk from artificial intelligence has led to some high-profile donations and investments. In January 2015, Elon Musk donated ten million dollars to the Future of Life Institute to fund research on understanding AI decision making. The goal of the institute is to ""grow wisdom with which we manage"" the growing power of technology. Musk also funds companies developing artificial intelligence such as Google DeepMind and Vicarious to ""just keep an eye on what's going on with artificial intelligence. I think there is potentially a dangerous outcome there.""
Development of militarized artificial intelligence is a related concern. Currently, 50+ countries are researching battlefield robots, including the United States, China, Russia, and the United Kingdom. Many people concerned about risk from superintelligent AI also want to limit the use of artificial soldiers.


==== Devaluation of humanity ====

Joseph Weizenbaum wrote that AI applications cannot, by definition, successfully simulate genuine human empathy and that the use of AI technology in fields such as customer service or psychotherapy was deeply misguided. Weizenbaum was also bothered that AI researchers (and some philosophers) were willing to view the human mind as nothing more than a computer program (a position now known as computationalism). To Weizenbaum these points suggest that AI research devalues human life.


==== Decrease in demand for human labor ====

The relationship between automation and employment is complicated. While automation eliminates old jobs, it also creates new jobs through micro-economic and macro-economic effects. Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist states that ""the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution"" is ""worth taking seriously"". Subjective estimates of the risk vary widely; for example, Michael Osborne and Carl Benedikt Frey estimate 47% of U.S. jobs are at ""high risk"" of potential automation, while an OECD report classifies only 9% of U.S. jobs as ""high risk"". Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy. Author Martin Ford and others go further and argue that a large number of jobs are routine, repetitive and (to an AI) predictable; Ford warns that these jobs may be automated in the next couple of decades, and that many of the new jobs may not be ""accessible to people with average capability"", even with retraining. Economists point out that in the past technology has tended to increase rather than reduce total employment, but acknowledge that ""we're in uncharted territory"" with AI.


==== Artificial moral agents ====
This raises the issue of how ethically the machine should behave towards both humans and other AI agents. This issue was addressed by Wendell Wallach in his book titled Moral Machines in which he introduced the concept of artificial moral agents (AMA). For Wallach, AMAs have become a part of the research landscape of artificial intelligence as guided by its two central questions which he identifies as ""Does Humanity Want Computers Making Moral Decisions"" and ""Can (Ro)bots Really Be Moral"". For Wallach the question is not centered on the issue of whether machines can demonstrate the equivalent of moral behavior in contrast to the constraints which society may place on the development of AMAs.


==== Machine ethics ====

The field of machine ethics is concerned with giving machines ethical principles, or a procedure for discovering a way to resolve the ethical dilemmas they might encounter, enabling them to function in an ethically responsible manner through their own ethical decision making. The field was delineated in the AAAI Fall 2005 Symposium on Machine Ethics: ""Past research concerning the relationship between technology and ethics has largely focused on responsible and irresponsible use of technology by human beings, with a few people being interested in how human beings ought to treat machines. In all cases, only human beings have engaged in ethical reasoning. The time has come for adding an ethical dimension to at least some machines. Recognition of the ethical ramifications of behavior involving machines, as well as recent and potential developments in machine autonomy, necessitate this. In contrast to computer hacking, software property issues, privacy issues and other topics normally ascribed to computer ethics, machine ethics is concerned with the behavior of machines towards human users and other machines. Research in machine ethics is key to alleviating concerns with autonomous systems—it could be argued that the notion of autonomous machines without such a dimension is at the root of all fear concerning machine intelligence. Further, investigation of machine ethics could enable the discovery of problems with current ethical theories, advancing our thinking about Ethics."" Machine ethics is sometimes referred to as machine morality, computational ethics or computational morality. A variety of perspectives of this nascent field can be found in the collected edition ""Machine Ethics"" that stems from the AAAI Fall 2005 Symposium on Machine Ethics.


==== Malevolent and friendly AI ====

Political scientist Charles T. Rubin believes that AI can be neither designed nor guaranteed to be benevolent. He argues that ""any sufficiently advanced benevolence may be indistinguishable from malevolence."" Humans should not assume machines or robots would treat us favorably, because there is no a priori reason to believe that they would be sympathetic to our system of morality, which has evolved along with our particular biology (which AIs would not share). Hyper-intelligent software may not necessarily decide to support the continued existence of humanity, and would be extremely difficult to stop. This topic has also recently begun to be discussed in academic publications as a real source of risks to civilization, humans, and planet Earth.
Physicist Stephen Hawking, Microsoft founder Bill Gates, and SpaceX founder Elon Musk have expressed concerns about the possibility that AI could evolve to the point that humans could not control it, with Hawking theorizing that this could ""spell the end of the human race"".
One proposal to deal with this is to ensure that the first generally intelligent AI is 'Friendly AI', and will then be able to control subsequently developed AIs. Some question whether this kind of check could really remain in place.
Leading AI researcher Rodney Brooks writes, ""I think it is a mistake to be worrying about us developing malevolent AI anytime in the next few hundred years. I think the worry stems from a fundamental error in not distinguishing the difference between the very real recent advances in a particular aspect of AI, and the enormity and complexity of building sentient volitional intelligence.""


=== Machine consciousness, sentience and mind ===

If an AI system replicates all key aspects of human intelligence, will that system also be sentient – will it have a mind which has conscious experiences? This question is closely related to the philosophical problem as to the nature of human consciousness, generally referred to as the hard problem of consciousness.


==== Consciousness ====


==== Computationalism and functionalism ====

Computationalism is the position in the philosophy of mind that the human mind or the human brain (or both) is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind-body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.


==== Strong AI hypothesis ====

The philosophical position that John Searle has named ""strong AI"" states: ""The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds."" Searle counters this assertion with his Chinese room argument, which asks us to look inside the computer and try to find where the ""mind"" might be.


==== Robot rights ====

Mary Shelley's Frankenstein considers a key issue in the ethics of artificial intelligence: if a machine can be created that has intelligence, could it also feel? If it can feel, does it have the same rights as a human? The idea also appears in modern science fiction, such as the film A.I.: Artificial Intelligence, in which humanoid machines have the ability to feel emotions. This issue, now known as ""robot rights"", is currently being considered by, for example, California's Institute for the Future, although many critics believe that the discussion is premature. Some critics of transhumanism argue that any hypothetical robot rights would lie on a spectrum with animal rights and human rights. The subject is profoundly discussed in the 2010 documentary film Plug & Pray.


=== Superintelligence ===

Are there limits to how intelligent machines – or human-machine hybrids – can be? A superintelligence, hyperintelligence, or superhuman intelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind. ‘’Superintelligence’’ may also refer to the form or degree of intelligence possessed by such an agent.


==== Technological singularity ====

If research into Strong AI produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to recursive self-improvement. The new intelligence could thus increase exponentially and dramatically surpass humans. Science fiction writer Vernor Vinge named this scenario ""singularity"". Technological singularity is when accelerating progress in technologies will cause a runaway effect wherein artificial intelligence will exceed human intellectual capacity and control, thus radically changing or even ending civilization. Because the capabilities of such an intelligence may be impossible to comprehend, the technological singularity is an occurrence beyond which events are unpredictable or even unfathomable.
Ray Kurzweil has used Moore's law (which describes the relentless exponential improvement in digital technology) to calculate that desktop computers will have the same processing power as human brains by the year 2029, and predicts that the singularity will occur in 2045.


==== Transhumanism ====

You awake one morning to find your brain has another lobe functioning. Invisible, this auxiliary lobe answers your questions with information beyond the realm of your own memory, suggests plausible courses of action, and asks questions that help bring out relevant facts. You quickly come to rely on the new lobe so much that you stop wondering how it works. You just use it. This is the dream of artificial intelligence.

Robot designer Hans Moravec, cyberneticist Kevin Warwick and inventor Ray Kurzweil have predicted that humans and machines will merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, which has roots in Aldous Huxley and Robert Ettinger, has been illustrated in fiction as well, for example in the manga Ghost in the Shell and the science-fiction series Dune.
In the 1980s artist Hajime Sorayama's Sexy Robots series were painted and published in Japan depicting the actual organic human form with lifelike muscular metallic skins and later ""the Gynoids"" book followed that was used by or influenced movie makers including George Lucas and other creatives. Sorayama never considered these organic robots to be real part of nature but always unnatural product of the human mind, a fantasy existing in the mind even when realized in actual form.
Edward Fredkin argues that ""artificial intelligence is the next stage in evolution"", an idea first proposed by Samuel Butler's ""Darwin among the Machines"" (1863), and expanded upon by George Dyson in his book of the same name in 1998.


== In fiction ==

Thought-capable artificial beings appeared as storytelling devices since antiquity.
The implications of a constructed machine exhibiting artificial intelligence have been a persistent theme in science fiction since the twentieth century. Early stories typically revolved around intelligent robots. The word ""robot"" itself was coined by Karel Čapek in his 1921 play R.U.R., the title standing for ""Rossum's Universal Robots"". Later, the SF writer Isaac Asimov developed the Three Laws of Robotics. He subsequently explored these in his many books, most notably the ""Multivac"" series about a super-intelligent computer of the same name. Asimov's laws are often brought up during layman discussions of machine ethics; while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.
The novel Do Androids Dream of Electric Sheep?, by Philip K. Dick, tells a science fiction story about Androids and humans clashing in a futuristic world. Elements of artificial intelligence include the empathy box, mood organ, and the androids themselves. Throughout the novel, Dick portrays the idea that human subjectivity is altered by technology created with artificial intelligence.
Nowadays AI is firmly rooted in popular culture; intelligent robots appear in innumerable works. HAL 9000, the murderous computer in charge of the Discovery One spaceship in Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), is an example of the common ""robotic rampage"" archetype in science fiction movies. The Terminator (1984) and The Matrix (1999) provide additional widely familiar examples. In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.


== See also ==


== Explanatory notes ==


== References ==


=== AI textbooks ===


=== History of AI ===


=== Other sources ===


== Further reading ==
DH Autor, ‘Why Are There Still So Many Jobs? The History and Future of Workplace Automation’ (2015) 29(3) Journal of Economic Perspectives 3.
TechCast Article Series, John Sagi, ""Framing Consciousness""
Boden, Margaret, Mind As Machine, Oxford University Press, 2006
Gopnik, Alison, ""Making AI More Human: Artificial intelligence has staged a revival by starting to incorporate what we know about how children learn"", Scientific American, vol. 316, no. 6 (June 2017), pp. 60–65.
Johnston, John (2008) The Allure of Machinic Life: Cybernetics, Artificial Life, and the New AI, MIT Press
Marcus, Gary, ""Am I Human?: Researchers need new ways to distinguish artificial intelligence from the natural kind"", Scientific American, vol. 316, no. 3 (March 2017), pp. 58–63. Multiple tests of artificial-intelligence efficacy are needed because, ""just as there is no single test of athletic prowess, there cannot be one ultimate test of intelligence."" One such test, a ""Construction Challenge"", would test perception and physical action—""two important elements of intelligent behavior that were entirely absent from the original Turing test."" Another proposal has been to give machines the same standardized tests of science and other disciplines that schoolchildren take. A so far insuperable stumbling block to artificial intelligence is an incapacity for reliable disambiguation. ""[V]irtually every sentence [that people generate] is ambiguous, often in multiple ways."" A prominent example is known as the ""pronoun disambiguation problem"": a machine has no way of determining to whom or what a pronoun in a sentence—such as ""he"", ""she"" or ""it""—refers.
E McGaughey, 'Will Robots Automate Your Job Away? Full Employment, Basic Income, and Economic Democracy' (2018) SSRN, part 2(3).
Myers, Courtney Boyd ed. (2009). ""The AI Report"". Forbes June 2009
Raphael, Bertram (1976). The Thinking Computer. W.H.Freeman and Company. ISBN 0-7167-0723-3. 
Serenko, Alexander (2010). ""The development of an AI journal ranking based on the revealed preference approach"" (PDF). Journal of Informetrics. 4 (4): 447–459. doi:10.1016/j.joi.2010.04.001. 
Serenko, Alexander; Michael Dohan (2011). ""Comparing the expert survey and citation impact journal ranking methods: Example from the field of Artificial Intelligence"" (PDF). Journal of Informetrics. 5 (4): 629–649. doi:10.1016/j.joi.2011.06.002. 
Sun, R. & Bookman, L. (eds.), Computational Architectures: Integrating Neural and Symbolic Processes. Kluwer Academic Publishers, Needham, MA. 1994.
Tom Simonite (29 December 2014). ""2014 in Computing: Breakthroughs in Artificial Intelligence"". MIT Technology Review. 


== External links ==

What Is AI? – An introduction to artificial intelligence by John McCarthy—a co-founder of the field, and the person who coined the term.
The Handbook of Artificial Intelligence Volume Ⅰ by Avron Barr and Edward A. Feigenbaum (Stanford University)
""Artificial Intelligence"". Internet Encyclopedia of Philosophy. 
Thomason, Richmond. ""Logic and Artificial Intelligence"". In Zalta, Edward N. Stanford Encyclopedia of Philosophy. 
AI at Curlie (based on DMOZ)
AITopics – A large directory of links and other resources maintained by the Association for the Advancement of Artificial Intelligence, the leading organization of academic AI researchers.
List of AI Conferences – A list of 225 AI conferences taking place all over the world."
1,Comparison of programming languages (string functions),3681422,109570,"String functions are used in computer programming languages to manipulate a string or query information about a string (some do both).
Most programming languages that have a string datatype will have some string functions although there may be other low-level ways within each language to handle strings directly. In object-oriented languages, string functions are often implemented as properties and methods of string objects. In functional and list-based languages a string is represented as a list (of character codes), therefore all list-manipulation procedures could be considered string functions. However such languages may implement a subset of explicit string-specific functions as well.
For function that manipulate strings, modern object-oriented languages, like C# and Java have immutable strings and return a copy (in newly allocated dynamic memory), while others, like C manipulate the original string unless the programmer copies data to a new string. See for example Concatenation below.
The most basic example of a string function is the length(string) function. This function returns the length of a string literal.
e.g. length(""hello world"") would return 11.
Other languages may have string functions with similar or exactly the same syntax or parameters or outcomes. For example, in many languages the length function is usually represented as len(string). The below list of common functions aims to help limit this confusion.


== Common string functions (multi language reference) ==
String functions common to many languages are listed below, including the different names used. The below list of common functions aims to help programmers find the equivalent function in a language. Note, string concatenation and regular expressions are handled in separate pages. Statements in guillemets (« … ») are optional.


=== CharAt ===

# Example in ALGOL 68 #
""Hello, World""[2];         // 'e'


=== Compare (integer result) ===


=== Compare (relational operator-based, Boolean result) ===


=== Concatenation ===


=== Contains ===

¢ Example in ALGOL 68 ¢
string in string(""e"", loc int, ""Hello mate"");      ¢ returns true ¢
string in string(""z"", loc int, ""word"");            ¢ returns false ¢


=== Equality ===
Tests if two strings are equal. See also #Compare and #Compare. Note that doing equality checks via a generic Compare with integer result is not only confusing for the programmer but is often a significantly more expensive operation; this is especially true when using ""C-strings"".


=== Find ===


=== Find character ===

^a Given a set of characters, SCAN returns the position of the first character found, while VERIFY returns the position of the first character that does not belong to the set.


=== Format ===

/* example in PL/I */
put string(some_string) edit('My ', 'pen', ' costs', 19.99)(a,a,a,p'$$$V.99')
/* returns ""My pen costs $19.99"" */


=== Inequality ===
Tests if two strings are not equal. See also #Equality.


=== index ===
see #Find


=== indexof ===
see #Find


=== instr ===
see #Find


=== instrrev ===
see #rfind


=== join ===


=== lastindexof ===
see #rfind


=== left ===


=== len ===
see #length


=== length ===


=== locate ===
see #Find


=== Lowercase ===


=== mid ===
see #substring


=== partition ===


=== replace ===


=== reverse ===


=== rfind ===


=== right ===


=== rpartition ===


=== slice ===
see #substring


=== split ===


=== sprintf ===
see #Format


=== strip ===
see #trim


=== strcmp ===
see #Compare (integer result)


=== substring ===


=== Uppercase ===


=== trim ===

trim or strip is used to remove whitespace from the beginning, end, or both beginning and end, of a string.
Other languages
In languages without a built-in trim function, it is usually simple to create a custom function which accomplishes the same task.


==== AWK ====
In AWK, one can use regular expressions to trim:

or:


==== C/C++ ====
There is no standard trim function in C or C++. Most of the available string libraries for C contain code which implements trimming, or functions that significantly ease an efficient implementation. The function has also often been called EatWhitespace in some non-standard C libraries.
In C, programmers often combine a ltrim and rtrim to implement trim:

The open source C++ library Boost has several trim variants, including a standard one:

Note that with boost's function named simply trim the input sequence is modified in-place, and does not return a result.
Another open source C++ library Qt has several trim variants, including a standard one:

The Linux kernel also includes a strip function, strstrip(), since 2.6.18-rc1, which trims the string ""in place"". Since 2.6.33-rc1, the kernel uses strim() instead of strstrip() to avoid false warnings.


==== Haskell ====
A trim algorithm in Haskell:

may be interpreted as follows: f drops the preceding whitespace, and reverses the string. f is then again applied to its own output. Note that the type signature (the second line) is optional.


==== J ====
The trim algorithm in J is a functional description:

That is: filter (#~) for non-space characters (' '&~:) between leading (+./\) and (*.) trailing (+./\.) spaces.


==== JavaScript ====
There is a built-in trim function in JavaScript 1.8.1 (Firefox 3.5 and later), and the ECMAScript 5 standard. In earlier versions it can be added to the String object's prototype as follows:


==== Perl ====
Perl 5 has no built-in trim function. However, the functionality is commonly achieved using regular expressions.
Example:

or:

These examples modify the value of the original variable $string.
Also available for Perl is StripLTSpace in String::Strip from CPAN.
There are, however, two functions that are commonly used to strip whitespace from the end of strings, chomp and chop:
chop removes the last character from a string and returns it.
chomp removes the trailing newline character(s) from a string if present. (What constitutes a newline is $INPUT_RECORD_SEPARATOR dependent).
In Perl 6, the upcoming major revision of the language, strings have a trim method.
Example:


==== Tcl ====
The Tcl string command has three relevant subcommands: trim, trimright and trimleft. For each of those commands, an additional argument may be specified: a string that represents a set of characters to remove—the default is whitespace (space, tab, newline, carriage return).
Example of trimming vowels:


==== XSLT ====
XSLT includes the function normalize-space(string) which strips leading and trailing whitespace, in addition to replacing any whitespace sequence (including line breaks) with a single space.
Example:

XSLT 2.0 includes regular expressions, providing another mechanism to perform string trimming.
Another XSLT technique for trimming is to utilize the XPath 2.0 substring() function.


== References ==


== External links ==
Perl String Functions
Python 2 String Methods
Python 3 String Methods
Scheme String Procedures
Erlang String Functions
.NET String Methods and Properties
Ruby String Class
PHP String functions
java.lang.String members
Haskell Hierarchical Libraries
Arrays in D (in D strings are regular arrays)
std.string from Phobos (D standard library)
Lua String Functions
OCaml String module
Common Lisp String manipulation
FreeBASIC String Functions
Reference.wolfram.com
Tcl reference for string commands"
2,Geographic information system,12398,77692,"A geographic information system (GIS) is a system designed to capture, store, manipulate, analyze, manage, and present spatial or geographic data. The acronym GIS is sometimes used for geographic information science (GIScience) to refer to the academic discipline that studies geographic information systems and is a large domain within the broader academic discipline of geoinformatics. What goes beyond a GIS is a spatial data infrastructure, a concept that has no such restrictive boundaries.
In general, the term describes any information system that integrates, stores, edits, analyzes, shares, and displays geographic information. GIS applications are tools that allow users to create interactive queries (user-created searches), analyze spatial information, edit data in maps, and present the results of all these operations. Geographic information science is the science underlying geographic concepts, applications, and systems.
GIS can refer to a number of different technologies, processes, and methods. It is attached to many operations and has many applications related to engineering, planning, management, transport/logistics, insurance, telecommunications, and business. For that reason, GIS and location intelligence applications can be the foundation for many location-enabled services that rely on analysis and visualization.
GIS can relate unrelated information by using location as the key index variable. Locations or extents in the Earth space–time may be recorded as dates/times of occurrence, and x, y, and z coordinates representing, longitude, latitude, and elevation, respectively. All Earth-based spatial–temporal location and extent references should be relatable to one another and ultimately to a ""real"" physical location or extent. This key characteristic of GIS has begun to open new avenues of scientific inquiry.


== History of development ==
The first known use of the term ""geographic information system"" was by Roger Tomlinson in the year 1968 in his paper ""A Geographic Information System for Regional Planning"". Tomlinson is also acknowledged as the ""father of GIS"".

Previously, one of the first applications of spatial analysis in epidemiology is the 1832 ""Rapport sur la marche et les effets du choléra dans Paris et le département de la Seine"". The French geographer Charles Picquet represented the 48 districts of the city of Paris by halftone color gradient according to the number of deaths by cholera per 1,000 inhabitants. In 1854 John Snow determined the source of a cholera outbreak in London by marking points on a map depicting where the cholera victims lived, and connecting the cluster that he found with a nearby water source. This was one of the earliest successful uses of a geographic methodology in epidemiology. While the basic elements of topography and theme existed previously in cartography, the John Snow map was unique, using cartographic methods not only to depict but also to analyze clusters of geographically dependent phenomena.
The early 20th century saw the development of photozincography, which allowed maps to be split into layers, for example one layer for vegetation and another for water. This was particularly used for printing contours – drawing these was a labour-intensive task but having them on a separate layer meant they could be worked on without the other layers to confuse the draughtsman. This work was originally drawn on glass plates but later plastic film was introduced, with the advantages of being lighter, using less storage space and being less brittle, among others. When all the layers were finished, they were combined into one image using a large process camera. Once color printing came in, the layers idea was also used for creating separate printing plates for each color. While the use of layers much later became one of the main typical features of a contemporary GIS, the photographic process just described is not considered to be a GIS in itself – as the maps were just images with no database to link them to.
Computer hardware development spurred by nuclear weapon research led to general-purpose computer ""mapping"" applications by the early 1960s.
The year 1960 saw the development of the world's first true operational GIS in Ottawa, Ontario, Canada, by the federal Department of Forestry and Rural Development. Developed by Dr. Roger Tomlinson, it was called the Canada Geographic Information System (CGIS) and was used to store, analyze, and manipulate data collected for the Canada Land Inventory – an effort to determine the land capability for rural Canada by mapping information about soils, agriculture, recreation, wildlife, waterfowl, forestry and land use at a scale of 1:50,000. A rating classification factor was also added to permit analysis.
CGIS was an improvement over ""computer mapping"" applications as it provided capabilities for overlay, measurement, and digitizing/scanning. It supported a national coordinate system that spanned the continent, coded lines as arcs having a true embedded topology and it stored the attribute and locational information in separate files. As a result of this, Tomlinson has become known as the ""father of GIS"", particularly for his use of overlays in promoting the spatial analysis of convergent geographic data.
CGIS lasted into the 1990s and built a large digital land resource database in Canada. It was developed as a mainframe-based system in support of federal and provincial resource planning and management. Its strength was continent-wide analysis of complex datasets. The CGIS was never available commercially.
In 1964 Howard T. Fisher formed the Laboratory for Computer Graphics and Spatial Analysis at the Harvard Graduate School of Design (LCGSA 1965–1991), where a number of important theoretical concepts in spatial data handling were developed, and which by the 1970s had distributed seminal software code and systems, such as SYMAP, GRID, and ODYSSEY – that served as sources for subsequent commercial development—to universities, research centers and corporations worldwide.
By the late 1970s two public domain GIS systems (MOSS and GRASS GIS) were in development, and by the early 1980s, M&S Computing (later Intergraph) along with Bentley Systems Incorporated for the CAD platform, Environmental Systems Research Institute (ESRI), CARIS (Computer Aided Resource Information System), MapInfo Corporation and ERDAS (Earth Resource Data Analysis System) emerged as commercial vendors of GIS software, successfully incorporating many of the CGIS features, combining the first generation approach to separation of spatial and attribute information with a second generation approach to organizing attribute data into database structures.
In 1986, Mapping Display and Analysis System (MIDAS), the first desktop GIS product was released for the DOS operating system. This was renamed in 1990 to MapInfo for Windows when it was ported to the Microsoft Windows platform. This began the process of moving GIS from the research department into the business environment.
By the end of the 20th century, the rapid growth in various systems had been consolidated and standardized on relatively few platforms and users were beginning to explore viewing GIS data over the Internet, requiring data format and transfer standards. More recently, a growing number of free, open-source GIS packages run on a range of operating systems and can be customized to perform specific tasks. Increasingly geospatial data and mapping applications are being made available via the World Wide Web (see List of GIS software § GIS as a service).
Several articles on the history of GIS have been published.


== GIS techniques and technology ==
Modern GIS technologies use digital information, for which various digitized data creation methods are used. The most common method of data creation is digitization, where a hard copy map or survey plan is transferred into a digital medium through the use of a CAD program, and geo-referencing capabilities. With the wide availability of ortho-rectified imagery (from satellites, aircraft, Helikites and UAVs), heads-up digitizing is becoming the main avenue through which geographic data is extracted. Heads-up digitizing involves the tracing of geographic data directly on top of the aerial imagery instead of by the traditional method of tracing the geographic form on a separate digitizing tablet (heads-down digitizing).


=== Relating information from different sources ===
GIS uses spatio-temporal (space-time) location as the key index variable for all other information. Just as a relational database containing text or numbers can relate many different tables using common key index variables, GIS can relate otherwise unrelated information by using location as the key index variable. The key is the location and/or extent in space-time.
Any variable that can be located spatially, and increasingly also temporally, can be referenced using a GIS. Locations or extents in Earth space–time may be recorded as dates/times of occurrence, and x, y, and z coordinates representing, longitude, latitude, and elevation, respectively. These GIS coordinates may represent other quantified systems of temporo-spatial reference (for example, film frame number, stream gage station, highway mile-marker, surveyor benchmark, building address, street intersection, entrance gate, water depth sounding, POS or CAD drawing origin/units). Units applied to recorded temporal-spatial data can vary widely (even when using exactly the same data, see map projections), but all Earth-based spatial–temporal location and extent references should, ideally, be relatable to one another and ultimately to a ""real"" physical location or extent in space–time.
Related by accurate spatial information, an incredible variety of real-world and projected past or future data can be analyzed, interpreted and represented. This key characteristic of GIS has begun to open new avenues of scientific inquiry into behaviors and patterns of real-world information that previously had not been systematically correlated.


=== GIS uncertainties ===
GIS accuracy depends upon source data, and how it is encoded to be data referenced. Land surveyors have been able to provide a high level of positional accuracy utilizing the GPS-derived positions. High-resolution digital terrain and aerial imagery, powerful computers and Web technology are changing the quality, utility, and expectations of GIS to serve society on a grand scale, but nevertheless there are other source data that affect overall GIS accuracy like paper maps, though these may be of limited use in achieving the desired accuracy.
In developing a digital topographic database for a GIS, topographical maps are the main source, and aerial photography and satellite imagery are extra sources for collecting data and identifying attributes which can be mapped in layers over a location facsimile of scale. The scale of a map and geographical rendering area representation type are very important aspects since the information content depends mainly on the scale set and resulting locatability of the map's representations. In order to digitize a map, the map has to be checked within theoretical dimensions, then scanned into a raster format, and resulting raster data has to be given a theoretical dimension by a rubber sheeting/warping technology process.
A quantitative analysis of maps brings accuracy issues into focus. The electronic and other equipment used to make measurements for GIS is far more precise than the machines of conventional map analysis. All geographical data are inherently inaccurate, and these inaccuracies will propagate through GIS operations in ways that are difficult to predict.


=== Data representation ===

GIS data represents real objects (such as roads, land use, elevation, trees, waterways, etc.) with digital data determining the mix. Real objects can be divided into two abstractions: discrete objects (e.g., a house) and continuous fields (such as rainfall amount, or elevations). Traditionally, there are two broad methods used to store data in a GIS for both kinds of abstractions mapping references: raster images and vector. Points, lines, and polygons are the stuff of mapped location attribute references. A new hybrid method of storing data is that of identifying point clouds, which combine three-dimensional points with RGB information at each point, returning a ""3D color image"". GIS thematic maps then are becoming more and more realistically visually descriptive of what they set out to show or determine.
For a list of popular GIS file formats, such as shapefiles, see GIS file formats § Popular GIS file formats.


=== Data capture ===

Data capture—entering information into the system—consumes much of the time of GIS practitioners. There are a variety of methods used to enter data into a GIS where it is stored in a digital format.
Existing data printed on paper or PET film maps can be digitized or scanned to produce digital data. A digitizer produces vector data as an operator traces points, lines, and polygon boundaries from a map. Scanning a map results in raster data that could be further processed to produce vector data.
Survey data can be directly entered into a GIS from digital data collection systems on survey instruments using a technique called coordinate geometry (COGO). Positions from a global navigation satellite system (GNSS) like Global Positioning System can also be collected and then imported into a GIS. A current trend in data collection gives users the ability to utilize field computers with the ability to edit live data using wireless connections or disconnected editing sessions. This has been enhanced by the availability of low-cost mapping-grade GPS units with decimeter accuracy in real time. This eliminates the need to post process, import, and update the data in the office after fieldwork has been collected. This includes the ability to incorporate positions collected using a laser rangefinder. New technologies also allow users to create maps as well as analysis directly in the field, making projects more efficient and mapping more accurate.
Remotely sensed data also plays an important role in data collection and consist of sensors attached to a platform. Sensors include cameras, digital scanners and lidar, while platforms usually consist of aircraft and satellites. In England in the mid 1990s, hybrid kite/balloons called helikites first pioneered the use of compact airborne digital cameras as airborne geo-information systems. Aircraft measurement software, accurate to 0.4 mm was used to link the photographs and measure the ground. Helikites are inexpensive and gather more accurate data than aircraft. Helikites can be used over roads, railways and towns where unmanned aerial vehicles (UAVs) are banned.
Recently aerial data collection is becoming possible with miniature UAVs. For example, the Aeryon Scout was used to map a 50-acre area with a ground sample distance of 1 inch (2.54 cm) in only 12 minutes.
The majority of digital data currently comes from photo interpretation of aerial photographs. Soft-copy workstations are used to digitize features directly from stereo pairs of digital photographs. These systems allow data to be captured in two and three dimensions, with elevations measured directly from a stereo pair using principles of photogrammetry. Analog aerial photos must be scanned before being entered into a soft-copy system, for high-quality digital cameras this step is skipped.
Satellite remote sensing provides another important source of spatial data. Here satellites use different sensor packages to passively measure the reflectance from parts of the electromagnetic spectrum or radio waves that were sent out from an active sensor such as radar. Remote sensing collects raster data that can be further processed using different bands to identify objects and classes of interest, such as land cover.
When data is captured, the user should consider if the data should be captured with either a relative accuracy or absolute accuracy, since this could not only influence how information will be interpreted but also the cost of data capture.
After entering data into a GIS, the data usually requires editing, to remove errors, or further processing. For vector data it must be made ""topologically correct"" before it can be used for some advanced analysis. For example, in a road network, lines must connect with nodes at an intersection. Errors such as undershoots and overshoots must also be removed. For scanned maps, blemishes on the source map may need to be removed from the resulting raster. For example, a fleck of dirt might connect two lines that should not be connected.


=== Raster-to-vector translation ===
Data restructuring can be performed by a GIS to convert data into different formats. For example, a GIS may be used to convert a satellite image map to a vector structure by generating lines around all cells with the same classification, while determining the cell spatial relationships, such as adjacency or inclusion.
More advanced data processing can occur with image processing, a technique developed in the late 1960s by NASA and the private sector to provide contrast enhancement, false color rendering and a variety of other techniques including use of two dimensional Fourier transforms. Since digital data is collected and stored in various ways, the two data sources may not be entirely compatible. So a GIS must be able to convert geographic data from one structure to another. In so doing, the implicit assumptions behind different ontologies and classifications require analysis. Object ontologies have gained increasing prominence as a consequence of object-oriented programming and sustained work by Barry Smith and co-workers.


=== Projections, coordinate systems, and registration ===

The earth can be represented by various models, each of which may provide a different set of coordinates (e.g., latitude, longitude, elevation) for any given point on the Earth's surface. The simplest model is to assume the earth is a perfect sphere. As more measurements of the earth have accumulated, the models of the earth have become more sophisticated and more accurate. In fact, there are models called datums that apply to different areas of the earth to provide increased accuracy, like NAD83 for U.S. measurements, and the World Geodetic System for worldwide measurements.


== Spatial analysis with geographical information system (GIS) ==

GIS spatial analysis is a rapidly changing field, and GIS packages are increasingly including analytical tools as standard built-in facilities, as optional toolsets, as add-ins or 'analysts'. In many instances these are provided by the original software suppliers (commercial vendors or collaborative non commercial development teams), while in other cases facilities have been developed and are provided by third parties. Furthermore, many products offer software development kits (SDKs), programming languages and language support, scripting facilities and/or special interfaces for developing one's own analytical tools or variants. The website ""Geospatial Analysis"" and associated book/ebook attempt to provide a reasonably comprehensive guide to the subject. The increased availability has created a new dimension to business intelligence termed ""spatial intelligence"" which, when openly delivered via intranet, democratizes access to geographic and social network data. Geospatial intelligence, based on GIS spatial analysis, has also become a key element for security. GIS as a whole can be described as conversion to a vectorial representation or to any other digitisation process.


=== Slope and aspect ===
Slope can be defined as the steepness or gradient of a unit of terrain, usually measured as an angle in degrees or as a percentage. Aspect can be defined as the direction in which a unit of terrain faces. Aspect is usually expressed in degrees from north. Slope, aspect, and surface curvature in terrain analysis are all derived from neighborhood operations using elevation values of a cell's adjacent neighbours. Slope is a function of resolution, and the spatial resolution used to calculate slope and aspect should always be specified. Various authors have compared techniques for calculating slope and aspect.
The following method can be used to derive slope and aspect:
The elevation at a point or unit of terrain will have perpendicular tangents (slope) passing through the point, in an east-west and north-south direction. These two tangents give two components, ∂z/∂x and ∂z/∂y, which then be used to determine the overall direction of slope, and the aspect of the slope. The gradient is defined as a vector quantity with components equal to the partial derivatives of the surface in the x and y directions.
The calculation of the overall 3x3 grid slope S and aspect A for methods that determine east-west and north-south component use the following formulas respectively:

  
    
      
        tan
        ⁡
        S
        =
        
          
            
              
                (
                
                  
                    
                      ∂
                      z
                    
                    
                      ∂
                      x
                    
                  
                
                )
              
              
                2
              
            
            +
            
              
                (
                
                  
                    
                      ∂
                      z
                    
                    
                      ∂
                      y
                    
                  
                
                )
              
              
                2
              
            
          
        
      
    
    {\displaystyle \tan S={\sqrt {\left({\frac {\partial z}{\partial x}}\right)^{2}+\left({\frac {\partial z}{\partial y}}\right)^{2}}}}
  

  
    
      
        tan
        ⁡
        A
        =
        
          (
          
            
              
                (
                
                  
                    
                      −
                      ∂
                      z
                    
                    
                      ∂
                      y
                    
                  
                
                )
              
              
                (
                
                  
                    
                      ∂
                      z
                    
                    
                      ∂
                      x
                    
                  
                
                )
              
            
          
          )
        
      
    
    {\displaystyle \tan A=\left({\frac {\left({\frac {-\partial z}{\partial y}}\right)}{\left({\frac {\partial z}{\partial x}}\right)}}\right)}
  
Zhou and Liu describe another formula for calculating aspect, as follows:

  
    
      
        A
        =
        
          270
          
            ∘
          
        
        +
        arctan
        ⁡
        
          (
          
            
              
                (
                
                  
                    
                      ∂
                      z
                    
                    
                      ∂
                      x
                    
                  
                
                )
              
              
                (
                
                  
                    
                      ∂
                      z
                    
                    
                      ∂
                      y
                    
                  
                
                )
              
            
          
          )
        
        −
        
          90
          
            ∘
          
        
        
          (
          
            
              
                (
                
                  
                    
                      ∂
                      z
                    
                    
                      ∂
                      y
                    
                  
                
                )
              
              
                |
                
                  
                    
                      ∂
                      z
                    
                    
                      ∂
                      y
                    
                  
                
                |
              
            
          
          )
        
      
    
    {\displaystyle A=270^{\circ }+\arctan \left({\frac {\left({\frac {\partial z}{\partial x}}\right)}{\left({\frac {\partial z}{\partial y}}\right)}}\right)-90^{\circ }\left({\frac {\left({\frac {\partial z}{\partial y}}\right)}{\left|{\frac {\partial z}{\partial y}}\right|}}\right)}
  


=== Data analysis ===
It is difficult to relate wetlands maps to rainfall amounts recorded at different points such as airports, television stations, and schools. A GIS, however, can be used to depict two- and three-dimensional characteristics of the Earth's surface, subsurface, and atmosphere from information points. For example, a GIS can quickly generate a map with isopleth or contour lines that indicate differing amounts of rainfall. Such a map can be thought of as a rainfall contour map. Many sophisticated methods can estimate the characteristics of surfaces from a limited number of point measurements. A two-dimensional contour map created from the surface modeling of rainfall point measurements may be overlaid and analyzed with any other map in a GIS covering the same area. This GIS derived map can then provide additional information - such as the viability of water power potential as a renewable energy source. Similarly, GIS can be used to compare other renewable energy resources to find the best geographic potential for a region.
Additionally, from a series of three-dimensional points, or digital elevation model, isopleth lines representing elevation contours can be generated, along with slope analysis, shaded relief, and other elevation products. Watersheds can be easily defined for any given reach, by computing all of the areas contiguous and uphill from any given point of interest. Similarly, an expected thalweg of where surface water would want to travel in intermittent and permanent streams can be computed from elevation data in the GIS.


=== Topological modeling ===
A GIS can recognize and analyze the spatial relationships that exist within digitally stored spatial data. These topological relationships allow complex spatial modelling and analysis to be performed. Topological relationships between geometric entities traditionally include adjacency (what adjoins what), containment (what encloses what), and proximity (how close something is to something else).


=== Geometric networks ===
Geometric networks are linear networks of objects that can be used to represent interconnected features, and to perform special spatial analysis on them. A geometric network is composed of edges, which are connected at junction points, similar to graphs in mathematics and computer science. Just like graphs, networks can have weight and flow assigned to its edges, which can be used to represent various interconnected features more accurately. Geometric networks are often used to model road networks and public utility networks, such as electric, gas, and water networks. Network modeling is also commonly employed in transportation planning, hydrology modeling, and infrastructure modeling.


=== Hydrological modeling ===
GIS hydrological models can provide a spatial element that other hydrological models lack, with the analysis of variables such as slope, aspect and watershed or catchment area. Terrain analysis is fundamental to hydrology, since water always flows down a slope. As basic terrain analysis of a digital elevation model (DEM) involves calculation of slope and aspect, DEMs are very useful for hydrological analysis. Slope and aspect can then be used to determine direction of surface runoff, and hence flow accumulation for the formation of streams, rivers and lakes. Areas of divergent flow can also give a clear indication of the boundaries of a catchment. Once a flow direction and accumulation matrix has been created, queries can be performed that show contributing or dispersal areas at a certain point. More detail can be added to the model, such as terrain roughness, vegetation types and soil types, which can influence infiltration and evapotranspiration rates, and hence influencing surface flow. One of the main uses of hydrological modeling is in environmental contamination research. Other applications of hydrological modeling include groundwater and surface water mapping, as well as flood risk maps.


=== Cartographic modeling ===

Dana Tomlin probably coined the term ""cartographic modeling"" in his PhD dissertation (1983); he later used it in the title of his book, Geographic Information Systems and Cartographic Modeling (1990). Cartographic modeling refers to a process where several thematic layers of the same area are produced, processed, and analyzed. Tomlin used raster layers, but the overlay method (see below) can be used more generally. Operations on map layers can be combined into algorithms, and eventually into simulation or optimization models.


=== Map overlay ===
The combination of several spatial datasets (points, lines, or polygons) creates a new output vector dataset, visually similar to stacking several maps of the same region. These overlays are similar to mathematical Venn diagram overlays. A union overlay combines the geographic features and attribute tables of both inputs into a single new output. An intersect overlay defines the area where both inputs overlap and retains a set of attribute fields for each. A symmetric difference overlay defines an output area that includes the total area of both inputs except for the overlapping area.
Data extraction is a GIS process similar to vector overlay, though it can be used in either vector or raster data analysis. Rather than combining the properties and features of both datasets, data extraction involves using a ""clip"" or ""mask"" to extract the features of one data set that fall within the spatial extent of another dataset.
In raster data analysis, the overlay of datasets is accomplished through a process known as ""local operation on multiple rasters"" or ""map algebra"", through a function that combines the values of each raster's matrix. This function may weigh some inputs more than others through use of an ""index model"" that reflects the influence of various factors upon a geographic phenomenon.


=== Geostatistics ===

Geostatistics is a branch of statistics that deals with field data, spatial data with a continuous index. It provides methods to model spatial correlation, and predict values at arbitrary locations (interpolation).
When phenomena are measured, the observation methods dictate the accuracy of any subsequent analysis. Due to the nature of the data (e.g. traffic patterns in an urban environment; weather patterns over the Pacific Ocean), a constant or dynamic degree of precision is always lost in the measurement. This loss of precision is determined from the scale and distribution of the data collection.
To determine the statistical relevance of the analysis, an average is determined so that points (gradients) outside of any immediate measurement can be included to determine their predicted behavior. This is due to the limitations of the applied statistic and data collection methods, and interpolation is required to predict the behavior of particles, points, and locations that are not directly measurable.

Interpolation is the process by which a surface is created, usually a raster dataset, through the input of data collected at a number of sample points. There are several forms of interpolation, each which treats the data differently, depending on the properties of the data set. In comparing interpolation methods, the first consideration should be whether or not the source data will change (exact or approximate). Next is whether the method is subjective, a human interpretation, or objective. Then there is the nature of transitions between points: are they abrupt or gradual. Finally, there is whether a method is global (it uses the entire data set to form the model), or local where an algorithm is repeated for a small section of terrain.
Interpolation is a justified measurement because of a spatial autocorrelation principle that recognizes that data collected at any position will have a great similarity to, or influence of those locations within its immediate vicinity.
Digital elevation models, triangulated irregular networks, edge-finding algorithms, Thiessen polygons, Fourier analysis, (weighted) moving averages, inverse distance weighting, kriging, spline, and trend surface analysis are all mathematical methods to produce interpolative data.


=== Address geocoding ===

Geocoding is interpolating spatial locations (X,Y coordinates) from street addresses or any other spatially referenced data such as ZIP Codes, parcel lots and address locations. A reference theme is required to geocode individual addresses, such as a road centerline file with address ranges. The individual address locations have historically been interpolated, or estimated, by examining address ranges along a road segment. These are usually provided in the form of a table or database. The software will then place a dot approximately where that address belongs along the segment of centerline. For example, an address point of 500 will be at the midpoint of a line segment that starts with address 1 and ends with address 1,000. Geocoding can also be applied against actual parcel data, typically from municipal tax maps. In this case, the result of the geocoding will be an actually positioned space as opposed to an interpolated point. This approach is being increasingly used to provide more precise location information.


=== Reverse geocoding ===
Reverse geocoding is the process of returning an estimated street address number as it relates to a given coordinate. For example, a user can click on a road centerline theme (thus providing a coordinate) and have information returned that reflects the estimated house number. This house number is interpolated from a range assigned to that road segment. If the user clicks at the midpoint of a segment that starts with address 1 and ends with 100, the returned value will be somewhere near 50. Note that reverse geocoding does not return actual addresses, only estimates of what should be there based on the predetermined range.


=== Multi-criteria decision analysis ===
Coupled with GIS, multi-criteria decision analysis methods support decision-makers in analysing a set of alternative spatial solutions, such as the most likely ecological habitat for restoration, against multiple criteria, such as vegetation cover or roads. MCDA uses decision rules to aggregate the criteria, which allows the alternative solutions to be ranked or prioritised. GIS MCDA may reduce costs and time involved in identifying potential restoration sites.


=== Data output and cartography ===
Cartography is the design and production of maps, or visual representations of spatial data. The vast majority of modern cartography is done with the help of computers, usually using GIS but production of quality cartography is also achieved by importing layers into a design program to refine it. Most GIS software gives the user substantial control over the appearance of the data.
Cartographic work serves two major functions:
First, it produces graphics on the screen or on paper that convey the results of analysis to the people who make decisions about resources. Wall maps and other graphics can be generated, allowing the viewer to visualize and thereby understand the results of analyses or simulations of potential events. Web Map Servers facilitate distribution of generated maps through web browsers using various implementations of web-based application programming interfaces (AJAX, Java, Flash, etc.).
Second, other database information can be generated for further analysis or use. An example would be a list of all addresses within one mile (1.6 km) of a toxic spill.


=== Graphic display techniques ===
Traditional maps are abstractions of the real world, a sampling of important elements portrayed on a sheet of paper with symbols to represent physical objects. People who use maps must interpret these symbols. Topographic maps show the shape of land surface with contour lines or with shaded relief.
Today, graphic display techniques such as shading based on altitude in a GIS can make relationships among map elements visible, heightening one's ability to extract and analyze information. For example, two types of data were combined in a GIS to produce a perspective view of a portion of San Mateo County, California.
The digital elevation model, consisting of surface elevations recorded on a 30-meter horizontal grid, shows high elevations as white and low elevation as black.
The accompanying Landsat Thematic Mapper image shows a false-color infrared image looking down at the same area in 30-meter pixels, or picture elements, for the same coordinate points, pixel by pixel, as the elevation information.
A GIS was used to register and combine the two images to render the three-dimensional perspective view looking down the San Andreas Fault, using the Thematic Mapper image pixels, but shaded using the elevation of the landforms. The GIS display depends on the viewing point of the observer and time of day of the display, to properly render the shadows created by the sun's rays at that latitude, longitude, and time of day.
An archeochrome is a new way of displaying spatial data. It is a thematic on a 3D map that is applied to a specific building or a part of a building. It is suited to the visual display of heat-loss data.


=== Spatial ETL ===
Spatial ETL tools provide the data processing functionality of traditional extract, transform, load (ETL) software, but with a primary focus on the ability to manage spatial data. They provide GIS users with the ability to translate data between different standards and proprietary formats, whilst geometrically transforming the data en route. These tools can come in the form of add-ins to existing wider-purpose software such as spreadsheets.


=== GIS data mining ===
GIS or spatial data mining is the application of data mining methods to spatial data. Data mining, which is the partially automated search for hidden patterns in large databases, offers great potential benefits for applied GIS-based decision making. Typical applications include environmental monitoring. A characteristic of such applications is that spatial correlation between data measurements require the use of specialized algorithms for more efficient data analysis.


== Applications ==
The implementation of a GIS is often driven by jurisdictional (such as a city), purpose, or application requirements. Generally, a GIS implementation may be custom-designed for an organization. Hence, a GIS deployment developed for an application, jurisdiction, enterprise, or purpose may not be necessarily interoperable or compatible with a GIS that has been developed for some other application, jurisdiction, enterprise, or purpose.
GIS provides, for every kind of location-based organization, a platform to update geographical data without wasting time to visit the field and update a database manually. GIS when integrated with other powerful enterprise solutions like SAP and the Wolfram Language helps creating powerful decision support system at enterprise level.

Many disciplines can benefit from GIS technology. An active GIS market has resulted in lower costs and continual improvements in the hardware and software components of GIS, and usage in the fields of science, government, business, and industry, with applications including real estate, public health, crime mapping, national defense, sustainable development, natural resources, climatology, landscape architecture, archaeology, regional and community planning, transportation and logistics. GIS is also diverging into location-based services, which allows GPS-enabled mobile devices to display their location in relation to fixed objects (nearest restaurant, gas station, fire hydrant) or mobile objects (friends, children, police car), or to relay their position back to a central server for display or other processing.


=== Open Geospatial Consortium standards ===

The Open Geospatial Consortium (OGC) is an international industry consortium of 384 companies, government agencies, universities, and individuals participating in a consensus process to develop publicly available geoprocessing specifications. Open interfaces and protocols defined by OpenGIS Specifications support interoperable solutions that ""geo-enable"" the Web, wireless and location-based services, and mainstream IT, and empower technology developers to make complex spatial information and services accessible and useful with all kinds of applications. Open Geospatial Consortium protocols include Web Map Service, and Web Feature Service.
GIS products are broken down by the OGC into two categories, based on how completely and accurately the software follows the OGC specifications.

Compliant Products are software products that comply to OGC's OpenGIS Specifications. When a product has been tested and certified as compliant through the OGC Testing Program, the product is automatically registered as ""compliant"" on this site.
Implementing Products are software products that implement OpenGIS Specifications but have not yet passed a compliance test. Compliance tests are not available for all specifications. Developers can register their products as implementing draft or approved specifications, though OGC reserves the right to review and verify each entry.


=== Web mapping ===

In recent years there has been a proliferation of free-to-use and easily accessible mapping software such as the proprietary web applications Google Maps and Bing Maps, as well as the free and open-source alternative OpenStreetMap. These services give the public access to huge amounts of geographic data; perceived by many users to be as trustworthy and usable as professional information.
Some of them, like Google Maps and OpenLayers, expose an application programming interface (API) that enable users to create custom applications. These toolkits commonly offer street maps, aerial/satellite imagery, geocoding, searches, and routing functionality. Web mapping has also uncovered the potential of crowdsourcing geodata in projects like OpenStreetMap, which is a collaborative project to create a free editable map of the world. These mashup projects have been proven to provide a high level of value and benefit to end users outside that possible through traditional geographic information.


=== Adding the dimension of time ===

The condition of the Earth's surface, atmosphere, and subsurface can be examined by feeding satellite data into a GIS. GIS technology gives researchers the ability to examine the variations in Earth processes over days, months, and years. As an example, the changes in vegetation vigor through a growing season can be animated to determine when drought was most extensive in a particular region. The resulting graphic represents a rough measure of plant health. Working with two variables over time would then allow researchers to detect regional differences in the lag between a decline in rainfall and its effect on vegetation.
GIS technology and the availability of digital data on regional and global scales enable such analyses. The satellite sensor output used to generate a vegetation graphic is produced for example by the advanced very-high-resolution radiometer (AVHRR). This sensor system detects the amounts of energy reflected from the Earth's surface across various bands of the spectrum for surface areas of about 1 square kilometer. The satellite sensor produces images of a particular location on the Earth twice a day. AVHRR and more recently the moderate-resolution imaging spectroradiometer (MODIS) are only two of many sensor systems used for Earth surface analysis.
In addition to the integration of time in environmental studies, GIS is also being explored for its ability to track and model the progress of humans throughout their daily routines. A concrete example of progress in this area is the recent release of time-specific population data by the U.S. Census. In this data set, the populations of cities are shown for daytime and evening hours highlighting the pattern of concentration and dispersion generated by North American commuting patterns. The manipulation and generation of data required to produce this data would not have been possible without GIS.
Using models to project the data held by a GIS forward in time have enabled planners to test policy decisions using spatial decision support systems.


== Semantics ==
Tools and technologies emerging from the World Wide Web Consortium's Semantic Web are proving useful for data integration problems in information systems. Correspondingly, such technologies have been proposed as a means to facilitate interoperability and data reuse among GIS applications. and also to enable new analysis mechanisms.
Ontologies are a key component of this semantic approach as they allow a formal, machine-readable specification of the concepts and relationships in a given domain. This in turn allows a GIS to focus on the intended meaning of data rather than its syntax or structure. For example, reasoning that a land cover type classified as deciduous needleleaf trees in one dataset is a specialization or subset of land cover type forest in another more roughly classified dataset can help a GIS automatically merge the two datasets under the more general land cover classification. Tentative ontologies have been developed in areas related to GIS applications, for example the hydrology ontology developed by the Ordnance Survey in the United Kingdom and the SWEET ontologies developed by NASA's Jet Propulsion Laboratory. Also, simpler ontologies and semantic metadata standards are being proposed by the W3C Geo Incubator Group to represent geospatial data on the web. GeoSPARQL is a standard developed by the Ordnance Survey, United States Geological Survey, Natural Resources Canada, Australia's Commonwealth Scientific and Industrial Research Organisation and others to support ontology creation and reasoning using well-understood OGC literals (GML, WKT), topological relationships (Simple Features, RCC8, DE-9IM), RDF and the SPARQL database query protocols.
Recent research results in this area can be seen in the International Conference on Geospatial Semantics and the Terra Cognita – Directions to the Geospatial Semantic Web workshop at the International Semantic Web Conference.


== Implications of GIS in society ==

With the popularization of GIS in decision making, scholars have begun to scrutinize the social and political implications of GIS. GIS can also be misused to distort reality for individual and political gain. It has been argued that the production, distribution, utilization, and representation of geographic information are largely related with the social context and has the potential to increase citizen trust in government. Other related topics include discussion on copyright, privacy, and censorship. A more optimistic social approach to GIS adoption is to use it as a tool for public participation.


=== GIS in education ===

At the end of the 20th century, GIS began to be recognized as tools that could be used in the classroom. The benefits of GIS in education seem focused on developing spatial thinking, but there is not enough bibliography or statistical data to show the concrete scope of the use of GIS in education around the world, although the expansion has been faster in those countries where the curriculum mentions them.
GIS seem to provide many advantages in teaching geography because they allow for analyses based on real geographic data and also help raise many research questions from teachers and students in classrooms, as well as they contribute to improvement in learning by developing spatial and geographical thinking and, in many cases, student motivation.


=== GIS in local government ===
GIS is proven as an organization-wide, enterprise and enduring technology that continues to change how local government operates. Government agencies have adopted GIS technology as a method to better manage the following areas of government organization:
Public Safety operations such as Emergency Operations Centers, Fire Prevention, Police and Sheriff mobile technology and dispatch, and mapping weather risks.
Parks and Recreation departments and their functions in asset inventory, land conservation, land management, and cemetery management.
Public Works and Utilities, tracking water and stormwater drainage, electrical assets, engineering projects, and public transportation assets and trends.
Fiber Network Management for interdepartmental network assets
School analytical and demographic data, asset management, and improvement/expansion planning
Public Administration for election data, property records, and zoning/management.
The Open Data initiative is pushing local government to take advantage of technology such as GIS technology, as it encompasses the requirements to fit the Open Data/Open Government model of transparency. With Open Data, local government organizations can implement Citizen Engagement applications and online portals, allowing citizens to see land information, report potholes and signage issues, view and sort parks by assets, view real-time crime rates and utility repairs, and much more. The push for open data within government organizations is driving the growth in local government GIS technology spending, and database management.


== See also ==


== References ==


== Further reading ==
Berry, J. K. (1993) Beyond Mapping: Concepts, Algorithms and Issues in GIS. Fort Collins, CO: GIS World Books.
Bolstad, P. (2005) GIS Fundamentals: A first text on Geographic Information Systems, Second Edition. White Bear Lake, MN: Eider Press, 543 pp.
Burrough, P. A. and McDonnell, R. A. (1998). Principles of geographical information systems. Oxford University Press, Oxford, 327 pp.
Buzai, G.D.; Robinson, D. (2010). ""Geographical Information Systems in Latin America, 1987-2010. A Preliminary Overview"". Journal of Latin American Geography. 9 (3): 9–31. doi:10.1353/lag.2010.0027. 
Chang, K. (2007) Introduction to Geographic Information System, 4th Edition. McGraw Hill, ISBN 978-0071267588
de Smith MJ, Goodchild MF, Longley PA (2007). Geospatial analysis: A comprehensive guide to principles, techniques and software tools (2nd ed.). Troubador, UK. ISBN 978-1848761582. 
Elangovan,K (2006)""GIS: Fundamentals, Applications and Implementations"", New India Publishing Agency, New Delhi""208 pp.
Fu, P., and J. Sun. 2010. Web GIS: Principles and Applications. ESRI Press. Redlands, CA. ISBN 1-58948-245-X.
Harvey, Francis(2008) A Primer of GIS, Fundamental geographic and cartographic concepts. The Guilford Press, 31 pp.
Heywood, I., Cornelius, S., and Carver, S. (2006) An Introduction to Geographical Information Systems. Prentice Hall. 3rd edition.
Longley, P.A., Goodchild, M.F., Maguire, D.J. and Rhind, D.W. (2005) Geographic Information Systems and Science. Chichester: Wiley. 2nd edition.
Maguire, D.J., Goodchild M.F., Rhind D.W. (1997) ""Geographic Information Systems: principles, and applications"" Longman Scientific and Technical, Harlow.
Maliene V, Grigonis V, Palevičius V, Griffiths S (2011). ""Geographic information system: Old principles with new capabilities"". Urban Design International. 16 (1): 1–6. doi:10.1057/udi.2010.25. 
Mennecke, Brian E.; Lawrence, A. West Jr. (October 2001). ""Geographic Information Systems in Developing Countries: Issues in Data Collection, Implementation and Management"". Journal of Global Information Management. 9 (4): 45–55. 
Ott, T. and Swiaczny, F. (2001) Time-integrative GIS. Management and analysis of spatio-temporal data, Berlin / Heidelberg / New York: Springer.
Sajeevan G (March 2008). ""Latitude and longitude – A misunderstanding"" (PDF). Current Science. 94 (5): 568. 
Sajeevan G (2006). ""Customise and empower"". Geospatial Today. 4 (7): 40–43. 
Thurston, J., Poiker, T.K. and J. Patrick Moore. (2003) Integrated Geospatial Technologies: A Guide to GPS, GIS, and Data Logging. Hoboken, New Jersey: Wiley.
Tomlinson, R.F., (2005) Thinking About GIS: Geographic Information System Planning for Managers. ESRI Press. 328 pp.
Wise, S. (2002) GIS Basics. London: Taylor & Francis.
Worboys, Michael; Duckham, Matt (2004). GIS: a computing perspective. Boca Raton: CRC Press. ISBN 0415283752. 
Wheatley, David and Gillings, Mark (2002) Spatial Technology and Archaeology. The Archaeological Application of GIS. London, New York, Taylor & Francis.
Holdstock, David (2017). Strategic GIS Planning and Management in Local Government. Boca Raton, FL: CRC Press. ISBN 9781466556508. 


== External links ==
 Media related to Geographic information systems at Wikimedia Commons"
3,Computational creativity,16300571,61153,"Computational creativity (also known as artificial creativity, mechanical creativity, creative computing or creative computation) is a multidisciplinary endeavour that is located at the intersection of the fields of artificial intelligence, cognitive psychology, philosophy, and the arts.
The goal of computational creativity is to model, simulate or replicate creativity using a computer, to achieve one of several ends:
To construct a program or computer capable of human-level creativity.
To better understand human creativity and to formulate an algorithmic perspective on creative behavior in humans.
To design programs that can enhance human creativity without necessarily being creative themselves.
The field of computational creativity concerns itself with theoretical and practical issues in the study of creativity. Theoretical work on the nature and proper definition of creativity is performed in parallel with practical work on the implementation of systems that exhibit creativity, with one strand of work informing the other.


== Theoretical issues ==
As measured by the amount of activity in the field (e.g., publications, conferences and workshops), computational creativity is a growing area of research. But the field is still hampered by a number of fundamental problems. Creativity is very difficult, perhaps even impossible, to define in objective terms. Is it a state of mind, a talent or ability, or a process? Creativity takes many forms in human activity, some eminent (sometimes referred to as ""Creativity"" with a capital C) and some mundane.
These are problems that complicate the study of creativity in general, but certain problems attach themselves specifically to computational creativity:
Can creativity be hard-wired? In existing systems to which creativity is attributed, is the creativity that of the system or that of the system's programmer or designer?
How do we evaluate computational creativity? What counts as creativity in a computational system? Are natural language generation systems creative? Are machine translation systems creative? What distinguishes research in computational creativity from research in artificial intelligence generally?
If eminent creativity is about rule-breaking or the disavowal of convention, how is it possible for an algorithmic system to be creative? In essence, this is a variant of Ada Lovelace's objection to machine intelligence, as recapitulated by modern theorists such as Teresa Amabile. If a machine can do only what it was programmed to do, how can its behavior ever be called creative?
Indeed, not all computer theorists would agree with the premise that computers can only do what they are programmed to do—a key point in favor of computational creativity.


== Defining creativity in computational terms ==
Because no single perspective or definition seems to offer a complete picture of creativity, the AI researchers Newell, Shaw and Simon developed the combination of novelty and usefulness into the cornerstone of a multi-pronged view of creativity, one that uses the following four criteria to categorize a given answer or solution as creative:
The answer is novel and useful (either for the individual or for society)
The answer demands that we reject ideas we had previously accepted
The answer results from intense motivation and persistence
The answer comes from clarifying a problem that was originally vague
Whereas the above reflects a ""top-down"" approach to computational creativity, an alternative thread has developed among ""bottom-up"" computational psychologists involved in artificial neural network research. During the late 1980s and early 1990s, for example, such generative neural systems were driven by genetic algorithms. Experiments involving recurrent nets were successful in hybridizing simple musical melodies and predicting listener expectations.
Concurrent with such research, a number of computational psychologists took the perspective, popularized by Stephen Wolfram, that system behaviors perceived as complex, including the mind's creative output, could arise from what would be considered simple algorithms. As neuro-philosophical thinking matured, it also became evident that language actually presented an obstacle to producing a scientific model of cognition, creative or not, since it carried with it so many unscientific aggrandizements that were more uplifting than accurate. Thus questions naturally arose as to how ""rich,"" ""complex,"" and ""wonderful"" creative cognition actually was.


== Artificial neural networks ==
Before 1989, artificial neural networks have been used to model certain aspects of creativity. Peter Todd (1989) first trained a neural network to reproduce musical melodies from a training set of musical pieces. Then he used a change algorithm to modify the network's input parameters. The network was able to randomly generate new music in a highly uncontrolled manner. In 1992, Todd extended this work, using the so-called distal teacher approach that had been developed by Paul Munro, Paul Werbos, D. Nguyen and Bernard Widrow, Michael I. Jordan and David Rumelhart. In the new approach there are two neural networks, one of which is supplying training patterns to another. In later efforts by Todd, a composer would select a set of melodies that define the melody space, position them on a 2-d plane with a mouse-based graphic interface, and train a connectionist network to produce those melodies, and listen to the new ""interpolated"" melodies that the network generates corresponding to intermediate points in the 2-d plane.
More recently a neurodynamical model of semantic networks has been developed to study how the connectivity structure of these networks relates to the richness of the semantic constructs, or ideas, they can generate. It was demonstrated that semantic neural networks that have richer semantic dynamics than those with other connectivity structures may provide insight into the important issue of how the physical structure of the brain determines one of the most profound features of the human mind – its capacity for creative thought.


== Key concepts from the literature ==
Some high-level and philosophical themes recur throughout the field of computational creativity.


=== Important categories of creativity ===
Margaret Boden refers to creativity that is novel merely to the agent that produces it as ""P-creativity"" (or ""psychological creativity""), and refers to creativity that is recognized as novel by society at large as ""H-creativity"" (or ""historical creativity""). Stephen Thaler has suggested a new category he calls ""V-"" or ""Visceral creativity"" wherein significance is invented to raw sensory inputs to a Creativity Machine architecture, with the ""gateway"" nets perturbed to produce alternative interpretations, and downstream nets shifting such interpretations to fit the overarching context. An important variety of such V-creativity is consciousness itself, wherein meaning is reflexively invented to activation turnover within the brain.


=== Exploratory and transformational creativity ===
Boden also distinguishes between the creativity that arises from an exploration within an established conceptual space, and the creativity that arises from a deliberate transformation or transcendence of this space. She labels the former as exploratory creativity and the latter as transformational creativity, seeing the latter as a form of creativity far more radical, challenging, and rarer than the former. Following the criteria from Newell and Simon elaborated above, we can see that both forms of creativity should produce results that are appreciably novel and useful (criterion 1), but exploratory creativity is more likely to arise from a thorough and persistent search of a well-understood space (criterion 3) -- while transformational creativity should involve the rejection of some of the constraints that define this space (criterion 2) or some of the assumptions that define the problem itself (criterion 4). Boden's insights have guided work in computational creativity at a very general level, providing more an inspirational touchstone for development work than a technical framework of algorithmic substance. However, Boden's insights are more recently also the subject of formalization, most notably in the work by Geraint Wiggins.


=== Generation and evaluation ===
The criterion that creative products should be novel and useful means that creative computational systems are typically structured into two phases, generation and evaluation. In the first phase, novel (to the system itself, thus P-Creative) constructs are generated; unoriginal constructs that are already known to the system are filtered at this stage. This body of potentially creative constructs are then evaluated, to determine which are meaningful and useful and which are not. This two-phase structure conforms to the Geneplore model of Finke, Ward and Smith, which is a psychological model of creative generation based on empirical observation of human creativity.


=== Combinatorial creativity ===
A great deal, perhaps all, of human creativity can be understood as a novel combination of pre-existing ideas or objects. Common strategies for combinatorial creativity include:
Placing a familiar object in an unfamiliar setting (e.g., Marcel Duchamp's Fountain) or an unfamiliar object in a familiar setting (e.g., a fish-out-of-water story such as The Beverly Hillbillies)
Blending two superficially different objects or genres (e.g., a sci-fi story set in the Wild West, with robot cowboys, as in Westworld, or the reverse, as in Firefly; Japanese haiku poems, etc.)
Comparing a familiar object to a superficially unrelated and semantically distant concept (e.g., ""Makeup is the Western burka""; ""A zoo is a gallery with living exhibits"")
Adding a new and unexpected feature to an existing concept (e.g., adding a scalpel to a Swiss Army knife; adding a camera to a mobile phone)
Compressing two incongruous scenarios into the same narrative to get a joke (e.g., the Emo Philips joke ""Women are always using me to advance their careers. Damned anthropologists!"")
Using an iconic image from one domain in a domain for an unrelated or incongruous idea or product (e.g., using the Marlboro Man image to sell cars, or to advertise the dangers of smoking-related impotence).
The combinatorial perspective allows us to model creativity as a search process through the space of possible combinations. The combinations can arise from composition or concatenation of different representations, or through a rule-based or stochastic transformation of initial and intermediate representations. Genetic algorithms and neural networks can be used to generate blended or crossover representations that capture a combination of different inputs.


==== Conceptual blending ====

Mark Turner and Gilles Fauconnier propose a model called Conceptual Integration Networks that elaborates upon Arthur Koestler's ideas about creativity as well as more recent work by Lakoff and Johnson, by synthesizing ideas from Cognitive Linguistic research into mental spaces and conceptual metaphors. Their basic model defines an integration network as four connected spaces:
A first input space (contains one conceptual structure or mental space)
A second input space (to be blended with the first input)
A generic space of stock conventions and image-schemas that allow the input spaces to be understood from an integrated perspective
A blend space in which a selected projection of elements from both input spaces are combined; inferences arising from this combination also reside here, sometimes leading to emergent structures that conflict with the inputs.
Fauconnier and Turner describe a collection of optimality principles that are claimed to guide the construction of a well-formed integration network. In essence, they see blending as a compression mechanism in which two or more input structures are compressed into a single blend structure. This compression operates on the level of conceptual relations. For example, a series of similarity relations between the input spaces can be compressed into a single identity relationship in the blend.
Some computational success has been achieved with the blending model by extending pre-existing computational models of analogical mapping that are compatible by virtue of their emphasis on connected semantic structures. More recently, Francisco Câmara Pereira presented an implementation of blending theory that employs ideas both from GOFAI and genetic algorithms to realize some aspects of blending theory in a practical form; his example domains range from the linguistic to the visual, and the latter most notably includes the creation of mythical monsters by combining 3-D graphical models.


== Linguistic creativity ==
Language provides continuous opportunity for creativity, evident in the generation of novel sentences, phrasings, puns, neologisms, rhymes, allusions, sarcasm, irony, similes, metaphors, analogies, witticisms, and jokes. Native speakers of morphologically rich languages frequently create new word-forms that are easily understood, although they will never find their way to the dictionary. The area of natural language generation has been well studied, but these creative aspects of everyday language have yet to be incorporated with any robustness or scale.


=== Story generation ===
Substantial work has been conducted in this area of linguistic creation since the 1970s, with the development of James Meehan's TALE-SPIN  system. TALE-SPIN viewed stories as narrative descriptions of a problem-solving effort, and created stories by first establishing a goal for the story's characters so that their search for a solution could be tracked and recorded. The MINSTREL system represents a complex elaboration of this basis approach, distinguishing a range of character-level goals in the story from a range of author-level goals for the story. Systems like Bringsjord's BRUTUS elaborate these ideas further to create stories with complex inter-personal themes like betrayal. Nonetheless, MINSTREL explicitly models the creative process with a set of Transform Recall Adapt Methods (TRAMs) to create novel scenes from old. The MEXICA model of Rafael Pérez y Pérez and Mike Sharples is more explicitly interested in the creative process of storytelling, and implements a version of the engagement-reflection cognitive model of creative writing.
The company Narrative Science makes computer generated news and reports commercially available, including summarizing team sporting events based on statistical data from the game. It also creates financial reports and real estate analyses.


=== Metaphor and simile ===
Example of a metaphor: ""She was an ape.""
Example of a simile: ""Felt like a tiger-fur blanket."" The computational study of these phenomena has mainly focused on interpretation as a knowledge-based process. Computationalists such as Yorick Wilks, James Martin, Dan Fass, John Barnden, and Mark Lee have developed knowledge-based approaches to the processing of metaphors, either at a linguistic level or a logical level. Tony Veale and Yanfen Hao have developed a system, called Sardonicus, that acquires a comprehensive database of explicit similes from the web; these similes are then tagged as bona-fide (e.g., ""as hard as steel"") or ironic (e.g., ""as hairy as a bowling ball"", ""as pleasant as a root canal""); similes of either type can be retrieved on demand for any given adjective. They use these similes as the basis of an on-line metaphor generation system called Aristotle that can suggest lexical metaphors for a given descriptive goal (e.g., to describe a supermodel as skinny, the source terms ""pencil"", ""whip"", ""whippet"", ""rope"", ""stick-insect"" and ""snake"" are suggested).


=== Analogy ===
The process of analogical reasoning has been studied from both a mapping and a retrieval perspective, the latter being key to the generation of novel analogies. The dominant school of research, as advanced by Dedre Gentner, views analogy as a structure-preserving process; this view has been implemented in the structure mapping engine or SME, the MAC/FAC retrieval engine (Many Are Called, Few Are Chosen), ACME (Analogical Constraint Mapping Engine) and ARCS (Analogical Retrieval Constraint System). Other mapping-based approaches include Sapper, which situates the mapping process in a semantic-network model of memory. Analogy is a very active sub-area of creative computation and creative cognition; active figures in this sub-area include Douglas Hofstadter, Paul Thagard, and Keith Holyoak. Also worthy of note here is Peter Turney and Michael Littman's machine learning approach to the solving of SAT-style analogy problems; their approach achieves a score that compares well with average scores achieved by humans on these tests.


=== Joke generation ===

Humour is an especially knowledge-hungry process, and the most successful joke-generation systems to date have focussed on pun-generation, as exemplified by the work of Kim Binsted and Graeme Ritchie. This work includes the JAPE system, which can generate a wide range of puns that are consistently evaluated as novel and humorous by young children. An improved version of JAPE has been developed in the guise of the STANDUP system, which has been experimentally deployed as a means of enhancing linguistic interaction with children with communication disabilities. Some limited progress has been made in generating humour that involves other aspects of natural language, such as the deliberate misunderstanding of pronominal reference (in the work of Hans Wim Tinholt and Anton Nijholt), as well as in the generation of humorous acronyms in the HAHAcronym system of Oliviero Stock and Carlo Strapparava.


=== Neologism ===
The blending of multiple word forms is a dominant force for new word creation in language; these new words are commonly called ""blends"" or ""portmanteau words"" (after Lewis Carroll). Tony Veale has developed a system called ZeitGeist that harvests neological headwords from Wikipedia and interprets them relative to their local context in Wikipedia and relative to specific word senses in WordNet. ZeitGeist has been extended to generate neologisms of its own; the approach combines elements from an inventory of word parts that are harvested from WordNet, and simultaneously determines likely glosses for these new words (e.g., ""food traveller"" for ""gastronaut"" and ""time traveller"" for ""chrononaut""). It then uses Web search to determine which glosses are meaningful and which neologisms have not been used before; this search identifies the subset of generated words that are both novel (""H-creative"") and useful. Neurolinguistic inspirations have been used to analyze the process of novel word creation in the brain, understand neurocognitive processes responsible for intuition, insight, imagination and creativity and to create a server that invents novel names for products, based on their description. Further, the system Nehovah blends two source words into a neologism that blends the meanings of the two source words. Nehovah searches WordNet for synonyms and TheTopTens.com for pop culture hyponyms. The synonyms and hyponyms are blended together to create a set of candidate neologisms. The neologisms are then scored based on their word structure, how unique the word is, how apparent the concepts are conveyed, and if the neologism has a pop culture reference. Nehovah loosely follows conceptual blending.


=== Poetry ===

Like jokes, poems involve a complex interaction of different constraints, and no general-purpose poem generator adequately combines the meaning, phrasing, structure and rhyme aspects of poetry. Nonetheless, Pablo Gervás has developed a noteworthy system called ASPERA that employs a case-based reasoning (CBR) approach to generating poetic formulations of a given input text via a composition of poetic fragments that are retrieved from a case-base of existing poems. Each poem fragment in the ASPERA case-base is annotated with a prose string that expresses the meaning of the fragment, and this prose string is used as the retrieval key for each fragment. Metrical rules are then used to combine these fragments into a well-formed poetic structure. Racter is an example of such a software project.


== Musical creativity ==
Computational creativity in the music domain has focused both on the generation of musical scores for use by human musicians, and on the generation of music for performance by computers. The domain of generation has included classical music (with software that generates music in the style of Mozart and Bach) and jazz. Most notably, David Cope has written a software system called ""Experiments in Musical Intelligence"" (or ""EMI"") that is capable of analyzing and generalizing from existing music by a human composer to generate novel musical compositions in the same style. EMI's output is convincing enough to persuade human listeners that its music is human-generated to a high level of competence.
In the field of contemporary classical music, Iamus is the first computer that composes from scratch, and produces final scores that professional interpreters can play. The London Symphony Orchestra played a piece for full orchestra, included in Iamus' debut CD, which New Scientist described as ""The first major work composed by a computer and performed by a full orchestra"". Melomics, the technology behind Iamus, is able to generate pieces in different styles of music with a similar level of quality.
Creativity research in jazz has focused on the process of improvisation and the cognitive demands that this places on a musical agent: reasoning about time, remembering and conceptualizing what has already been played, and planning ahead for what might be played next. The robot Shimon, developed by Gil Weinberg of Georgia Tech, has demonstrated jazz improvisation. Virtual improvisation software based on machine learning models of musical style include OMax, SoMax and PyOracle, are used to create improvisations in real-time by re-injecting variable length sequences learned on the fly from live performer.
In 1994, a Creativity Machine architecture (see above) was able to generate 11,000 musical hooks by training a synaptically perturbed neural net on 100 melodies that had appeared on the top ten list over the last 30 years. In 1996, a self-bootstrapping Creativity Machine observed audience facial expressions through an advanced machine vision system and perfected its musical talents to generate an album entitled ""Song of the Neurons""
In the field of musical composition, the patented works by René-Louis Baron allowed to make a robot that can create and play a multitude of orchestrated melodies so-called ""coherent"" in any musical style. All outdoor physical parameter associated with one or more specific musical parameters, can influence and develop each of these songs (in real time while listening to the song). The patented invention Medal-Composer raises problems of copyright.


== Visual and artistic creativity ==
Computational creativity in the generation of visual art has had some notable successes in the creation of both abstract art and representational art. The most famous program in this domain is Harold Cohen's AARON, which has been continuously developed and augmented since 1973. Though formulaic, Aaron exhibits a range of outputs, generating black-and-white drawings or colour paintings that incorporate human figures (such as dancers), potted plants, rocks, and other elements of background imagery. These images are of a sufficiently high quality to be displayed in reputable galleries.
Other software artists of note include the NEvAr system (for ""Neuro-Evolutionary Art"") of Penousal Machado. NEvAr uses a genetic algorithm to derive a mathematical function that is then used to generate a coloured three-dimensional surface. A human user is allowed to select the best pictures after each phase of the genetic algorithm, and these preferences are used to guide successive phases, thereby pushing NEvAr's search into pockets of the search space that are considered most appealing to the user.
The Painting Fool, developed by Simon Colton originated as a system for overpainting digital images of a given scene in a choice of different painting styles, colour palettes and brush types. Given its dependence on an input source image to work with, the earliest iterations of the Painting Fool raised questions about the extent of, or lack of, creativity in a computational art system. Nonetheless, in more recent work, The Painting Fool has been extended to create novel images, much as AARON does, from its own limited imagination. Images in this vein include cityscapes and forests, which are generated by a process of constraint satisfaction from some basic scenarios provided by the user (e.g., these scenarios allow the system to infer that objects closer to the viewing plane should be larger and more color-saturated, while those further away should be less saturated and appear smaller). Artistically, the images now created by the Painting Fool appear on a par with those created by Aaron, though the extensible mechanisms employed by the former (constraint satisfaction, etc.) may well allow it to develop into a more elaborate and sophisticated painter.
The artist Krasimira Dimtchevska and the software developer Svillen Ranev have created a computational system combining a rule-based generator of English sentences and a visual composition builder that converts sentences generated by the system into abstract art. The software generates automatically indefinite number of different images using different color, shape and size palettes. The software also allows the user to select the subject of the generated sentences or/and the one or more of the palettes used by the visual composition builder.
An emerging area of computational creativity is that of video games. ANGELINA is a system for creatively developing video games in Java by Michael Cook. One important aspect is Mechanic Miner, a system which can generate short segments of code which act as simple game mechanics. ANGELINA can evaluate these mechanics for usefulness by playing simple unsolvable game levels and testing to see if the new mechanic makes the level solvable. Sometimes Mechanic Miner discovers bugs in the code and exploits these to make new mechanics for the player to solve problems with.
In July 2015 Google released DeepDream – an open source computer vision program, created to detect faces and other patterns in images with the aim of automatically classifying images, which uses a convolutional neural network to find and enhance patterns in images via algorithmic pareidolia, thus creating a dreamlike psychedelic appearance in the deliberately over-processed images.
In August 2015 researchers from Tübingen, Germany created a convolutional neural network that uses neural representations to separate and recombine content and style of arbitrary images which is able to turn images into stylistic imitations of works of art by artists such as a Picasso or Van Gogh in about an hour. Their algorithm is put into use in the website DeepArt that allows users to create unique artistic images by their algorithm.
In early 2016, a global team of researchers explained how a new computational creativity approach known as the Digital Synaptic Neural Substrate (DSNS) could be used to generate original chess puzzles that were not derived from endgame databases. The DSNS is able to combine features of different objects (e.g. chess problems, paintings, music) using stochastic methods in order to derive new feature specifications which can be used to generate objects in any of the original domains. The generated chess puzzles have also been featured on YouTube.


== Creativity in problem solving ==
Creativity is also useful in allowing for unusual solutions in problem solving. In psychology and cognitive science, this research area is called creative problem solving. The Explicit-Implicit Interaction (EII) theory of creativity has recently been implemented using a CLARION-based computational model that allows for the simulation of incubation and insight in problem solving. The emphasis of this computational creativity project is not on performance per se (as in artificial intelligence projects) but rather on the explanation of the psychological processes leading to human creativity and the reproduction of data collected in psychology experiments. So far, this project has been successful in providing an explanation for incubation effects in simple memory experiments, insight in problem solving, and reproducing the overshadowing effect in problem solving.


== Debate about ""general"" theories of creativity ==
Some researchers feel that creativity is a complex phenomenon whose study is further complicated by the plasticity of the language we use to describe it. We can describe not just the agent of creativity as ""creative"" but also the product and the method. Consequently, it could be claimed that it is unrealistic to speak of a general theory of creativity. Nonetheless, some generative principles are more general than others, leading some advocates to claim that certain computational approaches are ""general theories"". Stephen Thaler, for instance, proposes that certain modalities of neural networks are generative enough, and general enough, to manifest a high degree of creative capabilities. Likewise, the Formal Theory of Creativity is based on a simple computational principle published by Jürgen Schmidhuber in 1991. The theory postulates that creativity and curiosity and selective attention in general are by-products of a simple algorithmic principle for measuring and optimizing learning progress.


=== Unified model of creativity ===
A unifying model of creativity was proposed by S. L. Thaler through a series of international patents in computational creativity, beginning in 1997 with the issuance of U.S. Patent 5,659,666. Based upon theoretical studies of traumatized neural networks and inspired by studies of damage-induced vibrational modes in simulated crystal lattices, this extensive intellectual property suite taught the application of a broad range of noise, damage, and disordering effects to a trained neural network so as to drive the formation of novel or confabulatory patterns that could potentially qualify as ideas and/or plans of action.
Thaler's scientific and philosophical papers both preceding and following the issuance of these patents described:
The aspects of creativity accompanying a broad gamut of cognitive functions (e.g., waking to dreaming to near-death trauma),
A shorthand notation for describing creative neural architectures and their function,
Quantitative modeling of the rhythm with which creative cognition occurs, and,
A prescription for critical perturbation regimes leading to the most efficient generation of useful information by a creative neural system.
A bottom-up model that links creativity and a wide range of psychopathologies. 
Thaler has also recruited his generative neural architectures into a theory of consciousness that closely models the temporal evolution of thought, creative or not, while also accounting for the subjective feel associated with this hotly debated mental phenomenon.
In 1989, in one of the most controversial reductions to practice of this general theory of creativity, one neural net termed the ""grim reaper,"" governed the synaptic damage (i.e., rule-changes) applied to another net that had learned a series of traditional Christmas carol lyrics. The former net, on the lookout for both novel and grammatical lyrics, seized upon the chilling sentence, ""In the end all men go to good earth in one eternal silent night,"" thereafter ceasing the synaptic degradation process. In subsequent projects, these systems produced more useful results across many fields of human endeavor, oftentimes bootstrapping their learning from a blank slate based upon the success or failure of self-conceived concepts and strategies seeded upon such internal network damage.


== Criticism of Computational Creativity ==
Traditional computers, as mainly used in the computational creativity application, do not support creativity, as they fundamentally transform a set of discrete, limited domain of input parameters into a set of discrete, limited domain of output parameters using a limited set of computational functions. As such, a computer cannot be creative, as everything in the output must have been already present in the input data or the algorithms. For some related discussions and references to related work are captured in some recent work on philosophical foundations of simulation.
Mathematically, the same set of arguments against creativity has been made by Chaitin. Similar observations come from a Model Theory perspective. All this criticism emphasizes that computational creativity is useful and may look like creativity, but it is not real creativity, as nothing new is created, just transformed in well defined algorithms.


== Events ==
The International Conference on Computational Creativity (ICCC) occurs annually, organized by The Association for Computational Creativity. Events in the series include:
ICCC 2017, Atlanta, Georgia, USA
ICCC 2016, Paris, France
ICCC 2015, Park City, Utah, USA. Keynote: Emily Short
ICCC 2014, Ljubljana, Slovenia. Keynote: Oliver Deussen
ICCC 2013, Sydney, Australia. Keynote: Arne Dietrich
ICCC 2012, Dublin, Ireland. Keynote: Steven Smith
ICCC 2011, Mexico City, Mexico. Keynote: George E Lewis
ICCC 2010, Lisbon, Portugal. Keynote/Inivited Talks: Nancy J Nersessian and Mary Lou Maher
Previously, the community of computational creativity has held a dedicated workshop, the International Joint Workshop on Computational Creativity, every year since 1999. Previous events in this series include:
IJWCC 2003, Acapulco, Mexico, as part of IJCAI'2003
IJWCC 2004, Madrid, Spain, as part of ECCBR'2004
IJWCC 2005, Edinburgh, UK, as part of IJCAI'2005
IJWCC 2006, Riva del Garda, Italy, as part of ECAI'2006
IJWCC 2007, London, UK, a stand-alone event
IJWCC 2008, Madrid, Spain, a stand-alone event
The 1st Conference on Computer Simulation of Musical Creativity will be held
CCSMC 2016, 17–19 June, University of Huddersfield, UK. Keynotes: Geraint Wiggins and Graeme Bailey.


== Publications and forums ==
Design Computing and Cognition is one conference that addresses computational creativity. The ACM Creativity and Cognition conference is another forum for issues related to computational creativity. Journées d'Informatique Musicale 2016 keynote by Shlomo Dubnov was on Information Theoretic Creativity.
A number of recent books provide either a good introduction or a good overview of the field of Computational Creativity. These include:
Pereira, F. C. (2007). ""Creativity and Artificial Intelligence: A Conceptual Blending Approach"". Applications of Cognitive Linguistics series, Mouton de Gruyter.
Veale, T. (2012). ""Exploding the Creativity Myth: The Computational Foundations of Linguistic Creativity"". Bloomsbury Academic, London.
McCormack, J. and d'Inverno, M. (eds.) (2012). ""Computers and Creativity"". Springer, Berlin.
Veale, T., Feyaerts, K. and Forceville, C. (2013, forthcoming). ""Creativity and the Agile Mind: A Multidisciplinary study of a Multifaceted phenomenon"". Mouton de Gruyter.
In addition to the proceedings of conferences and workshops, the computational creativity community has thus far produced these special journal issues dedicated to the topic:
New Generation Computing, volume 24, issue 3, 2006
Journal of Knowledge-Based Systems, volume 19, issue 7, November 2006
AI Magazine, volume 30, number 3, Fall 2009
Minds and Machines, volume 20, number 4, November 2010
Cognitive Computation, volume 4, issue 3, September 2012
AIEDAM, volume 27, number 4, Fall 2013
Computers in Entertainment, two special issues on Music Meta-Creation (MuMe), Fall 2016 (forthcoming)
In addition to these, a new journal has started which focuses on computational creativity within the field of music.
JCMS 2016, Journal of Creative Music Systems


== See also ==
Algorithmic art
Algorithmic composition
Applications of artificial intelligence
Computer art
Computer-generated music
Creative computing
Digital morphogenesis
Digital poetry
Generative systems
Musikalisches Würfelspiel (Musical dice game)
Procedural generation
Lists
List of emerging technologies
Outline of artificial intelligence


== References ==


== Further reading ==
An Overview of Artificial Creativity on Think Artificial
Cohen, H., ""the further exploits of AARON, Painter"", SEHR, volume 4, issue 2: Constructions of the Mind, 1995
Máquinas de computación, creatividad artificial y cine digital
Plotkin, R. ""The Genie in the Machine""


=== Documentaries ===
Noorderlicht: Margaret Boden and Stephen Thaler on Creative Computers
In Its Image"
4,Computational phylogenetics,3986130,58742,"Computational phylogenetics is the application of computational algorithms, methods, and programs to phylogenetic analyses. The goal is to assemble a phylogenetic tree representing a hypothesis about the evolutionary ancestry of a set of genes, species, or other taxa. For example, these techniques have been used to explore the family tree of hominid species and the relationships between specific genes shared by many types of organisms. Traditional phylogenetics relies on morphological data obtained by measuring and quantifying the phenotypic properties of representative organisms, while the more recent field of molecular phylogenetics uses nucleotide sequences encoding genes or amino acid sequences encoding proteins as the basis for classification. Many forms of molecular phylogenetics are closely related to and make extensive use of sequence alignment in constructing and refining phylogenetic trees, which are used to classify the evolutionary relationships between homologous genes represented in the genomes of divergent species. The phylogenetic trees constructed by computational methods are unlikely to perfectly reproduce the evolutionary tree that represents the historical relationships between the species being analyzed. The historical species tree may also differ from the historical tree of an individual homologous gene shared by those species.
Producing a phylogenetic tree requires a measure of homology among the characteristics shared by the taxa being compared. In morphological studies, this requires explicit decisions about which physical characteristics to measure and how to use them to encode distinct states corresponding to the input taxa. In molecular studies, a primary problem is in producing a multiple sequence alignment (MSA) between the genes or amino acid sequences of interest. Progressive sequence alignment methods produce a phylogenetic tree by necessity because they incorporate new sequences into the calculated alignment in order of genetic distance.


== Types of phylogenetic trees and networks ==
Phylogenetic trees generated by computational phylogenetics can be either rooted or unrooted depending on the input data and the algorithm used. A rooted tree is a directed graph that explicitly identifies a most recent common ancestor (MRCA), usually an imputed sequence that is not represented in the input. Genetic distance measures can be used to plot a tree with the input sequences as leaf nodes and their distances from the root proportional to their genetic distance from the hypothesized MRCA. Identification of a root usually requires the inclusion in the input data of at least one ""outgroup"" known to be only distantly related to the sequences of interest.
By contrast, unrooted trees plot the distances and relationships between input sequences without making assumptions regarding their descent. An unrooted tree can always be produced from a rooted tree, but a root cannot usually be placed on an unrooted tree without additional data on divergence rates, such as the assumption of the molecular clock hypothesis.
The set of all possible phylogenetic trees for a given group of input sequences can be conceptualized as a discretely defined multidimensional ""tree space"" through which search paths can be traced by optimization algorithms. Although counting the total number of trees for a nontrivial number of input sequences can be complicated by variations in the definition of a tree topology, it is always true that there are more rooted than unrooted trees for a given number of inputs and choice of parameters.
Both rooted and unrooted phylogenetic trees can be further generalized to rooted or unrooted phylogenetic networks, which allow for the modeling of evolutionary phenomena such as hybridization or horizontal gene transfer.


== Coding characters and defining homology ==


=== Morphological analysis ===
The basic problem in morphological phylogenetics is the assembly of a matrix representing a mapping from each of the taxa being compared to representative measurements for each of the phenotypic characteristics being used as a classifier. The types of phenotypic data used to construct this matrix depend on the taxa being compared; for individual species, they may involve measurements of average body size, lengths or sizes of particular bones or other physical features, or even behavioral manifestations. Of course, since not every possible phenotypic characteristic could be measured and encoded for analysis, the selection of which features to measure is a major inherent obstacle to the method. The decision of which traits to use as a basis for the matrix necessarily represents a hypothesis about which traits of a species or higher taxon are evolutionarily relevant. Morphological studies can be confounded by examples of convergent evolution of phenotypes. A major challenge in constructing useful classes is the high likelihood of inter-taxon overlap in the distribution of the phenotype's variation. The inclusion of extinct taxa in morphological analysis is often difficult due to absence of or incomplete fossil records, but has been shown to have a significant effect on the trees produced; in one study only the inclusion of extinct species of apes produced a morphologically derived tree that was consistent with that produced from molecular data.
Some phenotypic classifications, particularly those used when analyzing very diverse groups of taxa, are discrete and unambiguous; classifying organisms as possessing or lacking a tail, for example, is straightforward in the majority of cases, as is counting features such as eyes or vertebrae. However, the most appropriate representation of continuously varying phenotypic measurements is a controversial problem without a general solution. A common method is simply to sort the measurements of interest into two or more classes, rendering continuous observed variation as discretely classifiable (e.g., all examples with humerus bones longer than a given cutoff are scored as members of one state, and all members whose humerus bones are shorter than the cutoff are scored as members of a second state). This results in an easily manipulated data set but has been criticized for poor reporting of the basis for the class definitions and for sacrificing information compared to methods that use a continuous weighted distribution of measurements.
Because morphological data is extremely labor-intensive to collect, whether from literature sources or from field observations, reuse of previously compiled data matrices is not uncommon, although this may propagate flaws in the original matrix into multiple derivative analyses.


=== Molecular analysis ===
The problem of character coding is very different in molecular analyses, as the characters in biological sequence data are immediate and discretely defined - distinct nucleotides in DNA or RNA sequences and distinct amino acids in protein sequences. However, defining homology can be challenging due to the inherent difficulties of multiple sequence alignment. For a given gapped MSA, several rooted phylogenetic trees can be constructed that vary in their interpretations of which changes are ""mutations"" versus ancestral characters, and which events are insertion mutations or deletion mutations. For example, given only a pairwise alignment with a gap region, it is impossible to determine whether one sequence bears an insertion mutation or the other carries a deletion. The problem is magnified in MSAs with unaligned and nonoverlapping gaps. In practice, sizable regions of a calculated alignment may be discounted in phylogenetic tree construction to avoid integrating noisy data into the tree calculation.


== Distance-matrix methods ==

Distance-matrix methods of phylogenetic analysis explicitly rely on a measure of ""genetic distance"" between the sequences being classified, and therefore they require an MSA as an input. Distance is often defined as the fraction of mismatches at aligned positions, with gaps either ignored or counted as mismatches. Distance methods attempt to construct an all-to-all matrix from the sequence query set describing the distance between each sequence pair. From this is constructed a phylogenetic tree that places closely related sequences under the same interior node and whose branch lengths closely reproduce the observed distances between sequences. Distance-matrix methods may produce either rooted or unrooted trees, depending on the algorithm used to calculate them. They are frequently used as the basis for progressive and iterative types of multiple sequence alignments. The main disadvantage of distance-matrix methods is their inability to efficiently use information about local high-variation regions that appear across multiple subtrees.


=== UPGMA and WPGMA ===

The UPGMA (Unweighted Pair Group Method with Arithmetic mean) and WPGMA (Weighted Pair Group Method with Arithmetic mean) methods produce rooted trees and require a constant-rate assumption - that is, it assumes an ultrametric tree in which the distances from the root to every branch tip are equal.


=== Neighbor-joining ===

Neighbor-joining methods apply general cluster analysis techniques to sequence analysis using genetic distance as a clustering metric. The simple neighbor-joining method produces unrooted trees, but it does not assume a constant rate of evolution (i.e., a molecular clock) across lineages.


=== Fitch-Margoliash method ===
The Fitch-Margoliash method uses a weighted least squares method for clustering based on genetic distance. Closely related sequences are given more weight in the tree construction process to correct for the increased inaccuracy in measuring distances between distantly related sequences. The distances used as input to the algorithm must be normalized to prevent large artifacts in computing relationships between closely related and distantly related groups. The distances calculated by this method must be linear; the linearity criterion for distances requires that the expected values of the branch lengths for two individual branches must equal the expected value of the sum of the two branch distances - a property that applies to biological sequences only when they have been corrected for the possibility of back mutations at individual sites. This correction is done through the use of a substitution matrix such as that derived from the Jukes-Cantor model of DNA evolution. The distance correction is only necessary in practice when the evolution rates differ among branches. Another modification of the algorithm can be helpful, especially in case of concentrated distances (please report to concentration of measure phenomenon and curse of dimensionality): that modification, described in, has been shown to improve the efficiency of the algorithm and its robustness.
The least-squares criterion applied to these distances is more accurate but less efficient than the neighbor-joining methods. An additional improvement that corrects for correlations between distances that arise from many closely related sequences in the data set can also be applied at increased computational cost. Finding the optimal least-squares tree with any correction factor is NP-complete, so heuristic search methods like those used in maximum-parsimony analysis are applied to the search through tree space.


=== Using outgroups ===
Independent information about the relationship between sequences or groups can be used to help reduce the tree search space and root unrooted trees. Standard usage of distance-matrix methods involves the inclusion of at least one outgroup sequence known to be only distantly related to the sequences of interest in the query set. This usage can be seen as a type of experimental control. If the outgroup has been appropriately chosen, it will have a much greater genetic distance and thus a longer branch length than any other sequence, and it will appear near the root of a rooted tree. Choosing an appropriate outgroup requires the selection of a sequence that is moderately related to the sequences of interest; too close a relationship defeats the purpose of the outgroup and too distant adds noise to the analysis. Care should also be taken to avoid situations in which the species from which the sequences were taken are distantly related, but the gene encoded by the sequences is highly conserved across lineages. Horizontal gene transfer, especially between otherwise divergent bacteria, can also confound outgroup usage.


== Maximum parsimony ==
Maximum parsimony (MP) is a method of identifying the potential phylogenetic tree that requires the smallest total number of evolutionary events to explain the observed sequence data. Some ways of scoring trees also include a ""cost"" associated with particular types of evolutionary events and attempt to locate the tree with the smallest total cost. This is a useful approach in cases where not every possible type of event is equally likely - for example, when particular nucleotides or amino acids are known to be more mutable than others.
The most naive way of identifying the most parsimonious tree is simple enumeration - considering each possible tree in succession and searching for the tree with the smallest score. However, this is only possible for a relatively small number of sequences or species because the problem of identifying the most parsimonious tree is known to be NP-hard; consequently a number of heuristic search methods for optimization have been developed to locate a highly parsimonious tree, if not the best in the set. Most such methods involve a steepest descent-style minimization mechanism operating on a tree rearrangement criterion.


=== Branch and bound ===
The branch and bound algorithm is a general method used to increase the efficiency of searches for near-optimal solutions of NP-hard problems first applied to phylogenetics in the early 1980s. Branch and bound is particularly well suited to phylogenetic tree construction because it inherently requires dividing a problem into a tree structure as it subdivides the problem space into smaller regions. As its name implies, it requires as input both a branching rule (in the case of phylogenetics, the addition of the next species or sequence to the tree) and a bound (a rule that excludes certain regions of the search space from consideration, thereby assuming that the optimal solution cannot occupy that region). Identifying a good bound is the most challenging aspect of the algorithm's application to phylogenetics. A simple way of defining the bound is a maximum number of assumed evolutionary changes allowed per tree. A set of criteria known as Zharkikh's rules severely limit the search space by defining characteristics shared by all candidate ""most parsimonious"" trees. The two most basic rules require the elimination of all but one redundant sequence (for cases where multiple observations have produced identical data) and the elimination of character sites at which two or more states do not occur in at least two species. Under ideal conditions these rules and their associated algorithm would completely define a tree.


=== Sankoff-Morel-Cedergren algorithm ===
The Sankoff-Morel-Cedergren algorithm was among the first published methods to simultaneously produce an MSA and a phylogenetic tree for nucleotide sequences. The method uses a maximum parsimony calculation in conjunction with a scoring function that penalizes gaps and mismatches, thereby favoring the tree that introduces a minimal number of such events (an alternative view holds that the trees to be favored are those that maximize the amount of sequence similarity that can be interpreted as homology, a point of view that may lead to different optimal trees ). The imputed sequences at the interior nodes of the tree are scored and summed over all the nodes in each possible tree. The lowest-scoring tree sum provides both an optimal tree and an optimal MSA given the scoring function. Because the method is highly computationally intensive, an approximate method in which initial guesses for the interior alignments are refined one node at a time. Both the full and the approximate version are in practice calculated by dynamic programming.


=== MALIGN and POY ===
More recent phylogenetic tree/MSA methods use heuristics to isolate high-scoring, but not necessarily optimal, trees. The MALIGN method uses a maximum-parsimony technique to compute a multiple alignment by maximizing a cladogram score, and its companion POY uses an iterative method that couples the optimization of the phylogenetic tree with improvements in the corresponding MSA. However, the use of these methods in constructing evolutionary hypotheses has been criticized as biased due to the deliberate construction of trees reflecting minimal evolutionary events. This, in turn, has been countered by the view that such methods should be seen as heuristic approaches to find the trees that maximize the amount of sequence similarity that can be interpreted as homology.


== Maximum likelihood ==
The maximum likelihood method uses standard statistical techniques for inferring probability distributions to assign probabilities to particular possible phylogenetic trees. The method requires a substitution model to assess the probability of particular mutations; roughly, a tree that requires more mutations at interior nodes to explain the observed phylogeny will be assessed as having a lower probability. This is broadly similar to the maximum-parsimony method, but maximum likelihood allows additional statistical flexibility by permitting varying rates of evolution across both lineages and sites. In fact, the method requires that evolution at different sites and along different lineages must be statistically independent. Maximum likelihood is thus well suited to the analysis of distantly related sequences, but it is believed to be computationally intractable to compute due to its NP-hardness.
The ""pruning"" algorithm, a variant of dynamic programming, is often used to reduce the search space by efficiently calculating the likelihood of subtrees. The method calculates the likelihood for each site in a ""linear"" manner, starting at a node whose only descendants are leaves (that is, the tips of the tree) and working backwards toward the ""bottom"" node in nested sets. However, the trees produced by the method are only rooted if the substitution model is irreversible, which is not generally true of biological systems. The search for the maximum-likelihood tree also includes a branch length optimization component that is difficult to improve upon algorithmically; general global optimization tools such as the Newton-Raphson method are often used.


== Bayesian inference ==

Bayesian inference can be used to produce phylogenetic trees in a manner closely related to the maximum likelihood methods. Bayesian methods assume a prior probability distribution of the possible trees, which may simply be the probability of any one tree among all the possible trees that could be generated from the data, or may be a more sophisticated estimate derived from the assumption that divergence events such as speciation occur as stochastic processes. The choice of prior distribution is a point of contention among users of Bayesian-inference phylogenetics methods.
Implementations of Bayesian methods generally use Markov chain Monte Carlo sampling algorithms, although the choice of move set varies; selections used in Bayesian phylogenetics include circularly permuting leaf nodes of a proposed tree at each step and swapping descendant subtrees of a random internal node between two related trees. The use of Bayesian methods in phylogenetics has been controversial, largely due to incomplete specification of the choice of move set, acceptance criterion, and prior distribution in published work. Bayesian methods are generally held to be superior to parsimony-based methods; they can be more prone to long-branch attraction than maximum likelihood techniques, although they are better able to accommodate missing data.
Whereas likelihood methods find the tree that maximizes the probability of the data, a Bayesian approach recovers a tree that represents the most likely clades, by drawing on the posterior distribution. However, estimates of the posterior probability of clades (measuring their 'support') can be quite wide of the mark, especially in clades that aren't overwhelmingly likely. As such, other methods have been put forwards to estimate posterior probability.


== Model selection ==
Molecular phylogenetics methods rely on a defined substitution model that encodes a hypothesis about the relative rates of mutation at various sites along the gene or amino acid sequences being studied. At their simplest, substitution models aim to correct for differences in the rates of transitions and transversions in nucleotide sequences. The use of substitution models is necessitated by the fact that the genetic distance between two sequences increases linearly only for a short time after the two sequences diverge from each other (alternatively, the distance is linear only shortly before coalescence). The longer the amount of time after divergence, the more likely it becomes that two mutations occur at the same nucleotide site. Simple genetic distance calculations will thus undercount the number of mutation events that have occurred in evolutionary history. The extent of this undercount increases with increasing time since divergence, which can lead to the phenomenon of long branch attraction, or the misassignment of two distantly related but convergently evolving sequences as closely related. The maximum parsimony method is particularly susceptible to this problem due to its explicit search for a tree representing a minimum number of distinct evolutionary events.


=== Types of models ===

All substitution models assign a set of weights to each possible change of state represented in the sequence. The most common model types are implicitly reversible because they assign the same weight to, for example, a G>C nucleotide mutation as to a C>G mutation. The simplest possible model, the Jukes-Cantor model, assigns an equal probability to every possible change of state for a given nucleotide base. The rate of change between any two distinct nucleotides will be one-third of the overall substitution rate. More advanced models distinguish between transitions and transversions. The most general possible time-reversible model, called the GTR model, has six mutation rate parameters. An even more generalized model known as the general 12-parameter model breaks time-reversibility, at the cost of much additional complexity in calculating genetic distances that are consistent among multiple lineages. One possible variation on this theme adjusts the rates so that overall GC content - an important measure of DNA double helix stability - varies over time.
Models may also allow for the variation of rates with positions in the input sequence. The most obvious example of such variation follows from the arrangement of nucleotides in protein-coding genes into three-base codons. If the location of the open reading frame (ORF) is known, rates of mutation can be adjusted for position of a given site within a codon, since it is known that wobble base pairing can allow for higher mutation rates in the third nucleotide of a given codon without affecting the codon's meaning in the genetic code. A less hypothesis-driven example that does not rely on ORF identification simply assigns to each site a rate randomly drawn from a predetermined distribution, often the gamma distribution or log-normal distribution. Finally, a more conservative estimate of rate variations known as the covarion method allows autocorrelated variations in rates, so that the mutation rate of a given site is correlated across sites and lineages.


=== Choosing the best model ===
The selection of an appropriate model is critical for the production of good phylogenetic analyses, both because underparameterized or overly restrictive models may produce aberrant behavior when their underlying assumptions are violated, and because overly complex or overparameterized models are computationally expensive and the parameters may be overfit. The most common method of model selection is the likelihood ratio test (LRT), which produces a likelihood estimate that can be interpreted as a measure of ""goodness of fit"" between the model and the input data. However, care must be taken in using these results, since a more complex model with more parameters will always have a higher likelihood than a simplified version of the same model, which can lead to the naive selection of models that are overly complex. For this reason model selection computer programs will choose the simplest model that is not significantly worse than more complex substitution models. A significant disadvantage of the LRT is the necessity of making a series of pairwise comparisons between models; it has been shown that the order in which the models are compared has a major effect on the one that is eventually selected.
An alternative model selection method is the Akaike information criterion (AIC), formally an estimate of the Kullback–Leibler divergence between the true model and the model being tested. It can be interpreted as a likelihood estimate with a correction factor to penalize overparameterized models. The AIC is calculated on an individual model rather than a pair, so it is independent of the order in which models are assessed. A related alternative, the Bayesian information criterion (BIC), has a similar basic interpretation but penalizes complex models more heavily.
A comprehensive step-by-step protocol on constructing phylogenetic tree, including DNA/Amino Acid contiguous sequence assembly, multiple sequence alignment, model-test (testing best-fitting substitution models) and phylogeny reconstruction using Maximum Likelihood and Bayesian Inference, is available at Nature Protocol
A non traditional way of evaluating the phylogenetic tree is to compare it with clustering result. One can use a Multidimensional Scaling technique, so called Interpolative Joining to do dimensionality reduction to visualize the clustering result for the sequences in 3D, and then map the phylogenetic tree onto the clustering result. A better tree usually has a higher correlation with the clustering result.


== Evaluating tree support ==
As with all statistical analysis, the estimation of phylogenies from character data requires an evaluation of confidence. A number of methods exist to test the amount of support for a phylogenetic tree, either by evaluating the support for each sub-tree in the phylogeny (nodal support) or evaluating whether the phylogeny is significantly different from other possible trees (alternative tree hypothesis tests).


=== Nodal support ===
The most common method for assessing tree support is to evaluate the statistical support for each node on the tree. Typically, a node with very low support is not considered valid in further analysis, and visually may be collapsed into a polytomy to indicate that relationships within a clade are unresolved.


==== Consensus tree ====
Many methods for assessing nodal support involve consideration of multiple phylogenies. The consensus tree summarizes the nodes that are shared among a set of trees. In a *strict consensus,* only nodes found in every tree are shown, and the rest are collapsed into an unresolved polytomy. Less conservative methods, such as the *majority-rule consensus* tree, consider nodes that are supported by a given percentage of trees under consideration (such as at least 50%).
For example, in maximum parsimony analysis, there may be many trees with the same parsimony score. A strict consensus tree would show which nodes are found in all equally parsimonious trees, and which nodes differ. Consensus trees are also used to evaluate support on phylogenies reconstructed with Bayesian inference (see below).


==== Bootstrapping and jackknifing ====
In statistics, the bootstrap is a method for inferring the variability of data that has an unknown distribution using pseudoreplications of the original data. For example, given a set of 100 data points, a pseudoreplicate is a data set of the same size (100 points) randomly sampled from the original data, with replacement. That is, each original data point may be represented more than once in the pseudoreplicate, or not at all. Statistical support involves evaluation of whether the original data has similar properties to a large set of pseudoreplicates.
In phylogenetics, bootstrapping is conducted using the columns of the character matrix. Each pseudoreplicate contains the same number of species (rows) and characters (columns) randomly sampled from the original matrix, with replacement. A phylogeny is reconstructed from each pseudoreplicate, with the same methods used to reconstruct the phylogeny from the original data. For each node on the phylogeny, the nodal support is the percentage of pseudoreplicates containing that node.
The statistical rigor of the bootstrap test has been empirically evaluated using viral populations with known evolutionary histories, finding that 70% bootstrap support corresponds to a 95% probability that the clade exists. However, this was tested under ideal conditions (e.g. no change in evolutionary rates, symmetric phylogenies). In practice, values above 70% are generally supported and left to the researcher or reader to evaluate confidence. Nodes with support lower than 70% are typically considered unresolved.
Jackknifing in phylogenetics is a similar procedure, except the columns of the matrix are sampled without replacement. Pseudoreplicates are generated by randomly subsampling the data—for example, a ""10% jackknife"" would involve randomly sampling 10% of the matrix many times to evaluate nodal support.


==== Posterior probability ====
Reconstruction of phylogenies using Bayesian inference generates a posterior distribution of highly probable trees given the data and evolutionary model, rather than a single ""best"" tree. The trees in the posterior distribution generally have many different topologies. Most Bayesian inference methods utilize a Markov-chain Monte Carlo iteration, and the initial steps of this chain are not considered reliable reconstructions of the phylogeny. Trees generated early in the chain are usually discarded as burn-in. The most common method of evaluating nodal support in a Bayesian phylogenetic analysis is to calculate the percentage of trees in the posterior distribution (post-burn-in) which contain the node.
The statistical support for a node in Bayesian inference is expected to reflect the probability that a clade really exists given the data and evolutionary model. Therefore, the threshold for accepting a node as supported is generally higher than for bootstrapping.


==== Step counting methods ====
Bremer support counts the number of extra steps needed to contradict a clade.


=== Shortcomings ===
These measures each have their weaknesses. For example, smaller or larger clades tend to attract larger support values than mid-sized clades, simply as a result of the number of taxa in them.
Bootstrap support can provide high estimates of node support as a result of noise in the data rather than the true existence of a clade.


== Limitations and workarounds ==
Ultimately, there is no way to measure whether a particular phylogenetic hypothesis is accurate or not, unless the true relationships among the taxa being examined are already known (which may happen with bacteria or viruses under laboratory conditions). The best result an empirical phylogeneticist can hope to attain is a tree with branches that are well supported by the available evidence. Several potential pitfalls have been identified:


=== Homoplasy ===

Certain characters are more likely to evolve convergently than others; logically, such characters should be given less weight in the reconstruction of a tree. Weights in the form of a model of evolution can be inferred from sets of molecular data, so that maximum likelihood or Bayesian methods can be used to analyze them. For molecular sequences, this problem is exacerbated when the taxa under study have diverged substantially. As time since the divergence of two taxa increase, so does the probability of multiple substitutions on the same site, or back mutations, all of which result in homoplasies. For morphological data, unfortunately, the only objective way to determine convergence is by the construction of a tree – a somewhat circular method. Even so, weighting homoplasious characters does indeed lead to better-supported trees. Further refinement can be brought by weighting changes in one direction higher than changes in another; for instance, the presence of thoracic wings almost guarantees placement among the pterygote insects because, although wings are often lost secondarily, there is no evidence that they have been gained more than once.


=== Horizontal gene transfer ===
In general, organisms can inherit genes in two ways: vertical gene transfer and horizontal gene transfer. Vertical gene transfer is the passage of genes from parent to offspring, and horizontal (also called lateral) gene transfer occurs when genes jump between unrelated organisms, a common phenomenon especially in prokaryotes; a good example of this is the acquired antibiotic resistance as a result of gene exchange between various bacteria leading to multi-drug-resistant bacterial species. There have also been well-documented cases of horizontal gene transfer between eukaryotes.
Horizontal gene transfer has complicated the determination of phylogenies of organisms, and inconsistencies in phylogeny have been reported among specific groups of organisms depending on the genes used to construct evolutionary trees. The only way to determine which genes have been acquired vertically and which horizontally is to parsimoniously assume that the largest set of genes that have been inherited together have been inherited vertically; this requires analyzing a large number of genes.


=== Hybrids, speciation, introgressions and incomplete lineage sorting ===
The basic assumption underlying the mathematical model of cladistics is a situation where species split neatly in bifurcating fashion. While such an assumption may hold on a larger scale (bar horizontal gene transfer, see above), speciation is often much less orderly. Research since the cladistic method was introduced has shown that hybrid speciation, once thought rare, is in fact quite common, particularly in plants. Also paraphyletic speciation is common, making the assumption of a bifurcating pattern unsuitable, leading to phylogenetic networks rather than trees. Introgression can also move genes between otherwise distinct species and sometimes even genera, complicating phylogenetic analysis based on genes. This phenomenon can contribute to ""incomplete lineage sorting"" and is thought to be a common phenomenon across a number of groups. In species level analysis this can be dealt with by larger sampling or better whole genome analysis. Often the problem is avoided by restricting the analysis to fewer, not closely related specimens.


=== Taxon sampling ===
Owing to the development of advanced sequencing techniques in molecular biology, it has become feasible to gather large amounts of data (DNA or amino acid sequences) to infer phylogenetic hypotheses. For example, it is not rare to find studies with character matrices based on whole mitochondrial genomes (~16,000 nucleotides, in many animals). However, simulations have shown that it is more important to increase the number of taxa in the matrix than to increase the number of characters, because the more taxa there are, the more accurate and more robust is the resulting phylogenetic tree. This may be partly due to the breaking up of long branches.


=== Phylogenetic signal ===
Another important factor that affects the accuracy of tree reconstruction is whether the data analyzed actually contain a useful phylogenetic signal, a term that is used generally to denote whether a character evolves slowly enough to have the same state in closely related taxa as opposed to varying randomly. Tests for phylogenetic signal exist.


=== Continuous characters ===
Morphological characters that sample a continuum may contain phylogenetic signal, but are hard to code as discrete characters. Several methods have been used, one of which is gap coding, and there are variations on gap coding. In the original form of gap coding:

group means for a character are first ordered by size. The pooled within-group standard deviation is calculated ... and differences between adjacent means ... are compared relative to this standard deviation. Any pair of adjacent means is considered different and given different integer scores ... if the means are separated by a ""gap"" greater than the within-group standard deviation ... times some arbitrary constant.

If more taxa are added to the analysis, the gaps between taxa may become so small that all information is lost. Generalized gap coding works around that problem by comparing individual pairs of taxa rather than considering one set that contains all of the taxa.


=== Missing data ===
In general, the more data that are available when constructing a tree, the more accurate and reliable the resulting tree will be. Missing data are no more detrimental than simply having fewer data, although the impact is greatest when most of the missing data are in a small number of taxa. Concentrating the missing data across a small number of characters produces a more robust tree.


== The role of fossils ==
Because many characters involve embryological, or soft-tissue or molecular characters that (at best) hardly ever fossilize, and the interpretation of fossils is more ambiguous than that of living taxa, extinct taxa almost invariably have higher proportions of missing data than living ones. However, despite these limitations, the inclusion of fossils is invaluable, as they can provide information in sparse areas of trees, breaking up long branches and constraining intermediate character states; thus, fossil taxa contribute as much to tree resolution as modern taxa. Fossils can also constrain the age of lineages and thus demonstrate how consistent a tree is with the stratigraphic record; stratocladistics incorporates age information into data matrices for phylogenetic analyses.


== See also ==
List of phylogenetics software


== References ==


== Further reading ==
Charles Semple and Mike Steel (2003), Phylogenetics, Oxford University Press, ISBN 978-0-19-850942-4
Barry A. Cipra (2007), Algebraic Geometers See Ideal Approach to Biology, SIAM News, Volume 40, Number 6
Press, WH; Teukolsky, SA; Vetterling, WT; Flannery, BP (2007). ""Section 16.4. Hierarchical Clustering by Phylogenetic Trees"". Numerical Recipes: The Art of Scientific Computing (3rd ed.). New York: Cambridge University Press. ISBN 978-0-521-88068-8. 
Daniel H. Huson and Regula Rupp and Celine Scornavacca (2010) Phylogenetic Networks: Concepts, Algorithms and Applications, Cambridge University Press"
5,Computational immunology,3112875,56783,"In academia, computational immunology is a field of science that encompasses high-throughput genomic and bioinformatics approaches to immunology. The field's main aim is to convert immunological data into computational problems, solve these problems using mathematical and computational approaches and then convert these results into immunologically meaningful interpretations.


== Introduction ==
The immune system is a complex system of the human body and understanding it is one of the most challenging topics in biology. Immunology research is important for understanding the mechanisms underlying the defense of human body and to develop drugs for immunological diseases and maintain health. Recent findings in genomic and proteomic technologies have transformed the immunology research drastically. Sequencing of the human and other model organism genomes has produced increasingly large volumes of data relevant to immunology research and at the same time huge amounts of functional and clinical data are being reported in the scientific literature and stored in clinical records. Recent advances in bioinformatics or computational biology were helpful to understand and organize these large scale data and gave rise to new area that is called Computational immunology or immunoinformatics.
Computational immunology is a branch of bioinformatics and it is based on similar concepts and tools, such as sequence alignment and protein structure prediction tools. Immunomics is a discipline like genomics and proteomics. It is a science, which specifically combines Immunology with computer science, mathematics, chemistry, and biochemistry for large-scale analysis of immune system functions. It aims to study the complex protein–protein interactions and networks and allows a better understanding of immune responses and their role during normal, diseased and reconstitution states. Computational immunology is a part of immunomics, which is focused on analyzing large scale experimental data.


== History ==
Computational immunology began over 90 years ago with the theoretic modeling of malaria epidemiology. At that time, the emphasis was on the use of mathematics to guide the study of disease transmission. Since then, the field has expanded to cover all other aspects of immune system processes and diseases.


== Immunological database ==
After the recent advances in sequencing and proteomics technology, there have been many fold increase in generation of molecular and immunological data. The data are so diverse that they can be categorized in different databases according to their use in the research. Until now there are total 31 different immunological databases noted in the Nucleic Acids Research (NAR) Database Collection, which are given in the following table, together with some more immune related databases. The information given in the table is taken from the database descriptions in NAR Database Collection.
Online resources for allergy information are also available on http://www.allergen.org. Such data is valuable for investigation of cross-reactivity between known allergens and analysis of potential allergenicity in proteins. The Structural Database of Allergen Proteins (SDAP) stores information of allergenic proteins. The Food Allergy Research and Resource Program (FARRP) Protein Allergen-Online Database contains sequences of known and putative allergens derived from scientific literature and public databases. Allergome emphasizes the annotation of allergens that result in an IgE-mediated disease.


== Tools ==
A variety of computational, mathematical and statistical methods are available and reported. These tools are helpful for collection, analysis, and interpretation of immunological data. They include text mining, information management, sequence analysis, analysis of molecular interactions, and mathematical models that enable advanced simulations of immune system and immunological processes. Attempts are being made for the extraction of interesting and complex patterns from non-structured text documents in the immunological domain. Such as categorization of allergen cross-reactivity information, identification of cancer-associated gene variants and the classification of immune epitopes.
Immunoinformatics is using the basic bioinformatics tools such as ClustalW, BLAST, and TreeView, as well as specialized immunoinformatics tools, such as EpiMatrix, IMGT/V-QUEST for IG and TR sequence analysis, IMGT/ Collier-de-Perles and IMGT/StructuralQuery for IG variable domain structure analysis. Methods that rely on sequence comparison are diverse and have been applied to analyze HLA sequence conservation, help verify the origins of human immunodeficiency virus (HIV) sequences, and construct homology models for the analysis of hepatitis B virus polymerase resistance to lamivudine and emtricitabine.
There are also some computational models which focus on protein–protein interactions and networks. There are also tools which are used for T and B cell epitope mapping, proteasomal cleavage site prediction, and TAP– peptide prediction. The experimental data is very much important to design and justify the models to predict various molecular targets. Computational immunology tools is the game between experimental data and mathematically designed computational tools.


== Applications ==


=== Allergies ===
Allergies, while a critical subject of immunology, also vary considerably among individuals and sometimes even among genetically similar individuals. The assessment of protein allergenic potential focuses on three main aspects: (i) immunogenicity; (ii) cross-reactivity; and (iii) clinical symptoms. Immunogenicity is due to responses of an IgE antibody-producing B cell and/or of a T cell to a particular allergen. Therefore, immunogenicity studies focus mainly on identifying recognition sites of B-cells and T-cells for allergens. The three-dimensional structural properties of allergens control their allergenicity.
The use of immunoinformatics tools can be useful to predict protein allergenicity and will become increasingly important in the screening of novel foods before their wide-scale release for human use. Thus, there are major efforts under way to make reliable broad based allergy databases and combine these with well validated prediction tools in order to enable the identification of potential allergens in genetically modified drugs and foods. Though the developments are on primary stage, the World Health organization and Food and Agriculture Organization have proposed guidelines for evaluating allergenicity of genetically modified foods. According to the Codex alimentarius, a protein is potentially allergenic if it possesses an identity of ≥6 contiguous amino acids or ≥35% sequence similarity over an 80 amino acid window with a known allergen. Though there are rules, their inherent limitations have started to become apparent and exceptions to the rules have been well reported 


=== Infectious diseases and host responses ===
In the study of infectious diseases and host responses, the mathematical and computer models are a great help. These models were very useful in characterizing the behavior and spread of infectious disease, by understanding the dynamics of the pathogen in the host and the mechanisms of host factors which aid pathogen persistence. Examples include Plasmodium falciparum and nematode infection in ruminants.
Much has been done in understanding immune responses to various pathogens by integrating genomics and proteomics with bioinformatics strategies. Many exciting developments in large-scale screening of pathogens are currently taking place. National Institute of Allergy and Infectious Diseases (NIAID) has initiated an endeavor for systematic mapping of B and T cell epitopes of category A-C pathogens. These pathogens include Bacillus anthracis (anthrax), Clostridium botulinum toxin (botulism), Variola major (smallpox), Francisella tularensis (tularemia), viral hemorrhagic fevers, Burkholderia pseudomallei, Staphylococcus enterotoxin B, yellow fever, influenza, rabies, Chikungunya virus etc. Rule-based systems have been reported for the automated extraction and curation of influenza A records.
This development would lead to the development of an algorithm which would help to identify the conserved regions of pathogen sequences and in turn would be useful for vaccine development. This would be helpful in limiting the spread of infectious disease. Examples include a method for identification of vaccine targets from protein regions of conserved HLA binding and computational assessment of cross-reactivity of broadly neutralizing antibodies against viral pathogens. These examples illustrate the power of immunoinformatics applications to help solve complex problems in public health. Immunoinformatics could accelerate the discovery process dramatically and potentially shorten the time required for vaccine development. Immunoinformatics tools have been used to design the vaccine against Dengue virus  and Leishmania 


=== Immune system function ===
Using this technology it is possible to know the model behind immune system. It has been used to model T-cell-mediated suppression, peripheral lymphocyte migration, T-cell memory, tolerance, thymic function, and antibody networks. Models are helpful to predicts dynamics of pathogen toxicity and T-cell memory in response to different stimuli. There are also several models which are helpful in understanding the nature of specificity in immune network and immunogenicity.
For example, it was useful to examine the functional relationship between TAP peptide transport and HLA class I antigen presentation. TAP is a transmembrane protein responsible for the transport of antigenic peptides into the endoplasmic reticulum, where MHC them class I molecules can bind them and presented to T cells. As TAP does not bind all peptides equally, TAP-binding affinity could influence the ability of a particular peptide to gain access to the MHC class I pathway. Artificial neural network (ANN), a computer model was used to study peptide binding to human TAP and its relationship with MHC class I binding. The affinity of HLA-binding peptides for TAP was found to differ according to the HLA supertype concerned using this method. This research could have important implications for the design of peptide based immuno-therapeutic drugs and vaccines. It shows the power of the modeling approach to understand complex immune interactions.
There exist also methods which integrate peptide prediction tools with computer simulations that can provide detailed information on the immune response dynamics specific to the given pathogen's peptides .


=== Cancer Informatics ===
Cancer is the result of somatic mutations which provide cancer cells with a selective growth advantage. Recently it has been very important to determine the novel mutations. Genomics and proteomics techniques are used worldwide to identify mutations related to each specific cancer and their treatments. Computational tools are used to predict growth and surface antigens on cancerous cells. There are publications explaining a targeted approach for assessing mutations and cancer risk. Algorithm CanPredict was used to indicate how closely a specific gene resembles known cancer-causing genes. Cancer immunology has been given so much importance that the data related to it is growing rapidly. Protein–protein interaction networks provide valuable information on tumorigenesis in humans. Cancer proteins exhibit a network topology that is different from normal proteins in the human interactome. Immunoinformatics have been useful in increasing success of tumour vaccination. Recently, pioneering works have been conducted to analyse the host immune system dynamics in response to artificial immunity induced by vaccination strategies.. Other simulation tools use predicted cancer peptides to forecast immune specific anticancer responses that is dependent on the specified HLA. These resources are likely to grow significantly in the near future and immunoinformatics will be a major growth area in this domain.


== See also ==
Computational biology
Immunology
Genetics
Cancer
Immunity


== References ==


== External links ==
Boston University Center for Computational Immunology
York Computational Immunology Lab
Immunoinformatics Immunological Software and Web Services from Gajendra Pal Singh Raghava group"
6,List of important publications in computer science,454351,53953,"This is a list of important publications in computer science, organized by field.
Some reasons why a particular publication might be regarded as important:
Topic creator – A publication that created a new topic
Breakthrough – A publication that changed scientific knowledge significantly
Influence – A publication which has significantly influenced the world or has had a massive impact on the teaching of computer science.


== Artificial intelligence ==


=== Computing Machinery and Intelligence ===
Alan Turing
Mind, 59:433–460, 1950.
Online copy
Description: This paper discusses whether machines can think and suggested the Turing test as a method for checking it.


=== A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence ===
John McCarthy
Marvin Minsky
N. Rochester
C.E. Shannon
Online copy
Description: This summer research proposal inaugurated and defined the field. It contains the first use of the term artificial intelligence and this succinct description of the philosophical foundation of the field: ""every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it."" (See philosophy of AI) The proposal invited researchers to the Dartmouth conference, which is widely considered the ""birth of AI"". (See history of AI.)


=== Fuzzy sets ===
Lotfi Zadeh
Information and Control, Vol. 8, pp. 338–353. (1965).
Description: The seminal paper published in 1965 provides details on the mathematics of fuzzy set theory.


=== Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference ===
Judea Pearl
ISBN 1-55860-479-0 Publisher: Morgan Kaufmann Pub, 1988
Description: This book introduced Bayesian methods to AI.


=== Artificial Intelligence: A Modern Approach ===
Stuart J. Russell and Peter Norvig
Prentice Hall, Englewood Cliffs, New Jersey, 1995, ISBN 0-13-080302-2
Textbook's website
Description: The standard textbook in Artificial Intelligence. The book web site lists over 1100 colleges.


=== Machine learning ===


==== An Inductive Inference Machine ====
Ray Solomonoff
IRE Convention Record, Section on Information Theory, Part 2, pp. 56–62, 1957
(A longer version of this, a privately circulated report, 1956, is online).
Description: The first paper written on machine learning. Emphasized the importance of training sequences, and the use of parts of previous solutions to problems in constructing trial solutions to new problems.


==== Language identification in the limit ====
E. Mark Gold
Information and Control, 10(5):447–474, 1967
Online version: (HTML) (PDF)
Description: This paper created Algorithmic learning theory.


==== On the uniform convergence of relative frequencies of events to their probabilities ====
V. Vapnik, A. Chervonenkis
Theory of Probability and its Applications, 16(2):264—280, 1971
Description: Computational learning theory, VC theory, statistical uniform convergence and the VC dimension.


==== A theory of the learnable ====
Leslie Valiant
Communications of the ACM, 27(11):1134–1142 (1984)
Description: The Probably approximately correct learning (PAC learning) framework.


==== Learning representations by back-propagating errors ====
David E. Rumelhart, Geoffrey E. Hinton and Ronald J. Williams
Nature, 323, 533—536, 1986
Description: Development of Backpropagation algorithm for artificial neural networks. Note that the algorithm was first described by Paul Werbos in 1974.


==== Induction of Decision Trees ====
J.R. Quinlan
Machine Learning, 1. 81—106, 1986.
Description: Decision Trees are a common learning algorithm and a decision representation tool. Development of decision trees was done by many researchers in many areas, even before this paper. Though this paper is one of the most influential in the field.


==== Learning Quickly When Irrelevant Attributes Abound: A New Linear-threshold Algorithm ====
Nick Littlestone
Machine Learning 2: 285–318, 1988
Online version(PDF)
Description: One of the papers that started the field of on-line learning. In this learning setting, a learner receives a sequence of examples, making predictions after each one, and receiving feedback after each prediction. Research in this area is remarkable because (1) the algorithms and proofs tend to be very simple and beautiful, and (2) the model makes no statistical assumptions about the data. In other words, the data need not be random (as in nearly all other learning models), but can be chosen arbitrarily by ""nature"" or even an adversary. Specifically, this paper introduced the winnow algorithm.


==== Learning to predict by the method of Temporal difference ====
Richard S. Sutton
Machine Learning 3(1): 9–44
Online copy
Description: The Temporal difference method for reinforcement learning.


==== Learnability and the Vapnik–Chervonenkis dimension ====
A. Blumer
A. Ehrenfeucht
D. Haussler
M. K. Warmuth
Journal of the ACM, 36(4):929–965, 1989.
Description: The complete characterization of PAC learnability using the VC dimension.


==== Cryptographic limitations on learning boolean formulae and finite automata ====
M. Kearns
L. G. Valiant
In Proceedings of the 21st Annual ACM Symposium on Theory of Computing, pages 433–444, New York. ACM.
Online version(HTML)
Description: Proving negative results for PAC learning.


==== The strength of weak learnability ====
Robert E. Schapire
Machine Learning, 5(2):197–227, 1990.
Online version(HTML)
Description: Proving that weak and strong learnability are equivalent in the noise free PAC framework. The proof was done by introducing the boosting method.


==== A training algorithm for optimum margin classifiers ====
Bernhard E. Boser
Isabelle M. Guyon
Vladimir N. Vapnik
Proceedings of the Fifth Annual Workshop on Computational Learning Theory 5 144–152, Pittsburgh (1992).
Online version(HTML)
Description: This paper presented support vector machines, a practical and popular machine learning algorithm. Support vector machines often use the kernel trick.


==== A fast learning algorithm for deep belief nets ====
Geoffrey E. Hinton
Simon Osindero
Yee-Whye Teh
Neural Computation (2006)
Online PDF
Description: This paper presented a tractable greedy layer-wise learning algorithm for deep belief networks which led to great advancement in the field of deep learning.


==== Knowledge-based analysis of microarray gene expression data by using support vector machines ====
MP Brown
WN Grundy
D Lin
Nello Cristianini
CW Sugnet
TS Furey
M Ares Jr,
David Haussler
PNAS, 2000 January 4;97(1):262–7 <http://www.pnas.org/cgi/content/abstract/97/1/262>
Description: The first application of supervised learning to gene expression data, in particular Support Vector Machines. The method is now standard, and the paper one of the most cited in the area.


== Collaborative networks ==
Camarinha-Matos, L. M.; Afsarmanesh, H. (2005). ""Collaborative networks: A new scientific discipline, J"". Intelligent Manufacturing. 16 (4–5): 439–452. doi:10.1007/s10845-005-1656-3. 
Camarinha-Matos, L. M.; Afsarmanesh, H. (2008). Collaborative Networks: Reference Modeling, Springer: New York.


== Compilers ==


=== On the translation of languages from left to right ===
Knuth, D. E. (July 1965). ""On the translation of languages from left to right"" (PDF). Information and Control. 8 (6): 607–639. doi:10.1016/S0019-9958(65)90426-2. Retrieved 29 May 2011. 
Description: LR parser, which does bottom up parsing for deterministic context-free languages. Later derived parsers, such as the LALR parser, have been and continue to be standard practice, such as in Yacc and descendents.


=== Semantics of Context-Free Languages. ===
Donald Knuth
Math. Systems Theory 2:2 (1968), 127–145.
Description: About grammar attribution, the base for yacc's s-attributed and zyacc's LR-attributed approach.


=== A program data flow analysis procedure ===
Frances E. Allen, J. Cocke
Commun. ACM, 19, 137—147.
Description: From the abstract: ""The global data relationships in a program can be exposed and codified by the static analysis methods described in this paper. A procedure is given which determines all the definitions which can possibly reach each node of the control flow graph of the program and all the definitions that are live on each edge of the graph.""


=== A Unified Approach to Global Program Optimization ===
Gary Kildall
Proceedings of ACM SIGACT-SIGPLAN 1973 Symposium on Principles of Programming Languages.
pdf
Description: Formalized the concept of data-flow analysis as fixpoint computation over lattices, and showed that most static analyses used for program optimization can be uniformly expressed within this framework.


=== YACC: Yet another compiler-compiler ===
Stephen C. Johnson
Unix Programmer's Manual Vol 2b, 1979
Online copy (HTML)
Description: Yacc is a tool that made compiler writing much easier.


=== gprof: A Call Graph Execution Profiler ===
Susan L. Graham, Peter B. Kessler, Marshall Kirk McKusick
Proceedings of the ACM SIGPLAN 1982 Symposium on Compiler Construction, SIGPLAN Notices 17, 6, Boston, MA. June 1982.
Online copy; pdf
Description: The gprof profiler


=== Compilers: Principles, Techniques and Tools ===
Alfred V. Aho
Ravi Sethi
Jeffrey D. Ullman
Monica Lam
Addison-Wesley, 1986. ISBN 0-201-10088-6
Description: This book became a classic in compiler writing. It is also known as the Dragon book, after the (red) dragon that appears on its cover.


== Computer architecture ==


=== Colossus computer ===
T. H. Flowers
Annals of the History of Computing, Vol. 5 (No. 3), 1983, pp. 239–252.
The Design of Colossus
Description: The Colossus machines were early computing devices used by British codebreakers to break German messages encrypted with the Lorenz Cipher during World War II. Colossus was an early binary electronic digital computer. The design of Colossus was later described in the referenced paper.


=== First Draft of a Report on the EDVAC ===
John von Neumann
June 30, 1945, the ENIAC project.
First Draft of a report on the EDVAC (PDF)
Description: It contains the first published description of the logical design of a computer using the stored-program concept, which has come to be known as the von Neumann architecture.


=== Architecture of the IBM System/360 ===
Gene Amdahl, Fred Brooks, G. A. Blaauw
IBM Journal of Research and Development, 1964.
Architecture of the IBM System/360
Description: The IBM System/360 (S/360) is a mainframe computer system family announced by IBM on April 7, 1964. It was the first family of computers making a clear distinction between architecture and implementation.


=== The case for the reduced instruction set computer ===
DA Patterson, DR Ditzel
Computer ArchitectureNews, vol. 8, no. 6, October 1980, pp 25–33.
Online version(PDF)
Description: The reduced instruction set computer( RISC) CPU design philosophy. The RISC is a CPU design philosophy that favors a reduced set of simpler instructions.


=== Comments on ""the Case for the Reduced Instruction Set Computer"" ===
DW Clark, WD Strecker
Computer Architecture News, 1980.
Online version(PDF)
Description:


=== The CRAY-1 Computer System ===
DW Clark, WD Strecker
Communications of the ACM, January 1978, volume 21, number 1, pages 63–72.
Online version(PDF)
Description: The Cray-1 was a supercomputer designed by a team including Seymour Cray for Cray Research. The first Cray-1 system was installed at Los Alamos National Laboratory in 1976, and it went on to become one of the best known and most successful supercomputers in history.


=== Validity of the Single Processor Approach to Achieving Large Scale Computing Capabilities ===
Gene Amdahl
AFIPS 1967 Spring Joint Computer Conference, Atlantic City, N.J.
Online version(PDF)
Description: The Amdahl's Law.


=== A Case for Redundant Arrays of Inexpensive Disks (RAID) ===
David A. Patterson, Garth Gibson, Randy H. Katz
In International Conference on Management of Data, pages 109—116, 1988.
Online version(PDF)
Description: This paper discusses the concept of RAID disks, outlines the different levels of RAID, and the benefits of each level. It is a good paper for discussing issues of reliability and fault tolerance of computer systems, and the cost of providing such fault-tolerance.


=== The case for a single-chip multiprocessor ===
Kunle Olukotun, Basem Nayfeh, Lance Hammond, Ken Wilson, Kunyung Chang
In SIGOPS Oper. Syst. Rev. 30, pages 2–11, 1996.
Online version(PDF)
Description: This paper argues that the approach taken to improving the performance of processors by adding multiple instruction issue and out-of-order execution cannot continue to provide speedups indefinitely. It lays out the case for making single chip processors that contain multiple ""cores"". With the mainstream introduction of multicore processors by Intel in 2005, and their subsequent domination of the market, this paper was shown to be prescient.


== Computer graphics ==


=== The Rendering Equation ===
J. Kajiya
SIGGRAPH: ACM Special Interest Group on Computer Graphics and Interactive Techniques pages 143—150


=== Elastically deformable models ===
Demetri Terzopoulos, John Platt, Alan Barr, Kurt Fleischer
Computer Graphics, 21(4), 1987, 205–214, Proc. ACM SIGGRAPH'87 Conference, Anaheim, CA, July 1987.
Online version(PDF)
Description: The Academy of Motion Picture Arts and Sciences cited this paper as a ""milestone in computer graphics"".


== Computer vision ==


=== The Phase Correlation Image Alignment Method ===
C.D. Kuglin and D.C. Hines
IEEE 1975 Conference on Cybernetics and Society, 1975, New York, pp. 163–165, September
Description: A correlation method based upon the inverse Fourier transform


=== Determining Optical Flow ===
B.K.P. Horn and B.G. Schunck
Artificial Intelligence, Volume 17, 185–203, 1981
OA article here: doi:10.1016/0004-3702(81)90024-2
Description: A method for estimating the image motion of world points between 2 frames of a video sequence.


=== An Iterative Image Registration Technique with an Application to Stereo Vision ===
Lucas, B.D. and Kanade, T.
Proceedings of the 7th International Joint Conference on Artificial Intelligence, 674–679, Vancouver, Canada, 1981
Online version
Description: This paper provides efficient technique for image registration


=== The Laplacian Pyramid as a compact image code ===
Peter J. Burt and Edward H. Adelson
IEEE Transactions on Communications, volume = ""COM-31,4"", pp. 532–540, 1983.
Online version
Description: A technique for image encoding using local operators of many scales.


=== Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images ===
Stuart Geman and Donald Geman
IEEE Transactions on Pattern Analysis and Machine Intelligence, 1984
Description: introduced 1) MRFs for image analysis 2) the Gibbs sampling which revolutionized computational Bayesian statistics and thus had paramount impact in many other fields in addition to Computer Vision.


=== Snakes: Active contour models ===
Michael Kass, Andrew Witkin, and Demetri Terzopoulos
Description: An interactive variational technique for image segmentation and visual tracking.


=== Condensation – conditional density propagation for visual tracking ===
M. Isard and A. Blake
International Journal of Computer Vision, 29(1):5–28, 1998.
Online version
Description: A technique for visual tracking


=== Object recognition from local scale-invariant features ===
David Lowe
International Conference on Computer Vision, pp. 1150–1157, 1999
[1]
Description: A technique (scale-invariant feature transform) for robust feature description


== Concurrent, parallel, and distributed computing ==

Topics covered: concurrent computing, parallel computing, and distributed computing.


== Databases ==


=== A relational model for large shared data banks ===
E. F. Codd
Communications of the ACM, 13(6):377–387, June 1970
Description: This paper introduced the relational model for databases. This model became the number one model.


=== Binary B-Trees for Virtual Memory ===
Rudolf Bayer
ACM-SIGFIDET Workshop 1971, San Diego, California, Session 5B, p. 219–235.
Description: This paper introduced the B-Trees data structure. This model became the number one model.


=== Relational Completeness of Data Base Sublanguages ===
E. F. Codd
In: R. Rustin (ed.): Database Systems: 65-98, Prentice Hall and IBM Research Report RJ 987, San Jose, California : (1972)
Online version (PDF)
Description: Completeness of Data Base Sublanguages


=== The Entity Relationship Model – Towards a Unified View of Data ===
Peter Chen
ACM Transactions on Database Systems, Vol. 1, No. 1, March 1976, pp. 9–36
Description: This paper introduced the entity-relationship diagram(ERD) method of database design.


=== SEQUEL: A structured English query language ===
Donald D. Chamberlin, Raymond F. Boyce
International Conference on Management of Data, Proceedings of the 1974 ACM SIGFIDET (now SIGMOD) workshop on Data description, access and control, Ann Arbor, Michigan, pp. 249–264
Description: This paper introduced the SQL language.


=== The notions of consistency and predicate locks in a database system ===
K.P. Eswaran, Jim Gray, R.A. Lorie, I.L. Traiger
Communications of the ACM 19, 1976, 624—633
Description: This paper defined the concepts of transaction, consistency and schedule. It also argued that a transaction needs to lock a logical rather than a physical subset of the database.


=== Federated database systems for managing distributed, heterogeneous, and autonomous databases ===
Amit Sheth, J.A. Larson,""
ACM Computing Surveys - Special issue on heterogeneous databases Surveys, Volume 22 Issue 3, Pages 183 - 236, Sept. 1990
ACM source
Description: Introduced federated database systems concept leading huge impact on data interoperability and integration of hetereogenous data sources.


=== Mining association rules between sets of items in large databases ===
Rakesh Agrawal, Tomasz Imielinski, Arun Swami
Proc. of the ACM SIGMOD Conference on Management of Data, pages 207–216, Washington, D.C., May 1993
Online copy (HTML)
Description: Association rules, a very common method for data mining.


== History of computation ==


=== The Computer from Pascal to von Neumann ===
Goldstine, Herman H. (1972). The Computer from Pascal to von Neumann. Princeton University Press. ISBN 0-691-08104-2. 
Description: Perhaps the first book on the history of computation.


=== A History of Computing in the Twentieth Century ===
edited by:
Nicholas Metropolis
J. Howlett
Gian-Carlo Rota
Academic Press, 1980, ISBN 0-12-491650-3
Description: Several chapters by pioneers of computing.


== Information retrieval ==


=== A Vector Space Model for Automatic Indexing ===
Gerard Salton, A. Wong, C. S. Yang
Commun. ACM 18(11): 613–620 (1975)
Description: Presented the vector space model.


=== Extended Boolean Information Retrieval ===
Gerard Salton, Edward A. Fox, Harry Wu
Commun. ACM 26(11): 1022–1036 (1983)
Description: Presented the inverted index


=== A Statistical Interpretation of Term Specificity and Its Application in Retrieval ===
Karen Spärck Jones
Journal of Documentation 28: 11–21 (1972). doi:10.1108/eb026526.
Description: Conceived a statistical interpretation of term specificity called Inverse document frequency (IDF), which became a cornerstone of term weighting.


== Networking ==


=== Data Communications and Networking ===
Behrouz A. Forouzan. ISBN 0073376221, Copyright year: 2013, Publisher:: McGraw hill education.
Description: This book presents a comprehensive and accessible approach to data communications and networking that has made this book a favorite with students and professionals alike. More than 830 figures and 150 tables accompany the text and provide a visual and intuitive opportunity for understanding the material.


== Operating systems ==


=== An experimental timesharing system. ===
Fernando J. Corbató, M. Merwin-Daggett, and R.C. Daley
Proceedings of the AFIPS FJCC, pages 335–344, 1962.
Online copy (HTML)
Description: This paper discuss time-sharing as a method of sharing computer resource. This idea changed the interaction with computer systems.


=== The Working Set Model for Program Behavior ===
Peter J. Denning
Communications of the ACM, Vol. 11, No. 5, May 1968, pp 323–333
Online version(PDF)
Description: The beginning of cache. For more information see SIGOPS Hall of Fame.


=== Virtual Memory, Processes, and Sharing in MULTICS ===
Robert C. Daley, Jack B. Dennis
Communications of the ACM, Vol. 11, No. 5, May 1968, pp. 306–312.
Online version(PDF)
Description: The classic paper on Multics, the most ambitious operating system in the early history of computing. Difficult reading, but it describes the implications of trying to build a system that takes information sharing to its logical extreme. Most operating systems since Multics have incorporated a subset of its facilities.


=== The nucleus of a multiprogramming system ===
Per Brinch Hansen
Communications of the ACM, Vol. 13, No. 4, April 1970, pp. 238–242
Online version(PDF)
Description: Classic paper on the extensible nucleus architecture of the RC 4000 multiprogramming system, and what became known as the operating system kernel and microkernel architecture.


=== Operating System Principles ===
Per Brinch Hansen
Prentice Hall, Englewood Cliffs, NJ, July 1973
Online version (ACM Digital Library)
Description: The first comprehensive textbook on operating systems. Includes the first monitor notation (Chapter 7).


=== A note on the confinement problem ===
Butler W. Lampson
Communications of the ACM, 16(10):613–615, October 1973.
Online version(PDF)
Description: This paper addresses issues in constraining the flow of information from untrusted programs. It discusses covert channels, but more importantly it addresses the difficulty in obtaining full confinement without making the program itself effectively unusable. The ideas are important when trying to understand containment of malicious code, as well as aspects of trusted computing.


=== The UNIX Time-Sharing System ===
Dennis M. Ritchie and Ken Thompson
Communications of the ACM 17(7), July 1974.
Online copy
Description: The Unix operating system and its principles were described in this paper. The main importance is not of the paper but of the operating system, which had tremendous effect on operating system and computer technology.


=== Weighted voting for replicated data ===
David K. Gifford
Proceedings of the 7th ACM Symposium on Operating Systems Principles, pages 150–159, December 1979. Pacific Grove, California
Online copy (few formats)
Description: This paper describes the consistency mechanism known as quorum consensus. It is a good example of algorithms that provide a continuous set of options between two alternatives (in this case, between the read-one write-all, and the write-one read-all consistency methods). There have been many variations and improvements by researchers in the years that followed, and it is one of the consistency algorithms that should be understood by all. The options available by choosing different size quorums provide a useful structure for discussing of the core requirements for consistency in distributed systems.


=== Experiences with Processes and Monitors in Mesa ===
Butler W. Lampson, David D. Redell
Communications of the ACM, Vol. 23, No. 2, February 1980, pp. 105–117.
Online copy (PDF)
Description: This is the classic paper on synchronization techniques, including both alternate approaches and pitfalls.


=== Scheduling Techniques for Concurrent Systems ===
J. K. Ousterhout
Proceedings of Third International Conference on Distributed Computing Systems, 1982, 22—30.
Description: Algorithms for coscheduling of related processes were given


=== A Fast File System for UNIX ===
Marshall Kirk Mckusick, William N. Joy, Samuel J. Leffler, Robert S. Fabry
IACM Transactions on Computer Systems, Vol. 2, No. 3, August 1984, pp. 181–197.
Online copy (PDF)
Description: The file system of UNIX. One of the first papers discussing how to manage disk storage for high-performance file systems. Most file-system research since this paper has been influenced by it, and most high-performance file systems of the last 20 years incorporate techniques from this paper.


=== The Design of the UNIX Operating System ===
Maurice J. Bach, AT&T Bell Labs
Prentice Hall • 486 pp • Published 05/27/1986
This definitive description principally covered the System V Release 2 kernel, with some new features from Release 3 and BSD.


=== The Design and Implementation of a Log-Structured File System ===
Mendel Rosenblum, J. K. Ousterhout
ACM Transactions on Computer Systems, Vol. 10, No. 1 (February 1992), pp. 26–52.
Online version
Description: Log-structured file system.


=== Microkernel operating system architecture and Mach ===
David L. Black, David B. Golub, Daniel P. Julin, Richard F. Rashid, Richard P. Draves, Randall W. Dean, Alessandro Forin, Joseph Barrera, Hideyuki Tokuda, Gerald Malan, David Bohman
Proceedings of the USENIX Workshop on Microkernels and Other Kernel Architectures, pages 11–30, April 1992.
Description: This is a good paper discussing one particular microkernel architecture and contrasting it with monolithic kernel design. Mach underlies Mac OS X, and its layered architecture had a significant impact on the design of the Windows NT kernel and modern microkernels like L4. In addition, its memory-mapped files feature was added to many monolithic kernels.


=== An Implementation of a Log-Structured File System for UNIX ===
Margo Seltzer, Keith Bostic, Marshall Kirk McKusick, Carl Staelin
Proceedings of the Winter 1993 USENIX Conference, San Diego, CA, January 1993, 307-326
Online version
Description: The paper was the first production-quality implementation of that idea which spawned much additional discussion of the viability and short-comings of log-structured filesystems. While ""The Design and Implementation of a Log-Structured File System"" was certainly the first, this one was important in bringing the research idea to a usable system.


=== Soft Updates: A Solution to the Metadata Update problem in File Systems ===
G. Ganger, M. McKusick, C. Soules, Y. Patt
ACM Transactions on Computer Systems 18, 2. pp 127–153, May 2000
Online version
Description: A new way of maintaining filesystem consistency.


== Programming languages ==


=== The FORTRAN Automatic Coding System ===
John Backus et al.
Proceedings of the WJCC (Western Joint Computer Conference), Los Angeles, California, February 1957.
Online version(PDF)
Description: This paper describes the design and implementation of the first FORTRAN compiler by the IBM team. Fortran is a general-purpose, procedural, imperative programming language that is especially suited to numeric computation and scientific computing.


=== Recursive functions of symbolic expressions and their computation by machine, part I ===
John McCarthy.
Communications of the ACM, 3(4):184–195, April 1960.
Several online versions
Description: This paper introduced LISP, the first functional programming language, which was used heavily in many areas of computer science, especially in AI. LISP also has powerful features for manipulating LISP programs within the language.


=== ALGOL 60 ===
Revised Report on the Algorithmic Language Algol 60 by Peter Naur, et al. – The very influential ALGOL definition; with the first formally defined syntax.
Brian Randell and L. J. Russell, ALGOL 60 Implementation: The Translation and Use of ALGOL 60 Programs on a Computer. Academic Press, 1964. The design of the Whetstone Compiler. One of the early published descriptions of implementing a compiler. See the related papers: Whetstone Algol Revisited, and The Whetstone KDF9 Algol Translator by Brian Randell
Edsger W. Dijkstra, Algol 60 translation: an Algol 60 translator for the x1 and making a translator for Algol 60, report MR 35/61. Mathematisch Centrum, Amsterdam, 1961.
Description: Algol 60 introduced block structure.


=== The next 700 programming languages ===
Peter Landin
Communications of the ACM 9(3):157–65, March 1966
Description: This seminal paper proposed an ideal language ISWIM, which without being ever implemented influenced the whole later development.


=== Fundamental Concepts in Programming Languages ===
Christopher Strachey
pdf
Description: Fundamental Concepts in Programming Languages introduced much programming language terminology still in use today, including R-values, L-values, parametric polymorphism, and ad hoc polymorphism.


=== Lambda Papers ===
Gerald Jay Sussman and Guy L. Steele, Jr.
AI Memos, 1975–1980
Links to pdf's
Description: This series of papers and reports first defined the influential Scheme programming language and questioned the prevailing practices in programming language design, employing lambda calculus extensively to model programming language concepts and guide efficient implementation without sacrificing expressive power.


=== Structure and Interpretation of Computer Programs ===
Harold Abelson and Gerald Jay Sussman
MIT Press, 1984, 1996
Description: This textbook explains core computer programming concepts, and is widely considered a classic text in computer science.
Online course


=== Comprehending Monads ===
Philip Wadler
Mathematical structures in computer science 2.04 (1992): 461-493.
Online copy
Description: This paper introduced monads to functional programming.


=== Towards a Theory of Type Structure ===
John Reynolds
Programming Symposium. Springer Berlin Heidelberg, 1974.
online copy
Description: This paper introduced System F and created the modern notion of Parametric polymorphism


=== An axiomatic basis for computer programming ===
Tony Hoare
Communications of the ACM, Volume 12 Issue 10, Oct. 1969, Pages 576-580
Description: This paper introduce Hoare logic, which forms the foundation of program verification


== Scientific computing ==

Wilkinson, J. H.; Reinsch, C. (1971). Linear algebra, volume II of Handbook for Automatic Computation. Springer. ISBN 978-0-387-05414-8. 
Golub, Gene H.; van Loan, Charles F. (1996) [1983], Matrix Computations, 3rd edition, Johns Hopkins University Press;, ISBN 978-0-8018-5414-9 


=== Computational linguistics ===
Booth, T. L. (1969). ""Probabilistic representation of formal languages"". IEEE Conference Record of the 1969 Tenth Annual Symposium on Switching and Automata Theory. pp. 74–81. 
Contains the first presentation of stochastic context-free grammars.
Koskenniemi, Kimmo (1983), Two-level morphology: A general computational model of word-form recognition and production (PDF), Department of General Linguistics, University of Helsinki 
The first published description of computational morphology using finite state transducers. (Kaplan and Kay had previously done work in this field and presented this at a conference; the linguist Johnson had remarked the possibility in 1972, but not produced any implementation.)
Rabiner, Lawrence R. (1989). ""A tutorial on hidden Markov models and selected applications in speech recognition"". Proceedings of the IEEE. 77 (2): 257–286. doi:10.1109/5.18626. 
An overview of hidden Markov models geared toward speech recognition and other NLP fields, describing the Viterbi and forward-backward algorithms.
Brill, Eric (1995). ""Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging"". Computational Linguistics. 21 (4): 543–566. 
Describes a now commonly used POS tagger based on transformation-based learning.
Manning, Christopher D.; Schütze, Hinrich (1999), Foundation of Statistical Natural Language Processing, MIT Press 
Textbook on statistical and probabilistic methods in NLP.
Frost, Richard A. (2006). ""Realization of Natural-Language Interfaces Using Lazy Functional Programming"" (PDF). ACM Computing Surveys. 38 (4). 
This survey documents relatively less researched importance of lazy functional programming languages (i.e. Haskell) to construct Natural Language Processors and to accommodated many linguistic theories.


== Software engineering ==


=== Software engineering: Report of a conference sponsored by the NATO Science Committee ===
Peter Naur, Brian Randell (eds.)
Garmisch, Germany, 7–11 October 1968, Brussels, Scientific Affairs Division, NATO (1969) 231pp.
Online copy (PDF)
Description: Conference of leading people in software field c. 1968
The paper defined the field of Software engineering


=== A Description of the Model-View-Controller User Interface Paradigm in the Smalltalk-80 System ===
Krasner, Glenn E.; Pope, Stephen T.
The Journal of Object Technology, Aug-Sep 1988
Online copy (PDF)
Description: A description of the system that originated the (now dominant) GUI programming paradigm of Model–view–controller


=== Go To Statement Considered Harmful ===
Dijkstra, E. W.
Communications of the ACM, 11(3):147–148, March 1968
Online copy
Description: Don't use goto – the beginning of structured programming.


=== On the criteria to be used in decomposing systems into modules ===
David Parnas
Communications of the ACM, Volume 15, Issue 12:1053–1058, December 1972.
Online copy (PDF)
Description: The importance of modularization and information hiding. Note that information hiding was first presented in a different paper of the same author – ""Information Distributions Aspects of Design Methodology"", Proceedings of IFIP Congress '71, 1971, Booklet TA-3, pp. 26–30


=== Hierarchical Program Structures ===
Ole-Johan Dahl, C. A. R. Hoare
in Dahl, Dijkstra and Hoare, Structured Programming, Academic Press, London and New York, pp. 175–220, 1972.
Description: The beginning of Object-oriented programming. This paper argued that programs should be decomposed to independent components with small and simple interfaces. They also argued that objects should have both data and related methods.


=== A technique for software module specification with examples ===
David Parnas
Comm. ACM 15, 5 (May 1972), 330–336.
Online copy (PDF)
Description: software specification.


=== Structured Design ===
Wayne Stevens, Glenford Myers, and Larry Constantine
IBM Systems Journal, 13 (2), 115–139, 1974.
On-line copy (PDF)
Description: Seminal paper on Structured Design, data flow diagram, coupling, and cohesion.


=== The Emperor's Old Clothes ===
C.A.R. Hoare
Communications of the ACM, Vol. 24, No. 2, February 1981, pp. 75–83.
Archived copy (PDF)
Description: Illustrates the ""second-system effect"" and the importance of simplicity.


=== The Mythical Man-Month: Essays on Software Engineering ===
Brooks, Jr., F. P.
Addison Wesley Professional. 2nd edition, 1995.
Description: Throwing more people at the task will not speed its completion...


=== No Silver Bullet: Essence and Accidents of Software Engineering ===
Brooks, Frederick. P., Jr. (April 1987). ""No Silver Bullet: Essence and Accidents of Software Engineering"". Computer. 20 (4): 10–19. doi:10.1109/MC.1987.1663532. 


=== The Cathedral and the Bazaar ===
Raymond, E.S.
First Monday, 3, 3 (March 1998)
Online copy (HTML)
Description: Open source methodology.


=== Design Patterns: Elements of Reusable Object Oriented Software ===
E. Gamma, R. Helm, R. Johnson, J. Vlissides
Addison–Wesley, Reading, Massachusetts, 1995.
Description: This book was the first to define and list design patterns in computer science.


=== Statecharts: A Visual Formalism For Complex Systems ===
David Harel
D. Harel. Statecharts: A visual formalism for complex systems. Science of Computer Programming, 8:231—274, 1987
Online version
Description: Statecharts are a visual modeling method. They are an extension of state machine that might be exponentially more efficient. Therefore, statcharts enable formal modeling of applications that were too complex before. Statecharts are part of the UML diagrams.


== Security ==


=== Anonymity Systems ===
David Chaum. Untraceable electronic mail, return addresses, and digital pseudonyms. Communications of the ACM, 4(2):84–88, February 1981.
Dingledine and Mathewson, Anonymity Loves Company: Usability and the Network Effect, Workshop on the Economics of Information Security (WEIS) 2006


=== Cryptography ===

Whitfield Diffie and Martin E. Hellman, New Directions in Cryptography, IEEE Transactions on Information Theory, November 1976
R. L. Rivest and A. Shamir and L. M. Adelman, A Method For Obtaining Digital Signatures And Public-Key Cryptosystems, MIT/LCS/TM-82, 1977
Merkle, R. Security, Authentication, and Public Key Systems, PhD Thesis, 1979 Stanford University. (Just read chapter 2, pages 11–15, in which Merkle invents cryptographic hash functions.)


=== Passwords ===
Morris, Robert and Thompson, Ken. Password security: a case history, Communications of the ACM CACM Homepage archive Volume 22 Issue 11, Nov. 1979 Pages 594-597. PDF
Mazurek et al., Measuring password guessability for an entire university, CCS '13 Proceedings of the 2013 ACM SIGSAC conference on Computer & communications security, Pages 173-186


=== System Security ===
Saltzer and Schroeder, The Protection of Information in Computer Systems, ACM Symposium on Operating System Principles (October 1973) HTML HTML2
Karger and Schell, Thirty Years later: Lessons from the Multics Security Evaluation, ACSAC 2002
Lamport, Butler. A Note on the Confinement Problem, Communications of the ACM, 16:10 (Oct. 1973), pp. 613–615. PDF
Thompson, Reflections on Trusting Trust, Communications of the ACM, 27:8, Aug 1984
J.E. Forrester and B.P. Miller, An Empirical Study of the Robustness of Windows NT Applications Using Random Testing, 4th USENIX Windows Systems Symposium, Seattle, August 2000.
Zissis D and Lekkas D, Addressing cloud computing security issues, Future Generation Computer Systems, 28/3, pp. 583-592, Elsevier 2012


=== Usable Security ===
Whitten, Alma, Why Johnny Can't Encrypt: A Usability Evaluation of PGP 5.0, Proceedings of the 8th conference on USENIX Security Symposium, Volume 8, Pages 14–28
Garfinkel, Simson and Shelat, Abhi, Remembrance of Data Passed, IEEE Security and Privacy, Volume 1 Issue 1, January 2003, Page 17-27


== Theoretical computer science ==

Topics covered: theoretical computer science, including computability theory, computational complexity theory, algorithms, algorithmic information theory, information theory and formal verification.


== See also ==
DBLP (Digital Bibliography & Library Project in computer science)
List of open problems in computer science
List of computer science journals
List of computer science conferences
The Collection of Computer Science Bibliographies
Paris Kanellakis Award, a prize given to honor specific theoretical accomplishments that have had a significant and demonstrable effect on the practice of computing.


== References ==


== External links ==
ACM Classic Books Series
Most cited articles in Computer Science (CiteSeer Database)
50 most influential papers ACM SIGPLAN papers published in PLDI from 1979 through 1999; organized into a special SIGPLAN proceedings.


=== Academic Search Engines ===
Google Scholar
CiteSeer
Live Academic
Odysci
ISI Web of Science"
7,Church–Turing thesis,6854,52072,"In computability theory, the Church–Turing thesis (also known as computability thesis, the Turing–Church thesis, the Church–Turing conjecture, Church's thesis, Church's conjecture, and Turing's thesis) is a hypothesis about the nature of computable functions. It states that a function on the natural numbers is computable by a human being following an algorithm, ignoring resource limitations, if and only if it is computable by a Turing machine. The thesis is named after American mathematician Alonzo Church and the British mathematician Alan Turing. Before the precise definition of computable function, mathematicians often used the informal term effectively calculable to describe functions that are computable by paper-and-pencil methods. In the 1930s, several independent attempts were made to formalize the notion of computability:
In 1933, Austrian-American mathematician Kurt Gödel, with Jacques Herbrand, created a formal definition of a class called general recursive functions. The class of general recursive functions is the smallest class of functions (possibly with more than one argument) which includes all constant functions, projections, the successor function, and which is closed under function composition, recursion, and minimization.
In 1936, Alonzo Church created a method for defining functions called the λ-calculus. Within λ-calculus, he defined an encoding of the natural numbers called the Church numerals. A function on the natural numbers is called λ-computable if the corresponding function on the Church numerals can be represented by a term of the λ-calculus.
Also in 1936, before learning of Church's work, Alan Turing created a theoretical model for machines, now called Turing machines, that could carry out calculations from inputs by manipulating symbols on a tape. Given a suitable encoding of the natural numbers as sequences of symbols, a function on the natural numbers is called Turing computable if some Turing machine computes the corresponding function on encoded natural numbers.
Church and Turing proved that these three formally defined classes of computable functions coincide: a function is λ-computable if and only if it is Turing computable if and only if it is general recursive. This has led mathematicians and computer scientists to believe that the concept of computability is accurately characterized by these three equivalent processes. Other formal attempts to characterize computability have subsequently strengthened this belief (see below).
On the other hand, the Church–Turing thesis states that the above three formally-defined classes of computable functions coincide with the informal notion of an effectively calculable function. Since, as an informal notion, the concept of effective calculability does not have a formal definition, the thesis, although it has near-universal acceptance, cannot be formally proven.
Since its inception, variations on the original thesis have arisen, including statements about what can physically be realized by a computer in our universe (physical Church-Turing thesis) and what can be efficiently computed (Church–Turing thesis (complexity theory)). These variations are not due to Church or Turing, but arise from later work in complexity theory and digital physics. The thesis also has implications for the philosophy of mind (see below).


== Statement in Church's and Turing's words ==

J.B. Rosser (1939) addresses the notion of ""effective computability"" as follows: ""Clearly the existence of CC and RC (Church's and Rosser's proofs) presupposes a precise definition of 'effective'. 'Effective method' is here used in the rather special sense of a method each step of which is precisely predetermined and which is certain to produce the answer in a finite number of steps"". Thus the adverb-adjective ""effective"" is used in a sense of ""1a: producing a decided, decisive, or desired effect"", and ""capable of producing a result"".
In the following, the words ""effectively calculable"" will mean ""produced by any intuitively 'effective' means whatsoever"" and ""effectively computable"" will mean ""produced by a Turing-machine or equivalent mechanical device"". Turing's ""definitions"" given in a footnote in his 1939 Ph.D. thesis Systems of Logic Based on Ordinals, supervised by Church, are virtually the same:

† We shall use the expression “computable function” to mean a function calculable by a machine, and let “effectively calculable” refer to the intuitive idea without particular identification with any one of these definitions.

The thesis can be stated as: Every effectively calculable function is a computable function.
Turing stated it this way:

It was stated… that “a function is effectively calculable if its values can be found by some purely mechanical process.” We may take this literally, understanding that by a purely mechanical process one which could be carried out by a machine. The development… leads to… an identification of computability† with effective calculability. [† is the footnote quoted above.]


== History ==

One of the important problems for logicians in the 1930s was David Hilbert's Entscheidungsproblem, which asked whether there was a mechanical procedure for separating mathematical truths from mathematical falsehoods. This quest required that the notion of ""algorithm"" or ""effective calculability"" be pinned down, at least well enough for the quest to begin. But from the very outset Alonzo Church's attempts began with a debate that continues to this day. Was the notion of ""effective calculability"" to be (i) an ""axiom or axioms"" in an axiomatic system, or (ii) merely a definition that ""identified"" two or more propositions, or (iii) an empirical hypothesis to be verified by observation of natural events, or (iv) or just a proposal for the sake of argument (i.e. a ""thesis"").


=== Circa 1930–1952 ===
In the course of studying the problem, Church and his student Stephen Kleene introduced the notion of λ-definable functions, and they were able to prove that several large classes of functions frequently encountered in number theory were λ-definable. The debate began when Church proposed to Gödel that one should define the ""effectively computable"" functions as the λ-definable functions. Gödel, however, was not convinced and called the proposal ""thoroughly unsatisfactory"". Rather, in correspondence with Church (ca 1934–5), Gödel proposed axiomatizing the notion of ""effective calculability""; indeed, in a 1935 letter to Kleene, Church reported that:
""His [Gödel's] only idea at the time was that it might be possible, in terms of effective calculability as an undefined notion, to state a set of axioms which would embody the generally accepted properties of this notion, and to do something on that basis"".
But Gödel offered no further guidance. Eventually, he would suggest his recursion, modified by Herbrand's suggestion, that Gödel had detailed in his 1934 lectures in Princeton NJ (Kleene and Rosser transcribed the notes). But he did not think that the two ideas could be satisfactorily identified ""except heuristically"".
Next, it was necessary to identify and prove the equivalence of two notions of effective calculability. Equipped with the λ-calculus and ""general"" recursion, Stephen Kleene with help of Church and J. Barkley Rosser produced proofs (1933, 1935) to show that the two calculi are equivalent. Church subsequently modified his methods to include use of Herbrand–Gödel recursion and then proved (1936) that the Entscheidungsproblem is unsolvable: there is no generalized algorithm that can determine whether a well formed formula has a ""normal form"".
Many years later in a letter to Davis (ca 1965), Gödel said that ""he was, at the time of these [1934] lectures, not at all convinced that his concept of recursion comprised all possible recursions"". By 1963–4 Gödel would disavow Herbrand–Gödel recursion and the λ-calculus in favor of the Turing machine as the definition of ""algorithm"" or ""mechanical procedure"" or ""formal system"".
A hypothesis leading to a natural law?: In late 1936 Alan Turing's paper (also proving that the Entscheidungsproblem is unsolvable) was delivered orally, but had not yet appeared in print. On the other hand, Emil Post's 1936 paper had appeared and was certified independent of Turing's work. Post strongly disagreed with Church's ""identification"" of effective computability with the λ-calculus and recursion, stating:
""Actually the work already done by Church and others carries this identification considerably beyond the working hypothesis stage. But to mask this identification under a definition… blinds us to the need of its continual verification.""
Rather, he regarded the notion of ""effective calculability"" as merely a ""working hypothesis"" that might lead by inductive reasoning to a ""natural law"" rather than by ""a definition or an axiom"". This idea was ""sharply"" criticized by Church.
Thus Post in his 1936 paper was also discounting Kurt Gödel's suggestion to Church in 1934–5 that the thesis might be expressed as an axiom or set of axioms.
Turing adds another definition, Rosser equates all three: Within just a short time, Turing's 1936–37 paper ""On Computable Numbers, with an Application to the Entscheidungsproblem"" appeared. In it he stated another notion of ""effective computability"" with the introduction of his a-machines (now known as the Turing machine abstract computational model). And in a proof-sketch added as an ""Appendix"" to his 1936–37 paper, Turing showed that the classes of functions defined by λ-calculus and Turing machines coincided. Church was quick to recognise how compelling Turing's analysis was. In his review of Turing's paper he made clear that Turing's notion made ""the identification with effectiveness in the ordinary (not explicitly defined) sense evident immediately"".
In a few years (1939) Turing would propose, like Church and Kleene before him, that his formal definition of mechanical computing agent was the correct one. Thus, by 1939, both Church (1934) and Turing (1939) had individually proposed that their ""formal systems"" should be definitions of ""effective calculability""; neither framed their statements as theses.
Rosser (1939) formally identified the three notions-as-definitions:
""All three definitions are equivalent, so it does not matter which one is used.""
Kleene proposes Church's Thesis: This left the overt expression of a ""thesis"" to Kleene. In his 1943 paper Recursive Predicates and Quantifiers Kleene proposed his ""THESIS I"":
""This heuristic fact [general recursive functions are effectively calculable] …led Church to state the following thesis(22). The same thesis is implicit in Turing's description of computing machines(23).
""THESIS I. Every effectively calculable function (effectively decidable predicate) is general recursive [Kleene's italics]

""Since a precise mathematical definition of the term effectively calculable (effectively decidable) has been wanting, we can take this thesis… as a definition of it…""""(22) references Church 1936
""(23) references Turing 1936–7

Kleene goes on to note that:
""…the thesis has the character of an hypothesis—a point emphasized by Post and by Church(24). If we consider the thesis and its converse as definition, then the hypothesis is an hypothesis about the application of the mathematical theory developed from the definition. For the acceptance of the hypothesis, there are, as we have suggested, quite compelling grounds.""
""(24) references Post 1936 of Post and Church's Formal definitions in the theory of ordinal numbers, Fund. Math. vol 28 (1936) pp.11–21 (see ref. #2, Davis 1965:286).

Kleene's Church–Turing Thesis: A few years later (1952) Kleene, who switched from presenting his work in the mathematical terminology of the lambda calculus of his phd advisor Alonzo Church to the theory of general recursive functions of his other teacher Kurt Gödel, would overtly name the Church–Turing thesis in his correction of Turing's paper ""The Word Problem in Semi-Groups with Cancellation"", defend, and express the two ""theses"" and then ""identify"" them (show equivalence) by use of his Theorem XXX:
""Heuristic evidence and other considerations led Church 1936 to propose the following thesis.
Thesis I. Every effectively calculable function (effectively decidable predicate) is general recursive.

Theorem XXX: ""The following classes of partial functions are coextensive, i.e. have the same members: (a) the partial recursive functions, (b) the computable functions…"".
Turing's thesis: ""Turing's thesis that every function which would naturally be regarded as computable is computable under his definition, i.e. by one of his machines, is equivalent to Church's thesis by Theorem XXX.""
Kleene himself never stated that Turing had made a mistake in his paper, important in its own right for helping to establish the unsolvability of problems in group theoretic computations, although corrections to Turing's paper were also made later by Boone who originally pointed out ""points in the proof require clarification, which can be given"" and Turing's only PhD student, Robin Gandy. That Kleene doesn't mention this mistake in the body of his textbook where his presents his work on Turing machines but buried the fact he was correcting Alan Turing in the appendix was appreciated by Turing himself can be surmised from the ending of Turing's last publication ""Solvable and Unsolvable Problems"" which ends not with a bibliography but the words,


=== Later developments ===
An attempt to understand the notion of ""effective computability"" better led Robin Gandy (Turing's student and friend) in 1980 to analyze machine computation (as opposed to human-computation acted out by a Turing machine). Gandy's curiosity about, and analysis of, ""cellular automata"", ""Conway's game of life"", ""parallelism"" and ""crystalline automata"" led him to propose four ""principles (or constraints) ... which it is argued, any machine must satisfy."" His most-important fourth, ""the principle of causality"" is based on the ""finite velocity of propagation of effects and signals; contemporary physics rejects the possibility of instantaneous action at a distance."" From these principles and some additional constraints—(1a) a lower bound on the linear dimensions of any of the parts, (1b) an upper bound on speed of propagation (the velocity of light), (2) discrete progress of the machine, and (3) deterministic behavior—he produces a theorem that ""What can be calculated by a device satisfying principles I–IV is computable.""
In the late 1990s Wilfried Sieg analyzed Turing's and Gandy's notions of ""effective calculability"" with the intent of ""sharpening the informal notion, formulating its general features axiomatically, and investigating the axiomatic framework"". In his 1997 and 2002 work Sieg presents a series of constraints on the behavior of a computor—""a human computing agent who proceeds mechanically"". These constraints reduce to:
""(B.1) (Boundedness) There is a fixed bound on the number of symbolic configurations a computor can immediately recognize.
""(B.2) (Boundedness) There is a fixed bound on the number of internal states a computor can be in.
""(L.1) (Locality) A computor can change only elements of an observed symbolic configuration.
""(L.2) (Locality) A computor can shift attention from one symbolic configuration to another one, but the new observed configurations must be within a bounded distance of the immediately previously observed configuration.
""(D) (Determinacy) The immediately recognizable (sub-)configuration determines uniquely the next computation step (and id [instantaneous description] )""; stated another way: ""A computor's internal state together with the observed configuration fixes uniquely the next computation step and the next internal state.""
The matter remains in active discussion within the academic community.


=== The thesis as a definition ===
The thesis can be viewed as nothing but an ordinary mathematical definition. Comments by Gödel on the subject suggest this view, e.g. “…the correct definition of mechanical computability was established beyond any doubt by Turing.” The case for viewing the thesis as nothing more than a definition is made explicitly by Robert I. Soare, where it is also argued that Turing's definition of computability is no less likely to be correct than the epsilon-delta definition of a continuous function.


== Success of the thesis ==
Other formalisms (besides recursion, the λ-calculus, and the Turing machine) have been proposed for describing effective calculability/computability. Stephen Kleene (1952) adds to the list the functions ""reckonable in the system S1"" of Kurt Gödel 1936, and Emil Post's (1943, 1946) ""canonical [also called normal] systems"". In the 1950s Hao Wang and Martin Davis greatly simplified the one-tape Turing-machine model (see Post–Turing machine). Marvin Minsky expanded the model to two or more tapes and greatly simplified the tapes into ""up-down counters"", which Melzak and Lambek further evolved into what is now known as the counter machine model. In the late 1960s and early 1970s researchers expanded the counter machine model into the register machine, a close cousin to the modern notion of the computer. Other models include combinatory logic and Markov algorithms. Gurevich adds the pointer machine model of Kolmogorov and Uspensky (1953, 1958): ""... they just wanted to ... convince themselves that there is no way to extend the notion of computable function.""
All these contributions involve proofs that the models are computationally equivalent to the Turing machine; such models are said to be Turing complete. Because all these different attempts at formalizing the concept of ""effective calculability/computability"" have yielded equivalent results, it is now generally assumed that the Church–Turing thesis is correct. In fact, Gödel (1936) proposed something stronger than this; he observed that there was something ""absolute"" about the concept of ""reckonable in S1"":
""It may also be shown that a function which is computable ['reckonable'] in one of the systems Si, or even in a system of transfinite type, is already computable [reckonable] in S1. Thus the concept 'computable' ['reckonable'] is in a certain definite sense 'absolute', while practically all other familiar metamathematical concepts (e.g. provable, definable, etc.) depend quite essentially on the system to which they are defined""


== Informal usage in proofs ==
Proofs in computability theory often invoke the Church–Turing thesis in an informal way to establish the computability of functions while avoiding the (often very long) details which would be involved in a rigorous, formal proof. To establish that a function is computable by Turing machine, it is usually considered sufficient to give an informal English description of how the function can be effectively computed, and then conclude ""by the Church–Turing thesis"" that the function is Turing computable (equivalently, partial recursive).
Dirk van Dalen gives the following example for the sake of illustrating this informal use of the Church–Turing thesis:
EXAMPLE: Each infinite RE set contains an infinite recursive set.
Proof: Let A be infinite RE. We list the elements of A effectively, n0, n1, n2, n3, ...
From this list we extract an increasing sublist: put m0=n0, after finitely many steps we find an nk such that nk > m0, put m1=nk. We repeat this procedure to find m2 > m1, etc. this yields an effective listing of the subset B={m0,m1,m2,...} of A, with the property mi < mi+1.
Claim. B is decidable. For, in order to test k in B we must check if k=mi for some i. Since the sequence of mi's is increasing we have to produce at most k+1 elements of the list and compare them with k. If none of them is equal to k, then k not in B. Since this test is effective, B is decidable and, by Church's thesis, recursive.
In order to make the above example completely rigorous, one would have to carefully construct a Turing Machine, or λ-function, or carefully invoke recursion axioms, or at best, cleverly invoke various theorems of computability theory. But because the computability theorist believes that Turing computability correctly captures what can be computed effectively, and because an effective procedure is spelled out in English for deciding the set B, the computability theorist accepts this as proof that the set is indeed recursive.


== Variations ==
The success of the Church–Turing thesis prompted variations of the thesis to be proposed. For example, the Physical Church–Turing thesis (PCTT) states:

All physically computable functions are Turing-computable.

The Church–Turing thesis says nothing about the efficiency with which one model of computation can simulate another. It has been proved for instance that a (multi-tape) universal Turing machine only suffers a logarithmic slowdown factor in simulating any Turing machine. A variation of the Church–Turing thesis addresses whether an arbitrary but ""reasonable"" model of computation can be efficiently simulated. This is called the Feasibility Thesis, also known as the (Classical) Complexity-Theoretic Church–Turing Thesis (SCTT) or the Extended Church–Turing Thesis, which is not due to Church or Turing, but rather was realized gradually in the development of complexity theory. It states:

A probabilistic Turing machine can efficiently simulate any realistic model of computation.

The word 'efficiently' here means up to polynomial-time reductions. This thesis was originally called Computational Complexity-Theoretic Church–Turing Thesis by Ethan Bernstein and Umesh Vazirani (1997). The Complexity-Theoretic Church–Turing Thesis, then, posits that all 'reasonable' models of computation yield the same class of problems that can be computed in polynomial time. Assuming the conjecture that probabilistic polynomial time (BPP) equals deterministic polynomial time (P), the word 'probabilistic' is optional in the Complexity-Theoretic Church–Turing Thesis. A similar thesis, called the Invariance Thesis, was introduced by Cees F. Slot and Peter van Emde Boas. It states: ""Reasonable"" machines can simulate each other within a polynomially bounded overhead in time and a constant-factor overhead in space. The thesis originally appeared in a paper at STOC'84, which was the first paper to show that polynomial-time overhead and constant-space overhead could be simultaneously achieved for a simulation of a Random Access Machine on a Turing machine.
If BQP is shown to be a strict superset of BPP, it would invalidate the Complexity-Theoretic Church–Turing Thesis. In other words, there would be efficient quantum algorithms that perform tasks that do not have efficient probabilistic algorithms. This would not however invalidate the original Church–Turing thesis, since a quantum computer can always be simulated by a Turing machine, but it would invalidate the classical Complexity-Theoretic Church–Turing thesis for efficiency reasons. Consequently, the Quantum Complexity-Theoretic Church–Turing thesis states:

A quantum Turing machine can efficiently simulate any realistic model of computation.

Eugene Eberbach and Peter Wegner claim that the Church–Turing thesis is sometimes interpreted too broadly, stating ""the broader assertion that algorithms precisely capture what can be computed is invalid."" They claim that forms of computation not captured by the thesis are relevant today, terms which they call super-Turing computation.


== Philosophical implications ==
Philosophers have interpreted the Church–Turing thesis as having implications for the philosophy of mind. B. Jack Copeland states that it's an open empirical question whether there are actual deterministic physical processes that, in the long run, elude simulation by a Turing machine; furthermore, he states that it is an open empirical question whether any such processes are involved in the working of the human brain. There are also some important open questions which cover the relationship between the Church–Turing thesis and physics, and the possibility of hypercomputation. When applied to physics, the thesis has several possible meanings:
The universe is equivalent to a Turing machine; thus, computing non-recursive functions is physically impossible. This has been termed the Strong Church–Turing thesis, or Church–Turing–Deutsch principle, and is a foundation of digital physics.
The universe is not equivalent to a Turing machine (i.e., the laws of physics are not Turing-computable), but incomputable physical events are not ""harnessable"" for the construction of a hypercomputer. For example, a universe in which physics involves random real numbers, as opposed to computable reals, would fall into this category.
The universe is a hypercomputer, and it is possible to build physical devices to harness this property and calculate non-recursive functions. For example, it is an open question whether all quantum mechanical events are Turing-computable, although it is known that rigorous models such as quantum Turing machines are equivalent to deterministic Turing machines. (They are not necessarily efficiently equivalent; see above.) John Lucas and Roger Penrose have suggested that the human mind might be the result of some kind of quantum-mechanically enhanced, ""non-algorithmic"" computation.
There are many other technical possibilities which fall outside or between these three categories, but these serve to illustrate the range of the concept.


== Non-computable functions ==
One can formally define functions that are not computable. A well-known example of such a function is the Busy Beaver function. This function takes an input n and returns the largest number of symbols that a Turing machine with n states can print before halting, when run with no input. Finding an upper bound on the busy beaver function is equivalent to solving the halting problem, a problem known to be unsolvable by Turing machines. Since the busy beaver function cannot be computed by Turing machines, the Church–Turing thesis states that this function cannot be effectively computed by any method.
Several computational models allow for the computation of (Church-Turing) non-computable functions. These are known as hypercomputers. Mark Burgin argues that super-recursive algorithms such as inductive Turing machines disprove the Church–Turing thesis. His argument relies on a definition of algorithm broader than the ordinary one, so that non-computable functions obtained from some inductive Turing machines are called computable. This interpretation of the Church–Turing thesis differs from the interpretation commonly accepted in computability theory, discussed above. The argument that super-recursive algorithms are indeed algorithms in the sense of the Church–Turing thesis has not found broad acceptance within the computability research community.


== See also ==
Abstract machine
Church's thesis in constructive mathematics
Church–Turing–Deutsch principle, which states that every physical process can be simulated by a universal computing device
Computability logic
Computability theory
Decidability
Hypercomputer
Model of computation
Oracle (computer science)
Super-recursive algorithm


== Footnotes ==


== References ==


== External links ==
The Church–Turing Thesis entry by B. Jack Copeland in the Stanford Encyclopedia of Philosophy.
Computation in Physical Systems entry by Gualtiero Piccinini in the Stanford Encyclopedia of Philosophy—a comprehensive philosophical treatment of relevant issues.
Kaznatcheev, Artem (September 11, 2014). ""Transcendental idealism and Post's variant of the Church-Turing thesis"". Theory, Evolution, and Games Group."
8,Computational complexity theory,7543,46919,"Computational complexity theory is a branch of the theory of computation in theoretical computer science that focuses on classifying computational problems according to their inherent difficulty, and relating those classes to each other. A computational problem is understood to be a task that is in principle amenable to being solved by a computer, which is equivalent to stating that the problem may be solved by mechanical application of mathematical steps, such as an algorithm.
A problem is regarded as inherently difficult if its solution requires significant resources, whatever the algorithm used. The theory formalizes this intuition, by introducing mathematical models of computation to study these problems and quantifying the amount of resources needed to solve them, such as time and storage. Other complexity measures are also used, such as the amount of communication (used in communication complexity), the number of gates in a circuit (used in circuit complexity) and the number of processors (used in parallel computing). One of the roles of computational complexity theory is to determine the practical limits on what computers can and cannot do.
Closely related fields in theoretical computer science are analysis of algorithms and computability theory. A key distinction between analysis of algorithms and computational complexity theory is that the former is devoted to analyzing the amount of resources needed by a particular algorithm to solve a problem, whereas the latter asks a more general question about all possible algorithms that could be used to solve the same problem. More precisely, computational complexity theory tries to classify problems that can or cannot be solved with appropriately restricted resources. In turn, imposing restrictions on the available resources is what distinguishes computational complexity from computability theory: the latter theory asks what kind of problems can, in principle, be solved algorithmically.


== Computational problems ==


=== Problem instances ===
A computational problem can be viewed as an infinite collection of instances together with a solution for every instance. The input string for a computational problem is referred to as a problem instance, and should not be confused with the problem itself. In computational complexity theory, a problem refers to the abstract question to be solved. In contrast, an instance of this problem is a rather concrete utterance, which can serve as the input for a decision problem. For example, consider the problem of primality testing. The instance is a number (e.g. 15) and the solution is ""yes"" if the number is prime and ""no"" otherwise (in this case ""no""). Stated another way, the instance is a particular input to the problem, and the solution is the output corresponding to the given input.
To further highlight the difference between a problem and an instance, consider the following instance of the decision version of the traveling salesman problem: Is there a route of at most 2000 kilometres passing through all of Germany's 15 largest cities? The quantitative answer to this particular problem instance is of little use for solving other instances of the problem, such as asking for a round trip through all sites in Milan whose total length is at most 10 km. For this reason, complexity theory addresses computational problems and not particular problem instances.


=== Representing problem instances ===
When considering computational problems, a problem instance is a string over an alphabet. Usually, the alphabet is taken to be the binary alphabet (i.e., the set {0,1}), and thus the strings are bitstrings. As in a real-world computer, mathematical objects other than bitstrings must be suitably encoded. For example, integers can be represented in binary notation, and graphs can be encoded directly via their adjacency matrices, or by encoding their adjacency lists in binary.
Even though some proofs of complexity-theoretic theorems regularly assume some concrete choice of input encoding, one tries to keep the discussion abstract enough to be independent of the choice of encoding. This can be achieved by ensuring that different representations can be transformed into each other efficiently.


=== Decision problems as formal languages ===

Decision problems are one of the central objects of study in computational complexity theory. A decision problem is a special type of computational problem whose answer is either yes or no, or alternately either 1 or 0. A decision problem can be viewed as a formal language, where the members of the language are instances whose output is yes, and the non-members are those instances whose output is no. The objective is to decide, with the aid of an algorithm, whether a given input string is a member of the formal language under consideration. If the algorithm deciding this problem returns the answer yes, the algorithm is said to accept the input string, otherwise it is said to reject the input.
An example of a decision problem is the following. The input is an arbitrary graph. The problem consists in deciding whether the given graph is connected, or not. The formal language associated with this decision problem is then the set of all connected graphs — to obtain a precise definition of this language, one has to decide how graphs are encoded as binary strings.


=== Function problems ===
A function problem is a computational problem where a single output (of a total function) is expected for every input, but the output is more complex than that of a decision problem, that is, it isn't just yes or no. Notable examples include the traveling salesman problem and the integer factorization problem.
It is tempting to think that the notion of function problems is much richer than the notion of decision problems. However, this is not really the case, since function problems can be recast as decision problems. For example, the multiplication of two integers can be expressed as the set of triples (a, b, c) such that the relation a × b = c holds. Deciding whether a given triple is a member of this set corresponds to solving the problem of multiplying two numbers.


=== Measuring the size of an instance ===
To measure the difficulty of solving a computational problem, one may wish to see how much time the best algorithm requires to solve the problem. However, the running time may, in general, depend on the instance. In particular, larger instances will require more time to solve. Thus the time required to solve a problem (or the space required, or any measure of complexity) is calculated as a function of the size of the instance. This is usually taken to be the size of the input in bits. Complexity theory is interested in how algorithms scale with an increase in the input size. For instance, in the problem of finding whether a graph is connected, how much more time does it take to solve a problem for a graph with 2n vertices compared to the time taken for a graph with n vertices?
If the input size is n, the time taken can be expressed as a function of n. Since the time taken on different inputs of the same size can be different, the worst-case time complexity T(n) is defined to be the maximum time taken over all inputs of size n. If T(n) is a polynomial in n, then the algorithm is said to be a polynomial time algorithm. Cobham's thesis says that a problem can be solved with a feasible amount of resources if it admits a polynomial time algorithm.


== Machine models and complexity measures ==


=== Turing machine ===

A Turing machine is a mathematical model of a general computing machine. It is a theoretical device that manipulates symbols contained on a strip of tape. Turing machines are not intended as a practical computing technology, but rather as a general model of a computing machine—anything from an advanced supercomputer to a mathematician with a pencil and paper. It is believed that if a problem can be solved by an algorithm, there exists a Turing machine that solves the problem. Indeed, this is the statement of the Church–Turing thesis. Furthermore, it is known that everything that can be computed on other models of computation known to us today, such as a RAM machine, Conway's Game of Life, cellular automata or any programming language can be computed on a Turing machine. Since Turing machines are easy to analyze mathematically, and are believed to be as powerful as any other model of computation, the Turing machine is the most commonly used model in complexity theory.
Many types of Turing machines are used to define complexity classes, such as deterministic Turing machines, probabilistic Turing machines, non-deterministic Turing machines, quantum Turing machines, symmetric Turing machines and alternating Turing machines. They are all equally powerful in principle, but when resources (such as time or space) are bounded, some of these may be more powerful than others.
A deterministic Turing machine is the most basic Turing machine, which uses a fixed set of rules to determine its future actions. A probabilistic Turing machine is a deterministic Turing machine with an extra supply of random bits. The ability to make probabilistic decisions often helps algorithms solve problems more efficiently. Algorithms that use random bits are called randomized algorithms. A non-deterministic Turing machine is a deterministic Turing machine with an added feature of non-determinism, which allows a Turing machine to have multiple possible future actions from a given state. One way to view non-determinism is that the Turing machine branches into many possible computational paths at each step, and if it solves the problem in any of these branches, it is said to have solved the problem. Clearly, this model is not meant to be a physically realizable model, it is just a theoretically interesting abstract machine that gives rise to particularly interesting complexity classes. For examples, see non-deterministic algorithm.


=== Other machine models ===
Many machine models different from the standard multi-tape Turing machines have been proposed in the literature, for example random access machines. Perhaps surprisingly, each of these models can be converted to another without providing any extra computational power. The time and memory consumption of these alternate models may vary. What all these models have in common is that the machines operate deterministically.
However, some computational problems are easier to analyze in terms of more unusual resources. For example, a non-deterministic Turing machine is a computational model that is allowed to branch out to check many different possibilities at once. The non-deterministic Turing machine has very little to do with how we physically want to compute algorithms, but its branching exactly captures many of the mathematical models we want to analyze, so that non-deterministic time is a very important resource in analyzing computational problems.


=== Complexity measures ===
For a precise definition of what it means to solve a problem using a given amount of time and space, a computational model such as the deterministic Turing machine is used. The time required by a deterministic Turing machine M on input x is the total number of state transitions, or steps, the machine makes before it halts and outputs the answer (""yes"" or ""no""). A Turing machine M is said to operate within time f(n), if the time required by M on each input of length n is at most f(n). A decision problem A can be solved in time f(n) if there exists a Turing machine operating in time f(n) that solves the problem. Since complexity theory is interested in classifying problems based on their difficulty, one defines sets of problems based on some criteria. For instance, the set of problems solvable within time f(n) on a deterministic Turing machine is then denoted by DTIME(f(n)).
Analogous definitions can be made for space requirements. Although time and space are the most well-known complexity resources, any complexity measure can be viewed as a computational resource. Complexity measures are very generally defined by the Blum complexity axioms. Other complexity measures used in complexity theory include communication complexity, circuit complexity, and decision tree complexity.
The complexity of an algorithm is often expressed using big O notation.


=== Best, worst and average case complexity ===

The best, worst and average case complexity refer to three different ways of measuring the time complexity (or any other complexity measure) of different inputs of the same size. Since some inputs of size n may be faster to solve than others, we define the following complexities:
Best-case complexity: This is the complexity of solving the problem for the best input of size n.
Worst-case complexity: This is the complexity of solving the problem for the worst input of size n.
Average-case complexity: This is the complexity of solving the problem on an average. This complexity is only defined with respect to a probability distribution over the inputs. For instance, if all inputs of the same size are assumed to be equally likely to appear, the average case complexity can be defined with respect to the uniform distribution over all inputs of size n.
For example, consider the deterministic sorting algorithm quicksort. This solves the problem of sorting a list of integers that is given as the input. The worst-case is when the input is sorted or sorted in reverse order, and the algorithm takes time O(n2) for this case. If we assume that all possible permutations of the input list are equally likely, the average time taken for sorting is O(n log n). The best case occurs when each pivoting divides the list in half, also needing O(n log n) time.


=== Upper and lower bounds on the complexity of problems ===
To classify the computation time (or similar resources, such as space consumption), one is interested in proving upper and lower bounds on the maximum amount of time required by the most efficient algorithm solving a given problem. The complexity of an algorithm is usually taken to be its worst-case complexity, unless specified otherwise. Analyzing a particular algorithm falls under the field of analysis of algorithms. To show an upper bound T(n) on the time complexity of a problem, one needs to show only that there is a particular algorithm with running time at most T(n). However, proving lower bounds is much more difficult, since lower bounds make a statement about all possible algorithms that solve a given problem. The phrase ""all possible algorithms"" includes not just the algorithms known today, but any algorithm that might be discovered in the future. To show a lower bound of T(n) for a problem requires showing that no algorithm can have time complexity lower than T(n).
Upper and lower bounds are usually stated using the big O notation, which hides constant factors and smaller terms. This makes the bounds independent of the specific details of the computational model used. For instance, if T(n) = 7n2 + 15n + 40, in big O notation one would write T(n) = O(n2).


== Complexity classes ==


=== Defining complexity classes ===
A complexity class is a set of problems of related complexity. Simpler complexity classes are defined by the following factors:
The type of computational problem: The most commonly used problems are decision problems. However, complexity classes can be defined based on function problems, counting problems, optimization problems, promise problems, etc.
The model of computation: The most common model of computation is the deterministic Turing machine, but many complexity classes are based on non-deterministic Turing machines, Boolean circuits, quantum Turing machines, monotone circuits, etc.
The resource (or resources) that are being bounded and the bounds: These two properties are usually stated together, such as ""polynomial time"", ""logarithmic space"", ""constant depth"", etc.
Some complexity classes have complicated definitions that do not fit into this framework. Thus, a typical complexity class has a definition like the following:
The set of decision problems solvable by a deterministic Turing machine within time f(n). (This complexity class is known as DTIME(f(n)).)
But bounding the computation time above by some concrete function f(n) often yields complexity classes that depend on the chosen machine model. For instance, the language {xx | x is any binary string} can be solved in linear time on a multi-tape Turing machine, but necessarily requires quadratic time in the model of single-tape Turing machines. If we allow polynomial variations in running time, Cobham-Edmonds thesis states that ""the time complexities in any two reasonable and general models of computation are polynomially related"" (Goldreich 2008, Chapter 1.2). This forms the basis for the complexity class P, which is the set of decision problems solvable by a deterministic Turing machine within polynomial time. The corresponding set of function problems is FP.


=== Important complexity classes ===

Many important complexity classes can be defined by bounding the time or space used by the algorithm. Some important complexity classes of decision problems defined in this manner are the following:
The logarithmic-space classes (necessarily) do not take into account the space needed to represent the problem.
It turns out that PSPACE = NPSPACE and EXPSPACE = NEXPSPACE by Savitch's theorem.
Other important complexity classes include BPP, ZPP and RP, which are defined using probabilistic Turing machines; AC and NC, which are defined using Boolean circuits; and BQP and QMA, which are defined using quantum Turing machines. #P is an important complexity class of counting problems (not decision problems). Classes like IP and AM are defined using Interactive proof systems. ALL is the class of all decision problems.


=== Hierarchy theorems ===

For the complexity classes defined in this way, it is desirable to prove that relaxing the requirements on (say) computation time indeed defines a bigger set of problems. In particular, although DTIME(n) is contained in DTIME(n2), it would be interesting to know if the inclusion is strict. For time and space requirements, the answer to such questions is given by the time and space hierarchy theorems respectively. They are called hierarchy theorems because they induce a proper hierarchy on the classes defined by constraining the respective resources. Thus there are pairs of complexity classes such that one is properly included in the other. Having deduced such proper set inclusions, we can proceed to make quantitative statements about how much more additional time or space is needed in order to increase the number of problems that can be solved.
More precisely, the time hierarchy theorem states that

  
    
      
        
          
            D
            T
            I
            M
            E
          
        
        
          
            (
          
        
        f
        (
        n
        )
        
          
            )
          
        
        ⊊
        
          
            D
            T
            I
            M
            E
          
        
        
          
            (
          
        
        f
        (
        n
        )
        ⋅
        
          log
          
            2
          
        
        ⁡
        (
        f
        (
        n
        )
        )
        
          
            )
          
        
      
    
    {\displaystyle {\mathsf {DTIME}}{\big (}f(n){\big )}\subsetneq {\mathsf {DTIME}}{\big (}f(n)\cdot \log ^{2}(f(n)){\big )}}
  .
The space hierarchy theorem states that

  
    
      
        
          
            D
            S
            P
            A
            C
            E
          
        
        
          
            (
          
        
        f
        (
        n
        )
        
          
            )
          
        
        ⊊
        
          
            D
            S
            P
            A
            C
            E
          
        
        
          
            (
          
        
        f
        (
        n
        )
        ⋅
        log
        ⁡
        (
        f
        (
        n
        )
        )
        
          
            )
          
        
      
    
    {\displaystyle {\mathsf {DSPACE}}{\big (}f(n){\big )}\subsetneq {\mathsf {DSPACE}}{\big (}f(n)\cdot \log(f(n)){\big )}}
  .
The time and space hierarchy theorems form the basis for most separation results of complexity classes. For instance, the time hierarchy theorem tells us that P is strictly contained in EXPTIME, and the space hierarchy theorem tells us that L is strictly contained in PSPACE.


=== Reduction ===

Many complexity classes are defined using the concept of a reduction. A reduction is a transformation of one problem into another problem. It captures the informal notion of a problem being at most as difficult as another problem. For instance, if a problem X can be solved using an algorithm for Y, X is no more difficult than Y, and we say that X reduces to Y. There are many different types of reductions, based on the method of reduction, such as Cook reductions, Karp reductions and Levin reductions, and the bound on the complexity of reductions, such as polynomial-time reductions or log-space reductions.
The most commonly used reduction is a polynomial-time reduction. This means that the reduction process takes polynomial time. For example, the problem of squaring an integer can be reduced to the problem of multiplying two integers. This means an algorithm for multiplying two integers can be used to square an integer. Indeed, this can be done by giving the same input to both inputs of the multiplication algorithm. Thus we see that squaring is not more difficult than multiplication, since squaring can be reduced to multiplication.
This motivates the concept of a problem being hard for a complexity class. A problem X is hard for a class of problems C if every problem in C can be reduced to X. Thus no problem in C is harder than X, since an algorithm for X allows us to solve any problem in C. The notion of hard problems depends on the type of reduction being used. For complexity classes larger than P, polynomial-time reductions are commonly used. In particular, the set of problems that are hard for NP is the set of NP-hard problems.
If a problem X is in C and hard for C, then X is said to be complete for C. This means that X is the hardest problem in C. (Since many problems could be equally hard, one might say that X is one of the hardest problems in C.) Thus the class of NP-complete problems contains the most difficult problems in NP, in the sense that they are the ones most likely not to be in P. Because the problem P = NP is not solved, being able to reduce a known NP-complete problem, Π2, to another problem, Π1, would indicate that there is no known polynomial-time solution for Π1. This is because a polynomial-time solution to Π1 would yield a polynomial-time solution to Π2. Similarly, because all NP problems can be reduced to the set, finding an NP-complete problem that can be solved in polynomial time would mean that P = NP.


== Important open problems ==


=== P versus NP problem ===

The complexity class P is often seen as a mathematical abstraction modeling those computational tasks that admit an efficient algorithm. This hypothesis is called the Cobham–Edmonds thesis. The complexity class NP, on the other hand, contains many problems that people would like to solve efficiently, but for which no efficient algorithm is known, such as the Boolean satisfiability problem, the Hamiltonian path problem and the vertex cover problem. Since deterministic Turing machines are special non-deterministic Turing machines, it is easily observed that each problem in P is also member of the class NP.
The question of whether P equals NP is one of the most important open questions in theoretical computer science because of the wide implications of a solution. If the answer is yes, many important problems can be shown to have more efficient solutions. These include various types of integer programming problems in operations research, many problems in logistics, protein structure prediction in biology, and the ability to find formal proofs of pure mathematics theorems. The P versus NP problem is one of the Millennium Prize Problems proposed by the Clay Mathematics Institute. There is a US$1,000,000 prize for resolving the problem.


=== Problems in NP not known to be in P or NP-complete ===
It was shown by Ladner that if P ≠ NP then there exist problems in NP that are neither in P nor NP-complete. Such problems are called NP-intermediate problems. The graph isomorphism problem, the discrete logarithm problem and the integer factorization problem are examples of problems believed to be NP-intermediate. They are some of the very few NP problems not known to be in P or to be NP-complete.
The graph isomorphism problem is the computational problem of determining whether two finite graphs are isomorphic. An important unsolved problem in complexity theory is whether the graph isomorphism problem is in P, NP-complete, or NP-intermediate. The answer is not known, but it is believed that the problem is at least not NP-complete. If graph isomorphism is NP-complete, the polynomial time hierarchy collapses to its second level. Since it is widely believed that the polynomial hierarchy does not collapse to any finite level, it is believed that graph isomorphism is not NP-complete. The best algorithm for this problem, due to László Babai and Eugene Luks has run time 
  
    
      
        O
        (
        
          2
          
            
              n
              log
              ⁡
              n
            
          
        
        )
      
    
    {\displaystyle O(2^{\sqrt {n\log n}})}
   for graphs with n vertices, although some recent work by Babai offers some potentially new perspectives on this.
The integer factorization problem is the computational problem of determining the prime factorization of a given integer. Phrased as a decision problem, it is the problem of deciding whether the input has a prime factor less than k. No efficient integer factorization algorithm is known, and this fact forms the basis of several modern cryptographic systems, such as the RSA algorithm. The integer factorization problem is in NP and in co-NP (and even in UP and co-UP). If the problem is NP-complete, the polynomial time hierarchy will collapse to its first level (i.e., NP will equal co-NP). The best known algorithm for integer factorization is the general number field sieve, which takes time 
  
    
      
        O
        (
        
          e
          
            
              
                (
                
                  
                    64
                    9
                  
                
                )
              
              
                1
                
                  /
                
                3
              
            
            (
            log
            ⁡
            n
            
              )
              
                1
                
                  /
                
                3
              
            
            (
            log
            ⁡
            log
            ⁡
            n
            
              )
              
                2
                
                  /
                
                3
              
            
          
        
        )
      
    
    {\displaystyle O(e^{\left({\frac {64}{9}}\right)^{1/3}(\log n)^{1/3}(\log \log n)^{2/3}})}
   to factor an integer n. However, the best known quantum algorithm for this problem, Shor's algorithm, does run in polynomial time. Unfortunately, this fact doesn't say much about where the problem lies with respect to non-quantum complexity classes.


=== Separations between other complexity classes ===
Many known complexity classes are suspected to be unequal, but this has not been proved. For instance P ⊆ NP ⊆ PP ⊆ PSPACE, but it is possible that P = PSPACE. If P is not equal to NP, then P is not equal to PSPACE either. Since there are many known complexity classes between P and PSPACE, such as RP, BPP, PP, BQP, MA, PH, etc., it is possible that all these complexity classes collapse to one class. Proving that any of these classes are unequal would be a major breakthrough in complexity theory.
Along the same lines, co-NP is the class containing the complement problems (i.e. problems with the yes/no answers reversed) of NP problems. It is believed that NP is not equal to co-NP; however, it has not yet been proven. It is clear that if these two complexity classes are not equal then P is not equal to NP, since if P=NP we would also have P=co-NP, since problems in NP are dual to those in co-NP.
Similarly, it is not known if L (the set of all problems that can be solved in logarithmic space) is strictly contained in P or equal to P. Again, there are many complexity classes between the two, such as NL and NC, and it is not known if they are distinct or equal classes.
It is suspected that P and BPP are equal. However, it is currently open if BPP = NEXP.


== Intractability ==

A problem that can be solved in theory (e.g. given large but finite resources, especially time), but for which in practice any solution takes too many resources to be useful, is known as an intractable problem. Conversely, a problem that can be solved in practice is called a tractable problem, literally ""a problem that can be handled"". The term infeasible (literally ""cannot be done"") is sometimes used interchangeably with intractable, though this risks confusion with a feasible solution in mathematical optimization.
Tractable problems are frequently identified with problems that have polynomial-time solutions (P, PTIME); this is known as the Cobham–Edmonds thesis. Problems that are known to be intractable in this sense include those that are EXPTIME-hard. If NP is not the same as P, then NP-hard problems are also intractable in this sense.
However, this identification is inexact: a polynomial-time solution with large exponent or large constant term grows quickly, and may be impractical for practical size problems; conversely, an exponential-time solution that grows slowly may be practical on realistic input, or a solution that takes a long time in the worst case may take a short time in most cases or the average case, and thus still be practical. Saying that a problem is not in P does not imply that all large cases of the problem are hard or even that most of them are. For example, the decision problem in Presburger arithmetic has been shown not to be in P, yet algorithms have been written that solve the problem in reasonable times in most cases. Similarly, algorithms can solve the NP-complete knapsack problem over a wide range of sizes in less than quadratic time and SAT solvers routinely handle large instances of the NP-complete Boolean satisfiability problem.
To see why exponential-time algorithms are generally unusable in practice, consider a program that makes 2n operations before halting. For small n, say 100, and assuming for the sake of example that the computer does 1012 operations each second, the program would run for about 4 × 1010 years, which is the same order of magnitude as the age of the universe. Even with a much faster computer, the program would only be useful for very small instances and in that sense the intractability of a problem is somewhat independent of technological progress. However, an exponential-time algorithm that takes 1.0001n operations is practical until n gets relatively large.
Similarly, a polynomial time algorithm is not always practical. If its running time is, say, n15, it is unreasonable to consider it efficient and it is still useless except on small instances. Indeed, in practice even n3 or n2 algorithms are often impractical on realistic sizes of problems.


== History ==
An early example of algorithm complexity analysis is the running time analysis of the Euclidean algorithm done by Gabriel Lamé in 1844.
Before the actual research explicitly devoted to the complexity of algorithmic problems started off, numerous foundations were laid out by various researchers. Most influential among these was the definition of Turing machines by Alan Turing in 1936, which turned out to be a very robust and flexible simplification of a computer.
The beginning of systematic studies in computational complexity is attributed to the seminal 1965 paper ""On the Computational Complexity of Algorithms"" by Juris Hartmanis and Richard E. Stearns, which laid out the definitions of time complexity and space complexity, and proved the hierarchy theorems. In addition, in 1965 Edmonds suggested to consider a ""good"" algorithm to be one with running time bounded by a polynomial of the input size.
Earlier papers studying problems solvable by Turing machines with specific bounded resources include John Myhill's definition of linear bounded automata (Myhill 1960), Raymond Smullyan's study of rudimentary sets (1961), as well as Hisao Yamada's paper on real-time computations (1962). Somewhat earlier, Boris Trakhtenbrot (1956), a pioneer in the field from the USSR, studied another specific complexity measure. As he remembers:

However, [my] initial interest [in automata theory] was increasingly set aside in favor of computational complexity, an exciting fusion of combinatorial methods, inherited from switching theory, with the conceptual arsenal of the theory of algorithms. These ideas had occurred to me earlier in 1955 when I coined the term ""signalizing function"", which is nowadays commonly known as ""complexity measure"".

In 1967, Manuel Blum developed an axiomatic complexity theory based on his axioms and proved an important result, the so-called, speed-up theorem. The field really began to flourish in 1971 when the US researcher Stephen Cook and, working independently, Leonid Levin in the USSR, proved that there exist practically relevant problems that are NP-complete. In 1972, Richard Karp took this idea a leap forward with his landmark paper, ""Reducibility Among Combinatorial Problems"", in which he showed that 21 diverse combinatorial and graph theoretical problems, each infamous for its computational intractability, are NP-complete.


== See also ==


== References ==


=== Citations ===


=== Textbooks ===
Arora, Sanjeev; Barak, Boaz (2009), Computational Complexity: A Modern Approach, Cambridge, ISBN 978-0-521-42426-4, Zbl 1193.68112 
Downey, Rod; Fellows, Michael (1999), Parameterized complexity, Berlin, New York: Springer-Verlag 
Du, Ding-Zhu; Ko, Ker-I (2000), Theory of Computational Complexity, John Wiley & Sons, ISBN 978-0-471-34506-0 
Garey, Michael R.; Johnson, David S. (1979), Computers and Intractability: A Guide to the Theory of NP-Completeness, W. H. Freeman, ISBN 0-7167-1045-5 
Goldreich, Oded (2008), Computational Complexity: A Conceptual Perspective, Cambridge University Press 
van Leeuwen, Jan, ed. (1990), Handbook of theoretical computer science (vol. A): algorithms and complexity, MIT Press, ISBN 978-0-444-88071-0 
Papadimitriou, Christos (1994), Computational Complexity (1st ed.), Addison Wesley, ISBN 0-201-53082-1 
Sipser, Michael (2006), Introduction to the Theory of Computation (2nd ed.), USA: Thomson Course Technology, ISBN 0-534-95097-3 


=== Surveys ===
Khalil, Hatem; Ulery, Dana (1976), ""A Review of Current Studies on Complexity of Algorithms for Partial Differential Equations"", Proceedings of the annual conference on - ACM 76, ACM '76 Proceedings of the 1976 Annual Conference: 197, doi:10.1145/800191.805573 
Cook, Stephen (1983), ""An overview of computational complexity"" (PDF), Commun. ACM, ACM, 26 (6): 400–408, doi:10.1145/358141.358144, ISSN 0001-0782 
Fortnow, Lance; Homer, Steven (2003), ""A Short History of Computational Complexity"" (PDF), Bulletin of the EATCS, 80: 95–133 
Mertens, Stephan (2002), ""Computational Complexity for Physicists"", Computing in Science and Eng., Piscataway, NJ, USA: IEEE Educational Activities Department, 4 (3): 31–47, arXiv:cond-mat/0012185 , doi:10.1109/5992.998639, ISSN 1521-9615 


== External links ==
The Complexity Zoo
Hazewinkel, Michiel, ed. (2001) [1994], ""Computational complexity classes"", Encyclopedia of Mathematics, Springer Science+Business Media B.V. / Kluwer Academic Publishers, ISBN 978-1-55608-010-4 
http://mathoverflow.net/questions/34487/what-are-the-most-important-results-and-papers-in-complexity-theory-that-every/"
9,Timeline of programming languages,23696,45448,"This is a record of historically important programming languages, by decade.


== Pre-1950 ==


== 1950s ==


== 1960s ==


== 1970s ==


== 1980s ==


== 1990s ==


== 2000s ==


== 2010s ==


== See also ==
Programming language
Timeline of computing
History of computing hardware
History of programming languages


== References ==


== External links ==
Online encyclopedia for the history of programming languages
Diagram & history of programming languages
Eric Levenez's timeline diagram of computer languages history"
10,History of compiler construction,21310186,43068,"In computing, a compiler is a computer program that transforms source code written in a programming language or computer language (the source language), into another computer language (the target language, often having a binary form known as object code or machine code). The most common reason for transforming source code is to create an executable program.
Any program written in a high level programming language must be translated to object code before it can be executed, so all programmers using such a language use a compiler or an interpreter. Thus, compilers are very important to programmers. Improvements to a compiler may lead to a large number of improved executable programs.
Compilers are large and complex programs, but systematic analysis and research by computer scientists has led to a clearer understanding of compiler construction and a large body of theory has been developed around them. Research into compiler construction has led to tools that make it much easier to create compilers, so that today computer science students can create their own small language and develop a simple compiler for it in a few weeks.


== First compilers ==
Software for early computers was primarily written in assembly language. It is usually more productive for a programmer to use a high-level language, and programs written in a high-level language can be reused on different kinds of computers. Even so, it took a while for compilers to become established, because they generated code that did not perform as well as hand-written assembler, they were daunting development projects in their own right, and the very limited memory capacity of early computers created many technical problems for practical compiler implementations.
The first compiler was written by Corrado Böhm, in 1951, for his PhD thesis. The term compiler was coined by Grace Hopper., referring to her A-0 system which functioned as a loader or linker, not the modern notion of a compiler. The FORTRAN team led by John W. Backus at IBM introduced the first commercially available compiler, in 1957, which took 18 person-years to create.
The first ALGOL 58 compiler was completed by the end of 1958 by Friedrich L. Bauer, Hermann Bottenbruch, Heinz Rutishauser, and Klaus Samelson for the Z22 computer. Bauer et al. had been working on compiler technology for the Sequentielle Formelübersetzung (i.e. sequential formula translation) in the previous years.
By 1960, an extended Fortran compiler, ALTAC, was available on the Philco 2000, so it is probable that a Fortran program was compiled for both IBM and Philco computer architectures in mid-1960. The first known demonstrated cross-platform high-level language was COBOL. In a demonstration in December 1960, a COBOL program was compiled and executed on both the UNIVAC II and the RCA 501.


== Self-hosting compilers ==

Like any other software, there are benefits from implementing a compiler in a high-level language. In particular, a compiler can be self-hosted – that is, written in the programming language it compiles. Building a self-hosting compiler is a bootstrapping problem, i.e. the first such compiler for a language must be either hand written machine code or compiled by a compiler written in another language, or compiled by running the compiler in an interpreter.


=== Corrado Böhm PhD dissertation ===
Corrado Böhm developed a language, a machine, and a translation method for compiling that language on the machine in his PhD dissertation dated 1951. He not only described a complete compiler, but also defined for the first time that compiler in its own language. The language was interesting in itself, because every statement (including input statements, output statements and control statements) was a special case of an assignment statement.


=== NELIAC ===
The Navy Electronics Laboratory International ALGOL Compiler or NELIAC was a dialect and compiler implementation of the ALGOL 58 programming language developed by the Naval Electronics Laboratory in 1958.
NELIAC was the brainchild of Harry Huskey — then Chairman of the ACM and a well known computer scientist (and later academic supervisor of Niklaus Wirth), and supported by Maury Halstead, the head of the computational center at NEL. The earliest version was implemented on the prototype USQ-17 computer (called the Countess) at the laboratory. It was the world's first self-compiling compiler - the compiler was first coded in simplified form in assembly language (the bootstrap), then re-written in its own language and compiled by the bootstrap, and finally re-compiled by itself, making the bootstrap obsolete.


=== Lisp ===
Another early self-hosting compiler was written for Lisp by Tim Hart and Mike Levin at MIT in 1962. They wrote a Lisp compiler in Lisp, testing it inside an existing Lisp interpreter. Once they had improved the compiler to the point where it could compile its own source code, it was self-hosting.
The compiler as it exists on the standard compiler tape is a machine language program that was obtained by having the S-expression definition of the compiler work on itself through the interpreter. (AI Memo 39)
This technique is only possible when an interpreter already exists for the very same language that is to be compiled. It borrows directly from the notion of running a program on itself as input, which is also used in various proofs in theoretical computer science, such as the proof that the halting problem is undecidable.


=== Forth ===
Forth is an example of a self-hosting compiler. The self compilation and cross compilation features of Forth are commonly confused with metacompilation and metacompilers. Like Lisp, Forth is an extensible programming language. It is the extensible programming language features of Forth and Lisp that enable them to generate new versions of themselves or port themselves to new environments.


== Context-free grammars and parsers ==
A parser is an important component of a compiler. It parses the source code of a computer programming language to create some form of internal representation. Programming languages tend to be specified in terms of a context-free grammar because fast and efficient parsers can be written for them. Parsers can be written by hand or generated by a parser generator. A context-free grammar provides a simple and precise mechanism for describing how programming language constructs are built from smaller blocks. The formalism of context-free grammars was developed in the mid-1950s by Noam Chomsky.
Block structure was introduced into computer programming languages by the ALGOL project (1957–1960), which, as a consequence, also featured a context-free grammar to describe the resulting ALGOL syntax.
Context-free grammars are simple enough to allow the construction of efficient parsing algorithms which, for a given string, determine whether and how it can be generated from the grammar. If a programming language designer is willing to work within some limited subsets of context-free grammars, more efficient parsers are possible.


=== LR parsing ===

The LR parser (left to right) was invented by Donald Knuth in 1965 in a paper, ""On the Translation of Languages from Left to Right"". An LR parser is a parser that reads input from Left to right (as it would appear if visually displayed) and produces a Rightmost derivation. The term LR(k) parser is also used, where k refers to the number of unconsumed lookahead input symbols that are used in making parsing decisions.
Knuth proved that LR(k) grammars can be parsed with an execution time essentially proportional to the length of the program, and that every LR(k) grammar for k > 1 can be mechanically transformed into an LR(1) grammar for the same language. In other words, it is only necessary to have one symbol lookahead to parse any deterministic context-free grammar(DCFG).
Korenjak (1969) was the first to show parsers for programming languages could be produced using these techniques. Frank DeRemer devised the more practical Simple LR (SLR) and Look-ahead LR (LALR) techniques, published in his PhD dissertation at MIT in 1969. This was an important breakthrough, because LR(k) translators, as defined by Donald Knuth, were much too large for implementation on computer systems in the 1960s and 1970s.
In practice, LALR offers a good solution; the added power of LALR(1) parsers over SLR(1) parsers (that is, LALR(1) can parse more complex grammars than SLR(1)) is useful, and, though LALR(1) is not comparable with LL(1) (LALR(1) cannot parse all LL(1) grammars), most LL(1) grammars encountered in practice can be parsed by LALR(1). LR(1) grammars are more powerful again than LALR(1); however, an LR(1) grammar requires a canonical LR parser which would be extremely large in size and is not considered practical. The syntax of many programming languages are defined by grammars that can be parsed with an LALR(1) parser, and for this reason LALR parsers are often used by compilers to perform syntax analysis of source code.
A recursive ascent parser implements an LALR parser using mutually-recursive functions rather than tables. Thus, the parser is directly encoded in the host language similar to recursive descent. Direct encoding usually yields a parser which is faster than its table-driven equivalent for the same reason that compilation is faster than interpretation. It is also (in principle) possible to hand edit a recursive ascent parser, whereas a tabular implementation is nigh unreadable to the average human.
Recursive ascent was first described by Thomas Pennello in his article ""Very fast LR parsing"" in 1986. The technique was later expounded upon by G.H. Roberts in 1988 as well as in an article by Leermakers, Augusteijn, Kruseman Aretz in 1992 in the journal Theoretical Computer Science.


=== LL parsing ===

An LL parser parses the input from Left to right, and constructs a Leftmost derivation of the sentence (hence LL, as opposed to LR). The class of grammars which are parsable in this way is known as the LL grammars. LL grammars are an even more restricted class of context-free grammars than LR grammars. Nevertheless, they are of great interest to compiler writers, because such a parser is simple and efficient to implement.
LL(k) grammars can be parsed by a recursive descent parser which is usually coded by hand, although a notation such as META II might alternatively be used.
The design of ALGOL sparked investigation of recursive descent, since the ALGOL language itself is recursive. The concept of recursive descent parsing was discussed in the January 1961 issue of CACM in separate papers by A.A. Grau and Edgar T. ""Ned"" Irons.   Richard Waychoff and colleagues also implemented recursive descent in the Burroughs ALGOL compiler in March 1961, the two groups used different approaches but were in at least informal contact.
The idea of LL(1) grammars was introduced by Lewis and Stearns (1968).
Recursive descent was popularised by Niklaus Wirth with PL/0, an educational programming language used to teach compiler construction in the 1970s.
LR parsing can handle a larger range of languages than LL parsing, and is also better at error reporting, i.e. it detects syntactic errors when the input does not conform to the grammar as soon as possible.


=== Earley parser ===
In 1970, Jay Earley invented what came to be known as the Earley parser. Earley parsers are appealing because they can parse all context-free languages reasonably efficiently.


== Grammar description languages ==
John Backus proposed ""metalinguistic formulas"" to describe the syntax of the new programming language IAL, known today as ALGOL 58 (1959). Backus's work was based on the Post canonical system devised by Emil Post.
Further development of ALGOL led to ALGOL 60; in its report (1963), Peter Naur named Backus's notation Backus normal form (BNF), and simplified it to minimize the character set used. However, Donald Knuth argued that BNF should rather be read as Backus–Naur form, and that has become the commonly accepted usage.
Niklaus Wirth defined extended Backus–Naur form (EBNF), a refined version of BNF, in the early 1970s for PL/0. Augmented Backus–Naur form (ABNF) is another variant. Both EBNF and ABNF are widely used to specify the grammar of programming languages, as the inputs to parser generators, and in other fields such as defining communication protocols.


== Parser generators ==

A parser generator generates the lexical-analyser portion of a compiler. It is a program that takes a description of a formal grammar of a specific programming language and produces a parser for that language. That parser can be used in a compiler for that specific language. The parser detects and identifies the reserved words and symbols of the specific language from a stream of text and returns these as tokens to the code which implements the syntactic validation and translation into object code. This second part of the compiler can also be created by a compiler-compiler using a formal rules-of-precedence syntax-description as input.
The first compiler-compiler to use that name was written by Tony Brooker in 1960 and was used to create compilers for the Atlas computer at the University of Manchester, including the Atlas Autocode compiler. However it was rather different from modern compiler-compilers, and today would probably be described as being somewhere between a highly customisable generic compiler and an extensible-syntax language. The name 'compiler-compiler' was far more appropriate for Brooker's system than it is for most modern compiler-compilers, which are more accurately described as parser generators. It is almost certain that the ""Compiler Compiler"" name has entered common use due to Yacc rather than Brooker's work being remembered.
In the early 1960s, Robert McClure at Texas Instruments invented a compiler-compiler called TMG, the name taken from ""transmogrification"". In the following years TMG was ported to several UNIVAC and IBM mainframe computers.
The Multics project, a joint venture between MIT and Bell Labs, was one of the first to develop an operating system in a high level language. PL/I was chosen as the language, but an external supplier could not supply a working compiler. The Multics team developed their own subset dialect of PL/I known as Early PL/I (EPL) as their implementation language in 1964. TMG was ported to GE-600 series and used to develop EPL by Douglas McIlroy, Robert Morris, and others.
Not long after Ken Thompson wrote the first version of Unix for the PDP-7 in 1969, Doug McIlroy created the new system's first higher-level language: an implementation of McClure's TMG. TMG was also the compiler definition tool used by Ken Thompson to write the compiler for the B language on his PDP-7 in 1970. B was the immediate ancestor of C.
An early LALR parser generator was called ""TWS"", created by Frank DeRemer and Tom Pennello.


=== XPL ===
XPL is a dialect of the PL/I programming language, used for the development of compilers for computer languages. It was designed and implemented in 1967 by a team with William M. McKeeman, James J. Horning, and David B. Wortman at Stanford University and the University of California, Santa Cruz. It was first announced at the 1968 Fall Joint Computer Conference in San Francisco.
XPL featured a relatively simple translator writing system dubbed ANALYZER, based upon a bottom-up compiler precedence parsing technique called MSP (mixed strategy precedence). XPL was bootstrapped through Burroughs Algol onto the IBM System/360 computer. (Some subsequent versions of XPL used on University of Toronto internal projects utilized an SLR(1) parser, but those implementations have never been distributed).


=== Yacc ===
Yacc is a parser generator (loosely, compiler-compiler), not to be confused with lex, which is a lexical analyzer frequently used as a first stage by Yacc. Yacc was developed by Stephen C. Johnson at AT&T for the Unix operating system. The name is an acronym for ""Yet Another Compiler Compiler."" It generates an LALR(1) compiler based on a grammar written in a notation similar to Backus–Naur form.
Johnson worked on Yacc in the early 1970s at Bell Labs. He was familiar with TMG and its influence can be seen in Yacc and the design of the C programming language. Because Yacc was the default compiler generator on most Unix systems, it was widely distributed and used. Derivatives such as GNU Bison are still in use.
The compiler generated by Yacc requires a lexical analyzer. Lexical analyzer generators, such as lex or flex are widely available. The IEEE POSIX P1003.2 standard defines the functionality and requirements for both Lex and Yacc.


== Metacompilers ==

Metacompilers differ from parser generators, taking as input a program written in a metalanguage. Their input consists grammar analyzing formula and code production transforms that output executable code. Many can be programmed in their own metalanguage enabling them to compile themselves, making them self-hosting extensible language compilers.
Many metacompilers build on the work of Dewey Val Schorre. His META II compiler, first released in 1964, was the first documented metacompiler. Able to define its own language and others, META II accepted syntax formula having imbedded output (code production)s. It also translated to one of the earliest instances of a virtual machine. Lexical analysis was performed by built token recognizing functions: .ID, .STRING, and .NUMBER. Quoted strings in syntax formula recognize lexemes that are not kept.
TREE-META, a second generation Schorre metacompiler, appeared around 1968. It extended the capabilities of META II, adding unparse rules separating code production from the grammar analysis. Tree transform operations in the syntax formula produce abstract syntax trees that the unparse rules operate on. The unparse tree pattern matching provided peephole optimization ability.
CWIC, described in a 1970 ACM publication is a third generation Schorre metacompiler that added lexing rules and backtracking operators to the grammar analysis. LISP 2 was married with the unparse rules of TREEMETA in the CWIC generator language. With LISP 2 processing, CWIC can generate fully optimized code. CWIC also provided binary code generation into named code sections. Single and multipass compiles could be implemented using CWIC.
CWIC compiled to 8 bit byte addressable machine code instructions primarily designed to produce IBM System/360 code.
Later generations are not publicly documented. One important feature would be the abstraction of the target processor instruction set, generating to a pseudo machine instruction set, macros, that could be separately defined or mapped to a real machine's instructions. Optimizations applying to sequential instructions could then be applied to the pseudo instruction before their expansion to target machine code.


== Cross compilation ==
A cross compiler runs in one environment but produces object code for another. Cross compilers are used for embedded development, where the target computer has limited capabilities.
An early example of cross compilation was AIMICO, where a FLOW-MATIC program on a UNIVAC II was used to generate assembly language for the IBM 705, which was then assembled on the IBM computer.
The ALGOL 68C compiler generated ZCODE output, that could then be either compiled into the local machine code by a ZCODE translator or run interpreted. ZCODE is a register-based intermediate language. This ability to interpret or compile ZCODE encouraged the porting of ALGOL 68C to numerous different computer platforms.


== Optimizing compilers ==
Compiler optimization is the process of improving the quality of object code without changing the results it produces.
The developers of the first FORTRAN compiler aimed to generate code that was better than the average hand-coded assembler, so that customers would actually use their product. In one of the first real compilers, they often succeeded.
Later compilers, like IBM's Fortran IV compiler, placed more priority on good diagnostics and executing more quickly, at the expense of object code optimization. It wasn't until the IBM System/360 series that IBM provided two separate compilers: a fast executing code checker, and a slower optimizing one.
Frances E. Allen, working alone and jointly with John Cocke, introduced many of the concepts for optimization. Allen's 1966 paper, Program Optimization, introduced the use of graph data structures to encode program content for optimization. Her 1970 papers, Control Flow Analysis and A Basis for Program Optimization established intervals as the context for efficient and effective data flow analysis and optimization. Her 1971 paper with Cocke, A Catalogue of Optimizing Transformations, provided the first description and systematization of optimizing transformations. Her 1973 and 1974 papers on interprocedural data flow analysis extended the analysis to whole programs. Her 1976 paper with Cocke describes one of the two main analysis strategies used in optimizing compilers today.
Allen developed and implemented her methods as part of compilers for the IBM 7030 Stretch-Harvest and the experimental Advanced Computing System. This work established the feasibility and structure of modern machine- and language-independent optimizers. She went on to establish and lead the PTRAN project on the automatic parallel execution of FORTRAN programs. Her PTRAN team developed new parallelism detection schemes and created the concept of the program dependence graph, the primary structuring method used by most parallelizing compilers.
Programming Languages and their Compilers by John Cocke and Jacob T. Schwartz, published early in 1970, devoted more than 200 pages to optimization algorithms. It included many of the now familiar techniques such as redundant code elimination and strength reduction.


=== Peephole Optimization ===
Peephole optimization is a very simple but effective optimization technique. It was invented by William M. McKeeman and published in 1965 in CACM. It was used in the XPL compiler that McKeeman helped develop.


=== Capex COBOL Optimizer ===
Capex Corporation developed the ""COBOL Optimizer"" in the mid 1970s for COBOL. This type of optimizer depended, in this case, upon knowledge of 'weaknesses' in the standard IBM COBOL compiler, and actually replaced (or patched) sections of the object code with more efficient code. The replacement code might replace a linear table lookup with a binary search for example or sometimes simply replace a relatively 'slow' instruction with a known faster one that was otherwise functionally equivalent within its context. This technique is now known as ""Strength reduction"". For example, on the IBM System/360 hardware the CLI instruction was, depending on the particular model, between twice and 5 times as fast as a CLC instruction for single byte comparisons.
Modern compilers typically provide optimization options, so programmers can choose whether or not to execute an optimization pass.


== Diagnostics ==
When a compiler is given a syntactically incorrect program, a good, clear error message is helpful. From the perspective of the compiler writer, it is often difficult to achieve.
The WATFIV Fortran compiler was developed at the University of Waterloo, Canada in the late 1960s. It was designed to give better error messages than IBM's Fortran compilers of the time. In addition, WATFIV was far more usable, because it combined compiling, linking and execution into one step, whereas IBM's compilers had three separate components to run.


=== PL/C ===
PL/C was a computer programming language developed at Cornell University in the early 1970s. While PL/C was a subset of IBM's PL/I language, it was designed with the specific goal of being used for teaching programming. The two researchers and academic teachers who designed PL/C were Richard W. Conway and Thomas R. Wilcox. They submitted the famous article ""Design and implementation of a diagnostic compiler for PL/I"" published in the Communications of ACM in March 1973.
PL/C eliminated some of the more complex features of PL/I, and added extensive debugging and error recovery facilities. The PL/C compiler had the unusual capability of never failing to compile any program, through the use of extensive automatic correction of many syntax errors and by converting any remaining syntax errors to output statements.


== Just in Time compilation ==

Just in time compilation (JIT) is the generation of executable code on-the-fly or as close as possible to its actual execution, to take advantage of run time metrics or other performance enhancing options.


== Intermediate representation ==

Most modern compilers have a lexer and parser that produce an intermediate representation of the program. The intermediate representation is a simple sequence of operations which can be used by an optimizer and a code generator which produces instructions in the machine language of the target processor. Because the code generator uses an intermediate representation, the same code generator can be used for many different high level languages.
There are many possibilities for the intermediate representation. Three-address code, also known as a quadruple or quad is a common form, where there is an operator, two operands, and a result. Two-address code or triples have a stack to which results are written, in contrast to the explicit variables of three-address code.
Static Single Assignment (SSA) was developed by Ron Cytron, Jeanne Ferrante, Barry K. Rosen, Mark N. Wegman, and F. Kenneth Zadeck, researchers at IBM in the 1980s. In SSA, a variable is given a value only once. A new variable is created rather than modifying an existing one. SSA simplifies optimization and code generation.


== Code Generation ==

A code generator generates machine language instructions for the target processor.


=== Register Allocation ===
Sethi–Ullman algorithm or Sethi-Ullman numbering is a method to minimise the number of registers needed to hold variables.


== Notable compilers ==

Amsterdam Compiler Kit by Andrew Tanenbaum and Ceriel Jacobs
Berkeley Pascal, created by Ken Thompson in 1975. Bill Joy and others at University of California, Berkeley added improvements
GNU Compiler Collection, formerly the GNU C Compiler. First created by Richard Stallman in 1987, GCC is the major compiler used to build Linux.
LLVM, formerly known as the Low Level Virtual Machine
Small-C by Ron Cain and James E Hendrix
Turbo Pascal, created by Anders Hejlsberg, first released in 1983.
WATFOR, created at the University of Waterloo. One of the first popular educational compilers, although now largely obsolete.


== See also ==
History of programming languages
Lex (and Flex lexical analyser), the token parser commonly used in conjunction with yacc (and Bison).
BNF, a metasyntax used to express context-free grammar: that is, a formal way to describe formal languages.
Self-interpreter, an interpreter written in a language it can interpret.


== References ==


== Further reading ==
Backus, John, et al., ""The FORTRAN Automatic Coding System"", Proceedings of the Western Joint Computer Conference, Los Angeles, California, February 1957. Describes the design and implementation of the first FORTRAN compiler by the IBM team.
Knuth, D. E., RUNCIBLE-algebraic translation on a limited computer, Communications of the ACM, Vol. 2, p. 18, (Nov. 1959).
Irons, Edgar T., A syntax directed compiler for ALGOL 60, Communications of the ACM, Vol. 4, p. 51. (Jan. 1961)
Dijkstra, Edsger W. (1961). ""ALGOL 60 Translation: An ALGOL 60 Translator for the X1 and Making a Translator for ALGOL 60 (PDF) (Technical report). Amsterdam: Mathematisch Centrum. 35. 
Conway, Melvin E., Design of a separable transition-diagram compiler, Communications of the ACM, Volume 6, Issue 7 (July 1963)
Floyd, R. W., Syntactic analysis and operator precedence, Journal of the ACM, Vol. 10, p. 316. (July 1963).
Cheatham, T. E., and Sattley, K., Syntax directed compilation, SJCC p. 31. (1964).
Randell, Brian; Russell, Lawford John, ALGOL 60 Implementation: The Translation and Use of ALGOL 60 Programs on a Computer, Academic Press, 1964
Knuth, D. E. (July 1965). ""On the translation of languages from left to right"" (PDF). Information and Control. 8 (6): 607–639. doi:10.1016/S0019-9958(65)90426-2. Retrieved 29 May 2011. 
Cocke, John; Schwartz, Jacob T., Programming Languages and their Compilers: Preliminary Notes, Courant Institute of Mathematical Sciences technical report, New York University, 1969.
Bauer, Friedrich L.; Eickel, Jürgen (Eds.), Compiler Construction, An Advanced Course, 2nd ed. Lecture Notes in Computer Science 21, Springer 1976, ISBN 3-540-07542-9
Gries, David, Compiler Construction for Digital Computers, New York : Wiley, 1971. ISBN 0-471-32776-X


== External links ==
Compiler Construction before 1980 — Annotated literature list by Dick Grune"
11,String literal,199706,42371,"A string literal or anonymous string is a type of literal in programming for the representation of a string value within the source code of a computer program. Most often in modern languages this is a quoted sequence of characters (formally ""bracketed delimiters""), as in x = ""foo"", where ""foo"" is a string literal with value foo – the quotes are not part of the value, and one must use a method such as escape sequences to avoid the problem of delimiter collision and allow the delimiters themselves to be embedded in a string. However, there are numerous alternate notations for specifying string literals, particularly more complicated cases, and the exact notation depends on the individual programming language in question. Nevertheless, there are some general guidelines that most modern programming languages follow.


== Syntax ==


=== Bracketed delimiters ===
Most modern programming languages use bracket delimiters (also balanced delimiters) to specify string literals. Double quotations are the most common quoting delimiters used:

 ""Hi There!""

An empty string is literally written by a pair of quotes with no character at all in between:

 """"

Some languages either allow or mandate the use of single quotations instead of double quotations (the string must begin and end with the same kind of quotation mark and the type of quotation mark may give slightly different semantics):

 'Hi There!'

Note that these quotation marks are unpaired (the same character is used as an opener and a closer), which is a hangover from the typewriter technology which was the precursor of the earliest computer input and output devices.
In terms of regular expressions, a basic quoted string literal is given as:

""[^""]*""

This means that a string literal is written as: a quote, followed by zero, one, or more non-quote characters, followed by a quote. In practice this is often complicated by escaping, other delimiters, and excluding newlines.


==== Paired delimiters ====
A number of languages provide for paired delimiters, where the opening and closing delimiters are different. These also often allow nested strings, so delimiters can be embedded, so long as they are paired, but still result in delimiter collision for embedding an unpaired closing delimiter. Examples include PostScript, which uses parentheses, as in (The quick (brown fox)) and m4, which uses the backtick (`) as the starting delimiter, and the apostrophe (') as the ending delimiter. Tcl allows both quotes (for interpolated strings) and braces (for raw strings), as in ""The quick brown fox"" or {The quick {brown fox}}; this derives from the single quotations in Unix shells and the use of braces in C for compound statements, since blocks of code is in Tcl syntactically the same thing as string literals – that the delimiters are paired is essential for making this feasible.
While the Unicode character set includes paired (separate opening and closing) versions of both single and double quotations, used in text, mostly in other languages than English, these are rarely used in programming languages (because ASCII is preferred, and these are not included in ASCII):

 “Hi There!”
 ‘Hi There!’
 „Hi There!“
 «Hi There!»

The paired double quotations can be used in Visual Basic .NET, but many other programming languages will not accept them. Unpaired marks are preferred for compatibility - many web browsers, text editors, and other tools will not correctly display unicode paired quotes, and so even in languages where they are permitted, many projects forbid their use for source code.


=== Whitespace delimiters ===
String literals might be ended by newlines.
One example is MediaWiki template parameters.

 {{Navbox
 |name=Nulls
 |title=[[wikt:Null|Nulls]] in [[computing]]
 }}

There might be special syntax for multi-line strings.
In YAML, string literals may be specified by the relative positioning of whitespace and indentation.


=== Declarative notation ===
In the original FORTRAN programming language (for example), string literals were written in so-called Hollerith notation, where a decimal count of the number of characters was followed by the letter H, and then the characters of the string:

This declarative notation style is contrasted with bracketed delimiter quoting, because it does not require the use of balanced ""bracketed"" characters on either side of the string.
Advantages:
eliminates text searching (for the delimiter character) and therefore requires significantly less overhead
avoids the problem of delimiter collision
enables the inclusion of metacharacters that might otherwise be mistaken as commands
can be used for quite effective data compression of plain text strings
Drawbacks:
this type of notation is error-prone if used as manual entry by programmers
special care is needed in case of multi byte encodings
This is however not a drawback when the prefix is generated by an algorithm as is most likely the case.


== Delimiter collision ==

When using quoting, if one wishes to represent the delimiter itself in a string literal, one runs into the problem of delimiter collision. For example, if the delimiter is a double quote, one cannot simply represent a double quote itself by the literal """""" as the second quote is interpreted as the end of the string literal, not as the value of the string, and similarly one cannot write ""This is ""in quotes"", but invalid."" as the middle quoted portion is instead interpreted as outside of quotes. There are various solutions, the most general-purpose of which is using escape sequences, such as ""\"""" or ""This is \""in quotes\"" and properly escaped."", but there are many other solutions.
Note that paired quotes, such as braces in Tcl, allow nested string, such as {foo {bar} zork} but do not otherwise solve the problem of delimiter collision, since an unbalanced closing delimiter cannot simply be included, as in {}}.


=== Doubling up ===
A number of languages, including Pascal, BASIC, DCL, Smalltalk, SQL, J, and Fortran, avoid delimiter collision by doubling up on the quotation marks that are intended to be part of the string literal itself:


=== Dual quoting ===
Some languages, such as Fortran, Modula-2, JavaScript, Python, and PHP allow more than one quoting delimiter; in the case of two possible delimiters, this is known as dual quoting. Typically, this consists of allowing the programmer to use either single quotations or double quotations interchangeably – each literal must use one or the other.

This does not allow having a single literal with both delimiters in it, however. This can be worked around by using several literals and using string concatenation:

Note that Python has string literal concatenation, so consecutive string literals are concatenated even without an operator, so this can be reduced to:

D supports a few quoting delimiters, with such strings starting with q""[ and ending with ]"" or similarly for other delimiter character (any of () <> {} or []). D also supports here document-style strings via similar syntax.
In some programming languages, such as sh and Perl, there are different delimiters that are treated differently, such as doing string interpolation or not, and thus care must be taken when choosing which delimiter to use; see different kinds of strings, below.


=== Multiple quoting ===
A further extension is the use of multiple quoting, which allows the author to choose which characters should specify the bounds of a string literal.
For example, in Perl:

all produce the desired result. Although this notation is more flexible, few languages support it; other than Perl, Ruby (influenced by Perl) and C++11 also support these. In C++11, raw strings can have various delimiters, beginning with R""delimiter( and end with )delimiter"". The delimiter can be from zero to 16 characters long and may contain any member of the basic source character set except whitespace characters, parentheses, or backslash. A variant of multiple quoting is the use of here document-style strings.
Lua (as of 5.1) provides a limited form of multiple quoting, particularly to allow nesting of long comments or embedded strings. Normally one uses [[ and ]] to delimit literal strings (initial newline stripped, otherwise raw), but the opening brackets can include any number of equal signs, and only closing brackets with the same number of signs close the string. For example:

Multiple quoting is particularly useful with regular expressions that contain usual delimiters such as quotes, as this avoids needing to escape them. An early example is sed, where in the substitution command s/regex/replacement/ the default slash / delimiters can be replaced by another character, as in s,regex,replacement, .


=== Constructor functions ===
Another option, which is rarely used in modern languages, is to use a function to construct a string, rather than representing it via a literal. This is generally not used in modern languages because the computation is done at run time, rather than at parse time.
For example, early forms of BASIC did not include escape sequences or any other workarounds listed here, and thus one instead was required to use the CHR$ function, which returns a string containing the character corresponding to its argument. In ASCII the quotation mark has the value 34, so to represent a string with quotes on an ASCII system one would write

In C, a similar facility is available via sprintf and the %c ""character"" format specifier, though in the presence of other workarounds this is generally not used:

These constructor functions can also be used to represent nonprinting characters, though escape sequences are generally used instead. A similar technique can be used in C++ with the std::string stringification operator.


== Escape sequences ==

Escape sequences are a general technique for representing characters that are otherwise difficult to represent directly, including delimiters, nonprinting characters (such as backspaces), newlines, and whitespace characters (which are otherwise impossible to distinguish visually), and have a long history. They are accordingly widely used in string literals, and adding an escape sequence (either to a single character or throughout a string) is known as escaping.
One character is chosen as a prefix to give encodings for characters that are difficult or impossible to include directly. Most commonly this is backslash; in addition to other characters, a key point is that backslash itself can be encoded as a double backslash \\ and for delimited strings the delimiter itself can be encoded by escaping, say by \"" for "". A regular expression for such escaped strings can be given as follows, as found in the ANSI C specification:

""(\\.|[^\\""])*""

meaning ""a quote; followed by zero or more of either an escaped character (backslash followed by something, possibly backslash or quote), or a non-escape, non-quote character; ending in a quote"" – the only issue is distinguishing the terminating quote from a quote preceded by a backslash, which may itself be escaped. Note that multiple characters can follow the backslash, such as \uFFFF, depending on the escaping scheme.
An escaped string must then itself be lexically analyzed, converting the escaped string into the unescaped string that it represents. This is done during the evaluation phase of the overall lexing of the computer language: the evaluator of the lexer of the overall language executes its own lexer for escaped string literals.
Among other things, it must be possible to encode the character that normally terminates the string constant, plus there must be some way to specify the escape character itself. Escape sequences are not always pretty or easy to use, so many compilers also offer other means of solving the common problems. Escape sequences, however, solve every delimiter problem and most compilers interpret escape sequences. When an escape character is inside a string literal, it means ""this is the start of the escape sequence"". Every escape sequence specifies one character which is to be placed directly into the string. The actual number of characters required in an escape sequence varies. The escape character is on the top/left of the keyboard, but the editor will translate it, therefore it is not directly tapeable into a string. The backslash is used to represent the escape character in a string literal.
Many languages support the use of metacharacters inside string literals. Metacharacters have varying interpretations depending on the context and language, but are generally a kind of 'processing command' for representing printing or nonprinting characters.
For instance, in a C string literal, if the backslash is followed by a letter such as ""b"", ""n"" or ""t"", then this represents a nonprinting backspace, newline or tab character respectively. Or if the backslash is followed by 1-3 octal digits, then this sequence is interpreted as representing the arbitrary character with the specified ASCII code. This was later extended to allow more modern hexadecimal character code notation:

Note: Not all sequences in the above list are supported by all parsers, and there may be other escape sequences which are not in the above list.


=== Nested escaping ===
When code in one programming language is embedded inside another, embedded strings may require multiple levels of escaping. This is particularly common in regular expressions and SQL query within other languages, or other languages inside shell scripts. This double-escaping is often difficult to read and author.
Incorrect quoting of nested strings can present a security vulnerability. Use of untrusted data, as in data fields of an SQL query, should use prepared statements to prevent a code injection attack. In PHP 2 through 5.3, there was a feature called magic quotes which automatically escaped strings (for convenience and security), but due to problems was removed from version 5.4 onward.


=== Raw strings ===
A few languages provide a method of specifying that a literal is to be processed without any language-specific interpretation. This avoids the need for escaping, and yields more legible strings.
Raw strings are particularly useful when a common character needs to be escaped, notably in regular expressions (nested as string literals), where backslash \ is widely used, and in DOS/Windows paths, where backslash is used as a path separator. The profusion of backslashes is known as leaning toothpick syndrome, and can be reduced by using raw strings. Compare escaped and raw pathnames:

Extreme examples occur when these are combined – Uniform Naming Convention paths begin with \\, and thus an escaped regular expression matching a UNC name begins with 8 backslashes, ""\\\\\\\\"", due to needing to escape the string and the regular expression. Using raw strings reduces this to 4 (escaping in the regular expression), as in C# @""\\\\"".
In XML documents, CDATA sections allows use of characters such as & and < without an XML parser attempting to interpret them as part of the structure of the document itself. This can be useful when including literal text and scripting code, to keep the document well formed.


== Multiline string literals ==
In many languages, string literals can contain literal newlines, spanning several lines. Alternatively, newlines can be escaped, most often as \n. For example:

and

are both valid bash, producing:

foo
bar

Languages that allow literal newlines include bash, Lua, Perl, PHP, R, and Tcl. In some other languages string literals cannot include newlines.
Two issues with multiline string literals are leading and trailing newlines, and indentation. If the initial or final delimiters are on separate lines, there are extra newlines, while if they are not, the delimiter makes the string harder to read, particularly for the first line, which is often indented differently from the rest. Further, the literal must be unindented, as leading whitespace is preserved – this breaks the flow of the code if the literal occurs within indented code.
The most common solution for these problems is here document-style string literals. Formally speaking, a here document is not a string literal, but instead a stream literal or file literal. These originate in shell scripts and allow a literal to be fed as input to an external command. The opening delimiter is <<END where END can be any word, and the closing delimiter is END on a line by itself, serving as a content boundary – the << is due to redirecting stdin from the literal. Due to the delimiter being arbitrary, these also avoid the problem of delimiter collision. These also allow initial tabs to be stripped via the variant syntax <<-END though leading spaces are not stripped. The same syntax has since been adopted for multiline string literals in a number of languages, most notably Perl, and are also referred to as here documents, and retain the syntax, despite being strings and not involving redirection. As with other string literals, these can sometimes have different behavior specified, such as variable interpolation.
Python, whose usual string literals do not allow literal newlines, instead has a special form of string, designed for multiline literals, called triple quoting. These use a tripled delimiter, either ''' or """""". These literals strip leading indentation and the trailing newline (but not the leading newline), and are especially used for inline documentation, known as docstrings.
Tcl allows literal newlines in strings and has no special syntax to assist with multiline strings, though delimiters can be placed on lines by themselves and leading and trailing newlines stripped via string trim, while string map can be used to strip indentation.


== String literal concatenation ==
A few languages provide string literal concatenation, where adjacent string literals are implicitly joined into a single literal at compile time. This is a feature of C, C++, D, Ruby, and Python, which copied it from C. Notably, this concatenation happens at compile time, during lexical analysis (as a phase following initial tokenization), and is contrasted with both run time string concatenation (generally with the + operator) and concatenation during constant folding, which occurs at compile time, but in a later phase (after phrase analysis or ""parsing""). Most languages, such as C#, Java and Perl, do not support implicit string literal concatenation, and instead require explicit concatenation, such as with the + operator (this is also possible in D and Python, but illegal in C/C++ – see below); in this case concatenation may happen at compile time, via constant folding, or may be deferred to run time.


=== Motivation ===
In C, where the concept and term originate, string literal concatenation was introduced for two reasons:
To allow long strings to span multiple lines with proper indentation in contrast to line continuation, which destroys the indentation scheme; and
To allow the construction of string literals by macros (via stringizing).
In practical terms, this allows string concatenation in early phases of compilation (""translation"", specifically as part of lexical analysis), without requiring phrase analysis or constant folding. For example, the following are valid C/C++:

However, the following are invalid:

This is because string literals have pointer type, char * (C) or const char [n] (C++), which cannot be added; this is not a restriction in most other languages.
This is particularly important when used in combination with the C preprocessor, to allow strings to be computed following preprocessing, particularly in macros. As a simple example:

will (if the file is called a.c) expand to:

which is then concatenated, being equivalent to:

A common use case is in constructing printf or scanf format strings, where format specifiers are given by macros.
A more complex example uses stringification of integers (by the preprocessor) to define a macro that expands to a sequence of string literals, which are then concatenated to a single string literal with the file name and line number:

Beyond syntactic requirements of C/C++, implicit concatenation is a form of syntactic sugar, making it simpler to split string literals across several lines, avoiding the need for line continuation (via backslashes) and allowing one to add comments to parts of strings. For example, in Python, one can comment a regular expression in this way:


=== Problems ===
Implicit string concatenation is not required by modern compilers, which implement constant folding, and causes hard-to-spot errors due to unintentional concatenation from omitting a comma, particularly in vertical lists of strings, as in:

Accordingly, it is not used in most languages, and it has been proposed for deprecation from D and Python. However, removing the feature breaks backwards compatibility, and replacing it with a concatenation operator introduces issues of precedence – string literal concatenation occurs during lexing, prior to operator evaluation, but concatenation via an explicit operator occurs at the same time as other operators, hence precedence is an issue, potentially requiring parentheses to ensure desired evaluation order.
A subtler issue is that in C and C++, there are different types of string literals, and concatenation of these has implementation-defined behavior, which poses a potential security risk.


== Different kinds of strings ==
Some languages provide more than one kind of literal, which have different behavior. This is particularly used to indicate raw strings (no escaping), or to disable or enable variable interpolation, but has other uses, such as distinguishing character sets. Most often this is done by changing the quoting character or adding a prefix. This is comparable to prefixes and suffixes to integer literals, such as to indicate hexadecimal numbers or long integers.
One of the oldest examples is in shell scripts, where single quotes indicate a raw string or ""literal string"", while double quotes have escape sequences and variable interpolation.
For example, in Python, raw strings are preceded by an r or R – compare 'C:\\Windows' with r'C:\Windows' (though, a Python raw string cannot end in an odd number of backslashes). Python 2 also distinguishes two types of strings: 8-bit ASCII (""bytes"") strings (the default), explicitly indicated with a b or B prefix, and Unicode strings, indicated with a u or U prefix.
C#'s notation for raw strings is called @-quoting.

While this disables escaping, it allows double-up quotes, which allow one to represent quotes within the string:

C++11 allows raw strings, unicode strings (UTF-8, UTF-16, and UTF-32), and wide character strings, determined by prefixes.
In Tcl, brace-delimited strings are literal, while quote-delimited strings have escaping and interpolation.
Perl has a wide variety of strings, which are more formally considered operators, and are known as quote and quote-like operators. These include both a usual syntax (fixed delimiters) and a generic syntax, which allows a choice of delimiters; these include:

REXX uses suffix characters to specify characters or strings using their hexadecimal or binary code. E.g.,

all yield the space character, avoiding the function call X2C(20).


== Variable interpolation ==

Languages differ on whether and how to interpret string literals as either 'raw' or 'variable interpolated'. Variable interpolation is the process of evaluating an expression containing one or more variables, and returning output where the variables are replaced with their corresponding values in memory. In sh-compatible Unix shells, quotation-delimited ("") strings are interpolated, while apostrophe-delimited (') strings are not. For example, the following Perl code:

produces the output:

Nancy said Hello World to the crowd of people.

The sigil character ($) is interpreted to indicate variable interpolation.
Similarly, the printf function produces the same output using notation such as:

The metacharacters (%s) indicate variable interpolation.
This is contrasted with ""raw"" strings:

which produce output like:

$name said $greeting to the crowd of people.

Here the $ characters are not sigils, and are not interpreted to have any meaning other than plain text.


== Embedding source code in string literals ==
Languages that lack flexibility in specifying string literals make it particularly cumbersome to write programming code that generates other programming code. This is particularly true when the generation language is the same or similar to the output language.
For example:
writing code to produce quines
generating an output language from within a web template;
using XSLT to generate XSLT, or SQL to generate more SQL
generating a PostScript representation of a document for printing purposes, from within a document-processing application written in C or some other language.
writing shaders
Nevertheless, some languages are particularly well-adapted to produce this sort of self-similar output, especially those that support multiple options for avoiding delimiter collision.
Using string literals as code that generates other code may have adverse security implications, especially if the output is based at least partially on untrusted user input. This is particularly acute in the case of Web-based applications, where malicious users can take advantage of such weaknesses to subvert the operation of the application, for example by mounting an SQL injection attack.


== See also ==
Character literal
Sigil (computer programming)


== Notes ==


== References ==


== External links ==
Literals In Programming"
12,C string handling,33691376,41536,"The C programming language has a set of functions implementing operations on strings (character strings and byte strings) in its standard library. Various operations, such as copying, concatenation, tokenization and searching are supported. For character strings, the standard library uses the convention that strings are null-terminated: a string of n characters is represented as an array of n + 1 elements, the last of which is a ""NUL"" character.
The only support for strings in the programming language proper is that the compiler translates quoted string constants into null-terminated strings.


== Definitions ==
A string is a contiguous sequence of code units terminated by the first zero code (\0, corresponding to the null character). In C, there are two types of strings: string, which is sometimes called byte string which uses the type char as code units (one char is at least 8 bits), and wide string which uses the type wchar_t as code units.
A common misconception is that all char arrays are strings, because string literals are converted to arrays during the compilation (or translation) phase. It is important to remember that a string ends at the first zero code unit. An array or string literal that contains a zero before the last byte therefore contains a string, or possibly several strings, but is not itself a string. Conversely, it is possible to create a char array that is not null-terminated and is thus not a string: char is often used as a small integer when needing to save memory.
The term pointer to a string is used in C to describe a pointer to the initial (lowest-addressed) byte of a string. In C, pointers are used to pass strings to functions. Documentation (including this page) will often use the term string to mean pointer to a string.
The term length of a string is used in C to describe the number of bytes preceding the zero byte. strlen is a standardised function commonly used to determine the length of a string. A common mistake is to not realize that a string uses one more unit of memory than this length, in order to store the zero that ends the string.


== Character encodings ==
Each string ends at the first occurrence of the zero code unit of the appropriate kind (char or wchar_t). Consequently, a byte string can contain non-NUL characters in ASCII or any ASCII extension, but not characters in encodings such as UTF-16 (even though a 16-bit code unit might be nonzero, its high or low byte might be zero). The encodings that can be stored in wide strings are defined by the width of wchar_t. In most implementations, wchar_t is at least 16 bits, and so all 16-bit encodings, such as UCS-2, can be stored. If wchar_t is 32-bits, then 32-bit encodings, such as UTF-32, can be stored.
Variable-width encodings can be used in both byte strings and wide strings. String length and offsets are measured in bytes or wchar_t, not in ""characters"", which can be confusing to beginning programmers. UTF-8 and Shift JIS are often used in C byte strings, while UTF-16 is often used in C wide strings when wchar_t is 16 bits. Truncating strings with variable length characters using functions like strncpy can produce invalid sequences at the end of the string. This can be unsafe if the truncated parts are interpreted by code that assumes the input is valid.
Support for Unicode literals such as char foo[512] = ""φωωβαρ"";(UTF-8) or wchar_t foo[512] = L""φωωβαρ""; (UTF-16 or UTF-32) is implementation defined, and may require that the source code be in the same encoding. Some compilers or editors will require entering all non-ASCII characters as \xNN sequences for each byte of UTF-8, and/or \uNNNN for each word of UTF-16.


== Overview of functions ==
Most of the functions that operate on C strings are declared in the string.h header (cstring in C++), while functions that operate on C wide strings are declared in the wchar.h header (cwchar in C++). These headers also contain declarations of functions used for handling memory buffers; the name is thus something of a misnomer.
Functions declared in string.h are extremely popular since, as a part of the C standard library, they are guaranteed to work on any platform which supports C. However, some security issues exist with these functions, such as potential buffer overflows when not used carefully and properly, causing the programmers to prefer safer and possibly less portable variants, out of which some popular ones are listed below. Some of these functions also violate const-correctness by accepting a const string pointer and returning a non-const pointer within the string. To correct this, some have been separated into two overloaded functions in the C++ version of the standard library.
In historical documentation the term ""character"" was often used instead of ""byte"" for C strings, which leads many to believe that these functions somehow do not work for UTF-8. In fact all lengths are defined as being in bytes and this is true in all implementations, and these functions work as well with UTF-8 as with single-byte encodings. The BSD documentation has been fixed to make this clear, but POSIX, Linux, and Windows documentation still uses ""character"" in many places where ""byte"" or ""wchar_t"" is the correct term.
Functions for handling memory buffers can process sequences of bytes that include null-byte as part of the data. Names of these functions typically start with mem, as opposite to the str prefix.


=== Constants and types ===


=== Functions ===


==== Multibyte functions ====
These functions all take a pointer to a mbstate_t object that the caller must maintain. This was originally intended to track shift states in the mb encodings, but modern ones such as UTF-8 do not need this. However these functions were designed on the assumption that the wc encoding is not a variable-width encoding and thus are designed to deal with exactly one wchar_t at a time, passing it by value rather than using a string pointer. As UTF-16 is a variable-width encoding, the mbstate_t has been reused to keep track of surrogate pairs in the wide encoding, though the caller must still detect and call mbtowc twice for a single character.


=== Numeric conversions ===
The C standard library contains several functions for numeric conversions. The functions that deal with byte strings are defined in the stdlib.h header (cstdlib header in C++). The functions that deal with wide strings are defined in the wchar.h header (cwchar header in C++).
The strtoxxx functions are not const-correct, since they accept a const string pointer and return a non-const pointer within the string. Also, since the Normative Amendment 1 (C95), atoxx functions are considered subsumed by strtoxxx functions, for which reason neither C95 nor any later standard provides wide-character versions of these functions.


== Popular extensions ==


== Replacements ==
Despite the well-established need to replace strcat and strcpy with functions that do not allow buffer overflows, no accepted standard has arisen. This is partly due to the mistaken belief by many C programmers that strncat and strncpy have the desired behavior; however, neither function was designed for this (they were intended to manipulate null-padded fixed-size string buffers, a data format less commonly used in modern software), and the behavior and arguments are non-intuitive and often written incorrectly even by expert programmers.
The most popular replacement are the strlcat and strlcpy functions, which appeared in OpenBSD 2.4 in December, 1998. These functions always write one NUL to the destination buffer, truncating the result if necessary, and return the size of buffer that would be needed, which allows detection of the truncation and provides a size for creating a new buffer that will not truncate. They have been criticized on the basis of allegedly being inefficient and encouraging the use of C strings (instead of some superior alternative form of string). Consequently, they have not been included in the GNU C library (used by software on Linux), although they are implemented in the C libraries for OpenBSD, FreeBSD, NetBSD, Solaris, OS X, and QNX, as well as in alternative C libraries for Linux, such as musl. The lack of GNU C library support has not stopped various software authors from using it and bundling a replacement, among other SDL, GLib, ffmpeg, rsync, and even internally in the Linux kernel. Open source implementations for these functions are available.
As part of its 2004 Security Development Lifecycle, Microsoft introduced a family of ""secure"" functions including strcpy_s and strcat_s (along with many others). These functions were standardized with some minor changes as part of the optional C11 (Annex K) proposed by ISO/IEC WDTR 24731. Experience with these functions has shown significant problems with their adoption and errors in usage, so the removal of Annex K is proposed for the next revision of the C standard. These functions perform various checks including whether the string is too long to fit in the buffer. If the checks fail, a user-specified ""runtime-constraint handler"" function is called, which usually aborts the program. Some functions perform destructive operations before calling the runtime-constraint handler; for example, strcat_s sets the destination to the empty string, which can make it difficult to recover from error conditions or debug them. These functions attracted considerable criticism because initially they were implemented only on Windows and at the same time warning messages started to be produced by Microsoft Visual C++ suggesting the programmers to use these functions instead of standard ones. This has been speculated by some to be an attempt by Microsoft to lock developers into its platform. Although open-source implementations of these functions are available, these functions are not present in common Unix C libraries.
If the string length is known, then memcpy or memmove can be more efficient than strcpy, so some programs use them to optimize C string manipulation. They accept a buffer length as a parameter, so they can be employed to prevent buffer overflows in a manner similar to the aforementioned functions.


== See also ==
C syntax § Strings –  source code syntax, including backslash escape sequences
String functions


== Notes ==


== References ==


== External links ==
Fast memcpy in C, multiple C coding examples to target different types of CPU instruction architectures"
13,Academic genealogy of computer scientists,30729888,40768,"The following is an academic genealogy of computer scientists and is constructed by following the pedigree of thesis advisors.
Smaller text indicates advisors or advisees specialized in a field unrelated to computer science.


== Europe ==


=== Denmark ===
Peter Naur
(Olivier Danvy)


=== Finland ===
Arto Salomaa


=== France ===
Many French computer scientists worked at the National Institute for Research in Computer Science and Control (INRIA).
Marcel-Paul Schützenberger
Maurice Nivat
Philippe Flajolet
Gérard Huet
Francois Fages
Thierry Coquand
Hugo Herbelin

Xavier Leroy
Christine Paulin-Mohring
Didier Rémy
François Pottier

Bruno Courcelle

Louis Nolin
Bernard Robinet
Emmanuel Saint-James
Olivier Danvy (Secondary advisor: Emmanuel Saint-James)

Jean-François Perrot
Jacques Sakarovitch
Jean-Eric Pin
Pascal Weil

Gérard Berry
Gilles Kahn
Patrick Cousot
Alain Colmerauer


=== Germany ===
Karl Steinbuch
Franz Baader
Carl Adam Petri
Martin Odersky
Bernhard Steffen


=== Italy ===
Corrado Böhm
Ugo Montanari
Paolo Ciancarini
Roberto Gorrieri
Nadia Busi

Davide Sangiorgi


=== Netherlands ===


==== Van Wijngaarden / Dijkstra ====
Adriaan van Wijngaarden was director of the computer science department at the Centrum Wiskunde & Informatica. It was influential in the development of ALGOL 68.
Cornelis Benjamin Biezeno (1933: honoris causa. Universiteit van Amsterdam)
Adriaan van Wijngaarden (1945: Enige toepassingen van Fourierintegralen op elastische problemen. Technische Universiteit Delft)
Willem van der Poel (1956: The Logical Principles of Some Simple Computers. Universiteit van Amsterdam)
Gerard Holzmann (1979: Coordination Problems in Multiprocessing Systems. Technische Universiteit Delft)

Edsger Dijkstra (1959: Communication with an Automatic Computer. Universiteit van Amsterdam)
Nico Habermann (1967: On the Harmonious Co-operation of Abstract Machines. Technische Universiteit Eindhoven)
Lawrence Snyder (1973: An Analysis of Parameter Evalutation Mechanisms for Recursive Procedures. Carnegie Mellon University)
Tim Teitelbaum (1975: Minimal Distance Analysis of Syntax Errors in Computer Programs. Carnegie Mellon University)
Sten Andler (1979: Predicate Path Expressions: A High-level Synchronization Mechanism. Carnegie Mellon University)
John Ousterhout (1980: Partitioning and Cooperation in a Distributed Multiprocessor Operating System: MEDUSA. Carnegie Mellon University)
Philip Wadler (1984: Listlessness Is Better than Laziness: An Algorithm that Transforms Applicative Programs to Eliminate Intermediate Lists. Carnegie Mellon University) (Secondary advisor: Guy L. Steele, Jr.)
David Notkin (1984: Interactive Structure-Oriented Computing. Carnegie Mellon University)

Martin Rem (1976: Associons and the Closure Statement. Technische Universiteit Eindhoven) (Secondary advisor: Frans Kruseman Aretz)
Jan L. A. van de Snepscheut (1983: Trace Theory and VLSI Design. Technische Universiteit Eindhoven) (Secondary advisor: Edsger Dijkstra)
Peter Hilbers (1989: Mappings of Algorithms on Processor Networks. Rijksuniversiteit Groningen)

Jan Tijmen Udding (1984: Classification and Composition of Delay-Insensitive Circuits. Technische Universiteit Eindhoven) (Secondary advisor: Edsger Dijkstra)
Anne Kaldewaij (1986: A Formalism for Concurrent Processes. Technische Universiteit Eindhoven) (Secondary advisor: Frans Kruseman Aretz)

Guus Zoutendijk (1960: Methods of Feasible Directions : A Study in Lineair and Non-linear Programming. Universiteit van Amsterdam)
Marc Nico Spijker (1968: Stability and Convergence of Finite-Difference Methods. Universiteit Leiden)

Jaco de Bakker (1967: Formal Definition of Programming Languages: with an Application to the Definition of ALGOL 60. Universiteit van Amsterdam)
Willem-Paul de Roever (1974: Recursive Program Schemes: Semantics and Proof Theory. Vrije Universiteit Amsterdam)
Paul Vitanyi (1978: Lindenmayer Systems: Structure, Languages, and Growth Functions. Vrije Universiteit Amsterdam) (Secondary advisor: Arto K. Salomaa)
Ronald Cramer (1997: Modular design of secure yet practical cryptographic protocols. Universiteit van Amsterdam) (Secondary advisor: Ivan Bjerre Damgård)
Peter Grünwald (1998: The minimum description length principle and reasoning under uncertainty. Universiteit van Amsterdam)

Anton Nijholt (1980: Context-Free Grammars : Covers, Normal Forms, and Parsing. Vrije Universiteit Amsterdam)
Giuseppe Scollo (1993: The Engineering of Logics. Universiteit Twente)
Ed Brinksma (1988: On the Design of Extended LOTOS; a Specification Language for Open Distributed Systems. Universiteit Twente) (Primary advisor: Christian Anton Vissers)

John-Jules Meyer (1985: Programming Calculi Based on Fixed Point Transformations: Semantics and Applications. Vrije Universiteit Amsterdam)
Wiebe van der Hoek (1992: Modalities for Reasoning about Knowledge and Quantities. Vrije Universiteit Amsterdam) (Secondary advisor: Johan van Benthem)

Joost Kok (1989: Semantic Models for Parallel Computation in Data Flow, Logic- and Object-Oriented Programming. Vrije Universiteit Amsterdam)
Jan Rutten (1989: A Parallel Object-Oriented Language: Design and Semantic Foundations. Vrije Universiteit Amsterdam)
Frank S. de Boer (1991: Reasoning about Dynamically Evolving Process Structures: A Proof Theory for the Parallel Object-0riented Language POOL. Vrije Universiteit Amsterdam)
Marcello Bonsangue (1996: Topological Dualities in Semantics. Vrije Universiteit Amsterdam) (Secondary advisor: Joost Kok)

Reinder van de Riet (1968: Algol 60 as Formula Manipulation Language. Universiteit van Amsterdam)
Peter Apers (1982: Query Processing and Data Allocation in Distributed Database Systems. Vrije Universiteit Amsterdam)
Arno Siebes (1990: On Complex Objects. Universiteit Twente) (Secondary advisor: Martin L. Kersten)

Martin L. Kersten (1985: A Model for a Secure Programming Environment. Vrije Universiteit Amsterdam) (Secondary advisor: Anthony Ira Wasserman)
Stefan Manegold (2002: Understanding, Modeling, and Improving Main-Memory Database Performance. Universiteit van Amsterdam)

Roel Wieringa (1990: Algebraic Foundations for Dynamic Conceptual Models. Vrije Universiteit Amsterdam)
Frances Brazier (1991: Design and Evaluation of a User Interface for Information Retrieval. Vrije Universiteit Amsterdam) (Primary advisor: Sipke D. Fokkema)

Hugo Brandt Corstius (1970: Exercises in Computational Linguistics. Universiteit van Amsterdam) (Secondary advisor: Frans Kruseman Aretz)
Maarten van Emden (1971: An Analysis of Complexity. Universiteit van Amsterdam)
Jonathan Schaeffer (1986: Experiments in Search and Knowledge. University of Waterloo) (Secondary advisor: Randy G. Goebel)

Peter van Emde Boas (1974: Abstract Resource-Bound Classes. Universiteit van Amsterdam) (Secondary advisor: Pieter Cornelis Baayen)
Arjen Lenstra (1984: Polynomial Time Algorithms for the Factorization of Polynomials. Universiteit van Amsterdam)
Leen Torenvliet (1986: Structural Concepts in Relativised Hierarchies. Universiteit van Amsterdam)
Harry Buhrman (1993: Resource Bounded Reductions. Universiteit van Amsterdam) (Primary advisor: Steven Elliot Homer)

Herman te Riele (1976: A Computational Study of Generalized Aliquot Sequences. Universiteit van Amsterdam)
Dick Grune (1982: On the Design of ALEPH. Universiteit van Amsterdam) (Secondary advisor: Cornelis H. A. Koster)


==== Brouwer / Van Dalen ====
Several of the students of Dirk van Dalen, a descendant of Brouwer, became the first Dutch theoretical computer scientists, which still has a strong focus on lambda calculus, rewrite systems and functional programming.
Luitzen Egbertus Jan Brouwer (1907: Over de grondslagen der wiskunde. Universiteit van Amsterdam)
Arend Heyting (1925: Intuitionistische axiomatiek der projectieve meetkunde. Universiteit van Amsterdam)
Dirk van Dalen (1963: Extension Problems in Intuitionistic Plane Projective Geometry. Universiteit van Amsterdam)
Henk Barendregt (1971: Some Extensional Terms for Combinatory Logics and Lambda-Calculi. Universiteit Utrecht)
Roel de Vrijer (1987: Surjective Pairing and Strong Normalization: Two Themes in Lambda Calculus. Universiteit van Amsterdam)
Pieter Hartel (1988: Performance Analysis of Storage Management in Combinator Graph Reduction. Universiteit van Amsterdam) (Primary advisor: Bob Hertzberger)
Mariangiola Dezani-Ciancaglini (1996: Logical Semantics for Concurrent Lambda-Calculus. Katholieke Universiteit Nijmegen) (Secondary advisor: Corrado Böhm)

Jan van Leeuwen (1972: Rule-Labeled Programs: A Study of a Generalization of Context-Free Grammars and Some Classes of Formal Languages. Universiteit Utrecht)
Mark Overmars (1983: The Design of Dynamic Data Structures. Universiteit Utrecht)
Mark de Berg (1992: Efficient Algorithms for Ray Shooting and Hidden Surface Removal. Universiteit Utrecht)
Marc van Kreveld (1992: New Results on Data Structures in Computational Geometry. Universiteit Utrecht)

Hans Bodlaender (1986: Distributed Computing - Structure and Complexity. Universiteit Utrecht)
Harry Wijshoff (1987: Data Organization in Parallel Computers. Universiteit Utrecht)
Gerard Tel (1989: The Structure of Distributed Algorithms. Universiteit Utrecht)

Jan Bergstra (1976: Computability and Continuity in Finite Types. Universiteit Utrecht)
Frits Vaandrager (1990: Algebraic Techniques for Concurrency and Their Application. Universiteit van Amsterdam)
Linda van der Gaag (1990: Probability-Based Models for Plausible Reasoning. Universiteit van Amsterdam)
Chris Verhoef (1990: Linear unary operators in process algebra. Universiteit van Amsterdam)
Jan Friso Groote (1991: Process Algebra and Structured Operational Semantics. Universiteit van Amsterdam)
Wan Fokkink (1994: Clocks, Trees and Stars in Process Theory. Universiteit van Amsterdam)
Jaco van de Pol (1996: Termination of Higher-Order Rewrite Systems. Universiteit Utrecht) (Secondary advisor: Marc Bezem)

Jan Willem Klop (1980: Combinatory reduction systems. Universiteit Utrecht)
Vincent van Oostrom (1994: Confluence for Abstract and Higher-Order Rewriting. Vrije Universiteit Amsterdam)

Albert Visser (1981: Aspects of Diagonalization & Provability. Universiteit Utrecht)
Wim Ruitenburg (1982: Intuitionistic Algebra, Theory and Sheaf Models. Universiteit Utrecht)
Catholijn Jonker (1994: Constraints and Negations in Logic Programming. Universiteit Utrecht) (Secondary advisor: Jan van Leeuwen)

Anne Sjerp Troelstra (1966: Intuitionistic General Topology. Universiteit van Amsterdam)
Gerard R. Renardel de Lavalette (1985: Theories with Type-free Application and Extended Bar Induction. Universiteit van Amsterdam)
Hans van Ditmarsch (2000: Knowledge Games. Rijksuniversiteit Groningen) (Secondary advisor: Johan van Benthem)

Ieke Moerdijk (1985: Topics in Intuitionism and Topos Theory. Universiteit van Amsterdam)
Marc Bezem (1986: Bar recursion and functionals of finite type. Universiteit Utrecht) (Secondary advisor: Dirk van Dalen)


=== Norway ===
Ole-Johan Dahl
Kristen Nygaard
Trygve Reenskaug


=== Poland ===
Grzegorz Rozenberg
Antoni W. Mazurkiewicz


=== Sweden ===
Bengt Nordström
Lennart Augustsson


=== United Kingdom ===
James H. Wilkinson


==== Edinburgh ====
Rod Burstall was one of the founders of the Laboratory for Foundations of Computer Science at the University of Edinburgh.
Rod Burstall (1956: Heuristic and Decision Tree Methods on Computers: Some Operational Research Applications. University of Birmingham)
Gordon Plotkin (1972: (dissertation title unknown). University of Edinburgh)
Glynn Winskel (1980: Events in Computation. University of Edinburgh)
Thomas Hildebrandt

Luca Cardelli (1982: An Algebraic Approach to Hardware Description and Verification. University of Edinburgh)
Eugenio Moggi (1988: The Partial Lambda Calculus. University of Edinburgh)
Philippa Gardner
Alex Simpson (computer scientist)

J Strother Moore (1973: Computational Logic: Structure Sharing and Proof of Program Properties. University of Edinburgh)
Panagiotis Manolios

Michael J. C. Gordon
Jeffrey Joyce

Don Sannella (1982: Semantics, Implementation and Pragmatics of Clear, a Program Specification Language. University of Edinburgh)
David Aspinall
Martin Hofmann (Secondary advisor: Gordon Plotkin)

Thorsten Altenkirch
Michael Mendler (Secondary advisor: Michael P. Fourman)
Masahito Hasegawa

Robin Popplestone
Alan Mycroft
Chistopher Longuet-Higgins, Richard Gregory, and Donald Mitchie founded the Department of Machine Intelligence and Perception at the University of Edinburgh
Christopher Longuet-Higgins (1947: Some problems in theoretical chemistry by the method of molecular orbitals. University of Oxford)
Geoffrey Hinton (1977: Relaxation and its role in vision. University of Edinburgh)
Mark Steedman (1973: The formal description of musical perception. University of Edinburgh)

Richard Gregory
Donald Mitchie
Gordon Plotkin (1972: Automatic methods of inductive inference. University of Edinburgh)
Austin Tate (1975: Using goal structure to direct search in a problem solver. University of Edinburgh)
Andrew Blake (scientist) (1983: Parallel computation in low level vision. University of Edinburgh)
Stephen Muggleton (1986: Inductive acquisition of expert knowledge. University of Edinburgh)


==== Cambridge ====
Maurice Wilkes was the first head of the University of Cambridge Computer Laboratory
Maurice Wilkes
Peter Wegner
Clement McGowan (Secondary advisor: Juris Hartmanis)
Daniel M. Berry (Secondary advisor: Clement McGowan)
Nancy Leveson (Secondary advisor: Anthony Ira Wasserman)

David Wheeler
Mathai Joseph
Roger Needham
Ross J. Anderson
David L. Tennenhouse
Peter G. Gyarmati

Robin Milner never did a Ph.D.
Robin Milner
Mads Tofte
Faron Moller
Chris Tofts


==== Oxford ====
Christopher Strachey was the first Professor of Computation at Oxford.
Christopher Strachey
Peter Landin (worked as the assistant of Strachey, did not do a PhD.)
Chris Wadsworth
Peter Mosses
Jens Palsberg

David Turner (Secondary advisor: Dana Scott)

Tony Hoare established the undergraduate computer science course and led the Oxford University Computing Laboratory for many years.
Tony Hoare
Cliff Jones (computer scientist)
Tobias Nipkow

Bill Roscoe
Peter Lauer (computer scientist)
Eike Best
Javier Esparza


==== Warwick ====
Mathai Joseph
Zhiming Liu
Paritosh Pandya

Mike Paterson
Leslie Valiant


== North America ==


=== Church ===
Siméon Poisson (1800: (dissertation title unknown). École Polytechnique)
Michel Chasles (1814: (dissertation title unknown). École Polytechnique)
H. A. Newton (1850: (dissertation title unknown). Yale University)
E. H. Moore (1885: Extensions of Certain Theorems of Clifford and Cayley in the Geometry of n Dimensions. Yale University)
Oswald Veblen (1903: A System of Axioms for Geometry. The University of Chicago)
Philip Franklin
Alan Perlis
Gary Lindstrom
David Parnas
Richard J. Lipton
Dan Boneh
Avi Wigderson

Alonzo Church (1927: Alternatives to Zermelo's Assumption. Princeton University)
Stephen Kleene (1934: A Theory of Positive Integers in Formal Logic. Princeton University)
Robert Lee Constable (1968: Extending and Refining Hierarchies of Computable Functions. University of Wisconsin-Madison)
Steven Muchnick
Uwe Frederik Pleban
Peter Lee

Kurt Mehlhorn
Edmund M. Clarke
Robert Harper (computer scientist) (1985: Aspects of the Implementation of Type Theory. Cornell University)
Benjamin C. Pierce (1991: Programming with Intersection Types and Bounded Polymorphism. Carnegie Mellon University) (Secondary advisor: John C. Reynolds)
Gregory Morrisett

John Rosser (1934: A Mathematical Logic without Variables. Princeton University)
Theodore Hailperin
Steven Orey
Elliott Mendelson
George Collins (logician)
Gerald Sacks

Alan Turing (1938: Systems of Logic Based on Ordinals. Princeton University)
Robin Gandy (1953: On Axiomatic Systems in Mathematics and Theories in Physics. University of Cambridge)

Hartley Rogers, Jr. (1952: Some Results on Definability and Decidability in Elementary Theories, (Parts I-V). Princeton University)
Patrick C. Fischer (1962: Theory of Provable Recursive Functions. Massachusetts Institute of Technology)
Arnold L. Rosenberg (1965: Nonwriting Extendsions of Finite Automata. Harvard University)
Dennis Ritchie (1968: Program Structure and Computational Complexity. Harvard University)
Albert R. Meyer (1972: On Complex Recursive Functions. Harvard University)
Nancy Lynch
Leonid Levin
Jeanne Ferrante
Charles Rackoff
Larry Stockmeyer
David Harel
Joseph Halpern
Daphne Koller

Robert L. Probert (1973: On the Complexity of Matrix Multiplication. Waterloo University)
Lawrence V. Saxton (1973: Input-Output Conventions and the Complexity of Transductions. Waterloo University)
Stan J. Thomas (1983: A Non-First-Normal-Form Relational Database Model. Vanderbilt University)
Dirk Van Gucht (1985: Theory of Unnormalized Relational Structures. Vanderbilt University)

David Park (1964: Set-Theoretic Constructions in Model Theory. Massachusetts Institute of Technology)
Mike Paterson
Ian Parberry
Leslie Valiant

John C. Mitchell (1984: Lambda Calculus Models of Typed Programming Languages. Massachusetts Institute of Technology)

Michael O. Rabin (1957: Recursive Unsolvability of Group Theoretic Problems. Princeton University)
Dana Scott (1958: Convergent Sequences of Complete Theories. Princeton University)
Jack Copeland
Angus Macintyre
Marko Petkovšek
Fred S. Roberts
Ketan Mulmuley
Michael Fourman (1974: Connections between category theory and logic. University of Oxford)

Peter B. Andrews (mathematician)
Frank Pfenning
Hongwei Xi Boston University

George David Birkhoff (1907: Asymptotic Properties of Certain Ordinary Differential Equations with Applications to Boundary Value and Expansion Problems. The University of Chicago)
Clarence Raymond Adams (1922: The General Theory of the Linear Partial q-Difference Equation and of the Linear Partial Difference Equation of the Intermediate Type. Harvard University)
Anthony Morse (1937: Convergence in Variation and Related Topics. Brown University)
Woody Bledsoe (1953: Separative Measures for Topological Spaces. University of California, Berkeley)
Robert S. Boyer (1971: Locking: A Restriction of Resolution. University of Texas at Austin)


=== Harvard ===
Alfred North Whitehead (1884: (dissertation title unknown). University of Cambridge)
Willard Van Orman Quine (1932: The Logic of Sequences: A Generalization of Principia Mathematica. Harvard University)
Hao Wang (academic) (1948: An Economical Ontology for Classical Arithmetic. Harvard University)
Stephen Cook (1966: On the Minimum Computation Time of Functions. Harvard University)


=== Hopcroft / Lefschetz ===
Felix Klein (1868: Über die Transformation der allgemeinen Gleichung des zweiten Grades zwischen Linien-Koordinaten auf eine kanonische Form. Rheinische Friedrich-Wilhelms-Universität Bonn)
Ferdinand von Lindemann (1873: Über unendlich kleine Bewegungen und über Kraftsysteme bei allgemeiner projektivischer Maßbestimmung. Friedrich-Alexander-Universität Erlangen-Nürnberg)
Arnold Sommerfeld (1891: Die willkürlichen Functionen in der mathematischen Physik. Universität Königsberg)
Ernst Guillemin (1926: Theorie der Frequenzvervielfachung durch Eisenkernkoppelung. Ludwig-Maximilians-Universität München)
Robert Fano (1947: Theoretical Limitations on the Broadband Matching of Arbitrary Impedances. Massachusetts Institute of Technology)
William Linvill (1949: Analysis and Design of Sampled-Data Control Systems. Massachusetts Institute of Technology)
Bernard Widrow (1956: A Study of Rough Amplitude Quantization by Means of Nyquist Sampling Theory. Massachusetts Institute of Technology)
Richard Mattson (1962: The Analysis and Synthesis of Adaptive Systems Which Use Networks of Threshold Elements. Stanford University)
John Hopcroft (1964: Synthesis of Threshold Logic Networks. Stanford University)
Alfred Aho (1967: Indexed Grammars: An Extension of Context Free Grammars. Princeton University)
Zvi Galil (1975: The Complexity of Resolution Procedures for Theorem Proving in the Propositional Calculus. Cornell University)
David Eppstein (1989: Efficient Algorithms for Sequence Analysis with Concave and Convex Gap Costs. Columbia University)

Merrick L. Furst (1980: A Subexponenial Algorithm for Trivalent Isomorphism. Cornell University)
Andrew Appel (1985: Compile-Time Evaluation and Code Generation in Semantics-Directed Compilers. Carnegie Mellon University) (Primary advisor: Ravi Sethi)

Oskar Bolza (1886: Über die Reduction hyperelliptischer Integrale erster Ordnung und erster Gattung auf elliptische, insbesondere über die Reduction durch eine Transformation vierten Grades. Georg-August-Universität Göttingen)
John Hector McDonald (1900: Concerning the System of the Binary Cubic and Quadratic with Application to the Reduction of Hyperelliptic Integrals to Elliptic Integrals by a Transformation of Order Four. The University of Chicago)
Waldemar Trjitzinsky (1926: The Elliptic Cylinder Differential Equation. University of California, Berkeley)
Richard Hamming (1942: Some Problems in the Boundary Value Theory of Linear Differential Equations. University of Illinois at Urbana-Champaign)

William Edward Story (1875: On the Algebraic Relations Existing Between the Polars of a Binary Quantic. Universität Leipzig)
Solomon Lefschetz (1911: On the Existence of Loci with Given Singularities. Clark University)
Albert W. Tucker (1932: An Abstract Approach to Manifolds. Princeton University)
Marvin Minsky (1954: Theory of Neural-Analog Reinforcement Systems and Its Application to the Brain Model Problem. Princeton University)
Manuel Blum (1964: A Machine-Independent Theory of the Complexity of Recursive Functions. Massachusetts Institute of Technology)
John Gill, III (1972: Probabilistic Turing Machines and Complexity of Computation. University of California, Berkeley)
Gary Miller (computer scientist) (1975: Riemann's Hypothesis and Tests for Primality. University of California, Berkeley)
F. Thomson Leighton (1981: Layouts for the Shuffle-Exchange Graph and Lower Bound Techniques for VLSI. Massachusetts Institute of Technology)
Peter Shor (1985: Random Planar Matching and Bin Packing. Massachusetts Institute of Technology)

Dana Angluin (1976: An Application of the Theory of Computational Complexity to the Study of Inductive Inference. University of California, Berkeley)
Leonard Adleman (1976: Number-Theoretic Aspects of Computational Complexity. University of California, Berkeley)
Michael Sipser (1980: Nondeterminism and the Size of Two-Way Finite Automata. University of California, Berkeley)
Lance Fortnow (1989: Complexity-Theoretic Aspects of Interactive Proof Systems. Massachusetts Institute of Technology)
Daniel Spielman (1995: Computationally Efficient Error-Correcting Codes and Holographic Proofs. Massachusetts Institute of Technology)

Jeffrey Shallit (1983: Metric Theory of Pierce Expansions. University of California, Berkeley)
Silvio Micali (1983: Randomness Versus Hardness. University of California, Berkeley)
Eric Bach (1984: Analytic Methods in the Analysis and Design of Number-Theoretic Algorithms. University of California, Berkeley)
Shafrira Goldwasser (1984: Probabilitstic Encryption: Theory and Applications. University of California, Berkeley)
Johan Håstad

Vijay Vazirani (1984: Maximum Matchings without Blossoms. University of California, Berkeley)
Umesh Vazirani (1986: Randomness, Adversaries and Computation. University of California, Berkeley)
Madhu Sudan (1992: Efficient Checking of Polynomials and Proofs and the Hardness of Approximation Problems. University of California, Berkeley)
Sanjeev Arora (1994: Probabilistic Checking of Proofs and Hardness of Approximation Problems. University of California, Berkeley)
Andris Ambainis (2001: Quantum Entanglement, Quantum Communication and the Limits of Quantum Computing. University of California, Berkeley)
Scott Aaronson (2004: Limits on Efficient Computation in the Physical World. University of California, Berkeley)

Steven Rudich (1989: Limits on the Provable Consequences of One-Way Functions. University of California, Berkeley)
Moni Naor (1989: Implicit Storage Schemes for Quick Retrieval. University of California, Berkeley)
Ronitt Rubinfeld (1990: A Mathematical Theory of Self-Checking, Self-Testing and Self-Correcting Programs. University of California, Berkeley)
Sampath Kannan (1990: Program Checkers for Algebraic Problems. University of California, Berkeley)
Russell Impagliazzo (1992: Pseudo-Random Generators for Probabilistic Algorithms and for Cryptography. University of California, Berkeley)
Mor Harchol-Balter (1996: Network Analysis without Exponentiality Assumptions. University of California, Berkeley)
Luis von Ahn (2005: Human Computation. Carnegie Mellon University)
Ryan Williams (computer scientist) (2007: Algorithms and Resource Requirements for Fundamental Problems. Carnegie Mellon University)

Gerald Sussman (1973: A Computational Model of Skill Acquisition. Massachusetts Institute of Technology)
Drew McDermott (1976: Flexibility and Efficiency in a Computer Program for Designing Circuits. Massachusetts Institute of Technology)
Guy Steele, Jr. (1980: The Definition and Implementation of a Computer Programming Language Based on Constraints. Massachusetts Institute of Technology)
Philip Wadler (1984: Listlessness Is Better than Laziness: An Algorithm that Transforms Applicative Programs to Eliminate Intermediate Lists. Carnegie Mellon University) (Primary advisor: Nico Habermann)

Ken Forbus (1984: Qualitative Process Theory. Massachusetts Institute of Technology)

Scott Fahlman (1977: A System for Representing and Using Real-World Knowledge. Massachusetts Institute of Technology) (Secondary advisor: Gerald Sussman)

John McCarthy (computer scientist) (1951: Projection Operators and Partial Differential Equations. Princeton University)
Barbara Huberman Liskov (1968: A Program to Play Chess End Games. Stanford University)


=== California Institute of Technology ===


==== Knuth ====
Søren Rasmusen ((year unknown): (dissertation title unknown). )
Bernt Michael Holmboe ((year unknown): (dissertation title unknown). )
Carl Anton Bjerknes ((year unknown): (dissertation title unknown). )
Marius Sophus Lie ((year unknown): (dissertation title unknown). )
Elling Bolt Holst ((year unknown): (dissertation title unknown). )
Axel Thue (1889: (dissertation title unknown). University of Christiania)
Thoralf Skolem (1926: Einige Sätze über ganzzahlige Lösungen gewisser Gleichungen und Ungleichungen. Universitetet i Oslo)
Øystein Ore (1924: (dissertation title unknown). Universitetet i Oslo)
Grace Hopper (1934: New Types of Irreducibility Criteria. Yale University)
Marshall Hall, Jr. (1936: An Isomorphism Between Linear Recurring Sequences and Algebraic Rings. Yale University)
Donald Knuth (1963: Finite Semifields and Projective Planes. California Institute of Technology)
Vaughan Pratt (1972: Shellsort and Sorting Networks. Stanford University)
David Harel (1978: Logics of Programs: Axiomatics and Descriptive Power. Massachusetts Institute of Technology)

Jeffrey Scott Vitter (1980: Analysis of Coalesced Hashing. Stanford University)


==== Hartmanis ====
Eric Temple Bell
Morgan Ward
Robert P. Dilworth
Juris Hartmanis
Edward Reingold
Dexter Kozen
Hubie Chen

Neil Immerman
Allan Borodin
David G. Kirkpatrick
Ian Munro (computer scientist)


=== Floyd ===
Bob Floyd never received a PhD, although he worked closely with Donald Knuth on The Art of Computer Programming.
Bob Floyd ((year unknown): (dissertation title unknown). The University of Chicago)
Zohar Manna (1968: Termination of Algorithms. Carnegie Mellon University)

Adi Shamir (1977: The Fixed Points Of Recursive Definitions. Weizmann Institute of Science)
Martín Abadi (1987: Temporal Theorem Proving. Stanford University)
Shmuel Katz (computer scientist) (1977: Invariants And The Logical Analysis Of Programs. Weizmann Institute of Science)
Nachum Dershowitz (1979: The Evolution of Programs. Weizmann Institute of Science)

Jay Earley (1968: An Efficient Context-Free Parsing Algorithm. Carnegie Mellon University)
Robert Tarjan (1972: An Efficient Planarity Algorithm. Stanford University)
Daniel Sleator (1981: An 
  
    
      
        O
        (
        n
        m
        log
        ⁡
        n
        )
      
    
    {\displaystyle O(nm\log n)}
   Algorithm for Maximum Network Flow. Princeton University)
John R. Gilbert (1981: Graph Separator Theorems and Sparse Gaussian Elimination. Stanford University)
Raimund Seidel (1987: Output-Size Sensitive Algorithms for Constructive Problems in Computational Geometry. Cornell University)
Nina Amenta (1994: Helly Theorems and Generalized Linear Programming. University of California, Berkeley)

Monika Henzinger (1993: Fully Dynamic Graph Algorithms and Their Data Structures. Princeton University)

Ronald Rivest (1974: Analysis of Associative Retrieval Algorithms. Stanford University)
Ron Pinter (1982: The Impact of Layer Assignment Methods on Layout Algorithms for Integrated Circuits. Massachusetts Institute of Technology)
Avrim Blum (1991: Algorithms for Approximate Graph Coloring. Massachusetts Institute of Technology)
Robert Schapire (1991: The Design and Analysis of Efficient Learning Algorithms. Massachusetts Institute of Technology)

David Plaisted (1976: Theorem Proving and Semantic Trees. Stanford University)


=== Ullman ===


=== Hilbert ===
David Hilbert (1885, University of Königsberg)
Hugo Steinhaus
Mark Kac
Harry Kesten
Ed Granirer
Tony Lau
Maria Klawe

Hermann Weyl
Saunders MacLane
Roger Conant Lyndon
Calvin Creston Elgot

Anil Nerode
Bob Soare
Richard Tenney

Micael Morley
Terry Millar
Mark Manasse

Kurt Schutte
Wolfgang Maass

Wilhelm Ackermann
Richard Courant
Haskell Curry
Hellmuth Kneser
Reinhold Baer
Earl J. Schweppe
Elizabeth A. Unger

Erich Hecke (1910, University of Göttingen)
Heinrich Behnke (1923, University of Hamburg)
Hans Langmaack (1960, University of Münster)
Ernst-Rüdiger Olderog (1981, University of Kiel)


=== Aiken ===
Emory Leon Chaffee ((year unknown): (dissertation title unknown). )
Howard Aiken ((year unknown): (dissertation title unknown). )
Gerrit Blaauw ((year unknown): (dissertation title unknown). )
Christian Vissers ((year unknown): (dissertation title unknown). )
Hendrik Brinksma ((year unknown): (dissertation title unknown). )

Fred Brooks (1956: The Analytic Design of Automatic Data Processing Systems. )
Anthony Oettinger (1954: A Study for the Design of an Automatic Dictionary. )
William Hines Bossert ((year unknown): (dissertation title unknown). )
Gerald J. Popek ((year unknown): (dissertation title unknown). )
John Heidemann ((year unknown): (dissertation title unknown). )

Sheila Greibach (1963: Inverses of Phrase Structure Generators. Harvard University)
Ronald Book ( : Grammars with Time Functions. )
Michael J. Fischer ((year unknown): (dissertation title unknown). )
Mitchell Wand ((year unknown): (dissertation title unknown). )
Michael Martin Hammer ((year unknown): (dissertation title unknown). )
Dennis McLeod ((year unknown): (dissertation title unknown). )

Jean Gallier (1978: Semantics and Correctness of Classes of Deterministic and Nondeterministic Recursive Programs. University of California, Los Angeles)
Wayne Snyder (1988: Complete Sets of Transformations for General Unification. University of Pennsylvania)

Richard Karp (1959: Some Applications of Logical Syntax to Digital Computer Programming. Harvard University)
Robert Keller (computer scientist) (1970: Closures of Parallel Program Schemata. University of California, Berkeley)
Paul Hudak (1982: Object and Task Reclamation in Distributed Applicative Processing Systems. University of Utah)
Kai Li ((year unknown): (dissertation title unknown). )

Kellogg Booth ((year unknown): (dissertation title unknown). )
Ron Shamir ((year unknown): (dissertation title unknown). )
Rajeev Motwani (1988: Probabilistic Analysis of Matching and network flow Algorithms. )

Eugene Lawler (1963: Some Aspects of Discrete Mathematical Programming. )
David Shmoys (1984: Approximation Algorithms for Problems in Sequencing, Scheduling, and Communication Network Design. )
Philip N. Klein ((year unknown): (dissertation title unknown). )
Ramamurthy Ravi ((year unknown): (dissertation title unknown). )

Clifford Stein ((year unknown): (dissertation title unknown). )

Lee J. White (1967: A Parametric Study of Matchings and Coverings in Weighted Graphs. University of Michigan)
Sargur Srihari (1976: Comparative Evaluation of Stored Pattern Classifiers. The Ohio State University)
Venu Govindaraju (1992: Locating Faces in Newspaper Photographs. University at Buffalo)


=== Stanford ===
George Forsythe
Ramon E. Moore
Cleve Moler
Jack Dongarra
Charles F. Van Loan

William M. McKeeman
Eric Hehner (Primary advisor: David Barkley Wortman)

Richard P. Brent (Primary advisor: Gene Howard Golub)
J. Alan George
Gaston Gonnet
Ricardo Baeza-Yates

Michael Alexander Malcolm
David Cheriton
Willy Zwaenepoel
John Carter
Lixin Zhang

Mootaz Elnozahy
Cliff Mercer

David S. Johnson
Peter Keleher


=== Other ===
Harold Stone (computer scientist)
Harold N. (Hal) Gabow
Matthias Stallmann
Manfred K. Warmuth
Yoav Freund

Franco P. Preparata
Roberto Tamassia

Georg Kreisel
Richard Statman

Herbert A. Simon
Allen Newell
Robert Kendall Lindsay
Terrence Wendall Pratt
Daniel Paul Friedman
Matthias Felleisen
Shriram Krishnamurthi

Charles Bachman
Edwin Boring
Cooper Harold Langford
Arthur Burks
John Henry Holland
Kenneth A De Jong
Edgar F. Codd
Stephen T. Hedetniemi (Primary advisor: Frank Harary)
Donald F. Stanat
Jon Bentley
Charles Leiserson (Primary advisor: Hsiang-Tsung Kung)
Guy Blelloch
Thomas H. Cormen

Gul Agha (Secondary advisor: Carl Hewitt)

Robert ""Bob"" Allen Paige
Friedrich ""Fritz"" Henglein

Carl Gustav Hempel
John Alan Robinson


== See also ==
List of computer scientists


== References ==


== Further reading ==
Johnson, David S. (Summer 1984). ""The genealogy of theoretical computer science: a preliminary report"". ACM SIGACT News. 16 (2): 36–49. doi:10.1145/1008959.1008960. 
Parberry, Ian; Johnson, David S. (June 1995). James Ford; Fillia Makedon; Samuel Rebelsky, eds. ""The SIGACT Theoretical Computer Science Genealogy: Preliminary Report"" (PDF). Proceedings of DAGS 95, ""Electronic Publishing and the Information Superhighway"". Boston, MA: Birkhauser: 197–205. 
Coonce, Harry B. (December 2004). ""Computer science and the mathematics genealogy project"". ACM SIGACT News. 35 (4). 


== External links ==
Software Engineering Academic Genealogy
SIGACT Theoretical Computer Science Genealogy (archived on 13 October 2007)
Mathematics Genealogy Project
AI Genealogy Project
Computer Engineering Academic Genealogy by Yuan Xie, Pennsylvania State University"
14,List of computer scientists,6834,39023,"This is a list of computer scientists, people who do work in computer science, in particular researchers and authors.
Some persons notable as programmers are included here because they work in research as well as program. A few of these people pre-date the invention of the digital computer; they are now regarded as computer scientists because their work can be seen as leading to the invention of the computer. Others are mathematicians whose work falls within what would now be called theoretical computer science, such as complexity theory and algorithmic information theory.


== A ==
Wil van der Aalst – business process management, process mining, Petri nets
Scott Aaronson – quantum computing and complexity theory
Hal Abelson – intersection of computing and teaching
Serge Abiteboul – database theory
Samson Abramsky – game semantics
Leonard Adleman – RSA, DNA computing
Manindra Agrawal – polynomial-time primality testing
Luis von Ahn – human-based computation
Alfred Aho – compilers book, the 'a' in AWK
Frances E. Allen – compiler optimization
Gene Amdahl – supercomputer developer, founder of Amdahl Corporation
David P. Anderson – volunteer computing
Andrew Appel – compiler of text books
Cecilia R. Aragon – inventor of the treap, human-centered data science
Bruce Arden – programming language compilers (GAT, MAD), virtual memory architecture, MTS
Sanjeev Arora – PCP theorem
Winifred ""Tim"" Alice Asprey – established the computer science curriculum at Vassar College
John Vincent Atanasoff – computer pioneer, creator of ABC or Atanasoff Berry Computer


== B ==
Charles Babbage (1791–1871) – invented first mechanical computer called the supreme mathematician
Charles Bachman – American computer scientist, known for Integrated Data Store
Roland Carl Backhouse – mathematics of program construction
John Backus – FORTRAN, Backus–Naur form, first complete compiler
David F. Bacon – Programming languages, garbage collection
David A. Bader
Victor Bahl
Anthony James Barr – SAS System
Jean Bartik (1924–2011) – one of the first computer programmers, on ENIAC (1946), one of the first Vacuum tube computers, back when ""programming"" involved using cables, dials, and switches to physically rewire the machine; worked with John Mauchly toward BINAC (1949), EDVAC (1949), UNIVAC (1951) to develop early ""stored program"" computers
Andrew Barto
Rudolf Bayer – B-tree
James C. Beatty (1934–1978) – compiler optimization, super-computing
Gordon Bell (born 1934) – computer designer DEC VAX, author: Computer Structures
Steven M. Bellovin – network security
Tim Berners-Lee – World Wide Web
Daniel J. Bernstein – qmail, software as protected speech
Peter Bernus
Abhay Bhushan
Dines Bjørner – Vienna Development Method (VDM), RAISE
Gerrit Blaauw – one of the principal designers of the IBM System 360 line of computers
Sue Black
David Blei
Dorothy Blum – National Security Agency
Lenore Blum – complexity
Manuel Blum – cryptography
Barry Boehm – software engineering economics, spiral development
Corrado Bohm – author of the structured program theorem
Kurt Bollacker
Jeff Bonwick – inventor of slab allocation and ZFS
Grady Booch – Unified Modeling Language, Object Management Group
George Boole – Boolean logic
Andrew Booth – developed the first rotating drum storage device
Kathleen Booth – developed the first assembly language
Anita Borg (1949–2003) – American computer scientist, founder of Anita Borg Institute for Women and Technology
Bert Bos – Cascading Style Sheets
Mikhail Botvinnik – World Chess Champion, computer scientist and electrical engineer, pioneer of early expert system AI,inventor of Computer chess
Jonathan Bowen – Z notation, formal methods
Stephen R. Bourne – Bourne shell, portable ALGOL 68C compiler
Harry Bouwman (born 1953) – Dutch Information systems researcher, and Professor at the Åbo Akademi University
Robert S. Boyer – string searching, ACL2 theorem prover
Karlheinz Brandenburg – Main mp3 contributor
Jack E. Bresenham – early computer-graphics contributions, including Bresenham's algorithm
Sergey Brin – co-founder of Google
David J. Brown – unified memory architecture, binary compatibility
Per Brinch Hansen (surname ""Brinch Hansen"") – concurrency
Sjaak Brinkkemper – methodology of product software development
Fred Brooks – System 360, OS/360, The Mythical Man-Month, No Silver Bullet
Rod Brooks
Michael Butler – Event-B


== C ==
Lee Calcote – cloud computing
Tracy Camp – wireless computing
Martin Campbell-Kelly – history of computing
Rosemary Candlin
Bryan Cantrill – inventor of DTrace
Luca Cardelli – objects
Edwin Catmull – computer graphics
Vinton Cerf – Internet, TCP/IP
Gregory Chaitin
Zhou Chaochen – duration calculus
Peter Chen – entity-relationship model, data modeling, conceptual model
Leonardo Chiariglione, founder of MPEG
Alonzo Church – mathematics of combinators, lambda calculus
Alberto Ciaramella - speech recognition, patent informatics
Edmund M. Clarke – model checking
John Cocke – RISC
Edgar F. Codd (1923–2003) – formulated the database relational model
Jacques Cohen – computer science professor
Simon Colton – computational creativity
Alain Colmerauer – Prolog
Paul Justin Compton – Ripple Down Rules
Gordon Cormack – co-inventor of dynamic Markov compression
Stephen Cook – NP-completeness
James Cooley – Fast Fourier transform (FFT)
Danese Cooper – Open Source Software
Fernando J. Corbató – Compatible Time-Sharing System (CTSS), Multics
Kit Cosper - Open Source Software
Patrick Cousot – abstract interpretation
Ingemar Cox – digital watermarking
Seymour Cray – Cray Research, supercomputer
Nello Cristianini – machine learning, pattern analysis, artificial intelligence
Jon Crowcroft – networking
W. Bruce Croft
Glen Culler – interactive computing, computer graphics, high performance computing
Haskell Curry


== D ==
Luigi Dadda – designer of the Dadda multiplier
Ole-Johan Dahl – Simula
Ryan Dahl – founder of node.js project
Andries van Dam – computer graphics, hypertext
Samir Das – Wireless Networks, Mobile Computing, Vehicular ad hoc network, Sensor Networks, Mesh networking, Wireless ad hoc network
Christopher J. Date – proponent of database relational model
Jeff Dean – Bigtable, MapReduce, Spanner of Google
Erik Demaine – computational origami
Tom DeMarco
Richard DeMillo – computer security, software engineering, educational technology
Dorothy E. Denning – computer security
Peter J. Denning – identified the use of an operating system's working set and balance set, President of ACM
Michael Dertouzos – Director of Massachusetts Institute of Technology (MIT) Laboratory for Computer Science (LCS) from 1974 to 2001
Alexander Dewdney
Vinod Dham – P5 Pentium processor
Jan Dietz (born 1945) (decay constant) – information systems theory and Design & Engineering Methodology for Organizations
Whitfield Diffie (born 1944) (linear response function) – public key cryptography, Diffie–Hellman key exchange
Edsger Dijkstra – algorithms, Goto considered harmful, semaphore (programming)
Alan Dix – literally wrote the book on human–computer interaction
Jack Dongarra – linear algebra high performance computing (HCI)
Marco Dorigo – ant colony optimization
Paul Dourish – human computer interaction
Charles Stark Draper (1901–1987) – designer of Apollo Guidance Computer, ""father of inertial navigation"", MIT professor
Susan Dumais – information retrieval


== E ==
Peter Eades – graph drawing
Annie J. Easley
Wim Ebbinkhuijsen – COBOL
John Presper Eckert – ENIAC
Brendan Eich – JavaScript, Mozilla
Philip Emeagwali – supercomputing
E. Allen Emerson – model checking
Douglas Engelbart – tiled windows, hypertext, computer mouse
David Eppstein
Andrey Ershov
Don Estridge (1937–1985) – led development of original IBM Personal Computer (PC); known as ""father of the IBM PC""
Oren Etzioni – MetaCrawler, Netbot
Christopher Riche Evans
David C. Evans – computer graphics
Shimon Even


== F ==
Scott Fahlman
Edward Feigenbaum – intelligence
Edward Felten – computer security
Tim Finin
Raphael Finkel
Donald Firesmith
Gary William Flake
Tommy Flowers – Colossus computer
Robert Floyd – NP-completeness
Sally Floyd – Internet congestion control
Lawrence J. Fogel – Evolutionary programming
James D. Foley
Ken Forbus
Lance Fortnow
Martin Fowler
Herbert W. Franke
Edward Fredkin
Yoav Freund
Daniel P. Friedman
Ping Fu


== G ==
Richard Gabriel
V. K. Govindan
Zvi Galil
Bernard Galler – MAD (programming language)
Hector Garcia-Molina
Michael Garey – NP-completeness
Hugo de Garis
Bill Gates – co-founder of Microsoft
David Gelernter
Charles Geschke
Zoubin Ghahramani
Sanjay Ghemawat
Juan E. Gilbert – Human-Centered Computing
Lee Giles – CiteSeer
Seymour Ginsburg – formal languages, automata theory, AFL theory, database theory
Robert L. Glass
Kurt Gödel – computability – not a computer scientist per se, but his work was invaluable in the field
Joseph Goguen
Adele Goldberg – Smalltalk
Andrew V. Goldberg -- algorithms, algorithm engineering
Ian Goldberg – cryptographer, off-the-record messaging
Oded Goldreich – cryptography, computational complexity theory
Shafi Goldwasser – cryptography, computational complexity theory
Gene Golub – Matrix computation
Martin Charles Golumbic – algorithmic graph theory
Gastón Gonnet – co-founder of Waterloo Maple Inc.
James Gosling – NeWS, Java
Paul Graham – Viaweb, On Lisp, Arc
Robert M. Graham – programming language compilers (GAT, MAD), virtual memory architecture, Multics
Susan L. Graham – compilers, programming environments
Jim Gray – database
Sheila Greibach – Greibach normal form, AFL theory
Ralph Griswold – SNOBOL
Bill Gropp – Message Passing Interface, PETSc
Tom Gruber
Ramanathan V. Guha – RDF, Netscape, RSS, Epinions
Neil J. Gunther – computer performance analysis, capacity planning
Peter G. Gyarmati – adaptivity in operating systems and networking


== H ==
Philipp Matthäus Hahn – mechanical calculator
Eldon C. Hall – Apollo Guidance Computer
Wendy Hall
Joseph Halpern
Margaret Hamilton – ultra-reliable software design
Richard Hamming – Hamming code, founder of the Association for Computing Machinery
Jiawei Han – data mining
Juris Hartmanis – computational complexity theory
Johan Håstad – computational complexity theory
Les Hatton – software failure and vulnerabilities
Igor Hawryszkiewycz, (born 1948), American computer scientist and organizational theorist
He Jifeng – provably correct systems
Eric Hehner – predicative programming, formal methods, quote notation
Martin Hellman – encryption
Gernot Heiser – development of L4 and founder of OK Labs
James Hendler – Semantic Web
John L. Hennessy – computer architecture
Andrew Herbert
Carl Hewitt
Danny Hillis – Connection Machine
Geoffrey Hinton
Julia Hirschberg
C. A. R. Hoare – logic, rigor, Communicating sequential processes (CSP)
Betty Holberton – ENIAC programmer, developed the first Sort Merge Generator
John Henry Holland – genetic algorithms
Herman Hollerith (1860–1929) – invented recording of data on a machine readable medium, using punched cards
Gerard Holzmann – software verification, logic model checking (SPIN)
John Hopcroft – compilers
Admiral Grace Hopper (1906–1992) – developed early compilers: FLOW-Matic, COBOL; worked on UNIVAC; gave speeches on computer history, where she gave out nano-seconds
Eric Horvitz – artificial intelligence
Alston Householder
Paul Hudak (1952–2015) – Haskell programming language design
David A. Huffman (1925–1999) – Huffman coding, used in data compression
John Hughes – structuring computations with arrows; QuickCheck randomized program testing framework; Haskell programming language design.
Watts Humphrey (1927–2010) – Personal Software Process (PSP), Software quality, Team Software Process (TSP)


== I ==
Jean Ichbiah – Ada
Dan Ingalls – Smalltalk, BitBlt, Lively Kernel
Mary Jane Irwin
Kenneth E. Iverson – APL, J


== J ==
Ivar Jacobson – Unified Modeling Language, Object Management Group
Anil K. Jain (born 1948)
Ramesh Jain
Jonathan James
David S. Johnson
Stephen C. Johnson
Cliff Jones – Vienna Development Method (VDM)
Michael I. Jordan
Mathai Joseph
Aravind K. Joshi
Bill Joy (born 1954) – Sun Microsystems, BSD UNIX, vi, csh
Dan Jurafsky – Natural language processing


== K ==
William Kahan – numerical analysis
Robert E. Kahn – TCP/IP
Avinash Kak – digital image processing
Poul-Henning Kamp – inventor of GBDE, FreeBSD Jails, Varnish cache
David Karger
Richard Karp – NP-completeness
Narendra Karmarkar – Karmarkar's algorithm
Marek Karpinski – NP optimization problems
Alan Kay – Dynabook, Smalltalk, overlapping windows
Neeraj Kayal – AKS primality test
John George Kemeny – BASIC
Ken Kennedy – compiling for parallel and vector machines
Brian Kernighan (born 1942) – Unix, the 'k' in AWK
Carl Kesselman – grid computing
Gregor Kiczales – CLOS, reflection, aspect-oriented programming
Peter T. Kirstein – Internet
Stephen Cole Kleene – Kleene closure, recursion theory
Dan Klein – Natural language processing, Machine translation
Leonard Kleinrock – ARPANET, queueing theory, packet switching, hierarchical routing
Donald Knuth – The Art of Computer Programming, MIX/MMIX, TeX, literate programming
Andrew Koenig – C++
Daphne Koller – Artificial intelligence, bayesian network
Michael Kölling – BlueJ
Andrey Nikolaevich Kolmogorov – algorithmic complexity theory
Janet L. Kolodner – case-based reasoning
David Korn – Korn shell
Kees Koster – ALGOL 68
Robert Kowalski – logic programming
John Koza – genetic programming
John Krogstie – SEQUAL framework
Joseph Kruskal – Kruskal's algorithm
Thomas E. Kurtz (born 1928) – BASIC programming language; Dartmouth College computer professor


== L ==
Monica S. Lam
Leslie Lamport – algorithms for distributed computing, LaTeX
Butler W. Lampson
Peter J. Landin
Tom Lane
Börje Langefors
Chris Lattner – creator of Swift (programming language) and LLVM compiler infrastructure
Steve Lawrence
Edward D. Lazowska
Joshua Lederberg
Manny M Lehman
Charles E. Leiserson – cache-oblivious algorithms, provably good work-stealing, coauthor of Introduction to Algorithms
Douglas Lenat – artificial intelligence, Cyc
Yann LeCun
Rasmus Lerdorf – PHP
Max Levchin – Gausebeck-Levchin test and PayPal
Leonid Levin – computational complexity theory
Kevin Leyton-Brown – artificial intelligence
J.C.R. Licklider
David Liddle
John Lions – Lions Book
Richard J. Lipton – computational complexity theory
Barbara Liskov – programming languages
Darrell Long – Computer data storage
Patricia D. Lopez – broadening participation in computing
Gillian Lovegrove
Ada Lovelace – first programmer
Eugene Luks
Nancy Lynch


== M ==
Nadia Magnenat Thalmann – computer graphics, virtual actor
Tom Maibaum
Zohar Manna – fuzzy logic
James Martin – information engineering
Robert C. Martin (Uncle Bob) – software craftsmanship
John Mashey
Yuri Matiyasevich – solving Hilbert's tenth problem
Yukihiro Matsumoto – Ruby (programming language)
John Mauchly (1907–1980) – designed ENIAC, first general-purpose electronic digital computer, as well as EDVAC, BINAC and UNIVAC I, the first commercial computer; worked with Jean Bartik on ENIAC and Grace Murray Hopper on UNIVAC
Derek McAuley – ubiquitous computing, computer architecture, networking
John McCarthy – Lisp (programming language), artificial intelligence
Andrew McCallum
Douglas McIlroy – pipes
Chris McKinstry – artificial intelligence, Mindpixel
Marshall Kirk McKusick – BSD, Berkeley Fast File System
Lambert Meertens – ALGOL 68, ABC (programming language)
Bertrand Meyer – Eiffel (programming language)
Silvio Micali – cryptography
Robin Milner – ML (programming language)
Jack Minker – database logic
Marvin Minsky – artificial intelligence, perceptrons, Society of Mind
Tom M. Mitchell
Paul Mockapetris – Domain Name System (DNS)
Cleve Moler – numerical analysis, MATLAB
John P. Moon – inventor, Apple Inc.
Charles H. Moore – Forth programming language
Edward F. Moore – Moore machine
Gordon Moore – Moore's law
J Strother Moore – string searching, ACL2 theorem prover
Hans Moravec – robotics
Carroll Morgan
Robert Tappan Morris – Morris worm
Joel Moses – Macsyma
Rajeev Motwani – randomized algorithm
Stephen Muggleton – Inductive Logic Programming
Alan Mycroft – programming languages


== N ==
Mihai Nadin – anticipation research
Makoto Nagao – machine translation, natural language processing, digital library
Frieder Nake – pioneered computer arts
Peter Naur – BNF, ALGOL 60
Roger Needham – computer security
James G. Nell – GERAM
Bernard de Neumann – massively parallel autonomous cellular processor, software engineering research
Klara Dan von Neumann (1911-1963) – early computers, ENIAC programmer and control designer
John von Neumann (1903–1957) – early computers, von Neumann machine, set theory, functional analysis, mathematics pioneer, linear programming, quantum mechanics
Allen Newell – artificial intelligence, Computer Structures
Max Newman – Colossus, MADM
Andrew Ng – artificial intelligence, machine learning, robotics
Nils Nilsson – artificial intelligence
G.M. Nijssen – NIAM
Tobias Nipkow – proof assistance
Jerre Noe – computerized banking
Peter Nordin – artificial intelligence, genetic programming, evolutionary robotics
Donald Norman – user interfaces, usability
Peter Norvig – artificial intelligence, Director of Research at Google
George Novacky – Assistant Department Chair and Senior Lecturer in Computer Science, Assistant Dean of CAS for Undergraduate Studies at University of Pittsburgh
Kristen Nygaard – Simula


== O ==
T. William Olle – Ferranti Mercury
Steve Omohundro
John Ousterhout – Tcl programming Language
Mark Overmars – game programming
Martin Odersky – Scala programming Language
Severo Ornstein
John O'Sullivan- wifi


== P ==
Larry Page – co-founder of Google
Sankar Pal
Paritosh Pandya
Christos Papadimitriou
David Parnas – information hiding, modular programming
Yale Patt – Instruction-level parallelism, speculative architectures
David A. Patterson
Mihai Pătraşcu – data structures
Lawrence Paulson – ML
Randy Pausch (1960–2008) – Human-Computer interaction, Carnegie professor, ""Last Lecture""
Juan Pavón – software agents
Judea Pearl – artificial intelligence, search algorithms
David Pearson – CADES, computer graphics
Alan Perlis – Programming Pearls
Radia Perlman – spanning tree protocol
Pier Giorgio Perotto – designer of Programma 101, arguably the first personal computer
Rózsa Péter – recursive function theory
Simon Peyton Jones – functional programming
Roberto Pieraccini – Speech technologist, technical director at Jibo Inc.
Gordon Plotkin
Amir Pnueli – temporal logic
Willem van der Poel – computer graphics, robotics, geographic information systems, imaging, multimedia, virtual environments, games
Emil Post – mathematics
Jon Postel – Internet
Franco Preparata – computer engineering, computational geometry, parallel algorithms, computational biology
William H. Press – numerical algorithms


== R ==
Rapelang Rabana
Roberto Ierusalimschy – Lua (programming language)
Michael O. Rabin – nondeterministic machine
Dragomir R. Radev – Natural language processing, Information Retrieval
T. V. Raman – accessibility, Emacspeak
Brian Randell – dependability
Raj Reddy – AI
David P. Reed
Trygve Reenskaug – Model-view-controller (MVC) software architecture pattern
John C. Reynolds
Joyce K. Reynolds – Internet
Bernard Richards – medical informatics
Martin Richards – BCPL
Adam Riese
C. J. van Rijsbergen
Dennis Ritchie – C (programming language), UNIX
Ron Rivest – RSA, MD5, RC4
Colette Rolland – REMORA methodology, meta modelling
Azriel Rosenfeld
Douglas T. Ross – Structured Analysis and Design Technique
Guido van Rossum – Python (programming language)
Winston W. Royce – Waterfall model
Rudy Rucker – mathematician, writer, educator
Steven Rudich – complexity theory, cryptography
Jeff Rulifson
James Rumbaugh – Unified Modeling Language, Object Management Group
Peter Ružička – Slovak computer scientist and mathematician


== S ==
George Sadowsky
Gerard Salton – information retrieval
Jean E. Sammet – programming languages
Claude Sammut – artificial-intelligence researcher
Carl Sassenrath – operating systems, programming languages, Amiga, REBOL
Mahadev Satyanarayanan – file systems, distributed systems, mobile computing, pervasive computing
Walter Savitch – discovery of complexity class NL, Savitch's theorem, natural language processing, mathematical linguistics
Jonathan Schaeffer
Wilhelm Schickard – one of the first calculating machines
Steve Schneider – formal methods, security
Bruce Schneier – cryptography, security
Fred B. Schneider – concurrent and distributed computing
Dana Scott – domain theory
Michael L. Scott – programming languages, algorithms, distributed computing
Ravi Sethi – compilers, 2nd Dragon Book
Nigel Shadbolt
Adi Shamir – RSA, cryptanalysis
Claude Shannon – information theory
David E. Shaw – computational finance, computational biochemistry, parallel architectures
Cliff Shaw – systems programmer, artificial intelligence
Scott Shenker – networking
Ben Shneiderman – human-computer interaction, information visualization
Edward H. Shortliffe – MYCIN (medical diagnostic expert system)
Joseph Sifakis – model checking
Herbert A. Simon – artificial intelligence
Munindar P. Singh – multiagent systems, software engineering, artificial intelligence, social networks
Ramesh Sitaraman – helped build Akamai's high performance network
Daniel Sleator – splay tree, amortized analysis
Aaron Sloman – artificial intelligence and cognitive science
Arne Sølvberg – information modelling
Brian Cantwell Smith – reflection (computer science), 3lisp
Steven Spewak – Enterprise architecture planning
Carol Spradling
Robert Sproull
Rohini Kesavan Srihari – Information Retrieval, Text Analytics, Multilingual Text Mining
Sargur Srihari – Pattern Recognition, Machine learning, Computational criminology, CEDAR-FOX
Maciej Stachowiak – GNOME, Safari, WebKit
Richard Stallman (born 1953) – GNU Project
Ronald Stamper
Richard E. Stearns – computational complexity theory
Guy L. Steele, Jr. – Scheme, Common Lisp
Thomas Sterling – creator of Beowulf clusters
W. Richard Stevens (1951–1999) – author of books, including TCP/IP Illustrated and Advanced Programming in the Unix Environment
Larry Stockmeyer – computational complexity, distributed computing
Salvatore Stolfo - computer security, machine learning
Michael Stonebraker – relational database practice and theory
Olaf Storaasli – finite element machine, linear algebra, high performance computing
Christopher Strachey – denotational semantics
Bjarne Stroustrup – C++
Madhu Sudan – computational complexity theory, coding theory
Gerald Jay Sussman – Scheme
Bert Sutherland – graphics, Internet
Ivan Sutherland – graphics
Mario Szegedy – complexity theory, quantum computing


== T ==
Roberto Tamassia – computational geometry, computer security
Andrew S. Tanenbaum – operating systems, MINIX
Bernhard Thalheim – conceptual modelling foundation
Éva Tardos
Gábor Tardos
Robert Tarjan – splay tree
Valerie Taylor
Mario Tchou – italian engineer, of Chinese descent, leader of Olivetti Elea project
Jaime Teevan
Shang-Hua Teng – analysis of algorithms
Larry Tesler – human-computer interaction, graphical user interface, Apple Macintosh
Avie Tevanian – Mach kernel team, NeXT, Mac OS X
Charles P. Thacker – Xerox Alto, Microsoft Research
Daniel Thalmann – computer graphics, virtual actor
Ken Thompson – Unix
Sebastian Thrun – AI researcher and inventor of autonomous driving
Walter F. Tichy – RCS
Seinosuke Toda – computation complexity, recipient of 1998 Gödel Prize
Linus Torvalds – Linux kernel, Git
Godfried Toussaint – computational geometry – computational music theory
Gloria Townsend
Edwin E. Tozer – business information systems
Joseph F Traub – computational complexity of scientific problems
John Tukey – founder of FFT algorithm, Box plot, Exploratory Data Analysis and Coining the term 'bit'
Murray Turoff – computer-mediated communication
Alan Turing (1912–1954) – British computing pioneer, Turing machine, algorithms, cryptology, computer architecture


== U ==
Jeffrey D. Ullman – compilers, databases, complexity theory
Umar Saif


== V ==
Leslie Valiant – computational complexity theory, computational learning theory
Vladimir Vapnik – pattern recognition, computational learning theory
Moshe Vardi – professor of computer science at Rice University
Dorothy Vaughan
Umesh Vazirani
Vijay Vazirani
Manuela M. Veloso
François Vernadat – enterprise modeling
Richard Veryard – enterprise modeling
Paul Vitanyi – Kolmogorov complexity, Information distance, Normalized compression distance, Normalized Google distance
Andrew Viterbi – Viterbi algorithm
Jeffrey Scott Vitter – external memory algorithms, compressed data structures, data compression, databases
Paul Vixie – DNS, BIND, PAIX, Internet Software Consortium, MAPS, DNSBL


== W ==
David Wagner – security, cryptography
Larry Wall – Perl programming language
David Waltz
James Z. Wang
Steve Ward
Manfred K. Warmuth – computational learning theory
David H. D. Warren – AI, logic programming, Prolog, the 'w' in WAM
Kevin Warwick – artificial intelligence
Jan Weglarz
Peter Wegner – object-oriented programming, interaction (computer science)
Peter J. Weinberger – programming language design, the 'w' in AWK
Mark Weiser – ubiquitous computing
Joseph Weizenbaum – artificial intelligence, ELIZA
David Wheeler – EDSAC, subroutines
Franklin H. Westervelt – use of computers in engineering education, conversational use of computers, MTS, ARPANET, distance learning
Steve Whittaker – human computer interaction, computer support for cooperative work, social media
Jennifer Widom – nontraditional data management
Gio Wiederhold – database management systems
Norbert Wiener – Cybernetics
Adriaan van Wijngaarden – Dutch pioneer; ARRA, ALGOL
Mary Allen Wilkes – LINC developer, assembler-linker designer
Maurice Vincent Wilkes – microprogramming, EDSAC
Yorick Wilks – computational linguistics, artificial intelligence
James H. Wilkinson – numerical analysis
Sophie Wilson – ARM architecture
Shmuel Winograd – Coppersmith–Winograd algorithm
Terry Winograd – artificial intelligence, SHRDLU
Patrick Winston – artificial intelligence
Niklaus Wirth – Pascal, Modula, Oberon (programming language)
Neil Wiseman – computer graphics
Dennis E. Wisnosky – Integrated Computer-Aided Manufacturing (ICAM), IDEF
Stephen Wolfram – Mathematica
Mike Woodger – Pilot ACE, ALGOL 60, Ada (programming language)
Beatrice Helen Worsley – wrote the first PhD dissertation involving modern computers; was one of the people who wrote Transcode
Steve Wozniak – engineered first generation personal computers at Apple Computer
Jie Wu – computer networks
William Wulf – compilers


== Y ==
Mihalis Yannakakis
Andrew Chi-Chih Yao
John Yen
Edward Yourdon – Structured Systems Analysis and Design Method
Moti Yung
Yash Khalkar


== Z ==
Lotfi Zadeh – fuzzy logic
Hans Zantema – termination analysis
Arif Zaman – pseudo-random number generator
Shlomo Zilberstein – artificial intelligence, anytime algorithms, automated planning, and decentralized POMDPs
Jill Zimmerman – James M. Beall Professor of Mathematics and Computer Science at Goucher College
Konrad Zuse – German pioneer of hardware and software
Mark Zuckerberg – Founder of Facebook


== See also ==


== References ==


== External links ==
CiteSeer list of the most cited authors in computer science
Computer scientists with h-index >= 40"
15,Computational linguistics,5561,38820,"Computational linguistics is an interdisciplinary field concerned with the statistical or rule-based modeling of natural language from a computational perspective, as well as the study of appropriate computational approaches to linguistic questions.
Traditionally, computational linguistics was performed by computer scientists who had specialized in the application of computers to the processing of a natural language. Today, computational linguists often work as members of interdisciplinary teams, which can include regular linguists, experts in the target language, and computer scientists. In general, computational linguistics draws upon the involvement of linguists, computer scientists, experts in artificial intelligence, mathematicians, logicians, philosophers, cognitive scientists, cognitive psychologists, psycholinguists, anthropologists and neuroscientists, among others.
Computational linguistics has theoretical and applied components. Theoretical computational linguistics focuses on issues in theoretical linguistics and cognitive science, and applied computational linguistics focuses on the practical outcome of modeling human language use.
The Association for Computational Linguistics defines computational linguistics as:
...the scientific study of language from a computational perspective. Computational linguists are interested in providing computational models of various kinds of linguistic phenomena.


== Origins ==
Computational linguistics is often grouped within the field of artificial intelligence, but actually was present before the development of artificial intelligence. Computational linguistics originated with efforts in the United States in the 1950s to use computers to automatically translate texts from foreign languages, particularly Russian scientific journals, into English. Since computers can make arithmetic calculations much faster and more accurately than humans, it was thought to be only a short matter of time before they could also begin to process language. Computational and quantitative methods are also used historically in attempted reconstruction of earlier forms of modern languages and subgrouping modern languages into language families. Earlier methods such as lexicostatistics and glottochronology have been proven to be premature and inaccurate. However, recent interdisciplinary studies which borrow concepts from biological studies, especially gene mapping, have proved to produce more sophisticated analytical tools and more trustful results.
When machine translation (also known as mechanical translation) failed to yield accurate translations right away, automated processing of human languages was recognized as far more complex than had originally been assumed. Computational linguistics was born as the name of the new field of study devoted to developing algorithms and software for intelligently processing language data. The term ""computational linguistics"" itself was first coined by David Hays, founding member of both the Association for Computational Linguistics and the International Committee on Computational Linguistics. When artificial intelligence came into existence in the 1960s, the field of computational linguistics became that sub-division of artificial intelligence dealing with human-level comprehension and production of natural languages.
In order to translate one language into another, it was observed that one had to understand the grammar of both languages, including both morphology (the grammar of word forms) and syntax (the grammar of sentence structure). In order to understand syntax, one had to also understand the semantics and the lexicon (or 'vocabulary'), and even something of the pragmatics of language use. Thus, what started as an effort to translate between languages evolved into an entire discipline devoted to understanding how to represent and process natural languages using computers.
Nowadays research within the scope of computational linguistics is done at computational linguistics departments, computational linguistics laboratories, computer science departments, and linguistics departments. Some research in the field of computational linguistics aims to create working speech or text processing systems while others aim to create a system allowing human-machine interaction. Programs meant for human-machine communication are called conversational agents.


== Approaches ==
Just as computational linguistics can be performed by experts in a variety of fields and through a wide assortment of departments, so too can the research fields broach a diverse range of topics. The following sections discuss some of the literature available across the entire field broken into four main area of discourse: developmental linguistics, structural linguistics, linguistic production, and linguistic comprehension.


=== Developmental approaches ===
Language is a cognitive skill which develops throughout the life of an individual. This developmental process has been examined using a number of techniques, and a computational approach is one of them. Human language development does provide some constraints which make it harder to apply a computational method to understanding it. For instance, during language acquisition, human children are largely only exposed to positive evidence. This means that during the linguistic development of an individual, only evidence for what is a correct form is provided, and not evidence for what is not correct. This is insufficient information for a simple hypothesis testing procedure for information as complex as language, and so provides certain boundaries for a computational approach to modeling language development and acquisition in an individual.
Attempts have been made to model the developmental process of language acquisition in children from a computational angle, leading to both statistical grammars and connectionist models. Work in this realm has also been proposed as a method to explain the evolution of language through history. Using models, it has been shown that languages can be learned with a combination of simple input presented incrementally as the child develops better memory and longer attention span. This was simultaneously posed as a reason for the long developmental period of human children. Both conclusions were drawn because of the strength of the neural network which the project created.
The ability of infants to develop language has also been modeled using robots in order to test linguistic theories. Enabled to learn as children might, a model was created based on an affordance model in which mappings between actions, perceptions, and effects were created and linked to spoken words. Crucially, these robots were able to acquire functioning word-to-meaning mappings without needing grammatical structure, vastly simplifying the learning process and shedding light on information which furthers the current understanding of linguistic development. It is important to note that this information could only have been empirically tested using a computational approach.
As our understanding of the linguistic development of an individual within a lifetime is continually improved using neural networks and learning robotic systems, it is also important to keep in mind that languages themselves change and develop through time. Computational approaches to understanding this phenomenon have unearthed very interesting information. Using the Price Equation and Pólya urn dynamics, researchers have created a system which not only predicts future linguistic evolution, but also gives insight into the evolutionary history of modern-day languages. This modeling effort achieved, through computational linguistics, what would otherwise have been impossible.
It is clear that the understanding of linguistic development in humans as well as throughout evolutionary time has been fantastically improved because of advances in computational linguistics. The ability to model and modify systems at will affords science an ethical method of testing hypotheses that would otherwise be intractable.


=== Structural approaches ===
In order to create better computational models of language, an understanding of language’s structure is crucial. To this end, the English language has been meticulously studied using computational approaches to better understand how the language works on a structural level. One of the most important pieces of being able to study linguistic structure is the availability of large linguistic corpora, or samples. This grants computational linguists the raw data necessary to run their models and gain a better understanding of the underlying structures present in the vast amount of data which is contained in any single language. One of the most cited English linguistic corpora is the Penn Treebank. Derived from widely-different sources, such as IBM computer manuals and transcribed telephone conversations, this corpus contains over 4.5 million words of American English. This corpus has been primarily annotated using part-of-speech tagging and syntactic bracketing and has yielded substantial empirical observations related to language structure.
Theoretical approaches to the structure of languages have also been developed. These works allow computational linguistics to have a framework within which to work out hypotheses that will further the understanding of the language in a myriad of ways. One of the original theoretical theses on internalization of grammar and structure of language proposed two types of models. In these models, rules or patterns learned increase in strength with the frequency of their encounter. The work also created a question for computational linguists to answer: how does an infant learn a specific and non-normal grammar (Chomsky Normal Form) without learning an overgeneralized version and getting stuck? Theoretical efforts like these set the direction for research to go early in the lifetime of a field of study, and are crucial to the growth of the field.
Structural information about languages allows for the discovery and implementation of similarity recognition between pairs of text utterances. For instance, it has recently been proven that based on the structural information present in patterns of human discourse, conceptual recurrence plots can be used to model and visualize trends in data and create reliable measures of similarity between natural textual utterances. This technique is a strong tool for further probing the structure of human discourse. Without the computational approach to this question, the vastly complex information present in discourse data would have remained inaccessible to scientists.
Information regarding the structural data of a language is available for English as well as other languages, such as Japanese. Using computational methods, Japanese sentence corpora were analyzed and a pattern of log-normality was found in relation to sentence length. Though the exact cause of this lognormality remains unknown, it is precisely this sort of intriguing information which computational linguistics is designed to uncover. This information could lead to further important discoveries regarding the underlying structure of Japanese, and could have any number of effects on the understanding of Japanese as a language. Computational linguistics allows for very exciting additions to the scientific knowledge base to happen quickly and with very little room for doubt.
Without a computational approach to the structure of linguistic data, much of the information that is available now would still be hidden under the vastness of data within any single language. Computational linguistics allows scientists to parse huge amounts of data reliably and efficiently, creating the possibility for discoveries unlike any seen in most other approaches.


=== Production approaches ===
The production of language is equally as complex in the information it provides and the necessary skills which a fluent producer must have. That is to say, comprehension is only half the problem of communication. The other half is how a system produces language, and computational linguistics has made some very interesting discoveries in this area.

In a now famous paper published in 1950 Alan Turing proposed the possibility that machines might one day have the ability to ""think"". As a thought experiment for what might define the concept of thought in machines, he proposed an ""imitation test"" in which a human subject has two text-only conversations, one with a fellow human and another with a machine attempting to respond like a human. Turing proposes that if the subject cannot tell the difference between the human and the machine, it may be concluded that the machine is capable of thought. Today this test is known as the Turing test and it remains an influential idea in the area of artificial intelligence.

One of the earliest and best known examples of a computer program designed to converse naturally with humans is the ELIZA program developed by Joseph Weizenbaum at MIT in 1966. The program emulated a Rogerian psychotherapist when responding to written statements and questions posed by a user. It appeared capable of understanding what was said to it and responding intelligently, but in truth it simply followed a pattern matching routine that relied on only understanding a few keywords in each sentence. Its responses were generated by recombining the unknown parts of the sentence around properly translated versions of the known words. For example, in the phrase ""It seems that you hate me"" ELIZA understands ""you"" and ""me"" which matches the general pattern ""you [some words] me"", allowing ELIZA to update the words ""you"" and ""me"" to ""I"" and ""you"" and replying ""What makes you think I hate you?"". In this example ELIZA has no understanding of the word ""hate"", but it is not required for a logical response in the context of this type of psychotherapy.
Some projects are still trying to solve the problem which first started computational linguistics off as its own field in the first place. However, the methods have become more refined and clever, and consequently the results generated by computational linguists have become more enlightening. In an effort to improve computer translation, several models have been compared, including hidden Markov models, smoothing techniques, and the specific refinements of those to apply them to verb translation. The model which was found to produce the most natural translations of German and French words was a refined alignment model with a first-order dependence and a fertility model[16]. They also provide efficient training algorithms for the models presented, which can give other scientists the ability to improve further on their results. This type of work is specific to computational linguistics, and has applications which could vastly improve understanding of how language is produced and comprehended by computers.
Work has also been done in making computers produce language in a more naturalistic manner. Using linguistic input from humans, algorithms have been constructed which are able to modify a system's style of production based on a factor such as linguistic input from a human, or more abstract factors like politeness or any of the five main dimensions of personality. This work takes a computational approach via parameter estimation models to categorize the vast array of linguistic styles we see across individuals and simplify it for a computer to work in the same way, making human-computer interaction much more natural.


==== Text-based interactive approach ====
Many of the earliest and simplest models of human-computer interaction, such as ELIZA for example, involve a text-based input from the user to generate a response from the computer. By this method, words typed by a user trigger the computer to recognize specific patterns and reply accordingly, through a process known as keyword spotting.


==== Speech-based interactive approach ====
Recent technologies have placed more of an emphasis on speech-based interactive systems. These systems, such as Siri of the iOS operating system, operate on a similar pattern-recognizing technique as that of text-based systems, but with the former, the user input is conducted through speech recognition. This branch of linguistics involves the processing of the user's speech as sound waves and the interpreting of the acoustics and language patterns in order for the computer to recognize the input.


=== Comprehension approaches ===
Much of the focus of modern computational linguistics is on comprehension. With the proliferation of the internet and the abundance of easily accessible written human language, the ability to create a program capable of understanding human language would have many broad and exciting possibilities, including improved search engines, automated customer service, and online education.
Early work in comprehension included applying Bayesian statistics to the task of optical character recognition, as illustrated by Bledsoe and Browing in 1959 in which a large dictionary of possible letters were generated by ""learning"" from example letters and then the probability that any one of those learned examples matched the new input was combined to make a final decision. Other attempts at applying Bayesian statistics to language analysis included the work of Mosteller and Wallace (1963) in which an analysis of the words used in The Federalist Papers was used to attempt to determine their authorship (concluding that Madison most likely authored the majority of the papers).
In 1971 Terry Winograd developed an early natural language processing engine capable of interpreting naturally written commands within a simple rule governed environment. The primary language parsing program in this project was called SHRDLU, which was capable of carrying out a somewhat natural conversation with the user giving it commands, but only within the scope of the toy environment designed for the task. This environment consisted of different shaped and colored blocks, and SHRDLU was capable of interpreting commands such as ""Find a block which is taller than the one you are holding and put it into the box."" and asking questions such as ""I don't understand which pyramid you mean."" in response to the user's input. While impressive, this kind of natural language processing has proven much more difficult outside the limited scope of the toy environment. Similarly a project developed by NASA called LUNAR was designed to provide answers to naturally written questions about the geological analysis of lunar rocks returned by the Apollo missions. These kinds of problems are referred to as question answering.
Initial attempts at understanding spoken language were based on work done in the 1960s and 1970s in signal modeling where an unknown signal is analyzed to look for patterns and to make predictions based on its history. An initial and somewhat successful approach to applying this kind of signal modeling to language was achieved with the use of hidden Markov models as detailed by Rabiner in 1989. This approach attempts to determine probabilities for the arbitrary number of models that could be being used in generating speech as well as modeling the probabilities for various words generated from each of these possible models. Similar approaches were employed in early speech recognition attempts starting in the late 70s at IBM using word/part-of-speech pair probabilities.
More recently these kinds of statistical approaches have been applied to more difficult tasks such as topic identification using Bayesian parameter estimation to infer topic probabilities in text documents.


== Applications ==
Modern computational linguistics is often a combination of studies in computer science and programming, math, particularly statistics, language structures, and natural language processing. Combined, these fields most often lead to the development of systems that can recognize speech and perform some task based on that speech. Examples include speech recognition software, such as Apple's Siri feature, spellcheck tools, speech synthesis programs, which are often used to demonstrate pronunciation or help the disabled, and machine translation programs and websites, such as Google Translate and Word Reference.
Computational linguistics can be especially helpful in situations involving social media and the Internet. For example, filters in chatrooms or on website searches require computational linguistics. Chat operators often use filters to identify certain words or phrases and deem them inappropriate so that users cannot submit them. Another example of using filters is on websites. Schools use filters so that websites with certain keywords are blocked from children to view. There are also many programs in which parents use Parental controls to put content filters in place. Computational linguists can also develop programs that group and organize content through Social media mining. An example of this is Twitter, in which programs can group tweets by subject or keywords. Computational linguistics is also used for document retrieval and clustering. When you do an online search, documents and websites are retrieved based on the frequency of unique labels related to what you typed into a search engine. For instance, if you search ""red, large, four-wheeled vehicle,"" with the intention of finding pictures of a red truck, the search engine will still find the information desired by matching words such as ""four-wheeled"" with ""car"".


== Subfields ==
Computational linguistics can be divided into major areas depending upon the medium of the language being processed, whether spoken or textual; and upon the task being performed, whether analyzing language (recognition) or synthesizing language (generation).
Speech recognition and speech synthesis deal with how spoken language can be understood or created using computers. Parsing and generation are sub-divisions of computational linguistics dealing respectively with taking language apart and putting it together. Machine translation remains the sub-division of computational linguistics dealing with having computers translate between languages. The possibility of automatic language translation, however, has yet to be realized and remains a notoriously hard branch of computational linguistics.
Some of the areas of research that are studied by computational linguistics include:
Computational complexity of natural language, largely modeled on automata theory, with the application of context-sensitive grammar and linearly bounded Turing machines.
Computational semantics comprises defining suitable logics for linguistic meaning representation, automatically constructing them and reasoning with them
Computer-aided corpus linguistics, which has been used since the 1970s as a way to make detailed advances in the field of discourse analysis
Design of parsers or chunkers for natural languages
Design of taggers like POS-taggers (part-of-speech taggers)
Machine translation as one of the earliest and most difficult applications of computational linguistics draws on many subfields.
Simulation and study of language evolution in historical linguistics/glottochronology.


== Legacy ==
The subject of computational linguistics has had a recurring impact on popular culture:
The 1983 film WarGames features a young computer hacker who interacts with an artificially intelligent supercomputer.
A 1997 film, Conceiving Ada, focuses on Ada Lovelace, considered one of the first computer scientists, as well as themes of computational linguistics.
Her, a 2013 film, depicts a man's interactions with the ""world's first artificially intelligent operating system.""
The 2014 film The Imitation Game follows the life of computer scientist Alan Turing, developer of the Turing Test.
The 2015 film Ex Machina centers around human interaction with artificial intelligence.


== See also ==


== References ==


== Further reading ==


== External links ==
Association for Computational Linguistics (ACL)
ACL Anthology of research papers
ACL Wiki for Computational Linguistics

CICLing annual conferences on Computational Linguistics
Computational Linguistics – Applications workshop
Free online introductory book on Computational Linguistics at the Wayback Machine (archived January 25, 2008)
Language Technology World
Resources for Text, Speech and Language Processing
The Research Group in Computational Linguistics"
16,Community informatics,816023,38542,"Community informatics (CI) is an interdisciplinary field that is concerned with using information and communication technology (ICT) to empower members of communities and support their social, cultural, and economic development.  Community informatics may contribute to enhancing democracy, supporting the development of social capital, and building well connected communities; moreover, it is probable that such similar actions may let people experience new positive social change. In community informatics, there are several considerations which are the social context, shared values, distinct processes that are taken by members in a community, and social and technical systems. It is formally located as an academic discipline within a variety of academic faculties including information science, information systems, computer science, planning, development studies, and library science among others and draws on insights on community development from a range of backgrounds and disciplines. It is an interdisciplinary approach interested in using ICTs for different forms of community action, as distinct from pure academic study about ICT effects.


== Background ==
Most humans live in communities. In some urban areas, community and neighborhood are conflated but this may be a limited definition. Communities are defined as people coming together in pursuit of common aims or shared practices through any means, including physical, electronic, and social networks. They proliferate even while the ability to define them is amorphous.
Cultures ensure their growth and survival by continuing the norms and mores that are the bases of their way of life. Communities can use the infrastructure of ICTs as a method of continuing cultures within the context of the Internet and the World Wide Web. Once a cultural identity is defined within the context of these technologies, it can be replicated and disseminated through various means, including the sharing of information through websites, applications, databases, and file sharing. In this manner, a group that defines its cultural identity within the construct of technology infrastructure is empowered to hold valuable exchanges within the spheres of economics, political power, high and popular culture, education, and entertainment.
Since the inception of the Internet and the World Wide Web, we have seen the exponential growth of enterprises ranging from electronic commerce, social networking, entertainment and education, as well as a myriad of other contrivances and file exchanges that allow for an ongoing cultural enrichment through technology. However, there has been a general lag as to which populations can benefit through these services through impediments such as geographic location, a lack of funds, gaps in technology and the expertise and skills that are required to operate these systems.
To date there has been very considerable investment in supporting the electronic development of business communities, one-to-many social tools (for example, corporate intranets, or purpose-built exchange and social networking services such as eBay, or Myspace), or in developing applications for individual use. There is far less understanding, or investment in human-technical networks and processes that are intended to deliberately result in social change or community change, particularly in communities for whom electronic communication is secondary to having an adequate income or social survival.
The communal dimension (and focus of Community Informatics) results in a strong interest in studying and developing strategies for how ICTs can enable and empower those living in physical communities. This is particularly the case in those communities where ICT access is done communally, through Telecentres, information kiosks, community multimedia centres, and other technologies. This latter set of approaches has become of very considerable interest as Information and Communications Technology for Development (ICT4D) has emerged as significant element in strategic (and funding) approaches to social and economic development in Less Developed Countries. ICT4D initiatives have been undertaken by public, NGO and private sector agencies concerned with development such as the United Nations Development Program, the World Bank, the Swiss Agency for Development and Cooperation (SDC), the MS Swaminathan Research Foundation; have emerged as a key element in the poverty alleviation component of the UN's Millennium Development Goals; and as important directions for private sector investment both from a market perspective (cf. the ""Bottom of the Pyramid"") and from companies concerned with finding a delivery channel for goods and services into rural and low income communities.
There is thus growing interest in Community Informatics as an approach to understanding of how different ICTs can enable and empower marginalized communities to achieve their collective goals.


== Understanding communities ==
It is crucial to know how communities are formed and evolved and how the participation to a community occurs and differs while formation process. Understanding the nature of communities and the participation process will surely ensure designing and implemenameting a successful ICT solution that benefits members of community while communicating with each other or performing certain tasks.  The following points include a brief description of the nature of each potential community formation.


=== Community as a place ===
A group of people may form a community according to the place in which they live, enjoy staying, and work. They usually participate in communities within these three places since they gather together on consistent basis so that it is highly expected that such community is formed. Beside the home and the work gathering, people usually like to spend their time at informal places called third places in where they meet their new or old friends or have a chance to meet new people.


=== Community as a socio-spatial entity ===
A group of people may form a community as they have frequent direct interactions or live in close proximity to each other. The members of such community may have strong bond and focused common goals which give them a higher status over other communities. Moreover, as the number of the members increases, the community may become reputable and has a higher status over other communities.


=== Community as links between people ===
A group of people may form a community as they have common shared identity. People may form such community to support and advocate common shared values, morals or norms in which they believe. Such a community may have a set of symbols and be associated with a status over other communities. The inclusion and the exclusion to such community depend on whether or not a member share the same identity with others in the community. For instance, people who descend from one origin may form a community in which only people from that origin can join the community even though they do not know each other in advance.


=== Community of interests ===
A group of people may form a community as they have similar affinity for a particular activity, experience, or subject. The geographical location is not necessary while forming such community, and the inclusion and the exclusion to such community depends on whether a new member has that affinity or not.


=== Communities linked to life stage ===
A group of people may form a community if they share a similar experience in a distinct life stage. The experience could be related to the members themselves or to their relatives, such as their children. For instance, parents of elementary school children may form a community in which they care about their children while in school. As it is mentioned in the previous type of community formation, the members of such community have a common interest which is caring about their children while in school. This type of community may persist over time, but the inclusion and the exclusion to it may happen consistently as people are no longer in that distinct life stage.


=== Communities of practice ===
A group of people who share a similar profession may form a community in which they work to attain their goals and advance in their profession. Three important concepts are considered while forming community of practice which are mutual engagement, joint enterprise, and shared repertoire. In a community of practice, the members have to be mutually engaged with each other by establishing collaborative relationships that will allow them to willingly work on certain joint activities. In the second concept which is joint enterprise, the members of a community of practice are supposed to discuss and agree upon the work responsibilities so that they can work in harmony, and each member knows his responsibility and his expected contributions to the community. In addition to these two concepts, the members of the community of practice have a shared repertoire of procedures or ways to perform certain tasks. They usually agree upon these procedures and practices that they establish and develop over time.


== Conceptual approaches ==
As an academic discipline, CI can be seen as a field of practice in applied information and communications technology. Community informatics is a technique for looking at economic and social development within the construct of technology—online health communities, social networking websites, cultural awareness and enhancement through online connections and networks, electronic commerce, information exchanges, as well as a myriad of other aspects that contributes to creating a personal and group identity. The term was brought to prominence by Michael Gurstein. Michael Gurstein says that community informatics is a technology strategy or discipline that connects at the community level economic and social development with the emergence of community and civic networks, electronic commerce, online participation, self-help, virtual health communities, ""Tele-centres"", as well as other types of online institutions and corporations. He brought out the first representative collection of academic papers, although others, such as Brian Loader and his colleagues at the University of Teesside used the term in the mid-1990s.
CI brings together the practices of community development and organization, and insights from fields such as sociology, planning, computer science, critical theory, women's studies, library and information sciences, management information systems, and management studies. Its outcomes—community networks and community-based ICT-enabled service applications—are of increasing interest to grassroots organizations, NGOs and civil society, governments, the private sector, and multilateral agencies among others. Self-organized community initiatives of all varieties, from different countries, are concerned with ways to harness ICT for social capital, poverty alleviation and for the empowerment of the ""local"" in relation to its larger economic, political and social environments. Some claim it is potentially a form of 'radical practice'.
Community informatics may in fact, not gel as a single field within the academy, but remain a convenient locale for interdisciplinary activity, drawing upon many fields of social practice and endeavour, as well as knowledge of community applications of technology. However, one can begin to see the emergence of a postmodern ""trans-discipline"" presenting a challenge to existing disciplinary ""stove-pipes"" from the perspectives of the rapidly evolving fields of technology practice, technology change, public policy and commercial interest. Whether or not such a ""trans-discipline"" can maintain its momentum remains to be seen given the incertitude about the boundaries of such disciplines as community development.
Furthermore, there is a continuing disconnect between those coming from an Information Science perspective for whom social theories, including general theories of organisation are unfamiliar or seemingly irrelevant to solving complex 'technical' problems, and those whose focus is upon the theoretical and practical issues around working with communities for democratic and social change 
Given that many of those most actively involved in early efforts were academics, it is only inevitable that a process of ""sense-making"" with respect to these efforts would follow from ""tool-making"" efforts. These academics, and some community activists connected globally through the medium.
A first formal meeting of researchers with an academic interest in these initiatives was held in conjunction with the 1999 Global Community Networking Conference in Buenos Aires, Argentina. This meeting began the process of linking community-based ICT initiatives in developed countries with initiatives undertaken in developing countries, which were often part of larger economic and social development programmes funded by agencies such as the UN Development Programme, World Bank, or the International Development Research Centre. Academics and researchers interested in ICT efforts in developed countries began to see common and overlapping interests with those interested in similar work in less developed countries. For example, the issue of sustainability as a technical, cultural, and economic problem for community informatics has resulted in a special issue of the Journal of Community Informatics  as well as the subject of ongoing conferences in Prato, Italy and other conferences in South Africa.
In Canada, the beginnings of CI can be recognized from various trials in community networking in the 1970s (Clement 1981). An essential development occurred in the 1990s, due to the change of cost of computers and modems. Moreover, examples of using computer networking to initiate and enhance social activities was acknowledged by women's groups (Balka 1992) and by the labor movement (Mazepa 1997). 


=== Social informatics beyond an immediate concern for a community ===

Social informatics refers to the body of research and study that examines social aspects of computerization—including the roles of information technology in social and organizational change, the uses of information technologies in social contexts, and the ways that the social organization of information technologies is influenced by social forces and social practices. Historically, social informatics research has been strong in the Scandinavian countries, the UK and Northern Europe. In Europe some researchers have pointed out that in order to create awareness of the importance of social issues of computing, one has to focus on didactics of social informatics. Within North America, the field is represented largely through independent research efforts at a number of diverse institutions. Social informatics research diverges from earlier, deterministic (both social and technological) models for measuring the social impacts of technology. Such technological deterministic models characterized information technologies as tools to be installed and used with a pre-determined set of impacts on society dictated by the technology's stated capabilities. Similarly, the socially deterministic theory represented by some proponents of the social construction of technology (SCOT) or social shaping of technology theory see technology as the product of human social forces.


== Criticisms ==
There is a tension between the practice and research ends of the field. To some extent this reflects the gap, familiar from other disciplines such as community development, community organizing and community based research. In addition, the difficulty that Information Systems has in recognising the qualitative dimension of technology research means that the kind of approach taken by supporters of community informatics is difficult to justify to a positive field oriented towards solutions of technical, rather than social problems. This is a difficulty also seen in the relationship between strict technology research and management research. Problems in conceptualising and evaluating complex social interventions relying on a technical base are familiar from community health and community education. There are long-standing debates about the desire for accountable - especially quantifiable and outcome-focused social development, typically practised by government or supported by foundations, and the more participatory, qualitatively rich, process-driven priorities of grass-roots community activists, familiar from theorists such as Paulo Freire, or Deweyan pragmatism.
Some of the theoretical and practical tensions are also familiar from such disciplines as program evaluation and social policy, and perhaps paradoxically, Management Information Systems, where there is continual debate over the relative virtue and values of different forms of research and action, spread around different understandings of the virtues or otherwise of allegedly ""scientific"" or ""value-free"" activity (frequently associated with ""responsible"" and deterministic public policy philosophies), and contrasted with more interpretive and process driven viewpoints in bottom-up or practice driven activity. Community informatics would in fact probably benefit from closer knowledge of, and relationship to, theorists, practitioners, and evaluators of rigorous qualitative research and practice.
A further concern is the potential for practice to be ""hijacked"" by policy or academic agendas, rather than being driven by community goals, both in developed and developing countries. The ethics of technology intervention in indigenous or other communities has not been sufficiently explored, even though ICTs are increasingly looked upon as an important tool for social and economic development in such communities. Moreover, neither explicit theoretical positions nor ideological positioning has yet emerged. Many projects appear to have developed with no particular disciplinary affiliation, arising more directly from policy or practice imperatives to 'do something' with technology as funding opportunities arise or as those at the grassroots (or working with the grassroots) identify ICT as possible resources to respond to local issues, problems or opportunities. The papers and documented outcomes (as questions or issues for further research or elaboration) on the wiki of the October 2006 Prato conference demonstrate that many of the social, rather than technical issues are key questions of concern to any practitioner in community settings: how to bring about change; the nature of authentic or manufactured community; ethical frameworks; or the politics of community research.
A different strain of critique has emerged from gender studies. Some theorists have argued that feminist contributions to the field have yet to be fully acknowledged and Community Informatics as a research area has yet to welcome feminist interventions. This exists despite the presence of several gender-oriented studies and leadership roles played by women in community informatics initiatives.


== Research and practice interests ==
Research and practice ranges from concerns with purely virtual communities; to situations in which virtual or online communication are used to enhance existing communities in urban, rural, or remote geographic locations in developed or developing countries; to applications of ICTs for the range of areas of interest for communities including social and economic development, environmental management, media and ""content"" production, public management and e-governance among others. A central concern, although one not always realized in practice is with ""enabling"" or ""empowering"" communities with ICT that is, ensuring that the technology is available for the community. This further implies an approach to development which is rather more ""bottom up"" than ""top down"".
Areas of concern range from small-scale projects in particular communities or organizations which might involve only a handful of people, such as telecentres; an on online community of disabled people; civic networks and to large national, government sponsored networking projects in countries such as Australia and Canada or local community projects such as working with Maori families in New Zealand. The Gates Foundation has been active in supporting public libraries in countries such as Chile. An area of rapidly developing interest is in the use of ICT as a means to enhance citizen engagement as an ""e-Governance"" counterpart (or counterweight) to transaction oriented initiatives.
A key conceptual element and framing concept for Community Informatics is that of ""effective use"" introduced initially by Michael Gurstein in a critique of a research pre-occupation with the Digital Divide as ICT ""access"". CI is concerned with how ICTs are used in practice and not simply facilitating ""access"" to them and the notion of ""effective use"" is a bridge between CI research (research and analysis of the constituent elements of effective use), CI policy (developing enabling structures and programmes supportive of ""effective use"") and practice (implementing applications and services in support of local communities).
Another way to understand CI is Clement and Shade's ""access rainbow"" (Clement and Shade 2000). Clement and Shade have contended that accomplishing insignificant specialized connectedness to the Internet is no assurance that an individual or group will prevail with regards to appropriating new ICTs in ways that advance their improvement, independence, or empowerment. It is an approach which has multi-layered socio-specialized model for universal access to ICTs. It is displayed as seven layers, starting with the fundamental technical components of connectedness and moving upward through layers that inexorably push the essential social framework of access. The seven layers are: 1. Carriage 2. Devices 3. Software tools Internet 4. Content/services 5. Service/access 6. Literacy / social facilitation 7. Governance.Even though that all elements are important, the most important one is the content /service layer in the middle, since this is where the actual utility is most direct. The upper layers focus on social dimensions and the lower layers focus on technical aspects.
Many practitioners would dispute any necessary connection to university research, regarding academic theorising and interventions as constraining or irrelevant to grassroots activity which should be beyond the control of traditional institutions, or simply irrelevant to practical local goals.
Some of the commonalities and differences may be in fact be due to national and cultural differences. For example, the capacity of many North American (and particularly US) universities to engage in service learning as part of progressive charters in communities large and small is part of a long-standing tradition absent elsewhere. The tradition of service learning is almost entirely absent in the UK, Australia, or New Zealand, (and of limited significance in Canada) where the State has traditionally played a much stronger role in the delivery of community services and information.
In some countries such as the UK, there is a tradition of locally based grassroots community technology, for example in Manchester, or in Hebden Bridge. In Italy and the Netherlands, there also appears to have been a strong connection between the development of local civic networks based around a tradition of civic oppositionism, connected into the work of progressive academics.
In Latin America, Africa and many parts of Asia these efforts have been driven by external funding agencies as part of larger programs and initiatives in support of broader economic and social development goals. However, these efforts have now become significantly ""indigenized"" (and particularly in Latin America) and ""bottom-up"" ICT efforts are increasingly playing a leading role in defining the future use of ICT within local communities.
In Canada, The Canadian Research Alliance for Community Innovation and Networking (CRACIN) was established in 2003. Their goal is to explore and archive the status and achievements of CI activities in Canada. It is a research partnership between scholastics, specialists, and public sector delegates. 


== Networks ==
There are emerging online and personal networks of researchers and practitioners in community informatics and community networking in many countries as well as international groupings. The past decade has also seen conferences in many countries, and there is an emerging literature for theoreticians and practitioners including the on-line Journal of Community Informatics.
It is surprising in fact, how much in common is found when people from developed and non-developed countries meet. A common theme is the struggle to convince policy makers of the legitimacy of this approach to developing electronically literate societies, instead of a top-down or trickle-down approach, or an approach dominated by technical, rather than social solutions which in the end, tend to help vendors rather than communities. A common criticism that is frequently raised amongst participants at events such as the Prato conferences is that a focus on technical solutions evades the social changes that communities need to achieve in their values, activities and other people-oriented outcomes in order to make better use of technology.
The field tends to have a progressive bent, being concerned about the use of technology for social and cultural development connected to a desire for capacity building or expanding social capital, and in a number of countries, governments and foundations have funded a variety of community informatics projects and initiatives, particularly from a more tightly controlled, though not well-articulated social planning perspective, though knowledge about long-term effects of such forms of social intervention on use of technology is still in its early stages.


=== Public libraries and community networks ===
Even though that community networks and public libraries have similitudes in various ways, there are some obstacles that upset the probability of cooperation in the future between them. Albeit both CNs and libraries are concerned with giving information services to the society, an exchange is by all accounts lacking between the two communities. The mission of libraries is frequently rather barely engaged and, with regards to managing people and different institutes, their methodology can be to some degree unbending. Thusly, CN specialists, while institutionally more adaptable, rush to expel the part of public libraries in the community, tending to see the library essentially as a store of books upheld by public subsidizing. Public libraries have a long-standing custom of association with their communities, yet their conditions and concerns contrast from those of community networks (CNs).


== See also ==


== References ==


== External links ==
Center for Community Informatics - Loyola University, Maryland
Center for Community Informatics Research, Development & Training
Community Informatics - University of Illinois at Urbana-Champaign
Community Informatics - Penn State
Community Informatics - University of Michigan
Community Informatics Research Group, University of Pittsburgh
Journal of Community Informatics
Community Informatics Research Network
Association for Community Networking"
17,List of important publications in theoretical computer science,24095830,38158,"This is a list of important publications in theoretical computer science, organized by field.
Some reasons why a particular publication might be regarded as important:
Topic creator – A publication that created a new topic
Breakthrough – A publication that changed scientific knowledge significantly
Influence – A publication which has significantly influenced the world or has had a massive impact on the teaching of theoretical computer science.


== Computability ==


=== Cutland's Computability: An Introduction to Recursive Function Theory (Cambridge) ===
Cutland, Nigel J. (1980). Computability: An Introduction to Recursive Function Theory. Cambridge University Press. ISBN 0-521-29465-7. 
The review of this early text by Carl Smith of Purdue University (in the Society for Industrial and Applied Mathematics Reviews), reports that this a text with an ""appropriate blend of intuition and rigor… in the exposition of proofs"" that presents ""the fundamental results of classical recursion theory [RT]... in a style... accessible to undergraduates with minimal mathematical background"". While he states that it ""would make an excellent introductory text for an introductory course in [RT] for mathematics students"", he suggests that an ""instructor must be prepared to substantially augment the material… "" when it used with computer science students (given a dearth of material on RT applications to this area).


=== Decidability of second order theories and automata on infinite trees ===
Michael O. Rabin
Transactions of the American Mathematical Society, vol. 141, pp. 1–35, 1969
Description: The paper presented the tree automaton, an extension of the automata. The tree automaton had numerous applications to proofs of correctness of programs.


=== Finite automata and their decision problems ===
Michael O. Rabin and Dana S. Scott
IBM Journal of Research and Development, vol. 3, pp. 114–125, 1959
Online version (Not Free)
Description: Mathematical treatment of automata, proof of core properties, and definition of non-deterministic finite automaton.


=== Introduction to Automata Theory, Languages, and Computation ===

John E. Hopcroft, Jeffrey D. Ullman, and Rajeev Motwani
Addison-Wesley, 2001, ISBN 0-201-02988-X
Description: A popular textbook.


=== On certain formal properties of grammars ===
Chomsky, N. (1959). ""On certain formal properties of grammars"". Information and Control. 2 (2): 137–167. doi:10.1016/S0019-9958(59)90362-6. 
Description: This article introduced what is now known as the Chomsky hierarchy, a containment hierarchy of classes of formal grammars that generate formal languages.


=== On computable numbers, with an application to the Entscheidungsproblem ===
Alan Turing
Proceedings of the London Mathematical Society, Series 2, vol. 42, pp. 230–265, 1937, doi:10.1112/plms/s2-42.1.230.
Errata appeared in vol. 43, pp. 544–546, 1938, doi:10.1112/plms/s2-43.6.544.
HTML version, PDF version
Description: This article set the limits of computer science. It defined the Turing Machine, a model for all computations. On the other hand, it proved the undecidability of the halting problem and Entscheidungsproblem and by doing so found the limits of possible computation.


=== Rekursive Funktionen ===
Péter, Rózsa (1951). Rekursive Funktionen. Academic Press. ISBN 9780125526500. 
The first textbook on the theory of recursive functions. The book went through many editions and earned Péter the Kossuth Prize from the Hungarian government. Reviews by Raphael M. Robinson and Stephen Kleene praised the book for providing an effective elementary introduction for students.


== Computational complexity theory ==


=== Arora & Barak's Computational Complexity and Goldreich's Computational Complexity (both Cambridge) ===
Sanjeev Arora and Boaz Barak, ""Computational Complexity: A Modern Approach,"" Cambridge University Press, 2009, 579 pages, Hardcover
Oded Goldreich, ""Computational Complexity: A Conceptual Perspective, Cambridge University Press, 2008, 606 pages, Hardcover
Besides the estimable press bringing these recent texts forward, they are very positively reviewed in ACM's SIGACT News by Daniel Apon of the University of Arkansas, who identifies them as ""textbooks for a course in complexity theory, aimed at early graduate… or... advanced undergraduate students… [with] numerous, unique strengths and very few weaknesses,"" and states that both are:

""excellent texts that thoroughly cover both the breadth and depth of computational complexity theory… [by] authors... each [who] are giants in theory of computing [where each will be] ...an exceptional reference text for experts in the field… [and that] ...theorists, researchers and instructors of any school of thought will find either book useful.""

The reviewer notes that there is ""a definite attempt in [Arora and Barak] to include very up-to-date material, while Goldreich focuses more on developing a contextual and historical foundation for each concept presented,"" and that he ""applaud[s] all… authors for their outstanding contributions.""


=== A machine-independent theory of the complexity of recursive functions ===
Blum, Manuel (1967). ""A Machine-Independent Theory of the Complexity of Recursive Functions"" (PDF). Journal of the ACM. 14 (2): 322–336. doi:10.1145/321386.321395. 
Description: The Blum axioms.


=== Algebraic methods for interactive proof systems ===
Lund, C.; Fortnow, L.; Karloff, H.; Nisan, N. (1992). ""Algebraic methods for interactive proof systems"". Journal of the ACM. 39 (4): 859–868. doi:10.1145/146585.146605. 
Description: This paper showed that PH is contained in IP.


=== The complexity of theorem proving procedures ===
Cook, Stephen A. (1971). ""The Complexity of Theorem-Proving Procedures"" (PDF). Proceedings of the 3rd Annual ACM Symposium on Theory of Computing: 151–158. doi:10.1145/800157.805047. 
Description: This paper introduced the concept of NP-Completeness and proved that Boolean satisfiability problem (SAT) is NP-Complete. Note that similar ideas were developed independently slightly later by Leonid Levin at ""Levin, Universal Search Problems. Problemy Peredachi Informatsii 9(3):265-266, 1973"".


=== Computers and Intractability: A Guide to the Theory of NP-Completeness ===
Garey, Michael R.; Johnson, David S. (1979). Computers and Intractability: A Guide to the Theory of NP-Completeness. New York: Freeman. ISBN 0-7167-1045-5. 
Description: The main importance of this book is due to its extensive list of more than 300 NP-Complete problems. This list became a common reference and definition. Though the book was published only few years after the concept was defined such an extensive list was found.


=== Degree of difficulty of computing a function and a partial ordering of recursive sets ===
Rabin, Michael O. (1960). ""Degree of difficulty of computing a function and a partial ordering of recursive sets"" (PDF). Technical Report No. 2. Jerusalem: Hebrew University. 
Description: This technical report was the first publication talking about what later was renamed computational complexity


=== How good is the simplex method? ===
Victor Klee and George J. Minty
Klee, Victor; Minty, George J. (1972). ""How good is the simplex algorithm?"". In Shisha, Oved. Inequalities III (Proceedings of the Third Symposium on Inequalities held at the University of California, Los Angeles, Calif., September 1–9, 1969, dedicated to the memory of Theodore S. Motzkin). New York-London: Academic Press. pp. 159–175. MR 0332165. 
Description: Constructed the ""Klee–Minty cube"" in dimension D, whose 2D corners are each visited by Dantzig's simplex algorithm for linear optimization.


=== How to construct random functions ===
Goldreich, O.; Goldwasser, S.; Micali, S. (1986). ""How to construct random functions"" (PDF). Journal of the ACM. 33 (4): 792–807. doi:10.1145/6490.6503. 
Description: This paper showed that the existence of one way functions leads to computational randomness.


=== IP = PSPACE ===
Shamir, A. (1992). ""IP = PSPACE"". Journal of the ACM. 39 (4): 869–877. doi:10.1145/146585.146609. 
Description: IP is a complexity class whose characterization (based on interactive proof systems) is quite different from the usual time/space bounded computational classes. In this paper, Shamir extended the technique of the previous paper by Lund, et al., to show that PSPACE is contained in IP, and hence IP = PSPACE, so that each problem in one complexity class is solvable in the other.


=== Reducibility among combinatorial problems ===

R. M. Karp
In R. E. Miller and J. W. Thatcher, editors, Complexity of Computer Computations, Plenum Press, New York, NY, 1972, pp. 85–103
Description: This paper showed that 21 different problems are NP-Complete and showed the importance of the concept.


=== The Knowledge Complexity of Interactive Proof Systems ===
Goldwasser, S.; Micali, S.; Rackoff, C. (1989). ""The Knowledge Complexity of Interactive Proof Systems"" (PDF). SIAM J. Comput. 18 (1): 186–208. doi:10.1137/0218012. 
Description: This paper introduced the concept of zero knowledge.


=== A letter from Gödel to von Neumann ===
Kurt Gödel
A Letter from Gödel to John von Neumann, March 20, 1956
Online version
Description: Gödel discusses the idea of efficient universal theorem prover.


=== On the computational complexity of algorithms ===
Hartmanis, Juris; Stearns, Richard (1965). ""On the computational complexity of algorithms"". Transactions of the American Mathematical Society. 117: 285–306. doi:10.1090/s0002-9947-1965-0170805-7. 
Description: This paper gave computational complexity its name and seed.


=== Paths, trees, and flowers ===
Edmonds, J. (1965). ""Paths, trees, and flowers"". Canadian Journal of Mathematics. 17: 449–467. doi:10.4153/CJM-1965-045-4. 
Description: There is a polynomial time algorithm to find a maximum matching in a graph that is not bipartite and another step toward the idea of computational complexity. For more information see [3].


=== Theory and applications of trapdoor functions ===
Yao, A. C. (1982). ""Theory and application of trapdoor functions"". 23rd Annual Symposium on Foundations of Computer Science (SFCS 1982). pp. 80–91. doi:10.1109/SFCS.1982.45. 
Description: This paper creates a theoretical framework for trapdoor functions and described some of their applications, like in cryptography. Note that the concept of trapdoor functions was brought at ""New directions in cryptography"" six years earlier (See section V ""Problem Interrelationships and Trap Doors."").


=== Computational Complexity ===
C.H. Papadimitriou
Addison-Wesley, 1994, ISBN 0-201-53082-1
Description: An introduction to computational complexity theory, the book explains its author's characterization of P-SPACE and other results.


=== Interactive proofs and the hardness of approximating cliques ===
Feige, U.; Goldwasser, S.; Lovász, L.; Safra, S.; Szegedy, M. (1996). ""Interactive proofs and the hardness of approximating cliques"". Journal of the ACM. 43 (2): 268–292. doi:10.1145/226643.226652. 


=== Probabilistic checking of proofs: a new characterization of NP ===
Arora, S.; Safra, S. (1998). ""Probabilistic checking of proofs: A new characterization of NP"". Journal of the ACM. 45: 70–122. doi:10.1145/273865.273901. 


=== Proof verification and the hardness of approximation problems ===
Arora, S.; Lund, C.; Motwani, R.; Sudan, M.; Szegedy, M. (1998). ""Proof verification and the hardness of approximation problems"". Journal of the ACM. 45 (3): 501–555. doi:10.1145/278298.278306. 
Description: These three papers established the surprising fact that certain problems in NP remain hard even when only an approximative solution is required. See PCP theorem.


=== The Instrinsic Computational Difficulty of Functions ===
Cobham, Alan (1964). ""The Instrinsic Computational Difficulty of Functions"" (PDF). Proc. of the 1964 International Congress for Logic, Methodology, and the Philosophy of Science: 24–30. 
Description: First definition of the complexity class P. One of the founding papers of complexity theory.


== Algorithms ==


=== ""A machine program for theorem proving"" ===
Davis, M.; Logemann, G.; Loveland, D. (1962). ""A machine program for theorem-proving"" (PDF). Communications of the ACM. 5 (7): 394–397. doi:10.1145/368273.368557. 
Description: The DPLL algorithm. The basic algorithm for SAT and other NP-Complete problems.


=== ""A machine-oriented logic based on the resolution principle"" ===
Robinson, J. A. (1965). ""A Machine-Oriented Logic Based on the Resolution Principle"". Journal of the ACM. 12: 23–41. doi:10.1145/321250.321253. 
Description: First description of resolution and unification used in automated theorem proving; used in Prolog and logic programming.


=== ""The traveling-salesman problem and minimum spanning trees"" ===
Held, M.; Karp, R. M. (1970). ""The Traveling-Salesman Problem and Minimum Spanning Trees"". Operations Research. 18 (6): 1138–1162. doi:10.1287/opre.18.6.1138. 
Description: The use of an algorithm for minimum spanning tree as an approximation algorithm for the NP-Complete travelling salesman problem. Approximation algorithms became a common method for coping with NP-Complete problems.


=== ""A polynomial algorithm in linear programming"" ===
L. G. Khachiyan
Soviet Mathematics - Doklady, vol. 20, pp. 191–194, 1979
Description: For long, there was no provably polynomial time algorithm for the linear programming problem. Khachiyan was the first to provide an algorithm that was polynomial (and not just was fast enough most of the time as previous algorithms). Later, Narendra Karmarkar presented a faster algorithm at: Narendra Karmarkar, ""A new polynomial time algorithm for linear programming"", Combinatorica, vol 4, no. 4, p. 373–395, 1984.


=== ""Probabilistic algorithm for testing primality"" ===
Rabin, M. (1980). ""Probabilistic algorithm for testing primality"". Journal of Number Theory. 12 (1): 128–138. doi:10.1016/0022-314X(80)90084-0. 
Description: The paper presented the Miller-Rabin primality test and outlined the program of randomized algorithms.


=== ""Optimization by simulated annealing"" ===
Kirkpatrick, S.; Gelatt, C. D.; Vecchi, M. P. (1983). ""Optimization by Simulated Annealing"". Science. 220 (4598): 671–680. Bibcode:1983Sci...220..671K. doi:10.1126/science.220.4598.671. PMID 17813860. 
Description: This article described simulated annealing which is now a very common heuristic for NP-Complete problems.


=== The Art of Computer Programming ===

Donald Knuth
Description: This monograph has three popular algorithms books and a number of fascicles. The algorithms are written in both English and MIX assembly language (or MMIX assembly language in more recent fascicles). This makes algorithms both understandable and precise. However, the use of a low-level programming language frustrates some programmers more familiar with modern structured programming languages.


=== Algorithms + Data Structures = Programs ===

Niklaus Wirth
Prentice Hall, 1976, ISBN 0-13-022418-9
Description: An early, influential book on algorithms and data structures, with implementations in Pascal.


=== The Design and Analysis of Computer Algorithms ===
Alfred V. Aho, John E. Hopcroft, and Jeffrey D. Ullman
Addison-Wesley, 1974, ISBN 0-201-00029-6
Description: One of the standard texts on algorithms for the period of approximately 1975–1985.


=== How to Solve It By Computer ===
Dromey, R. G. (1982). How to Solve it by Computer. Prentice-Hall International. ISBN 978-0-13-434001-2. 
Description: Explains the Whys of algorithms and data-structures. Explains the Creative Process, the Line of Reasoning, the Design Factors behind innovative solutions.


=== Algorithms ===
Robert Sedgewick
Addison-Wesley, 1983, ISBN 0-201-06672-6
Description: A very popular text on algorithms in the late 1980s. It was more accessible and readable (but more elementary) than Aho, Hopcroft, and Ullman. There are more recent editions.


=== Introduction to Algorithms ===

Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein
3rd Edition, MIT Press, 2009, ISBN 978-0-262-03384-8.
Description: This textbook has become so popular that it is almost the de facto standard for teaching basic algorithms. The 1st edition (with first three authors) was published in 1990, the 2nd edition in 2001, and the 3rd in 2009.


== Algorithmic information theory ==


=== ""On Tables of Random Numbers"" ===
Kolmogorov, Andrei N. (1963). ""On Tables of Random Numbers"". Sankhyā Ser. A. 25: 369–375. MR 0178484. 
Kolmogorov, Andrei N. (1963). ""On Tables of Random Numbers"". Theoretical Computer Science. 207 (2): 387–395. doi:10.1016/S0304-3975(98)00075-9. MR 1643414. 
Description: Proposed a computational and combinatorial approach to probability.


=== ""A formal theory of inductive inference"" ===
Ray Solomonoff
Information and Control, vol. 7, pp. 1–22 and 224–254, 1964
Online copy: part I, part II
Description: This was the beginning of algorithmic information theory and Kolmogorov complexity. Note that though Kolmogorov complexity is named after Andrey Kolmogorov, he said that the seeds of that idea are due to Ray Solomonoff. Andrey Kolmogorov contributed a lot to this area but in later articles.


=== ""Algorithmic information theory"" ===
Chaitin, Gregory (1977). ""Algorithmic information theory"" (PDF). IBM Journal of Research and Development. IBM. 21 (4): 350–359. doi:10.1147/rd.214.0350. Archived from the original (PDF) on 2009-05-30. 
Description: An introduction to algorithmic information theory by one of the important people in the area.


== Information theory ==


=== ""A mathematical theory of communication"" ===
Shannon, C.E. (1948). ""A mathematical theory of communication"". Bell System Technical Journal. 27: 379–423, 623–656. 
Description: This paper created the field of information theory.


=== ""Error detecting and error correcting codes"" ===
Hamming, Richard (1950). ""Error detecting and error correcting codes"". Bell System Technical Journal. 29: 147–160. doi:10.1002/j.1538-7305.1950.tb00463.x. 
Description: In this paper, Hamming introduced the idea of error-correcting code. He created the Hamming code and the Hamming distance and developed methods for code optimality proofs.


=== ""A method for the construction of minimum redundancy codes"" ===
Huffman, D. (1952). ""A Method for the Construction of Minimum-Redundancy Codes"" (PDF). Proceedings of the IRE. 40 (9): 1098–1101. doi:10.1109/JRPROC.1952.273898. 
Description: The Huffman coding.


=== ""A universal algorithm for sequential data compression"" ===
Ziv, J.; Lempel, A. (1977). ""A universal algorithm for sequential data compression"". IEEE Transactions on Information Theory. 23 (3): 337–343. doi:10.1109/TIT.1977.1055714. Archived from the original on 2003-12-04. 
Description: The LZ77 compression algorithm.


=== Elements of Information Theory ===
Cover, Thomas M.; Thomas, Joy A. (1991). Elements of Information Theory. Wiley. 
Description: A popular introduction to information theory.


== Formal verification ==


=== Assigning Meaning to Programs ===
Floyd, Robert (1967). ""Assigning Meaning to Programs"" (PDF). Mathematical Aspects of Computer Science. Proceedings of Symposia in Applied Mathematics. 19: 19–32. doi:10.1090/psapm/019/0235771. ISBN 9780821813195. 
Description: Robert Floyd's landmark paper Assigning Meanings to Programs introduces the method of inductive assertions and describes how a program annotated with first-order assertions may be shown to satisfy a pre- and post-condition specification - the paper also introduces the concepts of loop invariant and verification condition.


=== An Axiomatic Basis for Computer Programming ===
Hoare, C. A. R. (October 1969). ""An axiomatic basis for computer programming"" (PDF). Communications of the ACM. 12 (10): 576–580. doi:10.1145/363235.363259. Archived from the original (PDF) on 2016-03-04. 
Description: Tony Hoare's paper An Axiomatic Basis for Computer Programming describes a set of inference (i.e. formal proof) rules for fragments of an Algol-like programming language described in terms of (what are now called) Hoare-triples.


=== Guarded Commands, Nondeterminacy and Formal Derivation of Programs ===
Dijkstra, E. W. (1975). ""Guarded commands, nondeterminacy and formal derivation of programs"". Communications of the ACM. 18 (8): 453–457. doi:10.1145/360933.360975. 
Description: Edsger Dijkstra's paper Guarded Commands, Nondeterminacy and Formal Derivation of Programs (expanded by his 1976 postgraduate-level textbook A Discipline of Programming) proposes that, instead of formally verifying a program after it has been written (i.e. post facto), programs and their formal proofs should be developed hand-in-hand (using predicate transformers to progressively refine weakest pre-conditions), a method known as program (or formal) refinement (or derivation), or sometimes ""correctness-by-construction"".


=== Proving Assertions about Parallel Programs ===
Edward A. Ashcroft
J. Comput. Syst. Sci. 10(1): 110-135 (1975)
Description: The paper that introduced invariance proofs of concurrent programs.


=== An Axiomatic Proof Technique for Parallel Programs I ===
Susan S. Owicki, David Gries
Acta Inf. 6: 319-340 (1976)
Description: In this paper, along with the same authors paper ""Verifying Properties of Parallel Programs: An Axiomatic Approach. Commun. ACM 19(5): 279-285 (1976)"", the axiomatic approach to parallel programs verification was presented.


=== A Discipline of Programming ===
Edsger W. Dijkstra
1976
Description: Edsger Dijkstra's classic postgraduate-level textbook A Discipline of Programming extends his earlier paper Guarded Commands, Nondeterminacy and Formal Derivation of Programs and firmly establishes the principle of formally deriving programs (and their proofs) from their specification.


=== Denotational Semantics ===
Joe Stoy
1977
Description: Joe Stoy's Denotational Semantics is the first (postgraduate level) book-length exposition of the mathematical (or functional) approach to the formal semantics of programming languages (in contrast to the operational and algebraic approaches).


=== The Temporal Logic of Programs ===
Pnueli, A. (1977). ""The temporal logic of programs"". 18th Annual Symposium on Foundations of Computer Science (SFCS 1977). IEEE. pp. 46–57. doi:10.1109/SFCS.1977.32. 
Description: The use of temporal logic was suggested as a method for formal verification.


=== Characterizing correctness properties of parallel programs using fixpoints (1980) ===
E. Allen Emerson, Edmund M. Clarke
In Proc. 7th International Colloquium on Automata Languages and Programming, pages 169-181, 1980
Description: Model checking was introduced as a procedure to check correctness of concurrent programs.


=== Communicating Sequential Processes (1978) ===
C.A.R. Hoare
1978
Description: Tony Hoare's (original) communicating sequential processes (CSP) paper introduces the idea of concurrent processes (i.e. programs) that do not share variables but instead cooperate solely by exchanging synchronous messages.


=== A Calculus of Communicating Systems ===
Robin Milner
1980
Description: Robin Milner's A Calculus of Communicating Systems (CCS) paper describes a process algebra permitting systems of concurrent processes to be reasoned about formally, something which has not been possible for earlier models of concurrency (semaphores, critical sections, original CSP).


=== Software Development: A Rigorous Approach ===
Cliff Jones
1980
Description: Cliff Jones' textbook Software Development: A Rigorous Approach is the first full-length exposition of the Vienna Development Method (VDM), which had evolved (principally) at IBM's Vienna research lab over the previous decade and which combines the idea of program refinement as per Dijkstra with that of data refinement (or reification) whereby algebraically-defined abstract data types are formally transformed into progressively more ""concrete"" representations.


=== The Science of Programming ===
David Gries
1981
Description: David Gries' textbook The Science of Programming describes Dijkstra's weakest precondition method of formal program derivation, except in a very much more accessible manner than Dijkstra's earlier A Discipline of Programming.
It shows how to construct programs that work correctly (without bugs, other than from typing errors). It does this by showing how to use precondition and postcondition predicate expressions and program proving techniques to guide the way programs are created.
The examples in the book are all small-scale, and clearly academic (as opposed to real-world). They emphasize basic algorithms, such as sorting and merging, and string manipulation. Subroutines (functions) are included, but object-oriented and functional programming environments are not addressed.


=== Communicating Sequential Processes (1985) ===
C.A.R. Hoare
1985
Description: Tony Hoare's Communicating Sequential Processes (CSP) textbook (currently the third most cited computer science reference of all time) presents an updated CSP model in which cooperating processes do not even have program variables and which, like CCS, permits systems of processes to be reasoned about formally.


=== Linear logic (1987) ===
Girard, J.-Y (1987). ""Linear Logic"" (PDF). Theoretical Computer Science. London Mathematical Society. 50 (1): 1–102. doi:10.1016/0304-3975(87)90045-4. Archived from the original (PDF) on 2006-11-29. 
Description: Girard's linear logic was a breakthrough in designing typing systems for sequential and concurrent computation, especially for resource conscious typing systems.


=== A Calculus of Mobile Processes (1989) ===
R. Milner, J. Parrow, D. Walker
1989
Online version: Part 1 and Part 2
Description: This paper introduces the Pi-Calculus, a generalisation of CCS which allows process mobility. The calculus is extremely simple and has become the dominant paradigm in the theoretical study of programming languages, typing systems and program logics.


=== The Z Notation: A Reference Manual ===
Spivey, J. M. (1992). The Z Notation: A Reference Manual (2nd ed.). Prentice Hall International. ISBN 0-13-978529-9. 
Description: Mike Spivey's classic textbook The Z Notation: A Reference Manual summarises the formal specification language Z notation which, although originated by Jean-Raymond Abrial, had evolved (principally) at Oxford University over the previous decade.


=== Communication and Concurrency ===
Robin Milner
Prentice-Hall International, 1989
Description: Robin Milner's textbook Communication and Concurrency is a more accessible, although still technically advanced, exposition of his earlier CCS work.


=== a Practical Theory of Programming ===
Eric Hehner
Springer, 1993, current edition online here
Description: the up-to-date version of Predicative programming. The basis for C.A.R. Hoare's UTP. The simplest and most comprehensive formal methods.


== References =="
18,LP-type problem,34676009,37751,"In the study of algorithms, an LP-type problem (also called a generalized linear program) is an optimization problem that shares certain properties with low-dimensional linear programs and that may be solved by similar algorithms. LP-type problems include many important optimization problems that are not themselves linear programs, such as the problem of finding the smallest circle containing a given set of planar points. They may be solved by a combination of randomized algorithms in an amount of time that is linear in the number of elements defining the problem, and subexponential in the dimension of the problem.


== Definition ==
LP-type problems were defined by Sharir & Welzl (1992) as problems in which one is given as input a finite set S of elements, and a function f that maps subsets of S to values from a totally ordered set. The function is required to satisfy two key properties:
Monotonicity: for every two sets A ⊆ B ⊆ S, f(A) ≤ f(B) ≤ f(S).
Locality: for every two sets A ⊆ B ⊆ S and every element x in S, if f(A) = f(B) = f(A ∪ {x}), then f(A) = f(B ∪ {x}).
A basis of an LP-type problem is a set B ⊆ S with the property that every proper subset of B has a smaller value of f than B itself, and the dimension (or combinatorial dimension) of an LP-type problem is defined to be the maximum cardinality of a basis.
It is assumed that an optimization algorithm may evaluate the function f only on sets that are themselves bases or that are formed by adding a single element to a basis. Alternatively, the algorithm may be restricted to two primitive operations: a violation test that determines, for a basis B and an element x whether f(B) = f(B ∪ {x}), and a basis computation that (with the same inputs) finds a basis of B ∪ {x}. The task for the algorithm to perform is to evaluate f(S) by only using these restricted evaluations or primitives.


== Examples and applications ==
A linear program may be defined by a system of d non-negative real variables, subject to n linear inequality constraints, together with a non-negative linear objective function to be minimized. This may be placed into the framework of LP-type problems by letting S be the set of constraints, and defining f(A) (for a subset A of the constraints) to be the minimum objective function value of the smaller linear program defined by A. With suitable general position assumptions (in order to prevent multiple solution points having the same optimal objective function value), this satisfies the monotonicity and locality requirements of an LP-type problem, and has combinatorial dimension equal to the number d of variables. Similarly, an integer program (consisting of a collection of linear constraints and a linear objective function, as in a linear program, but with the additional restriction that the variables must take on only integer values) satisfies both the monotonicity and locality properties of an LP-type problem, with the same general position assumptions as for linear programs. Theorems of Bell (1977) and Scarf (1977) show that, for an integer program with d variables, the combinatorial dimension is at most 2d.
Many natural optimization problems in computational geometry are LP-type:

The smallest circle problem is the problem of finding the minimum radius of a circle containing a given set of n points in the plane. It satisfies monotonicity (adding more points can only make the circle larger) and locality (if the smallest circle for set A contains B and x, then the same circle also contains B ∪ {x}). Because the smallest circle is always determined by some three points, the smallest circle problem has combinatorial dimension three, even though it is defined using two-dimensional Euclidean geometry. More generally, the smallest enclosing ball of points in d dimensions forms an LP-type problem of combinatorial dimension d + 1. The smallest circle problem can be generalized to the smallest ball enclosing a set of balls, to the smallest ball that touches or surrounds each of a set of balls, to the weighted 1-center problem, or to similar smaller enclosing ball problems in non-Euclidean spaces such as the space with distances defined by Bregman divergence. The related problem of finding the smallest enclosing ellipsoid is also an LP-type problem, but with a larger combinatorial dimension, d(d + 3)/2.
Let K0, K1, ... be a sequence of n convex sets in d-dimensional Euclidean space, and suppose that we wish to find the longest prefix of this sequence that has a common intersection point. This may be expressed as an LP-type problem in which f(A) = −i where Ki is the first member of A that does not belong to an intersecting prefix of A, and where f(A) = −n if there is no such member. The combinatorial dimension of this system is d + 1.
Suppose we are given a collection of axis-aligned rectangular boxes in three-dimensional space, and wish to find a line directed into the positive octant of space that cuts through all the boxes. This may be expressed as an LP-type problem with combinatorial dimension 4.
The problem of finding the closest distance between two convex polytopes, specified by their sets of vertices, may be represented as an LP-type problem. In this formulation, the set S is the set of all vertices in both polytopes, and the function value f(A) is the negation of the smallest distance between the convex hulls of the two subsets A of vertices in the two polytopes. The combinatorial dimension of the problem is d + 1 if the two polytopes are disjoint, or d + 2 if they have a nonempty intersection.
Let S = {f0, f1, ...} be a set of quasiconvex functions. Then the pointwise maximum maxi fi is itself quasiconvex, and the problem of finding the minimum value of maxi fi is an LP-type problem. It has combinatorial dimension at most 2d + 1, where d is the dimension of the domain of the functions, but for sufficiently smooth functions the combinatorial dimension is smaller, at most d + 1. Many other LP-type problems can also be expressed using quasiconvex functions in this way; for instance, the smallest enclosing circle problem is the problem of minimizing maxi fi where each of the functions fi measures the Euclidean distance from one of the given points.
LP-type problems have also been used to determine the optimal outcomes of certain games in algorithmic game theory, improve vertex placement in finite element method meshes, solve facility location problems, analyze the time complexity of certain exponential-time search algorithms, and reconstruct the three-dimensional positions of objects from their two-dimensional images.


== Algorithms ==


=== Seidel ===
Seidel (1991) gave an algorithm for low-dimensional linear programming that may be adapted to the LP-type problem framework. Seidel's algorithm takes as input the set S and a separate set X (initially empty) of elements known to belong to the optimal basis. It then considers the remaining elements one-by-one in a random order, performing violation tests for each one and, depending on the result, performing a recursive call to the same algorithm with a larger set of known basis elements. It may be expressed with the following pseudocode:

In a problem with combinatorial dimension d, the violation test in the ith iteration of the algorithm fails only when x is one of the d − |X| remaining basis elements, which happens with probability at most (d − |X|)/i. Based on this calculation, it can be shown that overall the expected number of violation tests performed by the algorithm is O(d! n), linear in n but worse than exponential in d.


=== Clarkson ===
Clarkson (1995) defines two algorithms, a recursive algorithm and an iterative algorithm, for linear programming based on random sampling techniques, and suggests a combination of the two that calls the iterative algorithm from the recursive algorithm. The recursive algorithm repeatedly chooses random samples whose size is approximately the square root of the input size, solves the sampled problem recursively, and then uses violation tests to find a subset of the remaining elements that must include at least one basis element:

In each iteration, the expected size of V is O(√n), and whenever V is nonempty it includes at least one new element of the eventual basis of S. Therefore, the algorithm performs at most d iterations, each of which performs n violation tests and makes a single recursive call to a subproblem of size O(d√n).
Clarkson's iterative algorithm assigns weights to each element of S, initially all of them equal. It then chooses a set R of 9d2 elements from S at random, and computes the sets B and V as in the previous algorithm. If the total weight of V is at most 2/(9d − 1) times the total weight of S (as happens with constant probability) then the algorithm doubles the weights of every element of V, and as before it repeats this process until V becomes empty. In each iteration, the weight of the optimal basis can be shown to increase at a greater rate than the total weight of S, from which it follows that the algorithm must terminate within O(log n) iterations.
By using the recursive algorithm to solve a given problem, switching to the iterative algorithm for its recursive calls, and then switching again to Seidel's algorithm for the calls made by the iterative algorithm, it is possible solve a given LP-type problem using O(dn + d! dO(1) log n) violation tests.
When applied to a linear program, this algorithm can be interpreted as being a dual simplex method. With certain additional computational primitives beyond the violation test and basis computation primitives, this method can be made deterministic.


=== Matoušek, Sharir, and Welzl ===
Matoušek, Sharir & Welzl (1996) describe an algorithm that uses an additional property of linear programs that is not always held by other LP-type problems, that all bases have the same cardinality of each other. If an LP-type problem does not have this property, it can be made to have it by adding d new dummy elements and by modifying the function f to return the ordered pair of its old value f(A) and of the number min(d,|A|), ordered lexicographically.
Rather than adding elements of S one at a time, or finding samples of the elements, Matoušek, Sharir & Welzl (1996) describe an algorithm that removes elements one at a time. At each step it maintains a basis C that may initially be the set of dummy elements. It may be described with the following pseudocode:

In most of the recursive calls of the algorithm, the violation test succeeds and the if statement is skipped. However, with a small probability the violation test fails and the algorithm makes an additional basis computation and then an additional recursive call. As the authors show, the expected time for the algorithm is linear in n and exponential in the square root of d log n. By combining this method with Clarkson's recursive and iterative procedures, these two forms of time dependence can be separated out from each other, resulting in an algorithm that performs O(dn) violation tests in the outer recursive algorithm and a number that is exponential in the square root of d log d in the lower levels of the algorithm.


== Variations ==


=== Optimization with outliers ===
Matoušek (1995) considers a variation of LP-type optimization problems in which one is given, together with the set S and the objective function f, a number k; the task is to remove k elements from S in order to make the objective function on the remaining set as small as possible. For instance, when applied to the smallest circle problem, this would give the smallest circle that contains all but k of a given set of planar points. He shows that, for all non-degenerate LP-type problems (that is, problems in which all bases have distinct values) this problem may be solved in time O(nkd), by solving a set of O(kd) LP-type problems defined by subsets of S.


=== Implicit problems ===
Some geometric optimization problems may be expressed as LP-type problems in which the number of elements in the LP-type formulation is significantly greater than the number of input data values for the optimization problem. As an example, consider a collection of n points in the plane, each moving with constant velocity. At any point in time, the diameter of this system is the maximum distance between two of its points. The problem of finding a time at which the diameter is minimized can be formulated as minimizing the pointwise maximum of O(n2) quasiconvex functions, one for each pair of points, measuring the Euclidean distance between the pair as a function of time. Thus, it can be solved as an LP-type problem of combinatorial dimension two on a set of O(n2) elements, but this set is significantly larger than the number of input points.
Chan (2004) describes an algorithm for solving implicitly defined LP-type problems such as this one in which each LP-type element is determined by a k-tuple of input values, for some constant k. In order to apply his approach, there must exist a decision algorithm that can determine, for a given LP-type basis B and set S of n input values, whether B is a basis for the LP-type problem determined by S.
Chan's algorithm performs the following steps:
If the number of input values is below some threshold value, find the set of LP-type elements that it determines and solve the resulting explicit LP-type problem.
Otherwise, partition the input values into a suitable number greater than k of equal-sized subsets Si.
If f is the objective function for the implicitly defined LP-type problem to be solved, then define a function g that maps collections of subsets Si to the value of f on the union of the collection. Then the collection of subsets Si and the objective function g itself defines an LP-type problem, of the same dimension as the implicit problem to be solved.
Solve the (explicit) LP-type problem defined by g using Clarkson's algorithm, which performs a linear number of violation tests and a polylogarithmic number of basis evaluations. The basis evaluations for g may be performed by recursive calls to Chan's algorithm, and the violation tests may be performed by calls to the decision algorithm.
With the assumption that the decision algorithm takes an amount of time O(T(n)) that grows at least polynomially as a function of the input size n, Chan shows that the threshold for switching to an explicit LP formulation and the number of subsets in the partition can be chosen in such a way that the implicit LP-type optimization algorithm also runs in time O(T(n)).
For instance, for the minimum diameter of moving points, the decision algorithm needs only to calculate the diameter of a set of points at a fixed time, a problem that can be solved in O(n log n) time using the rotating calipers technique. Therefore, Chan's algorithm for finding the time at which the diameter is minimized also takes time O(n log n). Chan uses this method to find a point of maximal Tukey depth among a given collection of n points in d-dimensional Euclidean space, in time O(nd − 1 + n log n). A similar technique was used by Braß, Heinrich-Litan & Morin (2003) to find a point of maximal Tukey depth for the uniform distribution on a convex polygon.


== History and related problems ==
The discovery of linear time algorithms for linear programming and the observation that the same algorithms could in many cases be used to solve geometric optimization problems that were not linear programs goes back at least to Megiddo (1983, 1984), who gave a linear expected time algorithm for both three-variable linear programs and the smallest circle problem. However, Megiddo formulated the generalization of linear programming geometrically rather than combinatorially, as a convex optimization problem rather than as an abstract problem on systems of sets. Similarly, Dyer (1986) and Clarkson (in the 1988 conference version of Clarkson 1995) observed that their methods could be applied to convex programs as well as linear programs. Dyer (1992) showed that the minimum enclosing ellipsoid problem could also be formulated as a convex optimization problem by adding a small number of non-linear constraints. The use of randomization to improve the time bounds for low dimensional linear programming and related problems was pioneered by Clarkson and by Dyer & Frieze (1989).
The definition of LP-type problems in terms of functions satisfying the axioms of locality and monotonicity is from Sharir & Welzl (1992), but other authors in the same timeframe formulated alternative combinatorial generalizations of linear programs. For instance, in a framework developed by Gärtner (1995), the function f is replaced by a total ordering on the subsets of S. It is possible to break the ties in an LP-type problem to create a total order, but only at the expense of an increase in the combinatorial dimension. Additionally, as in LP-type problems, Gärtner defines certain primitives for performing computations on subsets of elements; however, his formalization does not have an analogue of the combinatorial dimension.
Another abstract generalization of both linear programs and linear complementarity problems, formulated by Stickney & Watson (1978) and later studied by several other authors, concerns orientations of the edges of a hypercube with the property that every face of the hypercube (including the whole hypercube as a face) has a unique sink, a vertex with no outgoing edges. An orientation of this type may be formed from an LP-type problem by corresponding the subsets of S with the vertices of a hypercube in such a way that two subsets differ by a single element if and only if the corresponding vertices are adjacent, and by orienting the edge between neighboring sets A ⊆ B towards B if f(A) ≠ f(B) and towards A otherwise. The resulting orientation has the additional property that it forms a directed acyclic graph, from which it can be shown that a randomized algorithm can find the unique sink of the whole hypercube (the optimal basis of the LP-type problem) in a number of steps exponential in the square root of n.
The more recently developed framework of violator spaces generalizes LP-type problems, in the sense that every LP-type problem can be modeled by a violator space but not necessarily vice versa. Violator spaces are defined similarly to LP-type problems, by a function f that maps sets to objective function values, but the values of f are not ordered. Despite the lack of ordering, every set S has a well-defined set of bases (the minimal sets with the same value as the whole set) that can be found by variations of Clarkson's algorithms for LP-type problems. Indeed, violator spaces have been shown to exactly characterize the systems that can be solved by Clarkson's algorithms.


== Notes ==


== References =="
19,Computational chemistry,6019,36814,"Computational chemistry is a branch of chemistry that uses computer simulation to assist in solving chemical problems. It uses methods of theoretical chemistry, incorporated into efficient computer programs, to calculate the structures and properties of molecules and solids. It is necessary because, apart from relatively recent results concerning the hydrogen molecular ion (dihydrogen cation, see references therein for more details), the quantum many-body problem cannot be solved analytically, much less in closed form. While computational results normally complement the information obtained by chemical experiments, it can in some cases predict hitherto unobserved chemical phenomena. It is widely used in the design of new drugs and materials.
Examples of such properties are structure (i.e., the expected positions of the constituent atoms), absolute and relative (interaction) energies, electronic charge density distributions, dipoles and higher multipole moments, vibrational frequencies, reactivity, or other spectroscopic quantities, and cross sections for collision with other particles.
The methods used cover both static and dynamic situations. In all cases, the computer time and other resources (such as memory and disk space) increase rapidly with the size of the system being studied. That system can be one molecule, a group of molecules, or a solid. Computational chemistry methods range from very approximate to highly accurate; the latter are usually feasible for small systems only. Ab initio methods are based entirely on quantum mechanics and basic physical constants. Other methods are called empirical or semi-empirical because they use additional empirical parameters.
Both ab initio and semi-empirical approaches involve approximations. These range from simplified forms of the first-principles equations that are easier or faster to solve, to approximations limiting the size of the system (for example, periodic boundary conditions), to fundamental approximations to the underlying equations that are required to achieve any solution to them at all. For example, most ab initio calculations make the Born–Oppenheimer approximation, which greatly simplifies the underlying Schrödinger equation by assuming that the nuclei remain in place during the calculation. In principle, ab initio methods eventually converge to the exact solution of the underlying equations as the number of approximations is reduced. In practice, however, it is impossible to eliminate all approximations, and residual error inevitably remains. The goal of computational chemistry is to minimize this residual error while keeping the calculations tractable.
In some cases, the details of electronic structure are less important than the long-time phase space behavior of molecules. This is the case in conformational studies of proteins and protein-ligand binding thermodynamics. Classical approximations to the potential energy surface are used, as they are computationally less intensive than electronic calculations, to enable longer simulations of molecular dynamics. Furthermore, cheminformatics uses even more empirical (and computationally cheaper) methods like machine learning based on physicochemical properties. One typical problem in cheminformatics is to predict the binding affinity of drug molecules to a given target.


== History ==
Building on the founding discoveries and theories in the history of quantum mechanics, the first theoretical calculations in chemistry were those of Walter Heitler and Fritz London in 1927. The books that were influential in the early development of computational quantum chemistry include Linus Pauling and E. Bright Wilson's 1935 Introduction to Quantum Mechanics – with Applications to Chemistry, Eyring, Walter and Kimball's 1944 Quantum Chemistry, Heitler's 1945 Elementary Wave Mechanics – with Applications to Quantum Chemistry, and later Coulson's 1952 textbook Valence, each of which served as primary references for chemists in the decades to follow.
With the development of efficient computer technology in the 1940s, the solutions of elaborate wave equations for complex atomic systems began to be a realizable objective. In the early 1950s, the first semi-empirical atomic orbital calculations were performed. Theoretical chemists became extensive users of the early digital computers. One major advance came with the 1951 paper in Reviews of Modern Physics by Clemens C. J. Roothaan in 1951, largely on the ""LCAO MO"" approach (Linear Combination of Atomic Orbitals Molecular Orbitals), for many years the second-most cited paper in that journal. A very detailed account of such use in the United Kingdom is given by Smith and Sutcliffe. The first ab initio Hartree–Fock method calculations on diatomic molecules were performed in 1956 at MIT, using a basis set of Slater orbitals. For diatomic molecules, a systematic study using a minimum basis set and the first calculation with a larger basis set were published by Ransil and Nesbet respectively in 1960. The first polyatomic calculations using Gaussian orbitals were performed in the late 1950s. The first configuration interaction calculations were performed in Cambridge on the EDSAC computer in the 1950s using Gaussian orbitals by Boys and coworkers. By 1971, when a bibliography of ab initio calculations was published, the largest molecules included were naphthalene and azulene. Abstracts of many earlier developments in ab initio theory have been published by Schaefer.
In 1964, Hückel method calculations (using a simple linear combination of atomic orbitals (LCAO) method to determine electron energies of molecular orbitals of π electrons in conjugated hydrocarbon systems) of molecules, ranging in complexity from butadiene and benzene to ovalene, were generated on computers at Berkeley and Oxford. These empirical methods were replaced in the 1960s by semi-empirical methods such as CNDO.
In the early 1970s, efficient ab initio computer programs such as ATMOL, Gaussian, IBMOL, and POLYAYTOM, began to be used to speed ab initio calculations of molecular orbitals. Of these four programs, only Gaussian, now vastly expanded, is still in use, but many other programs are now in use. At the same time, the methods of molecular mechanics, such as MM2 force field, were developed, primarily by Norman Allinger.
One of the first mentions of the term computational chemistry can be found in the 1970 book Computers and Their Role in the Physical Sciences by Sidney Fernbach and Abraham Haskell Taub, where they state ""It seems, therefore, that 'computational chemistry' can finally be more and more of a reality."" During the 1970s, widely different methods began to be seen as part of a new emerging discipline of computational chemistry. The Journal of Computational Chemistry was first published in 1980.
Computational chemistry has featured in several Nobel Prize awards, most notably in 1998 and 2013. Walter Kohn, ""for his development of the density-functional theory"", and John Pople, ""for his development of computational methods in quantum chemistry"", received the 1998 Nobel Prize in Chemistry. Martin Karplus, Michael Levitt and Arieh Warshel received the 2013 Nobel Prize in Chemistry for ""the development of multiscale models for complex chemical systems"".


== Fields of application ==
The term theoretical chemistry may be defined as a mathematical description of chemistry, whereas computational chemistry is usually used when a mathematical method is sufficiently well developed that it can be automated for implementation on a computer. In theoretical chemistry, chemists, physicists, and mathematicians develop algorithms and computer programs to predict atomic and molecular properties and reaction paths for chemical reactions. Computational chemists, in contrast, may simply apply existing computer programs and methodologies to specific chemical questions.
Computational chemistry has two different aspects:
Computational studies, used to find a starting point for a laboratory synthesis, or to assist in understanding experimental data, such as the position and source of spectroscopic peaks.
Computational studies, used to predict the possibility of so far entirely unknown molecules or to explore reaction mechanisms not readily studied via experiments.
Thus, computational chemistry can assist the experimental chemist or it can challenge the experimental chemist to find entirely new chemical objects.
Several major areas may be distinguished within computational chemistry:
The prediction of the molecular structure of molecules by the use of the simulation of forces, or more accurate quantum chemical methods, to find stationary points on the energy surface as the position of the nuclei is varied.
Storing and searching for data on chemical entities (see chemical databases).
Identifying correlations between chemical structures and properties (see quantitative structure–property relationship (QSPR) and quantitative structure–activity relationship (QSAR)).
Computational approaches to help in the efficient synthesis of compounds.
Computational approaches to design molecules that interact in specific ways with other molecules (e.g. drug design and catalysis).


== Accuracy ==
The words exact and perfect do not apply here, as very few aspects of chemistry can be computed exactly. However, almost every aspect of chemistry can be described in a qualitative or approximate quantitative computational scheme.
Molecules consist of nuclei and electrons, so the methods of quantum mechanics apply. Computational chemists often attempt to solve the non-relativistic Schrödinger equation, with relativistic corrections added, although some progress has been made in solving the fully relativistic Dirac equation. In principle, it is possible to solve the Schrödinger equation in either its time-dependent or time-independent form, as appropriate for the problem in hand; in practice, this is not possible except for very small systems. Therefore, a great number of approximate methods strive to achieve the best trade-off between accuracy and computational cost.
Accuracy can always be improved with greater computational cost. Significant errors can present themselves in ab initio models comprising many electrons, due to the computational cost of full relativistic-inclusive methods. This complicates the study of molecules interacting with high atomic mass unit atoms, such as transitional metals and their catalytic properties. Present algorithms in computational chemistry can routinely calculate the properties of small molecules that contain up to about 40 electrons with errors for energies less than a few kJ/mol. For geometries, bond lengths can be predicted within a few picometres and bond angles within 0.5 degrees. The treatment of larger molecules that contain a few dozen atoms is computationally tractable by more approximate methods such as density functional theory (DFT).
There is some dispute within the field whether or not the latter methods are sufficient to describe complex chemical reactions, such as those in biochemistry. Large molecules can be studied by semi-empirical approximate methods. Even larger molecules are treated by classical mechanics methods that use what are called molecular mechanics (MM). In QM-MM methods, small parts of large complexes are treated quantum mechanically (QM), and the remainder is treated approximately (MM).


== Methods ==
One molecular formula can represent more than one molecular isomer: a set of isomers. Each isomer is a local minimum on the energy surface (called the potential energy surface) created from the total energy (i.e., the electronic energy, plus the repulsion energy between the nuclei) as a function of the coordinates of all the nuclei. A stationary point is a geometry such that the derivative of the energy with respect to all displacements of the nuclei is zero. A local (energy) minimum is a stationary point where all such displacements lead to an increase in energy. The local minimum that is lowest is called the global minimum and corresponds to the most stable isomer. If there is one particular coordinate change that leads to a decrease in the total energy in both directions, the stationary point is a transition structure and the coordinate is the reaction coordinate. This process of determining stationary points is called geometry optimization.
The determination of molecular structure by geometry optimization became routine only after efficient methods for calculating the first derivatives of the energy with respect to all atomic coordinates became available. Evaluation of the related second derivatives allows the prediction of vibrational frequencies if harmonic motion is estimated. More importantly, it allows for the characterization of stationary points. The frequencies are related to the eigenvalues of the Hessian matrix, which contains second derivatives. If the eigenvalues are all positive, then the frequencies are all real and the stationary point is a local minimum. If one eigenvalue is negative (i.e., an imaginary frequency), then the stationary point is a transition structure. If more than one eigenvalue is negative, then the stationary point is a more complex one, and is usually of little interest. When one of these is found, it is necessary to move the search away from it if the experimenter is looking solely for local minima and transition structures.
The total energy is determined by approximate solutions of the time-dependent Schrödinger equation, usually with no relativistic terms included, and by making use of the Born–Oppenheimer approximation, which allows for the separation of electronic and nuclear motions, thereby simplifying the Schrödinger equation. This leads to the evaluation of the total energy as a sum of the electronic energy at fixed nuclei positions and the repulsion energy of the nuclei. A notable exception are certain approaches called direct quantum chemistry, which treat electrons and nuclei on a common footing. Density functional methods and semi-empirical methods are variants on the major theme. For very large systems, the relative total energies can be compared using molecular mechanics. The ways of determining the total energy to predict molecular structures are:


=== Ab initio methods ===

The programs used in computational chemistry are based on many different quantum-chemical methods that solve the molecular Schrödinger equation associated with the molecular Hamiltonian. Methods that do not include any empirical or semi-empirical parameters in their equations – being derived directly from theoretical principles, with no inclusion of experimental data – are called ab initio methods. This does not imply that the solution is an exact one; they are all approximate quantum mechanical calculations. It means that a particular approximation is rigorously defined on first principles (quantum theory) and then solved within an error margin that is qualitatively known beforehand. If numerical iterative methods must be used, the aim is to iterate until full machine accuracy is obtained (the best that is possible with a finite word length on the computer, and within the mathematical and/or physical approximations made).

The simplest type of ab initio electronic structure calculation is the Hartree–Fock method (HF), an extension of molecular orbital theory, in which the correlated electron-electron repulsion is not specifically taken into account; only its average effect is included in the calculation. As the basis set size is increased, the energy and wave function tend towards a limit called the Hartree–Fock limit. Many types of calculations (termed post-Hartree–Fock methods) begin with a Hartree–Fock calculation and subsequently correct for electron-electron repulsion, referred to also as electronic correlation. As these methods are pushed to the limit, they approach the exact solution of the non-relativistic Schrödinger equation. To obtain exact agreement with experiment, it is necessary to include relativistic and spin orbit terms, both of which are far more important for heavy atoms. In all of these approaches, along with choice of method, it is necessary to choose a basis set. This is a set of functions, usually centered on the different atoms in the molecule, which are used to expand the molecular orbitals with the linear combination of atomic orbitals (LCAO) molecular orbital method ansatz. Ab initio methods need to define a level of theory (the method) and a basis set.
The Hartree–Fock wave function is a single configuration or determinant. In some cases, particularly for bond breaking processes, this is inadequate, and several configurations must be used. Here, the coefficients of the configurations, and of the basis functions, are optimized together.
The total molecular energy can be evaluated as a function of the molecular geometry; in other words, the potential energy surface. Such a surface can be used for reaction dynamics. The stationary points of the surface lead to predictions of different isomers and the transition structures for conversion between isomers, but these can be determined without a full knowledge of the complete surface.
A particularly important objective, called computational thermochemistry, is to calculate thermochemical quantities such as the enthalpy of formation to chemical accuracy. Chemical accuracy is the accuracy required to make realistic chemical predictions and is generally considered to be 1 kcal/mol or 4 kJ/mol. To reach that accuracy in an economic way it is necessary to use a series of post-Hartree–Fock methods and combine the results. These methods are called quantum chemistry composite methods.


=== Density functional methods ===

Density functional theory (DFT) methods are often considered to be ab initio methods for determining the molecular electronic structure, even though many of the most common functionals use parameters derived from empirical data, or from more complex calculations. In DFT, the total energy is expressed in terms of the total one-electron density rather than the wave function. In this type of calculation, there is an approximate Hamiltonian and an approximate expression for the total electron density. DFT methods can be very accurate for little computational cost. Some methods combine the density functional exchange functional with the Hartree–Fock exchange term and are termed hybrid functional methods.


=== Semi-empirical methods ===

Semi-empirical quantum chemistry methods are based on the Hartree–Fock method formalism, but make many approximations and obtain some parameters from empirical data. They were very important in computational chemistry from the 60S to the 90s, especially for treating large molecules where the full Hartree–Fock method without the approximations were too costly. The use of empirical parameters appears to allow some inclusion of correlation effects into the methods.
Primitive semi-empirical methods were designed even before, where the two-electron part of the Hamiltonian is not explicitly included. For π-electron systems, this was the Hückel method proposed by Erich Hückel, and for all valence electron systems, the extended Hückel method proposed by Roald Hoffmann. Sometimes, Hückel methods are referred to as ""completely emprirical"" because they do not derive from a Hamiltonian.Yet, the term ""empirical methods"", or ""empirical force fields"" is usually used to describe Molecular Mechanics.


=== Molecular mechanics ===

In many cases, large molecular systems can be modeled successfully while avoiding quantum mechanical calculations entirely. Molecular mechanics simulations, for example, use one classical expression for the energy of a compound, for instance the harmonic oscillator. All constants appearing in the equations must be obtained beforehand from experimental data or ab initio calculations.
The database of compounds used for parameterization, i.e., the resulting set of parameters and functions is called the force field, is crucial to the success of molecular mechanics calculations. A force field parameterized against a specific class of molecules, for instance proteins, would be expected to only have any relevance when describing other molecules of the same class.
These methods can be applied to proteins and other large biological molecules, and allow studies of the approach and interaction (docking) of potential drug molecules.


=== Methods for solids ===

Computational chemical methods can be applied to solid state physics problems. The electronic structure of a crystal is in general described by a band structure, which defines the energies of electron orbitals for each point in the Brillouin zone. Ab initio and semi-empirical calculations yield orbital energies; therefore, they can be applied to band structure calculations. Since it is time-consuming to calculate the energy for a molecule, it is even more time-consuming to calculate them for the entire list of points in the Brillouin zone.


=== Chemical dynamics ===
Once the electronic and nuclear variables are separated (within the Born–Oppenheimer representation), in the time-dependent approach, the wave packet corresponding to the nuclear degrees of freedom is propagated via the time evolution operator (physics) associated to the time-dependent Schrödinger equation (for the full molecular Hamiltonian). In the complementary energy-dependent approach, the time-independent Schrödinger equation is solved using the scattering theory formalism. The potential representing the interatomic interaction is given by the potential energy surfaces. In general, the potential energy surfaces are coupled via the vibronic coupling terms.
The most popular methods for propagating the wave packet associated to the molecular geometry are:
the split operator technique,
the Chebyshev (real) polynomial,
the multi-configuration time-dependent Hartree method (MCTDH),
the semiclassical method.


=== Molecular dynamics ===

Molecular dynamics (MD) use either quantum mechanics, molecular mechanics or a mixture of both to calculate forces which are then used to solve Newton's laws of motion to examine the time-dependent behaviour of systems. The result of a molecular dynamics simulation is a trajectory that describes how the position and velocity of particles varies with time.


=== Quantum mechanics/Molecular mechanics (QM/MM) ===

QM/MM is a hybrid method that attempts to combine the accuracy of quantum mechanics with the speed of molecular mechanics. It is useful for simulating very large molecules such as enzymes.


== Interpreting molecular wave functions ==
The atoms in molecules (QTAIM) model of Richard Bader was developed to effectively link the quantum mechanical model of a molecule, as an electronic wavefunction, to chemically useful concepts such as atoms in molecules, functional groups, bonding, the theory of Lewis pairs, and the valence bond model. Bader has demonstrated that these empirically useful chemistry concepts can be related to the topology of the observable charge density distribution, whether measured or calculated from a quantum mechanical wavefunction. QTAIM analysis of molecular wavefunctions is implemented, for example, in the AIMAll software package.


== Software packages ==
Many self-sufficient computational chemistry software packages exist. Some include many methods covering a wide range, while others concentrate on a very specific range or even on one method. Details of most of them can be found in:
Biomolecular modelling programs: proteins, nucleic acid.
Molecular mechanics programs.
Quantum chemistry and solid state physics software supporting several methods.
Molecular design software
Semi-empirical programs.
Valence bond programs.


== See also ==


== Notes and references ==


== Bibliography ==
C. J. Cramer Essentials of Computational Chemistry, John Wiley & Sons (2002).
T. Clark A Handbook of Computational Chemistry, Wiley, New York (1985).
R. Dronskowski Computational Chemistry of Solid State Materials, Wiley-VCH (2005).
A.K. Hartmann, Practical Guide to Computer Simulations, World Scientific (2009)
F. Jensen Introduction to Computational Chemistry, John Wiley & Sons (1999).
K.I. Ramachandran, G Deepa and Krishnan Namboori. P.K. Computational Chemistry and Molecular Modeling Principles and applications Springer-Verlag GmbH ISBN 978-3-540-77302-3.
D. Rogers Computational Chemistry Using the PC, 3rd Edition, John Wiley & Sons (2003).
P. v. R. Schleyer (Editor-in-Chief). Encyclopedia of Computational Chemistry. Wiley, 1998. ISBN 0-471-96588-X.
D. Sherrill. Notes on Quantum Mechanics and Computational Chemistry.
J. Simons An introduction to Theoretical Chemistry, Cambridge (2003) ISBN 978-0-521-53047-7.
A. Szabo, N.S. Ostlund, Modern Quantum Chemistry, McGraw-Hill (1982).
D. Young Computational Chemistry: A Practical Guide for Applying Techniques to Real World Problems, John Wiley & Sons (2001).
D. Young's Introduction to Computational Chemistry.
Lewars, Errol G. (2011). Computational Chemistry. Heidelberg: Springer. doi:10.1007/978-90-481-3862-3. ISBN 978-90-481-3860-9. 


== Specialized journals on computational chemistry ==
Reviews in Computational Chemistry
Journal of Computational Chemistry
Journal of Chemical Information and Modeling
Journal of Computer-aided Molecular Design
Journal of Chemical Information and Modeling
Journal of Chemical Theory and Computation
Computational and Theoretical Polymer Science
Computational and Theoretical Chemistry
Journal of Theoretical and Computational Chemistry
Journal of Cheminformatics
Journal of Computer Chemistry Japan
Annual Reports in Computational Chemistry
Computers & Chemical Engineering
Journal of Chemical Software
Molecular Informatics
Journal of Computer Aided Chemistry
Theoretical Chemistry Accounts


== External links ==
NIST Computational Chemistry Comparison and Benchmark DataBase – Contains a database of thousands of computational and experimental results for hundreds of systems
American Chemical Society Division of Computers in Chemistry – American Chemical Society Computers in Chemistry Division, resources for grants, awards, contacts and meetings.
CSTB report Mathematical Research in Materials Science: Opportunities and Perspectives – CSTB Report
3.320 Atomistic Computer Modeling of Materials (SMA 5107) Free MIT Course
Chem 4021/8021 Computational Chemistry Free University of Minnesota Course
Technology Roadmap for Computational Chemistry
Applications of molecular and materials modelling.
Impact of Advances in Computing and Communications Technologies on Chemical Science and Technology CSTB Report
MD and Computational Chemistry applications on GPUs"
20,ACM/IEEE Supercomputing Conference,26528420,36587,"SC (formerly Supercomputing), the International Conference for High Performance Computing, Networking, Storage and Analysis, is the name of the annual conference established in 1988 by the Association for Computing Machinery and the IEEE Computer Society. In 2016, about 11,000 people participated overall. The not-for-profit conference is run by a committee of approximately 600 volunteers who spend roughly three years organizing each conference.
Not to be confused with the International Supercomputing Conference.


== Sponsorship and Governance ==
SC is sponsored by the Association for Computing Machinery and the IEEE Computer Society. From its formation through 2011, ACM sponsorship was managed through ACM's Special Interest Group on Computer Architecture (SIGARCH). Sponsors are listed on each proceedings page in the ACM DL; see for example. Beginning in 2012, ACM began the process of transitioning sponsorship from SIGARCH to the recently formed Special Interest Group on High Performance Computing (SIGHPC). This transition was completed after SC15, and for SC16 ACM sponsorship was vested exclusively in SIGHPC (IEEE sponsorship remained unchanged). The conference is non-profit.
The conference is governed by a steering committee that includes representatives of the sponsoring societies, the current conference general chair, the general chairs of the preceding two years, the general chairs of the next two conference years, and a number of elected members. All steering committee members are volunteers, with the exception of the two representatives of the sponsoring societies, who are employees of those societies. The committee selects the conference general chair, approves each year's conference budget, and is responsible for setting policy and strategy for the conference.


== Conference Components ==
Although each conference committee introduces slight variations on the program each year, the core components of the conference remain largely unchanged from year to year.


=== Technical Program ===
The SC Technical Program is competitive with an acceptance rate around 20% for papers (see History). Traditionally, the program includes invited talks, panels, research papers, tutorials, workshops, posters, and Birds of a Feather (BoF) sessions.


=== Awards ===
Each year, SC hosts the following conference and sponsoring society awards:
ACM Gordon Bell Prize
ACM/IEEE-CS George Michael Memorial HPC Fellowship
ACM/IEEE-CS Ken Kennedy Award
ACM SIGHPC Computational & Data Science Fellowships
IEEE-CS Seymour Cray Computer Engineering Award
IEEE-CS Sidney Fernbach Memorial Award
IEEE CS TCHPC Award for Excellence for Early Career Researchers in HPC
Test of Time Award


=== Exhibits ===
In addition to the technical program, SC hosts a research exhibition each year that includes universities, state-sponsored computing research organizations (such as the Federal labs in the US), and vendors of HPC-related hardware and software from many countries around the world. There were 353 exhibitors at SC16 in Salt Lake City, UT.


=== Student Program ===
SC's program for students has gone through a variety of changes and emphases over the years. Beginning with SC15 the program is called ""Students@SC"", and is oriented toward undergraduate and graduate students in computing related fields, and computing-oriented students in science and engineering. The program includes professional development programs, opportunities to learn from mentors, and engagement with SC’s technical sessions.


=== SCinet ===
SCinet is SC’s research network. Started in 1991, SCinet features emerging technologies for very high bandwidth, low latency wide area network communications in addition to operational services necessary to provide conference attendees with connectivity to the commodity Internet and to many national research and engineering networks.


== Name changes ==
Since its establishment in 1988, and until 1995, the full name of the conference was the ""ACM/IEEE Supercomputing Conference"" (sometimes: ""ACM/IEEE Conference on Supercomputing""). The conference's abbreviated (and more commonly used) formal name was ""Supercomputing 'XY"", where XY denotes the last two digits of the year. In 1996, according to the archived front matter of the conference proceedings, the full name was changed to the ACM/IEEE ""International Conference on High Performance Computing and Communications"". The latter document further announced that, as of 1997, the conference will undergo a name change and will be called ""SC97: High Performance Networking and Computing"". The document explained that

1997 [will mark] the first use of ""SC97"" as the name of the annual conference you've known as ""Supercomputing 'XY"". This change reflects our growing attention to networking, distributed computing, data-intensive applications, and other emerging technologies that push the frontiers of communications and computing.

A 1997 HPCwire article discussed at length the reasoning, considerations, and concerns that accompanied the decision to change the name of the conference series from ""Supercomputing 'XY"" to ""SC 'XY"", stating that

It's official: the age of supercomputing has ended. At any rate, the word ""supercomputing"" has been excised from the title of the annual trade shows, sponsored by the IEEE and ACM, that have been known for almost ten years as ""Supercomputing '(final two digits of year)"". The next event, to be held in San Jose next November, has been redesignated ""SC '97."" Like Lewis Carroll's Cheshire Cat, ""supercomputing"" has faded steadily away until only the smile, nose, and whiskers remain. [...] The loss is a real one. An enormous range of ordinary people had some idea, however vague, what ""supercomputing"" meant. No-caf, local alternatives like ""SC"" and ""HPC"" lack this authority. This is not a trivial issue. In these days of rapid change, passing technofancies, and information overload, a rose with the wrong name is just another thorn -- or forgotten immediately. After all, how can businessmen, ordinary consumers, and taxpayers be expected to pay money for something they can't comprehend? More important, will investors and grant-givers hand over money to support further R&D on something whose only identity is an arbitrary clump of capital letters?

Despite these concerns, the abbreviated name of the conference, ""SC"", is still used today, a reminiscent of the abbreviation of the conference's original name—""Supercomputing Conference"".
The full name, in contrast, underwent several changes. Between 1997 and 2003, the name ""High Performance Networking and Computing"" was specified in the front matter of the archived conference proceedings in some years (1997, 1998, 2000, 2002), whereas in other years it was omitted altogether in favor of the abbreviated name (1999, 2001, 2003). In 2004, the stated front matter full name was changed to ""High Performance Computing, Networking and Storage Conference"". In 2005, this name was replaced by the original name of the conference—""supercomputing""— in the front matter. Finally, in 2006, the current full name, as used today, emerged: ""The International Conference for High Performance Computing, Networking, Storage and Analysis"".
Despite all of the name variances in the proceedings through the years, the digital library of ACM, the co-sponsoring society, records the name of the conference as ""The ACM/IEEE Conference on Supercomputing"" from 1998 - 2008, when it changes to """"The International Conference for High Performance Computing, Networking, Storage and Analysis"". It is these two names that are used in the full citations to the conference proceedings provided in this article.


== History ==
The table below provides the location, name of the general chair, and acceptance statistics for each year of SC. Note that references for data in these tables apply to data preceding the reference to the left on the same row; for example, for SC17 the single reference substantiates all the information in that row, but for SC05 the source for the convention center and chair is different than the source for the acceptance statistics.
The following table details the keynote speakers during the history of the conference; as of SC17, 20% of the keynote speakers have been female, with a mix of speakers from corporate, academic, and national government organizations.


== See also ==
Gordon Bell Prize
Sidney Fernbach Award
Seymour Cray Award
Ken Kennedy Award
TOP500
Green500
HPC Challenge Awards
SCinet
Storcloud


== References ==


== External links ==
The SC Conference Website
SC12 - The International Conference for High Performance Computing, Networking, Storage and Analysis"
21,Neuroinformatics,3062721,35909,"Neuroinformatics is a research field concerned with the organization of neuroscience data by the application of computational models and analytical tools. These areas of research are important for the integration and analysis of increasingly large-volume, high-dimensional, and fine-grain experimental data. Neuroinformaticians provide computational tools, mathematical models, and create interoperable databases for clinicians and research scientists. Neuroscience is a heterogeneous field, consisting of many and various sub-disciplines (e.g., cognitive psychology, behavioral neuroscience, and behavioral genetics). In order for our understanding of the brain to continue to deepen, it is necessary that these sub-disciplines are able to share data and findings in a meaningful way; Neuroinformaticians facilitate this.
Neuroinformatics stands at the intersection of neuroscience and information science. Other fields, like genomics, have demonstrated the effectiveness of freely distributed databases and the application of theoretical and computational models for solving complex problems. In Neuroinformatics, such facilities allow researchers to more easily quantitatively confirm their working theories by computational modeling. Additionally, neuroinformatics fosters collaborative research—an important fact that facilitates the field's interest in studying the multi-level complexity of the brain.
There are three main directions where neuroinformatics has to be applied:
the development of tools and databases for management and sharing of neuroscience data at all levels of analysis,
the development of tools for analyzing and modeling neuroscience data,
the development of computational models of the nervous system and neural processes.
In the recent decade, as vast amounts of diverse data about the brain were gathered by many research groups, the problem was raised of how to integrate the data from thousands of publications in order to enable efficient tools for further research. The biological and neuroscience data are highly interconnected and complex, and by itself, integration represents a great challenge for scientists.
Combining informatics research and brain research provides benefits for both fields of science. On one hand, informatics facilitates brain data processing and data handling, by providing new electronic and software technologies for arranging databases, modeling and communication in brain research. On the other hand, enhanced discoveries in the field of neuroscience will invoke the development of new methods in information technologies (IT).


== History ==
Starting in 1989, the United States National Institute of Mental Health (NIMH), the National Institute of Drug Abuse (NIDA) and the National Science Foundation (NSF) provided the National Academy of Sciences Institute of Medicine with funds to undertake a careful analysis and study of the need to create databases, share neuroscientific data and to examine how the field of information technology could create the tools needed for the increasing volume and modalities of neuroscientific data. The positive recommendations were reported in 1991. This positive report enabled NIMH, now directed by Allan Leshner, to create the ""Human Brain Project"" (HBP), with the first grants awarded in 1993. The HBP was led by Koslow along with cooperative efforts of other NIH Institutes, the NSF, the National Aeronautics and Space Administration and the Department of Energy. The HPG and grant-funding initiative in this area slightly preceded the explosive expansion of the World Wide Web. From 1993 through 2004 this program grew to over 100 million dollars in funded grants.
Next, Koslow pursued the globalization of the HPG and neuroinformatics through the European Union and the Office for Economic Co-operation and Development (OECD), Paris, France. Two particular opportunities occurred in 1996.
The first was the existence of the US/European Commission Biotechnology Task force co-chaired by Mary Clutter from NSF. Within the mandate of this committee, of which Koslow was a member the United States European Commission Committee on Neuroinformatics was established and co-chaired by Koslow from the United States. This committee resulted in the European Commission initiating support for neuroinformatics in Framework 5 and it has continued to support activities in neuroinformatics research and training.
A second opportunity for globalization of neuroinformatics occurred when the participating governments of the Mega Science Forum (MSF) of the OECD were asked if they had any new scientific initiatives to bring forward for scientific cooperation around the globe. The White House Office of Science and Technology Policy requested that agencies in the federal government meet at NIH to decide if cooperation were needed that would be of global benefit. The NIH held a series of meetings in which proposals from different agencies were discussed. The proposal recommendation from the U.S. for the MSF was a combination of the NSF and NIH proposals. Jim Edwards of NSF supported databases and data-sharing in the area of biodiversity; Koslow proposed the HPG as a model for sharing neuroscientific data, with the new moniker of neuroinformatics.
The two related initiates were combined to form the United States proposal on ""Biological Informatics"". This initiative was supported by the White House Office of Science and Technology Policy and presented at the OECD MSF by Edwards and Koslow. An MSF committee was established on Biological Informatics with two subcommittees: 1. Biodiversity (Chair, James Edwards, NSF), and 2. Neuroinformatics (Chair, Stephen Koslow, NIH). At the end of two years the Neuroinformatics subcommittee of the Biological Working Group issued a report supporting a global neuroinformatics effort. Koslow, working with the NIH and the White House Office of Science and Technology Policy to establishing a new Neuroinformatics working group to develop specific recommendation to support the more general recommendations of the first report. The Global Science Forum (GSF; renamed from MSF) of the OECD supported this recommendation.


=== The International Neuroinformatics Coordinating Facility ===

This committee presented 3 recommendations to the member governments of GSF. These recommendations were:
National neuroinformatics programs should be continued or initiated in each country should have a national node to both provide research resources nationally and to serve as the contact for national and international coordination.
An International Neuroinformatics Coordinating Facility (INCF) should be established. The INCF will coordinate the implementation of a global neuroinformatics network through integration of national neuroinformatics nodes.
A new international funding scheme should be established. This scheme should eliminate national and disciplinary barriers and provide a most efficient approach to global collaborative research and data sharing. In this new scheme, each country will be expected to fund the participating researchers from their country.
The GSF neuroinformatics committee then developed a business plan for the operation, support and establishment of the INCF which was supported and approved by the GSF Science Ministers at its 2004 meeting. In 2006 the INCF was created and its central office established and set into operation at the Karolinska Institute, Stockholm, Sweden under the leadership of Sten Grillner. Sixteen countries (Australia, Canada, China, the Czech Republic, Denmark, Finland, France, Germany, India, Italy, Japan, the Netherlands, Norway, Sweden, Switzerland, the United Kingdom and the United States), and the EU Commission established the legal basis for the INCF and Programme in International Neuroinformatics (PIN). To date, eighteen countries (Australia, Belgium, Czech Republic, Finland, France, Germany, India, Italy, Japan, Malaysia, Netherlands, Norway, Poland, Republic of Korea, Sweden, Switzerland, the United Kingdom and the United States) are members of the INCF. Membership is pending for several other countries.
The goal of the INCF is to coordinate and promote international activities in neuroinformatics. The INCF contributes to the development and maintenance of database and computational infrastructure and support mechanisms for neuroscience applications. The system is expected to provide access to all freely accessible human brain data and resources to the international research community. The more general task of INCF is to provide conditions for developing convenient and flexible applications for neuroscience laboratories in order to improve our knowledge about the human brain and its disorders.


=== Society for Neuroscience Brain Information Group ===
On the foundation of all of these activities, Huda Akil, the 2003 President of the Society for Neuroscience (SfN) established the Brain Information Group (BIG) to evaluate the importance of neuroinformatics to neuroscience and specifically to the SfN. Following the report from BIG, SfN also established a neuroinformatics committee.
In 2004, SfN announced the Neuroscience Database Gateway (NDG) as a universal resource for neuroscientists through which almost any neuroscience databases and tools may be reached. The NDG was established with funding from NIDA, NINDS and NIMH. The Neuroscience Database Gateway has transitioned to a new enhanced platform, the Neuroscience Information Framework. Funded by the NIH Neuroscience BLueprint, the NIF is a dynamic portal providing access to neuroscience-relevant resources (data, tools, materials) from a single search interface. The NIF builds upon the foundation of the NDG, but provides a unique set of tools tailored especially for neuroscientists: a more expansive catalog, the ability to search multiple databases directly from the NIF home page, a custom web index of neuroscience resources, and a neuroscience-focused literature search function.


== Collaboration with other disciplines ==
Neuroinformatics is formed at the intersections of the following fields:

Biology is concerned with molecular data (from genes to cell specific expression); medicine and anatomy with the structure of synapses and systems level anatomy; engineering – electrophysiology (from single channels to scalp surface EEG), brain imaging; computer science – databases, software tools, mathematical sciences – models, chemistry – neurotransmitters, etc. Neuroscience uses all aforementioned experimental and theoretical studies to learn about the brain through its various levels. Medical and biological specialists help to identify the unique cell types, and their elements and anatomical connections. Functions of complex organic molecules and structures, including a myriad of biochemical, molecular, and genetic mechanisms which regulate and control brain function, are determined by specialists in chemistry and cell biology. Brain imaging determines structural and functional information during mental and behavioral activity. Specialists in biophysics and physiology study physical processes within neural cells neuronal networks. The data from these fields of research is analyzed and arranged in databases and neural models in order to integrate various elements into a sophisticated system; this is the point where neuroinformatics meets other disciplines.
Neuroscience provides the following types of data and information on which neuroinformatics operates:
Molecular and cellular data (ion channel, action potential, genetics, cytology of neurons, protein pathways),
Data from organs and systems (visual cortex, perception, audition, sensory system, pain, taste, motor system, spinal cord),
Cognitive data (language, emotion, motor learning, sexual behavior, decision making, social neuroscience),
Developmental information (neuronal differentiation, cell survival, synaptic formation, motor differentiation, injury and regeneration, axon guidance, growth factors),
Information about diseases and aging (autonomic nervous system, depression, anxiety, Parkinson's disease, addiction, memory loss),
Neural engineering data (brain-computer interface), and
Computational neuroscience data (computational models of various neuronal systems, from membrane currents, proteins to learning and memory).
Neuroinformatics uses databases, the Internet, and visualization in the storage and analysis of the mentioned neuroscience data.


== Research programs and groups ==


=== Australia ===
Neuroimaging & Neuroinformatics, Howard Florey Institute, University of Melbourne
Institute scientists utilize brain imaging techniques, such as magnetic resonance imaging, to reveal the organization of brain networks involved in human thought. Led by Gary Egan.


=== Canada ===
McGill Centre for Integrative Neuroscience (MCIN), Montreal Neurological Institute, McGill University
Led by Alan Evans, MCIN conducts computationally-intensive brain research using innovative mathematical and statistical approaches to integrate clinical, psychological and brain imaging data with genetics. MCIN researchers and staff also develop infrastructure and software tools in the areas of image processing, databasing, and high performance computing. The MCIN community, together with the Ludmer Centre for Neuroinformatics and Mental Health, collaborates with a broad range of researchers and increasingly focuses on open data sharing and open science, including for the Montreal Neurological Institute.


=== Denmark ===
The THOR Center for Neuroinformatics
Established April 1998 at the Department of Mathematical Modelling, Technical University of Denmark. Besides pursuing independent research goals, the THOR Center hosts a number of related projects concerning neural networks, functional neuroimaging, multimedia signal processing, and biomedical signal processing.


=== Germany ===
The Neuroinformatics Portal Pilot
The project is part of a larger effort to enhance the exchange of neuroscience data, data-analysis tools, and modeling software. The portal is supported from many members of the OECD Working Group on Neuroinformatics. The Portal Pilot is promoted by the German Ministry for Science and Education.
Computational Neuroscience, ITB, Humboldt-University Berlin
This group focuses on computational neurobiology, in particular on the dynamics and signal processing capabilities of systems with spiking neurons. Lead by Andreas VM Herz.
The Neuroinformatics Group in Bielefeld
Active in the field of Artificial Neural Networks since 1989. Current research programmes within the group are focused on the improvement of man-machine-interfaces, robot-force-control, eye-tracking experiments, machine vision, virtual reality and distributed systems.


=== Italy ===
Laboratory of Computational Embodied Neuroscience (LOCEN)
This group, part of the Institute of Cognitive Sciences and Technologies, Italian National Research Council (ISTC-CNR) in Rome and founded in 2006 is currently led by Gianluca Baldassarre. It has two objectives: (a) understanding the brain mechanisms underlying learning and expression of sensorimotor behaviour, and related motivations and higher-level cognition grounded on it, on the basis of embodied computational models; (b) transferring the acquired knowledge to building innovative controllers for autonomous humanoid robots capable of learning in an open-ended fashion on the basis of intrinsic and extrinsic motivations.


=== Japan ===
Japan national neuroinformatics resource
The Visiome Platform is the Neuroinformatics Search Service that provides access to mathematical models, experimental data, analysis libraries and related resources. An online portal for neurophysiological data sharing is also available at BrainLiner.jp as part of the MEXT Strategic Research Program for Brain Sciences (SRPBS).
Laboratory for Mathematical Neuroscience, RIKEN Brain Science Institute (Wako, Saitama)
The target of Laboratory for Mathematical Neuroscience is to establish mathematical foundations of brain-style computations toward construction of a new type of information science. Led by Shun-ichi Amari.


=== The Netherlands ===
Netherlands state program in neuroinformatics
Started in the light of the international OECD Global Science Forum which aim is to create a worldwide program in Neuroinformatics.


=== Pakistan ===
NUST-SEECS Neuroinformatics Research Lab
Establishment of the Neuro-Informatics Lab at SEECS-NUST has enabled Pakistani researchers and members of the faculty to actively participate in such efforts, thereby becoming an active part of the above-mentioned experimentation, simulation, and visualization processes. The lab collaborates with the leading international institutions to develop highly skilled human resource in the related field. This lab facilitates neuroscientists and computer scientists in Pakistan to conduct their experiments and analysis on the data collected using state of the art research methodologies without investing in establishing the experimental neuroscience facilities. The key goal of this lab is to provide state of the art experimental and simulation facilities, to all beneficiaries including higher education institutes, medical researchers/practitioners, and technology industry.


=== Switzerland ===
The Blue Brain Project
The Blue Brain Project was founded in May 2005, and uses an 8000 processor Blue Gene/L supercomputer developed by IBM. At the time, this was one of the fastest supercomputers in the world.
The project involves:
Databases: 3D reconstructed model neurons, synapses, synaptic pathways, microcircuit statistics, computer model neurons, virtual neurons.
Visualization: microcircuit builder and simulation results visualizator, 2D, 3D and immersive visualization systems are being developed.
Simulation: a simulation environment for large-scale simulations of morphologically complex neurons on 8000 processors of IBM's Blue Gene supercomputer.
Simulations and experiments: iterations between large-scale simulations of neocortical microcircuits and experiments in order to verify the computational model and explore predictions.

The mission of the Blue Brain Project is to understand mammalian brain function and dysfunction through detailed simulations. The Blue Brain Project will invite researchers to build their own models of different brain regions in different species and at different levels of detail using Blue Brain Software for simulation on Blue Gene. These models will be deposited in an internet database from which Blue Brain software can extract and connect models together to build brain regions and begin the first whole brain simulations.
The Institute of Neuroinformatics (INI)
Established at the University of Zurich at the end of 1995, the mission of the Institute is to discover the key principles by which brains work and to implement these in artificial systems that interact intelligently with the real world.


=== United Kingdom ===
Genes to Cognition Project
A neuroscience research programme that studies genes, the brain and behaviour in an integrated manner. It is engaged in a large-scale investigation of the function of molecules found at the synapse. This is mainly focused on proteins that interact with the NMDA receptor, a receptor for the neurotransmitter, glutamate, which is required for processes of synaptic plasticity such as long-term potentiation (LTP). Many of the techniques used are high-throughout in nature, and integrating the various data sources, along with guiding the experiments has raised numerous informatics questions. The program is primarily run by Professor Seth Grant at the Wellcome Trust Sanger Institute, but there are many other teams of collaborators across the world.
The CARMEN project
The CARMEN project is a multi-site (11 universities in the United Kingdom) research project aimed at using GRID computing to enable experimental neuroscientists to archive their datasets in a structured database, making them widely accessible for further research, and for modellers and algorithm developers to exploit.
EBI Computational Neurobiology, EMBL-EBI (Hinxton)
The main goal of the group is to build realistic models of neuronal function at various levels, from the synapse to the micro-circuit, based on the precise knowledge of molecule functions and interactions (Systems Biology). Led by Nicolas Le Novère.


=== United States ===
Neuroscience Information Framework
The Neuroscience Information Framework (NIF) is an initiative of the NIH Blueprint for Neuroscience Research, which was established in 2004 by the National Institutes of Health. Unlike general search engines, NIF provides deeper access to a more focused set of resources that are relevant to neuroscience, search strategies tailored to neuroscience, and access to content that is traditionally ""hidden"" from web search engines. The NIF is a dynamic inventory of neuroscience databases, annotated and integrated with a unified system of biomedical terminology (i.e. NeuroLex). NIF supports concept-based queries across multiple scales of biological structure and multiple levels of biological function, making it easier to search for and understand the results. NIF will also provide a registry through which resources providers can disclose availability of resources relevant to neuroscience research. NIF is not intended to be a warehouse or repository itself, but a means for disclosing and locating resources elsewhere available via the web.
Neurogenetics GeneNetwork
Genenetwork started as component of the NIH Human Brain Project in 1999 with a focus on the genetic analysis of brain structure and function. This international program consists of tightly integrated genome and phenome data sets for human, mouse, and rat that are designed specifically for large-scale systems and network studies relating gene variants to differences in mRNA and protein expression and to differences in CNS structure and behavior. The great majority of data are open access. GeneNetwork has a companion neuroimaging web site—the Mouse Brain Library—that contains high resolution images for thousands of genetically defined strains of mice.
The Neuronal Time Series Analysis (NTSA)
NTSA Workbench is a set of tools, techniques and standards designed to meet the needs of neuroscientists who work with neuronal time series data. The goal of this project is to develop information system that will make the storage, organization, retrieval, analysis and sharing of experimental and simulated neuronal data easier. The ultimate aim is to develop a set of tools, techniques and standards in order to satisfy the needs of neuroscientists who work with neuronal data.
The Cognitive Atlas
The Cognitive Atlas is a project developing a shared knowledge base in cognitive science and neuroscience. This comprises two basic kinds of knowledge: tasks and concepts, providing definitions and properties thereof, and also relationships between them. An important feature of the site is ability to cite literature for assertions (e.g. ""The Stroop task measures executive control"") and to discuss their validity. It contributes to NeuroLex and the Neuroscience Information Framework, allows programmatic access to the database, and is built around semantic web technologies.
Brain Big Data research group at the Allen Institute for Brain Science (Seattle, WA)
Led by Hanchuan Peng, this group has focused on using large-scale imaging computing and data analysis techniques to reconstruct single neuron models and mapping them in brains of different animals.


== Technologies and developments ==
The main technological tendencies in neuroinformatics are:
Application of computer science for building databases, tools, and networks in neuroscience;
Analysis and modeling of neuronal systems.
In order to organize and operate with neural data scientists need to use the standard terminology and atlases that precisely describe the brain structures and their relationships.
Neuron Tracing and Reconstruction is an essential technique to establish digital models of the morphology of neurons. Such morphology is useful for neuron classification and simulation.
BrainML is a system that provides a standard XML metaformat for exchanging neuroscience data.
The Biomedical Informatics Research Network (BIRN) is an example of a grid system for neuroscience. BIRN is a geographically distributed virtual community of shared resources offering vast scope of services to advance the diagnosis and treatment of disease. BIRN allows combining databases, interfaces and tools into a single environment.
Budapest Reference Connectome is a web-based 3D visualization tool to browse connections in the human brain. Nodes, and connections are calculated from the MRI datasets of the Human Connectome Project.
GeneWays is concerned with cellular morphology and circuits. GeneWays is a system for automatically extracting, analyzing, visualizing and integrating molecular pathway data from the research literature. The system focuses on interactions between molecular substances and actions, providing a graphical view on the collected information and allows researchers to review and correct the integrated information.
Neocortical Microcircuit Database (NMDB). A database of versatile brain's data from cells to complex structures. Researchers are able not only to add data to the database but also to acquire and edit one.
SenseLab. SenseLab is a long-term effort to build integrated, multidisciplinary models of neurons and neural systems. It was founded in 1993 as part of the original Human Brain Project. A collection of multilevel neuronal databases and tools. SenseLab contains six related databases that support experimental and theoretical research on the membrane properties that mediate information processing in nerve cells, using the olfactory pathway as a model system.
BrainMaps.org is an interactive high-resolution digital brain atlas using a high-speed database and virtual microscope that is based on over 12 million megapixels of scanned images of several species, including human.
Another approach in the area of the brain mappings is the probabilistic atlases obtained from the real data from different group of people, formed by specific factors, like age, gender, diseased etc. Provides more flexible tools for brain research and allow obtaining more reliable and precise results, which cannot be achieved with the help of traditional brain atlases.


== See also ==


== Notes and references ==


== Bibliography ==


== Further reading ==


=== Books ===


=== Journals ==="
22,Graph isomorphism problem,1950766,35744,"The graph isomorphism problem is the computational problem of determining whether two finite graphs are isomorphic.
The problem is not known to be solvable in polynomial time nor to be NP-complete, and therefore may be in the computational complexity class NP-intermediate. It is known that the graph isomorphism problem is in the low hierarchy of class NP, which implies that it is not NP-complete unless the polynomial time hierarchy collapses to its second level. At the same time, isomorphism for many special classes of graphs can be solved in polynomial time, and in practice graph isomorphism can often be solved efficiently.
This problem is a special case of the subgraph isomorphism problem, which asks whether a given graph G contains a subgraph that is isomorphic to another given graph H and which is known to be NP-complete. It is also known to be a special case of the non-abelian hidden subgroup problem over the symmetric group.
In the area of image recognition it is known as the exact graph matching.


== State of the art ==
The best currently accepted theoretical algorithm is due to Babai & Luks (1983), and is based on the earlier work by Luks (1982) combined with a subfactorial algorithm of V. N. Zemlyachenko (Zemlyachenko, Korneenko & Tyshkevich 1985). The algorithm has run time 2O(√n log n) for graphs with n vertices and relies on the classification of finite simple groups. Without CFSG, a slightly weaker bound 2O(√n log2 n) was obtained first for strongly regular graphs by László Babai (1980), and then extended to general graphs by Babai & Luks (1983). Improvement of the exponent √n is a major open problem; for strongly regular graphs this was done by Spielman (1996). For hypergraphs of bounded rank, a subexponential upper bound matching the case of graphs was obtained by Babai & Codenotti (2008).
In November 2015, Babai announced a quasipolynomial time algorithm for all graphs, that is, one with running time 
  
    
      
        
          2
          
            O
            (
            (
            log
            ⁡
            n
            
              )
              
                c
              
            
            )
          
        
      
    
    {\displaystyle 2^{O((\log n)^{c})}}
   for some fixed 
  
    
      
        c
        >
        0
      
    
    {\displaystyle c>0}
  . On January 4 2017, Babai retracted the quasi-polynomial claim and stated a sub-exponential time bound instead after Harald Helfgott discovered a flaw in the proof. On January 9 2017, Babai announced a correction (published in full on January 19) and restored the quasi-polynomial claim, with Helfgott confirming the fix. Helfgott further claims that one can take c = 3, so the running time is 2O((log n)3).. The new proof has not been fully peer-reviewed yet.
There are several competing practical algorithms for graph isomorphism, such as those due to McKay (1981), Schmidt & Druffel (1976), and Ullman (1976). While they seem to perform well on random graphs, a major drawback of these algorithms is their exponential time performance in the worst case.
The graph isomorphism problem is computationally equivalent to the problem of computing the automorphism group of a graph, and is weaker than the permutation group isomorphism problem and the permutation group intersection problem. For the latter two problems, Babai, Kantor & Luks (1983) obtained complexity bounds similar to that for graph isomorphism.


== Solved special cases ==
A number of important special cases of the graph isomorphism problem have efficient, polynomial-time solutions:
Trees
Planar graphs (In fact, planar graph isomorphism is in log space, a class contained in P)
Interval graphs
Permutation graphs
Circulant graphs
Bounded-parameter graphs
Graphs of bounded treewidth
Graphs of bounded genus (Note: planar graphs are graphs of genus 0)
Graphs of bounded degree
Graphs with bounded eigenvalue multiplicity
k-Contractible graphs (a generalization of bounded degree and bounded genus)
Color-preserving isomorphism of colored graphs with bounded color multiplicity (i.e., at most k vertices have the same color for a fixed k) is in class NC, which is a subclass of P


== Complexity class GI ==
Since the graph isomorphism problem is neither known to be NP-complete nor known to be tractable, researchers have sought to gain insight into the problem by defining a new class GI, the set of problems with a polynomial-time Turing reduction to the graph isomorphism problem. If in fact the graph isomorphism problem is solvable in polynomial time, GI would equal P.
As is common for complexity classes within the polynomial time hierarchy, a problem is called GI-hard if there is a polynomial-time Turing reduction from any problem in GI to that problem, i.e., a polynomial-time solution to a GI-hard problem would yield a polynomial-time solution to the graph isomorphism problem (and so all problems in GI). A problem 
  
    
      
        X
      
    
    {\displaystyle X}
   is called complete for GI, or GI-complete, if it is both GI-hard and a polynomial-time solution to the GI problem would yield a polynomial-time solution to 
  
    
      
        X
      
    
    {\displaystyle X}
  .
The graph isomorphism problem is contained in both NP and co-AM. GI is contained in and low for Parity P, as well as contained in the potentially much smaller class SPP. That it lies in Parity P means that the graph isomorphism problem is no harder than determining whether a polynomial-time nondeterministic Turing machine has an even or odd number of accepting paths. GI is also contained in and low for ZPPNP. This essentially means that an efficient Las Vegas algorithm with access to an NP oracle can solve graph isomorphism so easily that it gains no power from being given the ability to do so in constant time.


=== GI-complete and GI-hard problems ===


==== Isomorphism of other objects ====
There are a number of classes of mathematical objects for which the problem of isomorphism is a GI-complete problem. A number of them are graphs endowed with additional properties or restrictions:
digraphs
labelled graphs, with the proviso that an isomorphism is not required to preserve the labels, but only the equivalence relation consisting of pairs of vertices with the same label
""polarized graphs"" (made of a complete graph Km and an empty graph Kn plus some edges connecting the two; their isomorphism must preserve the partition)
2-colored graphs
explicitly given finite structures
multigraphs
hypergraphs
finite automata
Markov Decision Processes
commutative class 3 nilpotent (i.e., xyz = 0 for every elements x, y, z) semigroups
finite rank associative algebras over a fixed algebraically closed field with zero squared radical and commutative factor over the radical.
context-free grammars
balanced incomplete block designs
Recognizing combinatorial isomorphism of convex polytopes represented by vertex-facet incidences.


==== GI-complete classes of graphs ====
A class of graphs is called GI-complete if recognition of isomorphism for graphs from this subclass is a GI-complete problem. The following classes are GI-complete:
connected graphs
graphs of diameter 2 and radius 1
directed acyclic graphs
regular graphs
bipartite graphs without non-trivial strongly regular subgraphs
bipartite Eulerian graphs
bipartite regular graphs
line graphs
split graphs
chordal graphs
regular self-complementary graphs
polytopal graphs of general, simple, and simplicial convex polytopes in arbitrary dimensions.

Many classes of digraphs are also GI-complete.


==== Other GI-complete problems ====
There are other nontrivial GI-complete problems in addition to isomorphism problems.
The recognition of self-complementarity of a graph or digraph.
A clique problem for a class of so-called M-graphs. It is shown that finding an isomorphism for n-vertex graphs is equivalent to finding an n-clique in an M-graph of size n2. This fact is interesting because the problem of finding an (n − ε)-clique in a M-graph of size n2 is NP-complete for arbitrarily small positive ε.
The problem of homeomorphism of 2-complexes.


==== GI-hard problems ====
The problem of counting the number of isomorphisms between two graphs is polynomial-time equivalent to the problem of telling whether even one exists.
The problem of deciding whether two convex polytopes given by either the V-description or H-description are projectively or affinely isomorphic. The latter means existence of a projective or affine map between the spaces that contain the two polytopes (not necessarily of the same dimension) which induces a bijection between the polytopes.


== Program checking ==
Manuel Blum and Sampath Kannan (1995) have shown a probabilistic checker for programs for graph isomorphism. Suppose P is a claimed polynomial-time procedure that checks if two graphs are isomorphic, but it is not trusted. To check if G and H are isomorphic:
Ask P whether G and H are isomorphic.
If the answer is ""yes':
Attempt to construct an isomorphism using P as subroutine. Mark a vertex u in G and v in H, and modify the graphs to make them distinctive (with a small local change). Ask P if the modified graphs are isomorphic. If no, change v to a different vertex. Continue searching.
Either the isomorphism will be found (and can be verified), or P will contradict itself.

If the answer is ""no"":
Perform the following 100 times. Choose randomly G or H, and randomly permute its vertices. Ask P if the graph is isomorphic to G and H. (As in AM protocol for graph nonisomorphism).
If any of the tests are failed, judge P as invalid program. Otherwise, answer ""no"".

This procedure is polynomial-time and gives the correct answer if P is a correct program for graph isomorphism. If P is not a correct program, but answers correctly on G and H, the checker will either give the correct answer, or detect invalid behaviour of P. If P is not a correct program, and answers incorrectly on G and H, the checker will detect invalid behaviour of P with high probability, or answer wrong with probability 2−100.
Notably, P is used only as a blackbox.


== Applications ==
Graphs are commonly used to encode structural information in many fields, including computer vision and pattern recognition, and graph matching, i.e., identification of similarities between graphs, is an important tools in these areas. In these areas graph isomorphism problem is known as the exact graph matching. 
In cheminformatics and in mathematical chemistry, graph isomorphism testing is used to identify a chemical compound within a chemical database. Also, in organic mathematical chemistry graph isomorphism testing is useful for generation of molecular graphs and for computer synthesis.
Chemical database search is an example of graphical data mining, where the graph canonization approach is often used. In particular, a number of identifiers for chemical substances, such as SMILES and InChI, designed to provide a standard and human-readable way to encode molecular information and to facilitate the search for such information in databases and on the web, use canonization step in their computation, which is essentially the canonization of the graph which represents the molecule.
In electronic design automation graph isomorphism is the basis of the Layout Versus Schematic (LVS) circuit design step, which is a verification whether the electric circuits represented by a circuit schematic and an integrated circuit layout are the same.


== See also ==
Graph automorphism problem
Graph canonization


== Notes ==


== References =="
23,History of software,40601008,34227,"Software can be defined as programmed instructions stored in the memory of stored-program digital computers for execution by the processor. The design for what would have been the first piece of software was written by Ada Lovelace in the 19th century but was never implemented.
Alan Turing is credited with being the first person to come up with a theory for software, which led to the two academic fields of computer science and software engineering. The first generation of software for early stored program digital computers in the late 1940s had its instructions written directly in binary code. Early on, it was very expensive when it was in low quantities, but as it became more popular in the 1980s, prices dropped significantly. It went from being an item that only belonged to the elite to the majority of the population owning one.


== Before stored-program digital computers ==


=== Origins of computer science ===

An outline (algorithm) for what would have been the first piece of software was written by Ada Lovelace in the 19th century, for the planned Analytical Engine. However, neither the Analytical Engine nor any software for it was ever created.
The first theory about software –  prior to the creation of computers as we know them today –  was proposed by Alan Turing in his 1935 essay Computable numbers with an application to the Entscheidungsproblem (decision problem).
This eventually led to the creation of the twin academic fields of computer science and software engineering, which both study software and its creation. Computer science is more theoretical (Turing's essay is an example of computer science), whereas software engineering is focused on more practical concerns.
However, prior to 1946, software as we now understand it –  programs stored in the memory of stored-program digital computers –  did not yet exist. The very first electronic computing devices were instead rewired in order to ""reprogram"" them –  see History of computing hardware.


== Early days of computer software (1948–1979) ==
In his manuscript ""A Mathematical theory of Communication"", Claude Shannon (1916–2001) provided an outline for how binary logic could be implemented to program a computer. Subsequently, the first computer programmers used binary code to instruct computers to perform various tasks. Nevertheless, the process was very arduous. Computer programmers had to enter long strings of binary code to tell the computer what data to store. Computer programmers had to load information onto computers using various tedious mechanisms, including flicking switches or punching holes at predefined positions in cards and loading these punched cards into a computer. With such methods, if a mistake was made, the whole program might have to be loaded again from the beginning.
The very first time a stored-program computer held a piece of software in an electronic memory, and executed it successfully, was 11am, 21 June 1948, at the University of Manchester, on the Small Scale Experimental Machine, also known as the ""Baby"" computer. It was written by Tom Kilburn, and calculated the highest factor of the integer 2^18 = 262,144. Starting with a large trial divisor, it performed division of 262,144 by repeated subtraction then checked if the remainder was zero. If not, it decremented the trial divisor by one and repeated the process. Google released a tribute to the Manchester Baby, celebrating it as the ""birth of software"".


=== Bundling of software with hardware and its legal issues ===
Later, software was sold to multiple customers by being bundled with the hardware by original equipment manufacturers (OEMs) such as Data General, Digital Equipment and IBM. When a customer bought a minicomputer, at that time the smallest computer on the market, the computer did not come with Pre-installed software, but needed to be installed by engineers employed by the OEM.
This bundling attracted the attention of US antitrust regulators, who sued IBM for improper ""tying"" in 1969, alleging that it was an antitrust violation that customers who wanted to obtain its software had to also buy or lease its hardware in order to do so. Although the case was dropped by the US Justice Department after many years of attrition as ""without merit"".
Very quickly, commercial software started to be pirated, and commercial software producers were very unhappy at this. Bill Gates, cofounder of Microsoft, was an early moraliser against software piracy with his famous Open Letter to Hobbyists in 1976.
Data General also encountered legal problems related to bundling –  although in this case, it was due to a civil suit from a would-be competitor. When Data General introduced the Data General Nova, a company called Digidyne wanted to use its RDOS operating system on its own hardware clone. Data General refused to license their software and claimed their ""bundling rights"". The US Supreme Court set a precedent called Digidyne v. Data General in 1985 by letting a 9th circuit appeal court decision on the case stand, and Data General was eventually forced into licensing the operating system because it was ruled that restricting the license to only DG hardware was an illegal tying arrangement. Even though the District Court noted that ""no reasonable juror could find that within this large and dynamic market with much larger competitors"", Data General ""had the market power to restrain trade through an illegal tie-in arrangement"", the tying of the operating system to the hardware was ruled as per se illegal on appeal.
In 2008, Psystar Corporation was sued by Apple Inc. for distributing unauthorized Macintosh clones with OS X preinstalled, and countersued. One of the arguments in the countersuit - citing the Data General case - was that Apple dominates the market for OS X compatible computers by illegally tying the operating system to Apple computers. District Court Judge William Alsup rejected this argument, saying, as the District Court had ruled in the Data General case over 20 years prior, that the relevant market was not simply one operating system (Mac OS) but all PC operating systems, including Mac OS, and noting that Mac OS did not enjoy a dominant position in that broader market. Alsup's judgement also noted that the surprising Data General precedent that tying of copyrighted products was always illegal had since been ""implicitly overruled"" by the verdict in the Illinois Tool Works Inc. v. Independent Ink, Inc. case.


=== Unix (1970s–present) ===

Unix was an early operating system which became popular and very influential, and still exists today. The most popular variant of Unix today is macOS (previously OS X and Mac OS X), while Linux is closely related to Unix.


=== Pre-Internet source code sharing ===
Before the Internet –  and indeed in the period after the internet was created, but before it came into widespread use by the public –  computer programming enthusiasts had to find other ways to share their efforts with each other, and also with potentially-interested computer users who were not themselves programmers. Such sharing techniques included distribution of tapes, such as the DECUS tapes, and later, electronic bulletin board systems. However, a particularly popular and mainstream early technique involved computer magazines.


==== Source code listings in computer magazines ====

Tiny BASIC was published as a type-in program in Dr. Dobb's Journal in 1975, and developed collaboratively (in effect, an early example of open source software, although that particular term was not to be coined until two decades later).
It was an inconvenient and slow process to type in source code from a computer magazine, and a single mistyped –  or worse, misprinted –  character could render the program inoperable, yet people still did so. (Optical character recognition technology to scan in the listings rather than transcribe them by hand was not yet available).
However, even with the widespread use of cartridges and cassette tapes in the 1980s for distribution of commercial software, free programs (such as simple educational programs for the purpose of teaching programming techniques) were still often printed, because it was cheaper than manufacturing and attaching cassette tapes to each copy of a magazine. Many of today's IT professionals who were children at the time had a lifelong interest in computing in general or programming in particular sparked by such first encounters with source code.
However, eventually a combination of four factors brought this practice of printing complete source code listings of entire programs in computer magazines to an end:
programs started to become very large
floppy discs started to be used for distributing software, and then came down in price
more and more people started to use computers –  computing became a mass market phenomenon, and most ordinary people were far less likely to want to spend hours typing in listings than the earlier enthusiasts
partly as a consequence of all of the above factors, computer magazines started to attach free cassette tapes, and free floppy discs, with free or trial versions of software on them, to their covers


== 1980s–present ==
Before the microcomputer, a successful software program typically sold up to 1,000 units at $50,000–60,000 each. By the mid-1980s, personal computer software sold thousands of copies for $50–700 each. Companies like Microsoft, MicroPro, and Lotus Development had tens of millions of dollars in annual sales. Just like the auto industry, the software industry has grown from a few visionaries operating (figuratively or literally) out of their garage with prototypes. Steve Jobs and Bill Gates were the Henry Ford and Louis Chevrolet of their times, who capitalized on ideas already commonly known before they started in the business. A pivotal moment in computing history was the publication in the 1980s of the specifications for the IBM Personal Computer published by IBM employee Philip Don Estridge, which quickly led to the dominance of the PC in the worldwide desktop and later laptop markets –  a dominance which continues to this day.


=== Free and open source software ===


=== Recent developments ===


==== App stores ====

Applications for mobile devices (cellphones and tablets) have been termed ""apps"" in recent years. Apple chose to funnel iPhone and iPad app sales through their App Store, and thus both vet apps, and get a cut of every paid app sold. Apple does not allow apps which could be used to circumvent their app store (e.g. virtual machines such as the Java or Flash virtual machines).
The Android platform, by contrast, has multiple app stores available for it, and users can generally select which to use (although Google Play requires a compatible or rooted device).
This move was replicated for desktop operating systems with GNOME Software (for Linux), the Mac App Store (for macOS), and the Windows Store (for Windows). All of these platforms remain, as they have always been, non-exclusive: they allow applications to be installed from outside the app store, and indeed from other app stores.
The explosive rise in popularity of apps, for the iPhone in particular but also for Android, led to a kind of ""gold rush"", with some hopeful programmers dedicating a significant amount of time to creating apps in the hope of striking it rich. As in real gold rushes, not all of these hopeful entrepreneurs were successful.


== Formalization of software development ==
The development of curricula in computer science has resulted in improvements in software development. Components of these curricula include:
Structured and Object Oriented programming
Data structures
Analysis of Algorithms
Formal languages and compiler construction
Computer Graphics Algorithms
Sorting and Searching
Numerical Methods, Optimization and Statistics
Artificial Intelligence and Machine Learning


== How software has affected hardware ==
As more and more programs enter the realm of firmware, and the hardware itself becomes smaller, cheaper and faster as predicted by Moore's law, an increasing number of types of functionality of computing first carried out by software, have joined the ranks of hardware, as for example with graphics processing units. (However, the change has sometimes gone the other way for cost or other reasons, as for example with softmodems and microcode.)
Most hardware companies today have more software programmers on the payroll than hardware designers, since software tools have automated many tasks of printed circuit board (PCB) engineers.


== Computer software and programming language timeline ==
The following tables include year by year development of many different aspects of computer software including:
High level languages

Operating systems

Networking software and applications

Computer graphics hardware, algorithms and applications

Spreadsheets

Word processing

Computer aided design


=== 1971–1974 ===


=== 1975–1978 ===


=== 1979–1982 ===


=== 1983–1986 ===


=== 1987–1990 ===


=== 1991–1994 ===


=== 1995–1998 ===


=== 1999–2002 ===


=== 2003–2006 ===


=== 2007–2010 ===


=== 2011–2014 ===


== See also ==

Forensic software engineering
History of computing hardware
History of operating systems
History of software engineering
List of failed and overbudget custom software projects


== References =="
24,Computational neuroscience,271430,31195,"Computational neuroscience (also known as theoretical neuroscience or mathematical neuroscience) is a branch of neuroscience which employs mathematical models, theoretical analysis and abstractions of the brain to understand the principles that govern the development, structure, information-processing, physiology and cognitive abilities of the nervous system.
Computational neuroscience focuses on the description of functional and biologically realistic neurons (and neural systems) and their physiology and dynamics, distinguishing it from psychological connectionism and disciplines such as machine learning, neural networks, and computational learning theory.
These models are useful since they capture the essential features of the biological system at multiple spatial-temporal scales, from membrane currents, proteins, and chemical coupling to network oscillations, columnar and topographic architecture, and learning and memory. Furthermore, these computational models frame hypotheses that can be directly tested by biological or psychological experiments.


== History ==
The term ""computational neuroscience"" was introduced by Eric L. Schwartz, who organized a conference, held in 1985 in Carmel, California, at the request of the Systems Development Foundation to provide a summary of the current status of a field which until that point was referred to by a variety of names, such as neural modeling, brain theory and neural networks. The proceedings of this definitional meeting were published in 1990 as the book Computational Neuroscience. The first open international meeting focused on Computational Neuroscience was organized by James M. Bower and John Miller in San Francisco, California in 1989 and has continued each year since as the annual CNS meeting. The first graduate educational program in computational neuroscience was organized as the Computational and Neural Systems Ph.D. program at the California Institute of Technology in 1985.
The early historical roots of the field can be traced to the work of people such as Louis Lapicque, Hodgkin & Huxley, Hubel & Wiesel, and David Marr, to name a few. Lapicque introduced the integrate and fire model of the neuron in a seminal article published in 1907. This model is still popular today for mathematical, biological, and artificial neural networks studies because of its simplicity (see a recent review) and, in fact, has seen recent experimental and biophysical support.
About 40 years later, Hodgkin & Huxley developed the voltage clamp and created the first biophysical model of the action potential. Hubel & Wiesel discovered that neurons in the primary visual cortex, the first cortical area to process information coming from the retina, have oriented receptive fields and are organized in columns. David Marr's work focused on the interactions between neurons, suggesting computational approaches to the study of how functional groups of neurons within the hippocampus and neocortex interact, store, process, and transmit information. Computational modeling of biophysically realistic neurons and dendrites began with the work of Wilfrid Rall, with the first multicompartmental model using cable theory.


== Major topics ==
Research in computational neuroscience can be roughly categorized into several lines of inquiry. Most computational neuroscientists collaborate closely with experimentalists in analyzing novel data and synthesizing new models of biological phenomena.


=== Single-neuron modeling ===

Even single neurons have complex biophysical characteristics and can perform computations (e.g.). Hodgkin and Huxley's original model only employed two voltage-sensitive currents (Voltage sensitive ion channels are glycoprotein molecules which extend through the lipid bilayer, allowing ions to traverse under certain conditions through the axolemma), the fast-acting sodium and the inward-rectifying potassium. Though successful in predicting the timing and qualitative features of the action potential, it nevertheless failed to predict a number of important features such as adaptation and shunting. Scientists now believe that there are a wide variety of voltage-sensitive currents, and the implications of the differing dynamics, modulations, and sensitivity of these currents is an important topic of computational neuroscience.
The computational functions of complex dendrites are also under intense investigation. There is a large body of literature regarding how different currents interact with geometric properties of neurons.
Some models are also tracking biochemical pathways at very small scales such as spines or synaptic clefts.
There are many software packages, such as GENESIS and NEURON, that allow rapid and systematic in silico modeling of realistic neurons. Blue Brain, a project founded by Henry Markram from the École Polytechnique Fédérale de Lausanne, aims to construct a biophysically detailed simulation of a cortical column on the Blue Gene supercomputer.
Modeling the richness of biophysical properties on the single-neuron scale can supply mechanisms that serve as the building blocks for network dynamics. However, detailed neuron descriptions are computationally expensive and this can handicap the pursuit of realistic network investigations, where many neurons need to be simulated. As a result, researchers that study large neural circuits typically represent each neuron and synapse with an artificially simple model, ignoring much of the biological detail. Hence there is a drive to produce simplified neuron models that can retain significant biological fidelity at a low computational overhead. Algorithms have been developed to produce faithful, faster running, simplified surrogate neuron models from computationally expensive, detailed neuron models.


=== Development, axonal patterning, and guidance ===
Computational neuroscience aims to address a wide array of questions. How do axons and dendrites form during development? How do axons know where to target and how to reach these targets? How do neurons migrate to the proper position in the central and peripheral systems? How do synapses form? We know from molecular biology that distinct parts of the nervous system release distinct chemical cues, from growth factors to hormones that modulate and influence the growth and development of functional connections between neurons.
Theoretical investigations into the formation and patterning of synaptic connection and morphology are still nascent. One hypothesis that has recently garnered some attention is the minimal wiring hypothesis, which postulates that the formation of axons and dendrites effectively minimizes resource allocation while maintaining maximal information storage.


=== Sensory processing ===
Early models of sensory processing understood within a theoretical framework are credited to Horace Barlow. Somewhat similar to the minimal wiring hypothesis described in the preceding section, Barlow understood the processing of the early sensory systems to be a form of efficient coding, where the neurons encoded information which minimized the number of spikes. Experimental and computational work have since supported this hypothesis in one form or another.
Current research in sensory processing is divided among a biophysical modelling of different subsystems and a more theoretical modelling of perception. Current models of perception have suggested that the brain performs some form of Bayesian inference and integration of different sensory information in generating our perception of the physical world.


=== Memory and synaptic plasticity ===

Earlier models of memory are primarily based on the postulates of Hebbian learning. Biologically relevant models such as Hopfield net have been developed to address the properties of associative, rather than content-addressable, style of memory that occur in biological systems. These attempts are primarily focusing on the formation of medium- and long-term memory, localizing in the hippocampus. Models of working memory, relying on theories of network oscillations and persistent activity, have been built to capture some features of the prefrontal cortex in context-related memory.
One of the major problems in neurophysiological memory is how it is maintained and changed through multiple time scales. Unstable synapses are easy to train but also prone to stochastic disruption. Stable synapses forget less easily, but they are also harder to consolidate. One recent computational hypothesis involves cascades of plasticity that allow synapses to function at multiple time scales. Stereochemically detailed models of the acetylcholine receptor-based synapse with the Monte Carlo method, working at the time scale of microseconds, have been built. It is likely that computational tools will contribute greatly to our understanding of how synapses function and change in relation to external stimulus in the coming decades.


=== Behaviors of networks ===
Biological neurons are connected to each other in a complex, recurrent fashion. These connections are, unlike most artificial neural networks, sparse and usually specific. It is not known how information is transmitted through such sparsely connected networks, although specific areas of the brain, such as the Visual cortex, are understood in some detail. It is also unknown what the computational functions of these specific connectivity patterns are, if any.
The interactions of neurons in a small network can be often reduced to simple models such as the Ising model. The statistical mechanics of such simple systems are well-characterized theoretically. There has been some recent evidence that suggests that dynamics of arbitrary neuronal networks can be reduced to pairwise interactions. It is not known, however, whether such descriptive dynamics impart any important computational function. With the emergence of two-photon microscopy and calcium imaging, we now have powerful experimental methods with which to test the new theories regarding neuronal networks.
In some cases the complex interactions between inhibitory and excitatory neurons can be simplified using mean field theory, which gives rise to the population model of neural networks. While many neurotheorists prefer such models with reduced complexity, others argue that uncovering structural functional relations depends on including as much neuronal and network structure as possible. Models of this type are typically built in large simulation platforms like GENESIS or NEURON. There have been some attempts to provide unified methods that bridge and integrate these levels of complexity.


=== Cognition, discrimination, and learning ===
Computational modeling of higher cognitive functions has only recently begun. Experimental data comes primarily from single-unit recording in primates. The frontal lobe and parietal lobe function as integrators of information from multiple sensory modalities. There are some tentative ideas regarding how simple mutually inhibitory functional circuits in these areas may carry out biologically relevant computation.
The brain seems to be able to discriminate and adapt particularly well in certain contexts. For instance, human beings seem to have an enormous capacity for memorizing and recognizing faces. One of the key goals of computational neuroscience is to dissect how biological systems carry out these complex computations efficiently and potentially replicate these processes in building intelligent machines.
The brain's large-scale organizational principles are illuminated by many fields, including biology, psychology, and clinical practice. Integrative neuroscience attempts to consolidate these observations through unified descriptive models and databases of behavioral measures and recordings. These are the bases for some quantitative modeling of large-scale brain activity.
The Computational Representational Understanding of Mind (CRUM) is another attempt at modeling human cognition through simulated processes like acquired rule-based systems in decision making and the manipulation of visual representations in decision making.


=== Consciousness ===
One of the ultimate goals of psychology/neuroscience is to be able to explain the everyday experience of conscious life. Francis Crick and Christof Koch made some attempts to formulate a consistent framework for future work in neural correlates of consciousness (NCC), though much of the work in this field remains speculative.


=== Computational clinical neuroscience ===
It is a field that brings together experts in neuroscience, neurology, psychiatry, decision sciences and computational modeling to quantitatively define and investigate problems in neurological and psychiatric diseases, and to train scientists and clinicians that wish to apply these models to diagnosis and treatment.


== Notable persons ==
David Marr, neuroscientist and professor of psychology at MIT, noted for his theories of perception
Phil Husbands, professor of computer science and artificial intelligence at the English University of Sussex
Read Montague, American neuroscientist and popular science author
Tomaso Poggio, Eugene McDermott professor in the Department of Brain and Cognitive Sciences, investigator at the McGovern Institute for Brain Research, a member of the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) and director of both the Center for Biological and Computational Learning at MIT and the Center for Brains, Minds, and Machines.
Terry Sejnowski, investigator at the Howard Hughes Medical Institute and the Francis Crick Professor at The Salk Institute for Biological Studies where he directs the Computational Neurobiology Laboratory
Haim Sompolinsky, William N. Skirball Professor of Neuroscience at the Edmond and Lily Safra Center for Brain Sciences (formerly the Interdisciplinary Center for Neural Computation), and a Professor of Physics at the Racah Institute of Physics at The Hebrew University of Jerusalem, Israel.
Peter Dayan, Professor and Director of the Gatsby Computational Neuroscience Unit at University College London.


== See also ==
Biological neuron models
Bayesian Brain
Brain-computer interface
Brain simulation
Computational anatomy
Connectionism
Medical image computing
Mind uploading
Neural coding
Neural engineering
Neural network
Neurocomputational speech processing
Neuroinformatics
Simulated reality
Artificial consciousness
Cognitive architecture
Technological singularity, a hypothetical artificial intelligence that would exceed the capabilities of the human brain


== Notes and references ==


== Bibliography ==
Chklovskii DB (2004). ""Synaptic connectivity and neuronal morphology: two sides of the same coin"". Neuron. 43 (5): 609–17. doi:10.1016/j.neuron.2004.08.012. PMID 15339643. 
Sejnowski, Terrence J.; Churchland, Patricia Smith (1992). The computational brain. Cambridge, Mass: MIT Press. ISBN 0-262-03188-4. 
Gerstner, W.; Kistler, W.; Naud, R.; Paninski, L. (2014). Neuronal Dynamics. Cambridge, UK: Cambridge University Press. ISBN 9781107447615. 
Abbott, L. F.; Dayan, Peter (2001). Theoretical neuroscience: computational and mathematical modeling of neural systems. Cambridge, Mass: MIT Press. ISBN 0-262-04199-5. 
Eliasmith, Chris; Anderson, Charles H. (2003). Neural engineering: Representation, computation, and dynamics in neurobiological systems. Cambridge, Mass: MIT Press. ISBN 0-262-05071-4. 
Hodgkin AL, Huxley AF (28 August 1952). ""A quantitative description of membrane current and its application to conduction and excitation in nerve"". J. Physiol. 117 (4): 500–44. doi:10.1113/jphysiol.1952.sp004764. PMC 1392413 . PMID 12991237. 
William Bialek; Rieke, Fred; David Warland; Rob de Ruyter van Steveninck (1999). Spikes: exploring the neural code. Cambridge, Mass: MIT. ISBN 0-262-68108-0. CS1 maint: Multiple names: authors list (link)
Schutter, Erik de (2001). Computational neuroscience: realistic modeling for experimentalists. Boca Raton: CRC. ISBN 0-8493-2068-2. 
Sejnowski, Terrence J.; Hemmen, J. L. van (2006). 23 problems in systems neuroscience. Oxford [Oxfordshire]: Oxford University Press. ISBN 0-19-514822-3. 
Michael A. Arbib; Shun-ichi Amari; Prudence H. Arbib (2002). The Handbook of Brain Theory and Neural Networks. Cambridge, Massachusetts: The MIT Press. ISBN 0-262-01197-2. 


== External links ==


=== Journals ===
Network: Computation in Neural Systems
Biological Cybernetics
Journal of Computational Neuroscience
Neural Computation
Neural Networks
Neurocomputing
Cognitive Neurodynamics
Frontiers in Computational Neuroscience
PLoS Computational Biology
Frontiers in Neuroinformatics
Journal of Mathematical Neuroscience


=== Software ===
BRIAN, a Python based simulator
Budapest Reference Connectome, web based 3D visualization tool to browse connections in the human brain
Emergent, neural simulation software.
GENESIS, a general neural simulation system.


=== Conferences ===
Computational and Systems Neuroscience (COSYNE) – a computational neuroscience meeting with a systems neuroscience focus.
Annual Computational Neuroscience Meeting (CNS)– a yearly computational neuroscience meeting.
Neural Information Processing Systems (NIPS)– a leading annual conference covering other machine learning topics as well.
International Conference on Cognitive Neurodynamics (ICCN)– a yearly conference.
UK Mathematical Neurosciences Meeting– a new yearly conference, focused on mathematical aspects.
The NeuroComp Conference– a yearly computational neuroscience conference (France).
Bernstein Conference on Computational Neuroscience (BCCN)– a yearly conference in Germany, organized by the Bernstein Network for Computational Neuroscience.
AREADNE Conferences– a biennial meeting that includes theoretical and experimental results, held in even years in Santorini, Greece.


=== Websites ===
Perlewitz's computational neuroscience on the web
Encyclopedia of Computational Neuroscience, part of Scholarpedia, an online expert curated encyclopedia on computational neuroscience, dynamical systems and machine intelligence
Ph.D studies in Computational Neuroscience in Jerusalem


=== Research Groups ===
Laboratory of Computational Embodied Neuroscience (LOCEN), Institute of Cognitive Sciences and Technologies, Italian National Research Council (ISTC-CNR), Rome, Italy. This group, founded in 2006 and currently led by Gianluca Baldassarre, has two objectives: (a) understanding the brain mechanisms underlying learning and the expression of sensorimotor behaviour, the motivations driving it, and higher cognition grounded on it, on the basis of embodied computational models; (b) in synergies with these studies, building innovative controllers for autonomous humanoid robots able to learn in an open-ended fashion driven by intrinsic and extrinsic motivations."
25,History of programming languages,896120,31047,"The first electronic computers had limited speed and memory capacity, forcing programmers to write programs in assembly language, i.e., the native language of the hardware. Once computer capacity increased it became practical to implement higher level languages.
The first high-level programming language was Plankalkül, created by Konrad Zuse between 1942 and 1945. The first high-level language to have an associated compiler, was created by Corrado Böhm in 1951, for his PhD thesis. The first commercially available language was FORTRAN (FORmula TRANslation); developed in 1956 (first manual appeared in 1956, but first developed in 1954) by John Backus, a worker at IBM.
When FORTRAN was first introduced it was treated with suspicion because of the belief that programs compiled from high-level language would be less efficient than those written directly in machine code. FORTRAN became popular because it provided a means of porting existing code to new computers, in a hardware market that was rapidly evolving. FORTRAN eventually became known for its efficiency. Over the years, FORTRAN had been updated, with standards released for FORTRAN-66, FORTRAN-77 and FORTRAN-92.


== Early history ==
During a nine-month period in 1842–1843, Ada Lovelace translated the memoir of Italian mathematician Luigi Menabrea about Charles Babbage's newest proposed machine, the analytical engine. With the article she appended a set of notes which specified in complete detail a method for calculating Bernoulli numbers with the engine, recognized by some historians as the world's first computer program.
The first computer codes were specialized for their applications. In the first decades of the 20th century, numerical calculations were based on decimal numbers. Eventually it was realized that logic could be represented with numbers, not only with words. For example, Alonzo Church was able to express the lambda calculus in a formulatic way. The Turing machine was an abstraction of the operation of a tape-marking machine, for example, in use at the telephone companies. Turing machines set the basis for storage of programs as data in the von Neumann architecture of computers by representing a machine through a finite number. However, unlike the lambda calculus, Turing's code does not serve well as a basis for higher-level languages—its principal use is in rigorous analyses of algorithmic complexity.
To some people, what was the first modern programming language depends on how much power and human-readability is required before the status of ""programming language"" is granted. Jacquard Looms and Charles Babbage's Difference Engine both had simple, extremely limited languages for describing the actions that these machines should perform.


== First programming languages ==
In the 1940s, the first recognizably modern electrically powered computers were created. The limited speed and memory capacity forced programmers to write hand tuned assembly language programs. It was eventually realized that programming in assembly language required a great deal of intellectual effort.
The first programming languages designed to communicate instructions to a computer were written in the 1950s. An early high-level programming language to be designed for a computer was Plankalkül, developed by the Germans for Z1 by Konrad Zuse between 1943 and 1945. However, it was not implemented until 1998 and 2000.
John Mauchly's Short Code, proposed in 1949, was one of the first high-level languages ever developed for an electronic computer. Unlike machine code, Short Code statements represented mathematical expressions in understandable form. However, the program had to be translated into machine code every time it ran, making the process much slower than running the equivalent machine code.
At the University of Manchester, Alick Glennie developed Autocode in the early 1950s, with the second iteration developed for the Mark 1 by R. A. Brooker in 1954, known as the ""Mark 1 Autocode"". Brooker also developed an autocode for the Ferranti Mercury in the 1950s in conjunction with the University of Manchester. The version for the EDSAC 2 was devised by D. F. Hartley of University of Cambridge Mathematical Laboratory in 1961. Known as EDSAC 2 Autocode, it was a straight development from Mercury Autocode adapted for local circumstances, and was noted for its object code optimisation and source-language diagnostics which were advanced for the time. A contemporary but separate thread of development, Atlas Autocode was developed for the University of Manchester Atlas 1 machine.
In 1954, language FORTRAN was invented at IBM by a team led by John Backus; it was the first widely used high level general purpose programming language to have a functional implementation, as opposed to just a design on paper. It is still a popular language for high-performance computing and is used for programs that benchmark and rank the world's fastest supercomputers.
Another early programming language was devised by Grace Hopper in the US, called FLOW-MATIC. It was developed for the UNIVAC I at Remington Rand during the period from 1955 until 1959. Hopper found that business data processing customers were uncomfortable with mathematical notation, and in early 1955, she and her team wrote a specification for an English programming language and implemented a prototype. The FLOW-MATIC compiler became publicly available in early 1958 and was substantially complete in 1959. Flow-Matic was a major influence in the design of COBOL, since only it and its direct descendent AIMACO were in actual use at the time.
Other languages still in use today include LISP (1958), invented by John McCarthy and COBOL (1959), created by the Short Range Committee. Another milestone in the late 1950s was the publication, by a committee of American and European computer scientists, of ""a new language for algorithms""; the ALGOL 60 Report (the ""ALGOrithmic Language""). This report consolidated many ideas circulating at the time and featured three key language innovations:
nested block structure: code sequences and associated declarations could be grouped into blocks without having to be turned into separate, explicitly named procedures;
lexical scoping: a block could have its own private variables, procedures and functions, invisible to code outside that block, that is, information hiding.
Another innovation, related to this, was in how the language was described:
a mathematically exact notation, Backus–Naur form (BNF), was used to describe the language's syntax. Nearly all subsequent programming languages have used a variant of BNF to describe the context-free portion of their syntax.
Algol 60 was particularly influential in the design of later languages, some of which soon became more popular. The Burroughs large systems were designed to be programmed in an extended subset of Algol.
Algol's key ideas were continued, producing ALGOL 68:
syntax and semantics became even more orthogonal, with anonymous routines, a recursive typing system with higher-order functions, etc.;
not only the context-free part, but the full language syntax and semantics were defined formally, in terms of Van Wijngaarden grammar, a formalism designed specifically for this purpose.
Algol 68's many little-used language features (for example, concurrent and parallel blocks) and its complex system of syntactic shortcuts and automatic type coercions made it unpopular with implementers and gained it a reputation of being difficult. Niklaus Wirth actually walked out of the design committee to create the simpler Pascal language.

Some notable languages that were developed in this period include:


== Establishing fundamental paradigms ==

The period from the late 1960s to the late 1970s brought a major flowering of programming languages. Most of the major language paradigms now in use were invented in this period:
Speakeasy (computational environment), developed in 1964 at Argonne National Laboratory (ANL) by Stanley Cohen, is an OOPS (object-oriented programming, much like the later MATLAB, IDL (programming language) and Mathematica) numerical package. Speakeasy has a clear Fortran foundation syntax. It first addressed efficient physics computation internally at ANL, was modified for research use (as ""Modeleasy"") for the Federal Reserve Board in the early 1970s and then was made available commercially; Speakeasy and Modeleasy are still in use currently.
Simula, invented in the late 1960s by Nygaard and Dahl as a superset of Algol 60, was the first language designed to support object-oriented programming.
C, an early systems programming language, was developed by Dennis Ritchie and Ken Thompson at Bell Labs between 1969 and 1973.
Smalltalk (mid-1970s) provided a complete ground-up design of an object-oriented language.
Prolog, designed in 1972 by Colmerauer, Roussel, and Kowalski, was the first logic programming language.
ML built a polymorphic type system (invented by Robin Milner in 1973) on top of Lisp, pioneering statically typed functional programming languages.
Each of these languages spawned an entire family of descendants, and most modern languages count at least one of them in their ancestry.
The 1960s and 1970s also saw considerable debate over the merits of ""structured programming"", which essentially meant programming without the use of ""goto"". A significant fraction of programmers believed that, even in languages that provide ""goto"", it is bad programming style to use it except in rare circumstances. This debate was closely related to language design: some languages did not include a ""goto"" at all, which forced structured programming on the programmer.
To provide even faster compile times, some languages were structured for ""one-pass compilers"" which expect subordinate routines to be defined first, as with Pascal, where the main routine, or driver function, is the final section of the program listing.
Some notable languages that were developed in this period include:


== 1980s: consolidation, modules, performance ==

The 1980s were years of relative consolidation in imperative languages. Rather than inventing new paradigms, all of these movements elaborated upon the ideas invented in the previous decade. C++ combined object-oriented and systems programming. The United States government standardized Ada, a systems programming language intended for use by defense contractors. In Japan and elsewhere, vast sums were spent investigating so-called fifth-generation programming languages that incorporated logic programming constructs. The functional languages community moved to standardize ML and Lisp. Research in Miranda, a functional language with lazy evaluation, began to take hold in this decade.
One important new trend in language design was an increased focus on programming for large-scale systems through the use of modules, or large-scale organizational units of code. Modula, Ada, and ML all developed notable module systems in the 1980s. Module systems were often wedded to generic programming constructs---generics being, in essence, parametrized modules (see also polymorphism in object-oriented programming).
Although major new paradigms for imperative programming languages did not appear, many researchers expanded on the ideas of prior languages and adapted them to new contexts. For example, the languages of the Argus and Emerald systems adapted object-oriented programming to distributed systems.
The 1980s also brought advances in programming language implementation. The RISC movement in computer architecture postulated that hardware should be designed for compilers rather than for human assembly programmers. Aided by processor speed improvements that enabled increasingly aggressive compilation techniques, the RISC movement sparked greater interest in compilation technology for high-level languages.
Language technology continued along these lines well into the 1990s.
Some notable languages that were developed in this period include:


== 1990s: the Internet age ==

The rapid growth of the Internet in the mid-1990s was the next major historic event in programming languages. By opening up a radically new platform for computer systems, the Internet created an opportunity for new languages to be adopted. In particular, the JavaScript programming language rose to popularity because of its early integration with the Netscape Navigator web browser. Various other scripting languages achieved widespread use in developing customized applications for web servers such as PHP. The 1990s saw no fundamental novelty in imperative languages, but much recombination and maturation of old ideas. This era began the spread of functional languages. A big driving philosophy was programmer productivity. Many ""rapid application development"" (RAD) languages emerged, which usually came with an IDE, garbage collection, and were descendants of older languages. All such languages were object-oriented. These included Object Pascal, Visual Basic, and Java. Java in particular received much attention.
More radical and innovative than the RAD languages were the new scripting languages. These did not directly descend from other languages and featured new syntaxes and more liberal incorporation of features. Many consider these scripting languages to be more productive than even the RAD languages, but often because of choices that make small programs simpler but large programs more difficult to write and maintain. Nevertheless, scripting languages came to be the most prominent ones used in connection with the Web.
Some notable languages that were developed in this period include:


== Current trends ==
Programming language evolution continues, in both industry and research. Some of the recent trends have included:

Increasing support for functional programming in mainstream languages used commercially, including pure functional programming for making code easier to reason about and easier to parallelise (at both micro- and macro- levels)
Constructs to support concurrent and distributed programming.
Mechanisms for adding security and reliability verification to the language: extended static checking, dependent typing, information flow control, static thread safety.
Alternative mechanisms for composability and modularity: mixins, traits, delegates, aspects.
Component-oriented software development.
Metaprogramming, reflection or access to the abstract syntax tree
AOP or Aspect Oriented Programming allowing developers to insert code in another module or class at ""join points""
Domain specific languages and code generation
XML for graphical interface (XUL, XAML)

Increased interest in distribution and mobility.
Integration with databases, including XML and relational databases.
Open source as a developmental philosophy for languages, including the GNU Compiler Collection and languages such as Python, Ruby, and Scala.
Massively parallel languages for coding 2000 processor GPU graphics processing units and supercomputer arrays including OpenCL
Early research into (as-yet-unimplementable) quantum computing programming languages
More interest in visual programming languages like Scratch
Some notable languages developed during this period include:


== Prominent people ==

Some key people who helped develop programming languages:
Alan Cooper, developer of Visual Basic.
Alan Kay, pioneering work on object-oriented programming, and originator of Smalltalk.
Anders Hejlsberg, developer of Turbo Pascal, Delphi, C#, and TypeScript.
Bertrand Meyer, inventor of Eiffel.
Bjarne Stroustrup, developer of C++.
Brian Kernighan, co-author of the first book on the C programming language with Dennis Ritchie, coauthor of the AWK and AMPL programming languages.
Chris Lattner, creator of Swift and LLVM.
Dennis Ritchie, inventor of C. Unix Operating System, Plan 9 Operating System.
Grace Hopper, first to use the term compiler and developer of Flow-Matic, influenced development of COBOL. Popularized machine-independent programming languages and the term ""debugging"".
Guido van Rossum, creator of Python.
James Gosling, lead developer of Java and its precursor, Oak.
Jean Ichbiah, chief designer of Ada, Ada 83.
Jean-Yves Girard, co-inventor of the polymorphic lambda calculus (System F).
Jeff Bezanson, main designer, and one of the core developers of Julia.
Joe Armstrong, creator of Erlang.
John Backus, inventor of Fortran and cooperated in the design of ALGOL 58 and ALGOL 60.
John C. Reynolds, co-inventor of the polymorphic lambda calculus (System F).
John McCarthy, inventor of LISP.
John von Neumann, originator of the operating system concept.
Graydon Hoare, inventor of Rust.
Ken Thompson, inventor of B, Go Programming Language, Inferno Programming Language, and Unix Operating System co-author.
Kenneth E. Iverson, developer of APL, and co-developer of J along with Roger Hui.
Konrad Zuse, designed the first high-level programming language, Plankalkül (which influenced ALGOL 58).
Kristen Nygaard, pioneered object-oriented programming, co-invented Simula.
Larry Wall, creator of the Perl programming language (see Perl and Perl 6).
Martin Odersky, creator of Scala, and previously a contributor to the design of Java.
Nathaniel Rochester, inventor of first assembler (IBM 701).
Niklaus Wirth, inventor of Pascal, Modula and Oberon.
Ole-Johan Dahl, pioneered object-oriented programming, co-invented Simula.
Rasmus Lerdorf, creator of PHP
Rich Hickey, creator of Clojure.
Robin Milner, inventor of ML, and sharing credit for Hindley–Milner polymorphic type inference.
Stephen Wolfram, creator of Mathematica.
Tom Love and Brad Cox, creator of Objective-C.
Walter Bright, creator of D.
Yukihiro Matsumoto, creator of Ruby.


== See also ==


== References ==


== Further reading ==
Rosen, Saul, (editor), Programming Systems and Languages, McGraw-Hill, 1967.
Sammet, Jean E., Programming Languages: History and Fundamentals, Prentice-Hall, 1969.
Sammet, Jean E. (July 1972). ""Programming Languages: History and Future"". Communications of the ACM. 15 (7): 601–610. doi:10.1145/361454.361485. 
Richard L. Wexelblat (ed.): History of Programming Languages, Academic Press 1981.
Thomas J. Bergin and Richard G. Gibson (eds.): History of Programming Languages, Addison Wesley, 1996.


== External links ==
History and evolution of programming languages
Graph of programming language history"
26,Here document,1426425,30591,"In computing, a here document (here-document, here-text, heredoc, hereis, here-string or here-script) is a file literal or input stream literal: it is a section of a source code file that is treated as if it were a separate file. The term is also used for a form of multiline string literals that use similar syntax, preserving line breaks and other whitespace (including indentation) in the text.
Here documents originate in the Unix shell, and are found in sh, csh, ksh, bash and zsh, among others. Here document-style string literals are found in various high-level languages, notably the Perl programming language (syntax inspired by Unix shell) and languages influenced by Perl, such as PHP and Ruby. Other high-level languages such as Python and Tcl have other facilities for multiline strings.
Here documents can be treated either as files or strings. Some shells treat them as a format string literal, allowing variable substitution and command substitution inside the literal.
The most common syntax for here documents, originating in Unix shells, is << followed by a delimiting identifier (often EOF or END), followed, starting on the next line, by the text to be quoted, and then closed by the same delimiting identifier on its own line. This syntax is because here documents are formally stream literals, and the content of the document is redirected to stdin (standard input) of the preceding command; the here document syntax is by analogy with the syntax for input redirection, which is < ""take input from the following file"".
Other languages often use substantially similar syntax, but details of syntax and actual functionality can vary significantly. When used simply for string literals, the << does not indicate indirection, but is simply a starting delimiter convention. In some languages, such as Ruby, << is also used for input redirection, thus resulting in << being used twice if one wishes to redirect from a here document string literal.


== File literals ==
Narrowly speaking, here documents are file literals or stream literals. These originate in the Unix shell, though similar facilities are available in some other languages.


=== Unix shells ===
Here documents are available in many Unix shells.
In the following example, text is passed to the tr command (transliterating lower to upper-case) using a here document. This could be in a shell file, or entered interactively at a prompt.

END_TEXT was used as the delimiting identifier. It specified the start and end of the here document. The redirect and the delimiting identifier do not need to be separated by a space: <<END_TEXT or << END_TEXT both work equally well.
Appending a minus sign to the << has the effect that leading tabs are ignored. This allows indenting here documents in shell scripts (primarily for alignment with existing indentation) without changing their value:

This yields the same output, notably not indented.
By default, behavior is largely identical to the contents of double quotes: variables are interpolated, commands in backticks are evaluated, etc.

This can be disabled by quoting any part of the label, which is then ended by the unquoted value; the behavior is essentially identical to that if the contents were enclosed in single quotes. Thus for example by setting it in single quotes:

Double quotes may also be used, but this is subject to confusion, because expansion does occur in a double-quoted string, but does not occur in a here document with double-quoted delimiter. Single- and double-quoted delimiters are distinguished in some other languages, notably Perl (see below), where behavior parallels the corresponding string quoting.


==== Here strings ====
A here string (available in Bash, ksh, or zsh) is syntactically similar, consisting of <<<, and effects input redirection from a word (a sequence treated as a unit by the shell, in this context generally a string literal). In this case the usual shell syntax is used for the word (""here string syntax""), with the only syntax being the redirection: a here string is an ordinary string used for input redirection, not a special kind of string.
A single word need not be quoted:

In case of a string with spaces, it must be quoted:

This could also be written as:

Multiline strings are acceptable, yielding:

Note that leading and trailing newlines, if present, are included:

The key difference from here documents is that, in here documents, the delimiters are on separate lines; the leading and trailing newlines are stripped. Here, the terminating delimiter can be specified.
Here strings are particularly useful for commands that often take short input, such as the calculator bc:

Note that here string behavior can also be accomplished (reversing the order) via piping and the echo command, as in:

however here strings are particularly useful when the last command needs to run in the current process, as is the case with the read builtin:

yields nothing, while

This happens because in the previous example piping causes read to run in a subprocess, and as such can not affect the environment of the parent process.


=== Microsoft NMAKE ===
In Microsoft NMAKE, here documents are referred to as inline files. Inline files are referenced as << or <<pathname: the first notation creates a temporary file, the second notation creates (or overwrites) the file with the specified pathname. An inline file is terminated with << on a line by itself, optionally followed by the (case-insensitive) keyword KEEP or NOKEEP to indicate whether the created file should be kept.

target0: dependent0
    command0 <<
temporary inline file
...
<<

target1: dependent1
    command1 <<
temporary, but preserved inline file
...
<<KEEP

target2: dependent2
    command2 <<filename2
named, but discarded inline file
...
<<NOKEEP

target3: dependent3
    command3 <<filename3
named inline file
...
<<KEEP


=== R ===
R does not have file literals, but provides equivalent functionality by combining string literals with a string-to-file function. R allows arbitrary whitespace, including newlines, in strings. A string then can be turned into a file descriptor using the textConnection() function. For example, the following turns a data table embedded in the source code into a data-frame variable:


=== Data segment ===
Perl and Ruby have a form of file literal, which can be considered a form of data segment. In these languages, including the line __DATA__ (Perl) or __END__ (Ruby, old Perl) marks the end of the code segment and the start of the data segment. Only the contents prior to this line are executed, and the contents of the source file after this line are available as a file object: PACKAGE::DATA in Perl (e.g., main::DATA) and DATA in Ruby. As an inline file, these are semantically similar to here documents, though there can be only one per script. However, in these languages the term ""here document"" instead refers to multiline string literals, as discussed below.


=== Data URI Scheme ===
As further explained in Data URI scheme, all major web browsers understand URIs that start with data: as here document.


== Multiline string literals ==
The term ""here document"" or ""here string"" is also used for multiline string literals in various programming languages, notably Perl (syntax influenced by Unix shell), and languages influenced by Perl, notably PHP and Ruby. The shell-style << syntax is often retained, despite not being used for input redirection.


=== Perl-influenced ===


==== Perl ====
In Perl there are several different ways to invoke here docs. The delimiters around the tag have the same effect within the here doc as they would in a regular string literal: For example, using double quotes around the tag allows variables to be interpolated, but using single quotes doesn't, and using the tag without either behaves like double quotes. Using backticks as the delimiters around the tag runs the contents of the heredoc as a shell script. It is necessary to make sure that the end tag is at the beginning of the line or the tag will not be recognized by the interpreter.
Note that the here doc does not start at the tag—but rather starts on the next line. So the statement containing the tag continues on after the tag.
Here is an example with double quotes:

Output:

Dear Spike,

I wish you to leave Sunnydale and never return.

Not Quite Love,
Buffy the Vampire Slayer

Here is an example with single quotes:

Output:

Dear $recipient,

I wish you to leave Sunnydale and never return.

Not Quite Love,
$sender

And an example with backticks (may not be portable):

It is possible to start multiple heredocs on the same line:

The tag itself may contain whitespace, which may allow heredocs to be used without breaking indentation.

In addition to these strings, Perl also features file literals, namely the contents of the file following __DATA__ (formerly __END__) on a line by itself. This is accessible as the file object PACKAGE::DATA such as main::DATA, and can be viewed as a form of data segment.


==== PHP ====
In PHP, here documents are referred to as heredocs.

Outputs

This is a heredoc section.
For more information talk to Joe Smith, your local Programmer.

Thanks!

Hey Joe Smith! You can actually assign the heredoc section to a variable!

The line containing the closing identifier must not contain any other characters, except an optional ending semicolon. Otherwise, it will not be considered to be a closing identifier, and PHP will continue looking for one. If a proper closing identifier is not found, a parse error will result at the last line of the script.
In PHP 5.3 and later, like Perl, it is possible to not interpolate variables by surrounding the tag with single quotes; this is called a nowdoc:

In PHP 5.3+ it is also possible to surround the tag with double quotes, which like Perl has the same effect as not surrounding the tag with anything at all.


==== Ruby ====
The following Ruby code displays a grocery list by using a here document.

The result:

The << in a here document does not indicate input redirection, but Ruby also uses << for input redirection, so redirecting to a file from a here document involves using << twice, in different senses:

As with Unix shells, Ruby also allows for the delimiting identifier not to start on the first column of a line, if the start of the here document is marked with the slightly different starter ""<<-"". Besides, Ruby treats here documents as a double-quoted string, and as such, it is possible to use the #{} construct to interpolate code. The following example illustrates both of these features:

Ruby expands on this by providing the ""<<~"" syntax for omitting indentation on the here document:

The common indentation of two spaces is omitted from all lines:

Like Perl, Ruby allows for starting multiple here documents in one line:

As with Perl, Ruby features file literals, namely the contents of the file following __END__ on a line by itself. This is accessible as the file object DATA and can be viewed as a form of data segment.


=== Others ===


==== Python ====
Python supports multi-line strings as a ""verbatim"" string.

From 3.6, verbatim f-strings support variable and expression interpolation.


==== D ====
Since version 2.0, D has support for here document-style strings using the 'q' prefix character. These strings begin with q""IDENT followed immediately by a newline (for an arbitrary identifier IDENT), and end with IDENT"" at the start of a line.

D also supports a few quoting delimiters, with similar syntax, with such strings starting with q""[ and ending with ]"" or similarly for other delimiter character (any of () <> {} or []).


==== OS/JCL ====
On IBM's Job Control Language (JCL) used on its earlier MVS and current z/OS operating systems, data which is inline to a job stream can be identified by an * on a DD statement, such as //SYSIN DD * or //SYSIN DD *,DLM=text In the first case, the lines of text follow and are combined into a pseudo file with the DD name SYSIN. All records following the command are combined until either another OS/JCL command occurs (any line beginning with //), the default EOF sequence (/*) is found, or the physical end of data occurs. In the second case, the conditions are the same, except the DLM= operand is used to specify the text string signalling end of data, which can be used if a data stream contains JCL (again, any line beginning with //), or the /* sequence (such as comments in C or C++ source code). The following compiles and executes an assembly language program, supplied as in-line data to the assembler.

The //SYSIN DD * statement is the functional equivalent of <</* Indicating s stream of data follows, terminated by /*.


==== Racket ====
Racket's here strings start with #<< followed by characters that define a terminator for the string. The content of the string includes all characters between the #<< line and a line whose only content is the specified terminator. More precisely, the content of the string starts after a newline following #<<, and it ends before a newline that is followed by the terminator.

Outputs:

This is a simple here string in Racket.
  * One
  * Two
  * Three

No escape sequences are recognized between the starting and terminating lines; all characters are included in the string (and terminator) literally.

Outputs:

This string spans for multiple lines
and can contain any Unicode symbol.
So things like λ, ☠, α, β, are all fine.

In the next line comes the terminator. It can contain any Unicode symbol as well, even spaces and smileys!

Here strings can be used normally in contexts where normal strings would:

Outputs:

Dear Isaac,

Thanks for the insightful conversation yesterday.

                Carl

An interesting alternative is to use the language extension at-exp to write @-expressions. They look like this:

#lang at-exp racket

(displayln @string-append{
This is a long string,
very convenient when a
long chunk of text is
needed.

No worries about escaping
""quotes"" or \escapes. It's
also okay to have λ, γ, θ, ...

Embed code: @(number->string (+ 3 4))
})

Outputs:

This is a long string,
very convenient when a
long chunk of text is
needed.

No worries about escaping
""quotes"" or \escapes. It's
also okay to have λ, γ, θ, ...

Embed code: 7

An @-expression is not specific nor restricted to strings, it is a syntax form that can be composed with the rest of the language.


==== Windows PowerShell ====
In Windows PowerShell, here documents are referred to as here-strings. A here-string is a string which starts with an open delimiter (@"" or @') and ends with a close delimiter (""@ or '@) on a line by itself, which terminates the string. All characters between the open and close delimiter are considered the string literal. Using a here-string with double quotes allows variables to be interpreted, using single quotes doesn't. Variable interpolation occurs with simple variables (e.g. $x but NOT $x.y or $x[0]). You can execute a set of statements by putting them in $() (e.g. $($x.y) or $(Get-Process | Out-String)).
In the following PowerShell code, text is passed to a function using a here-string. The function ConvertTo-UpperCase is defined as follows:

Here is an example that demonstrates variable interpolation and statement execution using a here-string with double quotes:

Using a here-string with single quotes instead, the output would look like this: Output:


==== DIGITAL Command Language (DCL) ====
In DCL scripts, any input line which does not begin with a $ symbol is implicitly treated as input to the preceding command - all lines which do not begin with $ are here-documents. The input is either passed to the program, or can be explicitly referenced by the logical name SYS$INPUT (analogous to the Unix concept of stdin).
For instance, explicitly referencing the input as SYS$INPUT:

produces:

Additionally, the DECK command, initially intended for punched card support (hence its name: it signified the beginning of a data deck) can be used to supply input to the preceding command. The input deck is ended either by the command $ EOD, or the character pattern specified by the /DOLLARS parameter to DECK.
Example of a program totalling up monetary values:

Would produce the following output (presuming ADD_SUMS was written to read the values and add them):

Example of using DECK /DOLLARS to create one command file from another:


== See also ==
Pipeline (Unix) for information about pipes
String literal


== References ==


=== General ===


== External links ==
Here document. Link to Rosetta Code task with examples of here documents in over 15 languages."
27,History of computer science,3271413,29971,"The history of computer science began long before our modern discipline of computer science. Developments in previous centuries alluded to the disipline that we now know as computer science. This progression, from mechanical inventions and mathematical theories towards modern computer concepts and machines, led to the development of a major academic field and the basis of a massive worldwide industry.


== Prehistory ==
The earliest known tool for use in computation was the abacus, developed in the period between 2700–2300 BCE in Sumer. The Sumerians' abacus consisted of a table of successive columns which delimited the successive orders of magnitude of their sexagesimal number system. Its original style of usage was by lines drawn in sand with pebbles . Abaci of a more modern design are still used as calculation tools today, such as the Chinese abacus.
In the 5th century BC in ancient India, the grammarian Pāṇini formulated the grammar of Sanskrit in 3959 rules known as the Ashtadhyayi which was highly systematized and technical. Panini used metarules, transformations and recursions.
The Antikythera mechanism is believed to be an early mechanical analog computer. It was designed to calculate astronomical positions. It was discovered in 1901 in the Antikythera wreck off the Greek island of Antikythera, between Kythera and Crete, and has been dated to circa 100 BC.
Mechanical analog computer devices appeared again a thousand years later in the medieval Islamic world and were developed by Muslim astronomers, such as the mechanical geared astrolabe by Abū Rayhān al-Bīrūnī, and the torquetum by Jabir ibn Aflah. According to Simon Singh, Muslim mathematicians also made important advances in cryptography, such as the development of cryptanalysis and frequency analysis by Alkindus. Programmable machines were also invented by Muslim engineers, such as the automatic flute player by the Banū Mūsā brothers, and Al-Jazari's programmable humanoid automata and castle clock, which is considered to be the first programmable analog computer. Technological artifacts of similar complexity appeared in 14th century Europe, with mechanical astronomical clocks.
When John Napier discovered logarithms for computational purposes in the early 17th century, there followed a period of considerable progress by inventors and scientists in making calculating tools. In 1623 Wilhelm Schickard designed a calculating machine, but abandoned the project, when the prototype he had started building was destroyed by a fire in 1624 . Around 1640, Blaise Pascal, a leading French mathematician, constructed a mechanical adding device based on a design described by Greek mathematician Hero of Alexandria. Then in 1672 Gottfried Wilhelm Leibniz invented the Stepped Reckoner which he completed in 1694.
In 1837 Charles Babbage first described his Analytical Engine which is accepted as the first design for a modern computer. The analytical engine had expandable memory, an arithmetic unit, and logic processing capabilities able to interpret a programming language with loops and conditional branching. Although never built, the design has been studied extensively and is understood to be Turing equivalent. The analytical engine would have had a memory capacity of less than 1 kilobyte of memory and a clock speed of less than 10 Hertz .
Considerable advancement in mathematics and electronics theory was required before the first modern computers could be designed.


== Binary logic ==
In 1702, Gottfried Wilhelm Leibniz developed logic in a formal, mathematical sense with his writings on the binary numeral system. In his system, the ones and zeros also represent true and false values or on and off states. But it took more than a century before George Boole published his Boolean algebra in 1854 with a complete system that allowed computational processes to be mathematically modeled .
By this time, the first mechanical devices driven by a binary pattern had been invented. The industrial revolution had driven forward the mechanization of many tasks, and this included weaving. Punched cards controlled Joseph Marie Jacquard's loom in 1801, where a hole punched in the card indicated a binary one and an unpunched spot indicated a binary zero. Jacquard's loom was far from being a computer, but it did illustrate that machines could be driven by binary systems .


== Creation of the computer ==
Before the 1920s, computers (sometimes computors) were human clerks that performed computations. They were usually under the lead of a physicist. Many thousands of computers were employed in commerce, government, and research establishments. Most of these computers were women. Some performed astronomical calculations for calendars, others ballistic tables for the military.
After the 1920s, the expression computing machine referred to any machine that performed the work of a human computer, especially those in accordance with effective methods of the Church-Turing thesis. The thesis states that a mathematical method is effective if it could be set out as a list of instructions able to be followed by a human clerk with paper and pencil, for as long as necessary, and without ingenuity or insight.
Machines that computed with continuous values became known as the analog kind. They used machinery that represented continuous numeric quantities, like the angle of a shaft rotation or difference in electrical potential.
Digital machinery, in contrast to analog, were able to render a state of a numeric value and store each individual digit. Digital machinery used difference engines or relays before the invention of faster memory devices.
The phrase computing machine gradually gave way, after the late 1940s, to just computer as the onset of electronic digital machinery became common. These computers were able to perform the calculations that were performed by the previous human clerks.
Since the values stored by digital machines were not bound to physical properties like analog devices, a logical computer, based on digital equipment, was able to do anything that could be described ""purely mechanical."" The theoretical Turing Machine, created by Alan Turing, is a hypothetical device theorized in order to study the properties of such hardware.


== Emergence of a discipline ==


=== Charles Babbage and Ada Lovelace ===

Charles Babbage is often regarded as one of the first pioneers of computing. Beginning in the 1810s, Babbage had a vision of mechanically computing numbers and tables. Putting this into reality, Babbage designed a calculator to compute numbers up to 8 decimal points long. Continuing with the success of this idea, Babbage worked to develop a machine that could compute numbers with up to 20 decimal places. By the 1830s, Babbage had devised a plan to develop a machine that could use punched cards to perform arithmetical operations. The machine would store numbers in memory units, and there would be a form of sequential control. This means that one operation would be carried out before another in such a way that the machine would produce an answer and not fail. This machine was to be known as the “Analytical Engine”, which was the first true representation of what is the modern computer.
Ada Lovelace (Augusta Ada Byron) is credited as the pioneer of computer programming and is regarded as a mathematical genius, a result of the mathematically heavy tutoring regimen her mother assigned to her as a young girl. Lovelace began working with Charles Babbage as an assistant while Babbage was working on his “Analytical Engine”, the first mechanical computer. During her work with Babbage, Ada Lovelace became the designer of the first computer algorithm, which had the ability to compute Bernoulli numbers. Moreover, Lovelace’s work with Babbage resulted in her prediction of future computers to not only perform mathematical calculations, but also manipulate symbols, mathematical or not. While she was never able to see the results of her work, as the “Analytical Engine” was not created in her lifetime, her efforts in later years, beginning in the 1840s, did not go unnoticed.


=== Alan Turing and the Turing machine ===

The mathematical foundations of modern computer science began to be laid by Kurt Gödel with his incompleteness theorem (1931). In this theorem, he showed that there were limits to what could be proved and disproved within a formal system. This led to work by Gödel and others to define and describe these formal systems, including concepts such as mu-recursive functions and lambda-definable functions.
In 1936 Alan Turing and Alonzo Church independently, and also together, introduced the formalization of an algorithm, with limits on what can be computed, and a ""purely mechanical"" model for computing. This became the Church–Turing thesis, a hypothesis about the nature of mechanical calculation devices, such as electronic computers. The thesis claims that any calculation that is possible can be performed by an algorithm running on a computer, provided that sufficient time and storage space are available.
In 1936, Alan Turing also published his seminal work on the Turing machines, an abstract digital computing machine which is now simply referred to as the Universal Turing machine. This machine invented the principle of the modern computer and was the birthplace of the stored program concept that almost all modern day computers use. These hypothetical machines were designed to formally determine, mathematically, what can be computed, taking into account limitations on computing ability. If a Turing machine can complete the task, it is considered Turing computable or more commonly, Turing complete.
The Los Alamos physicist Stanley Frankel, has described John von Neumann's view of the fundamental importance of Turing's 1936 paper, in a letter:

I know that in or about 1943 or ‘44 von Neumann was well aware of the fundamental importance of Turing's paper of 1936… Von Neumann introduced me to that paper and at his urging I studied it with care. Many people have acclaimed von Neumann as the ""father of the computer"" (in a modern sense of the term) but I am sure that he would never have made that mistake himself. He might well be called the midwife, perhaps, but he firmly emphasized to me, and to others I am sure, that the fundamental conception is owing to Turing...


=== Akira Nakashima and switching circuit theory ===
Up to and during the 1930s, electrical engineers were able to build electronic circuits to solve mathematical and logic problems, but most did so in an ad hoc manner, lacking any theoretical rigor. This changed with NEC engineer Akira Nakashima's switching circuit theory in the 1930s. From 1934 to 1936, Nakashima published a series of papers showing that the two-valued Boolean algebra, which he discovered independently (he was unaware of George Boole's work until 1938), can describe the operation of switching circuits. This concept, of utilizing the properties of electrical switches to do logic, is the basic concept that underlies all electronic digital computers. Switching circuit theory provided the mathematical foundations and tools for digital system design in almost all areas of modern technology.
Nakashima's work was later cited and elaborated on in Claude Elwood Shannon's seminal 1937 master's thesis ""A Symbolic Analysis of Relay and Switching Circuits"". While taking an undergraduate philosophy class, Shannon had been exposed to Boole's work, and recognized that it could be used to arrange electromechanical relays (then used in telephone routing switches) to solve logic problems. His thesis became the foundation of practical digital circuit design when it became widely known among the electrical engineering community during and after World War II.


=== Early computer hardware ===
In 1941, Konrad Zuse developed the world's first functional program-controlled computer, the Z3. In 1998, it was shown to be Turing-complete in principle. Zuse also developed the S2 computing machine, considered the first process control computer. He founded one of the earliest computer businesses in 1941, producing the Z4, which became the world's first commercial computer. In 1946, he designed the first high-level programming language, Plankalkül.
In 1948, the Manchester Baby was completed, it was the world's first general purpose electronic digital computer that also ran stored programs like almost all modern computers. The influence on Max Newman of Turing's seminal 1936 paper on the Turing Machines and of his logico-mathematical contributions to the project, were both crucial to the successful development of the Manchester SSEM.
In 1950, Britain's National Physical Laboratory completed Pilot ACE, a small scale programmable computer, based on Turing's philosophy. With an operating speed of 1 MHz, the Pilot Model ACE was for some time the fastest computer in the world. Turing's design for ACE had much in common with today's RISC architectures and it called for a high-speed memory of roughly the same capacity as an early Macintosh computer, which was enormous by the standards of his day. Had Turing's ACE been built as planned and in full, it would have been in a different league from the other early computers.


=== Shannon and information theory ===
Claude Shannon went on to found the field of information theory with his 1948 paper titled A Mathematical Theory of Communication, which applied probability theory to the problem of how to best encode the information a sender wants to transmit. This work is one of the theoretical foundations for many areas of study, including data compression and cryptography .


=== Wiener and cybernetics ===
From experiments with anti-aircraft systems that interpreted radar images to detect enemy planes, Norbert Wiener coined the term cybernetics from the Greek word for ""steersman."" He published ""Cybernetics"" in 1948, which influenced artificial intelligence. Wiener also compared computation, computing machinery, memory devices, and other cognitive similarities with his analysis of brain waves.
The first actual computer bug was a moth. It was stuck in between the relays on the Harvard Mark II. While the invention of the term 'bug' is often but erroneously attributed to Grace Hopper, a future rear admiral in the U.S. Navy, who supposedly logged the ""bug"" on September 9, 1945, most other accounts conflict at least with these details. According to these accounts, the actual date was September 9, 1947 when operators filed this 'incident' — along with the insect and the notation ""First actual case of bug being found"" (see software bug for details).


=== John von Neumann and the von Neumann architecture ===

In 1946, a model for computer architecture was introduced and became known as Von Neumann architecture. Since 1950, the von Neumann model provided uniformity in subsequent computer designs. The von Neumann architecture was considered innovative as it introduced an idea of allowing machine instructions and data to share memory space. The von Neumann model is composed of three major parts, the arithmetic logic unit (ALU), the memory, and the instruction processing unit (IPU). In von Neumann machine design, the IPU passes addresses to memory, and memory, in turn, is routed either back to the IPU if an instruction is being fetched or to the ALU if data is being fetched.
Von Neumann’s machine design uses a RISC (Reduced instruction set computing) architecture, which means the instruction set uses a total of 21 instructions to perform all tasks. (This is in contrast to CISC, complex instruction set computing, instruction sets which have more instructions from which to choose.) With von Neumann architecture, main memory along with the accumulator (the register that holds the result of logical operations) are the two memories that are addressed. Operations can be carried out as simple arithmetic (these are performed by the ALU and include addition, subtraction, multiplication and division), conditional branches (these are more commonly seen now as if statements or while loops. The branches serve as go to statements), and logical moves between the different components of the machine, i.e., a move from the accumulator to memory or vice versa. Von Neumann architecture accepts fractions and instructions as data types. Finally, as the von Neumann architecture is a simple one, its register management is also simple. The architecture uses a set of seven registers to manipulate and interpret fetched data and instructions. These registers include the ""IR"" (instruction register), ""IBR"" (instruction buffer register), ""MQ"" (multiplier quotient register), ""MAR"" (memory address register), and ""MDR"" (memory data register)."" The architecture also uses a program counter (""PC"") to keep track of where in the program the machine is.


== See also ==
Computer Museum
History of computing
History of computing hardware
History of software
List of computer term etymologies, the origins of computer science words
List of prominent pioneers in computer science
Timeline of algorithms
History of personal computers


== References ==


== Further reading ==
Tedre, Matti (2014). The Science of Computing: Shaping a Discipline. Taylor and Francis / CRC Press. ISBN 978-1-4822-1769-8. 
Kak, Subhash : Computing Science in Ancient India; Munshiram Manoharlal Publishers Pvt. Ltd (2001)
The Development of Computer Science: A Sociocultural Perspective Matti Tedre's Ph.D. Thesis, University of Joensuu (2006)
Ceruzzi, Paul E. (1998). A History of a Modern Computing. The MIT Press. ISBN 978-0-262-03255-1. 
Copeland, B. Jack. ""The Modern History of Computing"". In Zalta, Edward N. Stanford Encyclopedia of Philosophy. 


== External links ==
Computer History Museum
Computers: From the Past to the Present
The First ""Computer Bug"" at the Naval History and Heritage Command Photo Archives.
Bitsavers, an effort to capture, salvage, and archive historical computer software and manuals from minicomputers and mainframes of the 1950s, 1960s, 1970s, and 1980s
Oral history interviews"
28,Association for Computing Machinery,2928,28997,"The Association for Computing Machinery (ACM) is an international learned society for computing. It was founded in 1947, and is the world's largest scientific and educational computing society. It is a not-for-profit professional membership group. Its membership is more than 100,000 as of 2011. Its headquarters are in New York City.
The ACM is an umbrella organization for academic and scholarly interests in computer science. Its motto is ""Advancing Computing as a Science & Profession"".


== History ==
The ACM was founded in 1947 under the name Eastern Association for Computing Machinery, which was changed the following year to the Association of Computing Machinery.


== Activities ==

ACM is organized into over 171 local chapters and 37 Special Interest Groups (SIGs), through which it conducts most of its activities. Additionally, there are over 500 college and university chapters. The first student chapter was founded in 1961 at the University of Louisiana at Lafayette.
Many of the SIGs, such as SIGGRAPH, SIGPLAN, SIGCSE and SIGCOMM, sponsor regular conferences, which have become famous as the dominant venue for presenting innovations in certain fields. The groups also publish a large number of specialized journals, magazines, and newsletters.
ACM also sponsors other computer science related events such as the worldwide ACM International Collegiate Programming Contest (ICPC), and has sponsored some other events such as the chess match between Garry Kasparov and the IBM Deep Blue computer.


== Services ==


=== Publications ===

ACM publishes over 50 journals including the prestigious Journal of the ACM, and two general magazines for computer professionals, Communications of the ACM (also known as Communications or CACM) and Queue. Other publications of the ACM include:
ACM XRDS, formerly ""Crossroads"", was redesigned in 2010 and is the most popular student computing magazine in the US.
ACM Interactions, an interdisciplinary HCI publication focused on the connections between experiences, people and technology, and the third largest ACM publication.
ACM Computing Surveys (CSUR)
ACM Computers in Entertainment (CIE)
ACM Special Interest Group: Computers and Society (SIGCAS) 
A number of journals, specific to subfields of computer science, titled ACM Transactions. Some of the more notable transactions include:
ACM Transactions on Computer Systems (TOCS)
IEEE/ACM Transactions on Computational Biology and Bioinformatics (TCBB)
ACM Transactions on Computational Logic (TOCL)
ACM Transactions on Computer-Human Interaction (TOCHI)
ACM Transactions on Database Systems (TODS)
ACM Transactions on Graphics (TOG)
ACM Transactions on Mathematical Software (TOMS)
ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)
IEEE/ACM Transactions on Networking (TON)
ACM Transactions on Programming Languages and Systems (TOPLAS)

Although Communications no longer publishes primary research, and is not considered a prestigious venue, many of the great debates and results in computing history have been published in its pages.
ACM has made almost all of its publications available to paid subscribers online at its Digital Library and also has a Guide to Computing Literature. Individual members additionally have access to Safari Books Online and Books24x7. ACM also offers insurance, online courses, and other services to its members.
In 1997, ACM Press published Wizards and Their Wonders: Portraits in Computing (ISBN 0897919602), written by Christopher Morgan, with new photographs by Louis Fabian Bachrach. The book is a collection of historic and current portrait photographs of figures from the computer industry.


== Portal and Digital Library ==
The ACM Portal is an online service of the ACM. Its core are two main sections: ACM Digital Library and the ACM Guide to Computing Literature.
The ACM Digital Library is the full-text collection of all articles published by the ACM in its articles, magazines and conference proceedings. The Guide is a bibliography in computing with over one million entries. The ACM Digital Library contains a comprehensive archive starting in the 1950s of the organization's journals, magazines, newsletters and conference proceedings. Online services include a forum called Ubiquity and Tech News digest. There is an extensive underlying bibliographic database containing key works of all genres from all major publishers of computing literature. This secondary database is a rich discovery service known as The ACM Guide to Computing Literature.
ACM adopted a hybrid Open Access (OA) publishing model in 2013. Authors who do not choose to pay the OA fee must grant ACM publishing rights by either a copyright transfer agreement or a publishing license agreement.
ACM was a ""green"" publisher before the term was invented. Authors may post documents on their own websites and in their institutional repositories with a link back to the ACM Digital Library's permanently maintained Version of Record.
All metadata in the Digital Library is open to the world, including abstracts, linked references and citing works, citation and usage statistics, as well as all functionality and services. Other than the free articles, the full-texts are accessed by subscription.
There is also a mounting challenge to the ACM's publication practices coming from the open access movement. Some authors see a centralized peer–review process as less relevant and publish on their home pages or on unreviewed sites like arXiv. Other organizations have sprung up which do their peer review entirely free and online, such as Journal of Artificial Intelligence Research (JAIR), Journal of Machine Learning Research (JMLR) and the Journal of Research and Practice in Information Technology.


== Membership grades ==
In addition to student and regular members, ACM has several advanced membership grades to recognize those with multiple years of membership and ""demonstrated performance that sets them apart from their peers"".


=== Fellows ===

The ACM Fellows Program was established by Council of the Association for Computing Machinery in 1993 ""to recognize and honor outstanding ACM members for their achievements in computer science and information technology and for their significant contributions to the mission of the ACM."" There are presently about 958 Fellows out of about 75,000 professional members.


=== Distinguished Members ===
In 2006 ACM began recognizing two additional membership grades, one which was called Distinguished Members. Distinguished Members (Distinguished Engineers, Distinguished Scientists, and Distinguished Educators) have at least 15 years of professional experience and 5 years of continuous ACM membership and ""have made a significant impact on the computing field"". Note that in 2006 when the Distinguished Members first came out, one of the three levels was called ""Distinguished Member"" and was changed about two years later to ""Distinguished Educator"". Those who already had the Distinguished Member title had their titles changed to one of the other three titles.


=== Senior Members ===
Also in 2006, ACM began recognizing Senior Members. Senior Members have ten or more years of professional experience and 5 years of continuous ACM membership.


=== Distinguished Speakers ===
While not technically a membership grade, the ACM recognizes distinguished speakers on topics in computer science. A distinguished speaker is appointed for a three-year period. There are usually about 125 current distinguished speakers. The ACM website describes these people as 'Renowned International Thought Leaders'. The distinguished speaker program is overseen by a committee 
Norman E. Gibbs served as the president of the ACM.


== Chapters ==
ACM has three kinds of chapters: Special Interest Groups, Professional Chapters, and Student Chapters.
As of 2011, ACM has professional & SIG Chapters in 56 countries.
As of 2014, there exist ACM student chapters in 41 different countries.


=== Special Interest Groups ===


== Conferences ==

ACM and its Special Interest Groups (SIGs) sponsors numerous conferences with 170 hosted worldwide in 2017. ACM Conferences page has an up-to-date complete list while a partial list is shown below. Most of the SIGs also have an annual conference. ACM conferences are often very popular publishing venues and are therefore very competitive. For example, the 2007 SIGGRAPH conference attracted about 30000 visitors, and CIKM only accepted 15% of the long papers that were submitted in 2005.

MobiHoc: International Symposium on Mobile Ad Hoc Networking and Computing
The ACM is a co–presenter and founding partner of the Grace Hopper Celebration of Women in Computing (GHC) with the Anita Borg Institute for Women and Technology.
There are some conferences hosted by ACM student branches; this includes Reflections Projections, which is hosted by UIUC ACM. . In addition, ACM sponsors regional conferences. Regional conferences facilitate increased opportunities for collaboration between nearby institutions and they are well attended.
For additional non-ACM conferences, see this list of computer science conferences.


== Awards ==
The ACM presents or co–presents a number of awards for outstanding technical and professional achievements and contributions in computer science and information technology.

Over 30 of ACM's Special Interest Groups also award individuals for their contributions with a few listed below.


== Leadership ==

The President of ACM for 2016–2018 is Vicki L. Hanson, Distinguished Professor in the Department of Information Sciences and Technologies at the Rochester Institute of Technology and Professor and Chair of Inclusive Technologies at the University of Dundee, UK. She is successor of Alexander L. Wolf (2014–2016), Dean of the Jack Baskin School of Engineering at the University of California, Santa Cruz; Vint Cerf (2012–2014), an American computer scientist who is recognized as one of ""the fathers of the Internet""; Alain Chesnais (2010–2012), a French citizen living in Toronto, Ontario, Canada, where he runs his company named Visual Transitions; and Dame Wendy Hall of the University of Southampton, UK (2008–2010).
ACM is led by a Council consisting of the President, Vice-President, Treasurer, Past President, SIG Governing Board Chair, Publications Board Chair, three representatives of the SIG Governing Board, and seven Members–At–Large. This institution is often referred to simply as ""Council"" in Communications of the ACM.


== Infrastructure ==
ACM has five ""Boards"" that make up various committees and subgroups, to help Headquarters staff maintain quality services and products. These boards are as follows:
Publications Board
SIG Governing Board
Education Board
Membership Services Board
Practitioners Board


== ACM Council on Women in Computing ==
ACM-W, the ACM council on women in computing, supports, celebrates, and advocates internationally for the full engagement of women in computing. ACM–W's main programs are regional celebrations of women in computing, ACM-W chapters, and scholarships for women CS students to attend research conferences. In India and Europe these activities are overseen by ACM-W India and ACM-W Europe respectively. ACM-W collaborates with organizations such as the Anita Borg Institute, the National Center for Women & Information Technology (NCWIT), and Committee on the Status of Women in Computing Research (CRA-W).


=== Athena Lectures ===
The ACM-W gives an annual Athena Lecturer Award to honor outstanding women researchers who have made fundamental contributions to computer science. This program began in 2006. Speakers are nominated by SIG officers.
2006–2007: Deborah Estrin of UCLA
2007–2008: Karen Spärck Jones of Cambridge University
2008–2009: Shafi Goldwasser of MIT and the Weitzmann Institute of Science
2009–2010: Susan J. Eggers of the University of Washington
2010–2011: Mary Jane Irwin of the Pennsylvania State University
2011–2012: Judith S. Olson of the University of California, Irvine
2012–2013: Nancy Lynch of MIT
2013–2014: Katherine Yelick of LBNL
2014–2015: Susan Dumais of Microsoft Research
2015–2016: Jennifer Widom of Stanford University
2016–2017: Jennifer Rexford of Princeton University


== Cooperation ==
ACM's primary partner has been the IEEE Computer Society (IEEE-CS), which is the largest subgroup of the Institute of Electrical and Electronics Engineers (IEEE). The IEEE focuses more on hardware and standardization issues than theoretical computer science, but there is considerable overlap with ACM's agenda. They have many joint activities including conferences, publications and awards. ACM and its SIGs co-sponsor about 20 conferences each year with IEEE-CS and other parts of IEEE. Eckert-Mauchly Award and Ken Kennedy Award, both major awards in computer science, are given jointly by ACM and the IEEE-CS. They occasionally cooperate on projects like developing computing curricula.
ACM has also jointly sponsored on events with other professional organizations like the Society for Industrial and Applied Mathematics (SIAM).


== See also ==


== References ==


== External links ==
Official website
ACM portal for publications
ACM Digital Library
Association for Computing Machinery Records, 1947-2009, Charles Babbage Institute, University of Minnesota."
29,Computer program,5783,28866,"A computer program is a structured collection of instruction sequences that perform a specific task when executed by a computer. A computer requires programs to function.
A computer program is usually written by a computer programmer in a programming language. From the program in its human-readable form of source code, a compiler can derive machine code—a form consisting of instructions that the computer can directly execute. Alternatively, a computer program may be executed with the aid of an interpreter.
The evolution of a process is directed by a pattern of rules called a program. People create programs to direct processes.
A formal model of some part of a computer program that performs a general and well-defined task is called an algorithm. A collection of computer programs, libraries, and related data are referred to as software. Computer programs may be categorized along functional lines, such as application software and system software.


== History ==


=== Early programmable machines ===
The earliest programmable machines preceded the invention of the digital computer. In 1801, Joseph-Marie Jacquard devised a loom that would weave a pattern by following a series of perforated cards. Patterns could be woven and repeated by arranging the cards.


=== Analytical Engine ===

In 1837, Charles Babbage was inspired by Jacquard's loom to attempt to build the Analytical Engine. The names of the components of the calculating device were borrowed from the textile industry. In the textile industry, yarn was brought from the store to be milled. The device would have had a ""store""—memory to hold 1,000 numbers of 40 decimal digits each. Numbers from the ""store"" would then have then been transferred to the ""mill"" (analogous to the CPU of a modern machine), for processing. It was programmed using two sets of perforated cards—one to direct the operation and the other for the input variables.  However, after more than 17,000 pounds of the British government's money, the thousands of cogged wheels and gears never fully worked together.
During a nine-month period in 1842–43, Ada Lovelace translated the memoir of Italian mathematician Luigi Menabrea. The memoir covered the Analytical Engine. The translation contained Note G which completely detailed a method for calculating Bernoulli numbers using the Analytical Engine. This note is recognized by some historians as the world's first written computer program.


=== Universal Turing machine ===
In 1936, Alan Turing introduced the Universal Turing machine—a theoretical device that can model every computation that can be performed on a Turing complete computing machine. It is a finite-state machine that has an infinitely long read/write tape. The machine can move the tape back and forth, changing its contents as it performs an algorithm. The machine starts in the initial state, goes through a sequence of steps, and halts when it encounters the halt state. This machine is considered by some to be the origin of the stored-program computer—used by John von Neumann (1946) for the ""Electronic Computing Instrument"" that now bears the von Neumann architecture name.


=== Early programmable computers ===
The Z3 computer, invented by Konrad Zuse (1941) in Germany, was a digital and programmable computer. A digital computer uses electricity as the calculating component. The Z3 contained 2,400 relays to create the circuits. The circuits provided a binary, floating-point, nine-instruction computer. Programming the Z3 was through a specially designed keyboard and punched tape.
The Electronic Numerical Integrator And Computer (Fall 1945) was a Turing complete, general-purpose computer that used 17,468 vacuum tubes to create the circuits. At its core, it was a series of Pascalines wired together. Its 40 units weighed 30 tons, occupied 1,800 square feet (167 m2), and consumed $650 per hour (in 1940s currency) in electricity when idle. It had 20 base-10 accumulators. Programming the ENIAC took up to two months. Three function tables were on wheels and needed to be rolled to fixed function panels. Function tables were connected to function panels using heavy black cables. Each function table had 728 rotating knobs. Programming the ENIAC also involved setting some of the 3,000 switches. Debugging a program took a week. The ENIAC featured parallel operations. Different sets of accumulators could simultaneously work on different algorithms. It used punched card machines for input and output, and it was controlled with a clock signal. It ran for eight years, calculating hydrogen bomb parameters, predicting weather patterns, and producing firing tables to aim artillery guns.
The Manchester Small-Scale Experimental Machine (June 1948) was a stored-program computer. Programming transitioned away from moving cables and setting dials; instead, a computer program was stored in memory as numbers. Only three bits of memory were available to store each instruction, so it was limited to eight instructions. 32 switches were available for programming.


=== Later computers ===

Computers manufactured until the 1970s had front-panel switches for programming. The computer program was written on paper for reference. An instruction was represented by a configuration of on/off settings. After setting the configuration, an execute button was pressed. This process was then repeated. Computer programs also were manually input via paper tape or punched cards. After the medium was loaded, the starting address was set via switches and the execute button pressed.
In 1961, the Burroughs B5000 was built specifically to be programmed in the ALGOL 60 language. The hardware featured circuits to ease the compile phase.
In 1964, the IBM System/360 was a line of six computers each having the same instruction set architecture. The Model 30 was the smallest and least expensive. Customers could upgrade and retain the same application software. Each System/360 model featured multiprogramming. With operating system support, multiple programs could be in memory at once. When one was waiting for input/output, another could compute. Each model also could emulate other computers. Customers could upgrade to the System/360 and retain their IBM 7094 or IBM 1401 application software.


== Computer programming ==

Computer programming is the process of writing or editing source code. Editing source code involves testing, analyzing, refining, and sometimes coordinating with other programmers on a jointly developed program. A person who practices this skill is referred to as a computer programmer, software developer, and sometimes coder.
The sometimes lengthy process of computer programming is usually referred to as software development. The term software engineering is becoming popular as the process is seen as an engineering discipline.


=== Programming languages ===

Computer programs can be categorized by the programming language paradigm used to produce them. Two of the main paradigms are imperative and declarative.


==== Imperative languages ====
Imperative programming languages specify a sequential algorithm using declarations, expressions, and statements:
A declaration couples a variable name to a datatype – for example: var x: integer;
An expression yields a value – for example: 2 + 2 yields 4
A statement might assign an expression to a variable or use the value of a variable to alter the program's control flow – for example: x := 2 + 2; if x = 4 then do_something();
One criticism of imperative languages is the side effect of an assignment statement on a class of variables called non-local variables.


==== Declarative languages ====
Declarative programming languages describe what computation should be performed and not how to compute it. Declarative programs omit the control flow and are considered sets of instructions. Two broad categories of declarative languages are functional languages and logical languages. The principle behind functional languages (like Haskell) is to not allow side effects, which makes it easier to reason about programs like mathematical functions. The principle behind logical languages (like Prolog) is to define the problem to be solved – the goal – and leave the detailed solution to the Prolog system itself. The goal is defined by providing a list of subgoals. Then each subgoal is defined by further providing a list of its subgoals, etc. If a path of subgoals fails to find a solution, then that subgoal is backtracked and another path is systematically attempted.


=== Compilation and interpretation ===
A computer program in the form of a human-readable, computer programming language is called source code. Source code may be converted into an executable image by a compiler or executed immediately with the aid of an interpreter.
Compilers are used to translate source code from a programming language into either object code or machine code. Object code needs further processing to become machine code, and machine code consists of the central processing unit's native instructions, ready for execution. Compiled computer programs are commonly referred to as executables, binary images, or simply as binaries – a reference to the binary file format used to store the executable code.
Interpreters are used to execute source code from a programming language line-by-line. The interpreter decodes each statement and performs its behavior. One advantage of interpreters is that they can easily be extended to an interactive session. The programmer is presented with a prompt, and individual lines of code are typed in and performed immediately.
The main disadvantage of interpreters is computer programs run slower than when compiled. Interpreting code is slower because the interpreter must decode each statement and then perform it. However, software development may be faster using an interpreter because testing is immediate when the compiling step is omitted. Another disadvantage of interpreters is an interpreter must be present on the executing computer. By contrast, compiled computer programs need no compiler present during execution.
Just in time compilers pre-compile computer programs just before execution. For example, the Java virtual machine Hotspot contains a Just In Time Compiler which selectively compiles Java bytecode into machine code - but only code which Hotspot predicts is likely to be used many times.
Either compiled or interpreted programs might be executed in a batch process without human interaction.
Scripting languages are often used to create batch processes. One common scripting language is Unix shell, and its executing environment is called the command-line interface.
No properties of a programming language require it to be exclusively compiled or exclusively interpreted. The categorization usually reflects the most popular method of language execution. For example, Java is thought of as an interpreted language and C a compiled language, despite the existence of Java compilers and C interpreters.


== Storage and execution ==

Typically, computer programs are stored in non-volatile memory until requested either directly or indirectly to be executed by the computer user. Upon such a request, the program is loaded into random-access memory, by a computer program called an operating system, where it can be accessed directly by the central processor. The central processor then executes (""runs"") the program, instruction by instruction, until termination. A program in execution is called a process. Termination is either by normal self-termination or by error – software or hardware error.


=== Simultaneous execution ===

Many operating systems support multitasking which enables many computer programs to appear to run simultaneously on one computer. Operating systems may run multiple programs through process scheduling – a software mechanism to switch the CPU among processes often so users can interact with each program while it runs. Within hardware, modern day multiprocessor computers or computers with multicore processors may run multiple programs.
Multiple lines of the same computer program may be simultaneously executed using threads. Multithreading processors are optimized to execute multiple threads efficiently.


=== Self-modifying programs ===

A computer program in execution is normally treated as being different from the data the program operates on. However, in some cases, this distinction is blurred when a computer program modifies itself. The modified computer program is subsequently executed as part of the same program. Self-modifying code is possible for programs written in machine code, assembly language, Lisp, C, COBOL, PL/1, and Prolog.


== Functional categories ==
Computer programs may be categorized along functional lines. The main functional categories are application software and system software. System software includes the operating system which couples computer hardware with application software. The purpose of the operating system is to provide an environment in which application software executes in a convenient and efficient manner. In addition to the operating system, system software includes embedded programs, boot programs, and micro programs. Application software designed for end users have a user interface. Application software not designed for the end user includes middleware, which couples one application with another. Application software also includes utility programs. The distinction between system software and application software is under debate.


=== Application software ===

There are many types of application software:
The word app came to being in 21st century. It is a clipping of the word ""application"". They have been designed for many platforms, but the word was first used for smaller mobile apps. Desktop apps are traditional computer programs that run on desktop computers. Mobile apps run on mobile devices. Web apps run inside a web browser. Both mobile and desktop apps may be downloaded from the developers' website or purchased from app stores such as Microsoft Store, Apple App Store, Mac App Store, Google Play or Intel AppUp.
An application suite consists of multiple applications bundled together. Examples include Microsoft Office, LibreOffice, and iWork. They bundle a word processor, spreadsheet, and other applications.
Enterprise applications bundle accounting, personnel, customer, and vendor applications. Examples include enterprise resource planning, customer relationship management, and supply chain management software.
Enterprise infrastructure software supports the enterprise's software systems. Examples include databases, email servers, and network servers.
Information worker software are designed for workers at the departmental level. Examples include time management, resource management, analytical, collaborative and documentation tools. Word processors, spreadsheets, email and blog clients, personal information system, and individual media editors may aid in multiple information worker tasks.
Media development software generates print and electronic media for others to consume, most often in a commercial or educational setting. These produce graphics, publications, animations, and videos.
Product engineering software is used to help develop large machines and other application software. Examples includes computer-aided design (CAD), computer-aided engineering (CAE), and integrated development environments.
Entertainment Software can refer to video games, movie recorders and players, and music recorders and players.


=== Utility programs ===
Utility programs are application programs designed to aid system administrators and computer programmers.


=== Operating system ===

An operating system is a computer program that acts as an intermediary between a user of a computer and the computer hardware. 
In the 1950s, the programmer, who was also the operator, would write a program and run it. After the program finished executing, the output may have been printed, or it may have been punched onto paper tape or cards for later processing. More often than not the program did not work.  The programmer then looked at the console lights and fiddled with the console switches. If less fortunate, a memory printout was made for further study. In the 1960s, programmers reduced the amount of wasted time by automating the operator's job. A program called an operating system was kept in the computer at all times.
Originally, operating systems were programmed in assembly; however, modern operating systems are typically written in C.


=== Boot program ===
A stored-program computer requires an initial computer program stored in its read-only memory to boot. The boot process is to identify and initialize all aspects of the system, from processor registers to device controllers to memory contents. Following the initialization process, this initial computer program loads the operating system and sets the program counter to begin normal operations.


=== Embedded programs ===

Independent of the host computer, a hardware device might have embedded firmware to control its operation. Firmware is used when the computer program is rarely or never expected to change, or when the program must not be lost when the power is off.


=== Microcode programs ===

Microcode programs control some central processing units and some other hardware. This code moves data between the registers, buses, arithmetic logic units, and other functional units in the CPU. Unlike conventional programs, microcode is not usually written by, or even visible to, the end users of systems, and is usually provided by the manufacturer, and is considered internal to the device.


== See also ==
Automatic programming
Firmware
Killer application
Software
Software bug


== References ==


== Further reading ==
Knuth, Donald E. (1997). The Art of Computer Programming, Volume 1, 3rd Edition. Boston: Addison-Wesley. ISBN 0-201-89683-4. 
Knuth, Donald E. (1997). The Art of Computer Programming, Volume 2, 3rd Edition. Boston: Addison-Wesley. ISBN 0-201-89684-2. 
Knuth, Donald E. (1997). The Art of Computer Programming, Volume 3, 3rd Edition. Boston: Addison-Wesley. ISBN 0-201-89685-0."
30,Computational electromagnetics,3849994,28632,"Computational electromagnetics, computational electrodynamics or electromagnetic modeling is the process of modeling the interaction of electromagnetic fields with physical objects and the environment.
It typically involves using computationally efficient approximations to Maxwell's equations and is used to calculate antenna performance, electromagnetic compatibility, radar cross section and electromagnetic wave propagation when not in free space.
A specific part of computational electromagnetics deals with electromagnetic radiation scattered and absorbed by small particles.


== Background ==
Several real-world electromagnetic problems like electromagnetic scattering, electromagnetic radiation, modeling of waveguides etc., are not analytically calculable, for the multitude of irregular geometries found in actual devices. Computational numerical techniques can overcome the inability to derive closed form solutions of Maxwell's equations under various constitutive relations of media, and boundary conditions. This makes computational electromagnetics (CEM) important to the design, and modeling of antenna, radar, satellite and other communication systems, nanophotonic devices and high speed silicon electronics, medical imaging, cell-phone antenna design, among other applications.
CEM typically solves the problem of computing the E (electric) and H (magnetic) fields across the problem domain (e.g., to calculate antenna radiation pattern for an arbitrarily shaped antenna structure). Also calculating power flow direction (Poynting vector), a waveguide's normal modes, media-generated wave dispersion, and scattering can be computed from the E and H fields. CEM models may or may not assume symmetry, simplifying real world structures to idealized cylinders, spheres, and other regular geometrical objects. CEM models extensively make use of symmetry, and solve for reduced dimensionality from 3 spatial dimensions to 2D and even 1D.
An eigenvalue problem formulation of CEM allows us to calculate steady state normal modes in a structure. Transient response and impulse field effects are more accurately modeled by CEM in time domain, by FDTD. Curved geometrical objects are treated more accurately as finite elements FEM, or non-orthogonal grids. Beam propagation method (BPM) can solve for the power flow in waveguides. CEM is application specific, even if different techniques converge to the same field and power distributions in the modeled domain.


== Overview of methods ==
One approach is to discretize the space in terms of grids (both orthogonal, and non-orthogonal) and solving Maxwell's equations at each point in the grid. Discretization consumes computer memory, and solving the equations takes significant time. Large-scale CEM problems face memory and CPU limitations. As of 2007, CEM problems require supercomputers, high performance clusters, vector processors and/or parallelism. Typical formulations involve either time-stepping through the equations over the whole domain for each time instant; or through banded matrix inversion to calculate the weights of basis functions, when modeled by finite element methods; or matrix products when using transfer matrix methods; or calculating integrals when using method of moments (MoM); or using fast fourier transforms, and time iterations when calculating by the split-step method or by BPM.


== Choice of methods ==
Choosing the right technique for solving a problem is important, as choosing the wrong one can either result in incorrect results, or results which take excessively long to compute. However, the name of a technique does not always tell one how it is implemented, especially for commercial tools, which will often have more than one solver.
Davidson gives two tables comparing the FEM, MoM and FDTD techniques in the way they are normally implemented. One table is for both open region (radiation and scattering problems) and another table is for guided wave problems.


== Maxwell's equations in hyperbolic PDE form ==
Maxwell's equations can be formulated as a hyperbolic system of partial differential equations. This gives access to powerful techniques for numerical solutions.
It is assumed that the waves propagate in the (x,y)-plane and restrict the direction of the magnetic field to be parallel to the z-axis and thus the electric field to be parallel to the (x,y) plane. The wave is called a transverse magnetic (TM) wave. In 2D and no polarization terms present, Maxwell's equations can then be formulated as:

  
    
      
        
          
            ∂
            
              ∂
              t
            
          
        
        
          
            
              u
              ¯
            
          
        
        +
        A
        
          
            ∂
            
              ∂
              x
            
          
        
        
          
            
              u
              ¯
            
          
        
        +
        B
        
          
            ∂
            
              ∂
              y
            
          
        
        
          
            
              u
              ¯
            
          
        
        +
        C
        
          
            
              u
              ¯
            
          
        
        =
        
          
            
              g
              ¯
            
          
        
      
    
    {\displaystyle {\frac {\partial }{\partial t}}{\bar {u}}+A{\frac {\partial }{\partial x}}{\bar {u}}+B{\frac {\partial }{\partial y}}{\bar {u}}+C{\bar {u}}={\bar {g}}}
  
where u, A, B, and C are defined as

  
    
      
        
          
            
              u
              ¯
            
          
        
        =
        
          (
          
            
              
                
                  
                    E
                    
                      x
                    
                  
                
              
              
                
                  
                    E
                    
                      y
                    
                  
                
              
              
                
                  
                    H
                    
                      z
                    
                  
                
              
            
          
          )
        
        ,
      
    
    {\displaystyle {\bar {u}}=\left({\begin{matrix}E_{x}\\E_{y}\\H_{z}\end{matrix}}\right),}
  

  
    
      
        A
        =
        
          (
          
            
              
                
                  0
                
                
                  0
                
                
                  0
                
              
              
                
                  0
                
                
                  0
                
                
                  
                    
                      1
                      ϵ
                    
                  
                
              
              
                
                  0
                
                
                  
                    
                      1
                      μ
                    
                  
                
                
                  0
                
              
            
          
          )
        
        ,
      
    
    {\displaystyle A=\left({\begin{matrix}0&0&0\\0&0&{\frac {1}{\epsilon }}\\0&{\frac {1}{\mu }}&0\end{matrix}}\right),}
  

  
    
      
        B
        =
        
          (
          
            
              
                
                  0
                
                
                  0
                
                
                  
                    
                      
                        −
                        1
                      
                      ϵ
                    
                  
                
              
              
                
                  0
                
                
                  0
                
                
                  0
                
              
              
                
                  
                    
                      
                        −
                        1
                      
                      μ
                    
                  
                
                
                  0
                
                
                  0
                
              
            
          
          )
        
        ,
      
    
    {\displaystyle B=\left({\begin{matrix}0&0&{\frac {-1}{\epsilon }}\\0&0&0\\{\frac {-1}{\mu }}&0&0\end{matrix}}\right),}
  

  
    
      
        C
        =
        
          (
          
            
              
                
                  
                    
                      σ
                      ϵ
                    
                  
                
                
                  0
                
                
                  0
                
              
              
                
                  0
                
                
                  
                    
                      σ
                      ϵ
                    
                  
                
                
                  0
                
              
              
                
                  0
                
                
                  0
                
                
                  0
                
              
            
          
          )
        
        .
      
    
    {\displaystyle C=\left({\begin{matrix}{\frac {\sigma }{\epsilon }}&0&0\\0&{\frac {\sigma }{\epsilon }}&0\\0&0&0\end{matrix}}\right).}
  
In this representation, 
  
    
      
        
          
            
              g
              ¯
            
          
        
      
    
    {\displaystyle {\bar {g}}}
   is the forcing function, and is in the same space as 
  
    
      
        
          
            
              u
              ¯
            
          
        
      
    
    {\displaystyle {\bar {u}}}
  . It can be used to express an externally applied field or to describe an optimization constraint. As formulated above:

  
    
      
        
          
            
              g
              ¯
            
          
        
        =
        
          (
          
            
              
                
                  
                    E
                    
                      x
                      ,
                      c
                      o
                      n
                      s
                      t
                      r
                      a
                      i
                      n
                      t
                    
                  
                
              
              
                
                  
                    E
                    
                      y
                      ,
                      c
                      o
                      n
                      s
                      t
                      r
                      a
                      i
                      n
                      t
                    
                  
                
              
              
                
                  
                    H
                    
                      z
                      ,
                      c
                      o
                      n
                      s
                      t
                      r
                      a
                      i
                      n
                      t
                    
                  
                
              
            
          
          )
        
        .
      
    
    {\displaystyle {\bar {g}}=\left({\begin{matrix}E_{x,constraint}\\E_{y,constraint}\\H_{z,constraint}\end{matrix}}\right).}
  

  
    
      
        
          
            
              g
              ¯
            
          
        
      
    
    {\displaystyle {\bar {g}}}
   may also be explicitly defined equal to zero to simplify certain problems, or to find a characteristic solution, which is often the first step in a method to find the particular inhomogeneous solution.


== Integral equation solvers ==


=== The discrete dipole approximation ===
The discrete dipole approximation is a flexible technique for computing scattering and absorption by targets of arbitrary geometry. The formulation is based on integral form of Maxwell equations. The DDA is an approximation of the continuum target by a finite array of polarizable points. The points acquire dipole moments in response to the local electric field. The dipoles of course interact with one another via their electric fields, so the DDA is also sometimes referred to as the coupled dipole approximation. The resulting linear system of equations is commonly solved using conjugate gradient iterations. The discretization matrix has symmetries (the integral form of Maxwell equations has form of convolution) enabling Fast Fourier Transform to multiply matrix times vector during conjugate gradient iterations.


=== Method of moments element method ===
The method of moments (MoM) or boundary element method (BEM) is a numerical computational method of solving linear partial differential equations which have been formulated as integral equations (i.e. in boundary integral form). It can be applied in many areas of engineering and science including fluid mechanics, acoustics, electromagnetics, fracture mechanics, and plasticity.
MoM has become more popular since the 1980s. Because it requires calculating only boundary values, rather than values throughout the space, it is significantly more efficient in terms of computational resources for problems with a small surface/volume ratio. Conceptually, it works by constructing a ""mesh"" over the modeled surface. However, for many problems, BEM are significantly computationally less efficient than volume-discretization methods (finite element method, finite difference method, finite volume method). Boundary element formulations typically give rise to fully populated matrices. This means that the storage requirements and computational time will tend to grow according to the square of the problem size. By contrast, finite element matrices are typically banded (elements are only locally connected) and the storage requirements for the system matrices typically grow linearly with the problem size. Compression techniques (e.g. multipole expansions or adaptive cross approximation/hierarchical matrices) can be used to ameliorate these problems, though at the cost of added complexity and with a success-rate that depends heavily on the nature and geometry of the problem.
BEM is applicable to problems for which Green's functions can be calculated. These usually involve fields in linear homogeneous media. This places considerable restrictions on the range and generality of problems suitable for boundary elements. Nonlinearities can be included in the formulation, although they generally introduce volume integrals which require the volume to be discretized before solution, removing an oft-cited advantage of BEM.


=== Fast multipole method ===
The fast multipole method (FMM) is an alternative to MoM or Ewald summation. It is an accurate simulation technique and requires less memory and processor power than MoM. The FMM was first introduced by Greengard and Rokhlin and is based on the multipole expansion technique. The first application of the FMM in computational electromagnetics was by Engheta et al.(1992). FMM can also be used to accelerate MoM.


=== Plane wave time-domain ===
While the fast multipole method is useful for accelerating MoM solutions of integral equations with static or frequency-domain oscillatory kernels, the plane wave time-domain (PWTD) algorithm employs similar ideas to accelerate the MoM solution of time-domain integral equations involving the retarded potential. The PWTD algorithm was introduced in 1998 by Ergin, Shanker, and Michielssen.


=== Partial element equivalent circuit method ===
The partial element equivalent circuit (PEEC) is a 3D full-wave modeling method suitable for combined electromagnetic and circuit analysis. Unlike MoM, PEEC is a full spectrum method valid from dc to the maximum frequency determined by the meshing. In the PEEC method, the integral equation is interpreted as Kirchhoff's voltage law applied to a basic PEEC cell which results in a complete circuit solution for 3D geometries. The equivalent circuit formulation allows for additional SPICE type circuit elements to be easily included. Further, the models and the analysis apply to both the time and the frequency domains. The circuit equations resulting from the PEEC model are easily constructed using a modified loop analysis (MLA) or modified nodal analysis (MNA) formulation. Besides providing a direct current solution, it has several other advantages over a MoM analysis for this class of problems since any type of circuit element can be included in a straightforward way with appropriate matrix stamps. The PEEC method has recently been extended to include nonorthogonal geometries. This model extension, which is consistent with the classical orthogonal formulation, includes the Manhattan representation of the geometries in addition to the more general quadrilateral and hexahedral elements. This helps in keeping the number of unknowns at a minimum and thus reduces computational time for nonorthogonal geometries.


== Differential equation solvers ==


=== Finite-difference time-domain ===
Finite-difference time-domain (FDTD) is a popular CEM technique. It is easy to understand. It has an exceptionally simple implementation for a full wave solver. It is at least an order of magnitude less work to implement a basic FDTD solver than either an FEM or MoM solver. FDTD is the only technique where one person can realistically implement oneself in a reasonable time frame, but even then, this will be for a quite specific problem. Since it is a time-domain method, solutions can cover a wide frequency range with a single simulation run, provided the time step is small enough to satisfy the Nyquist–Shannon sampling theorem for the desired highest frequency.
FDTD belongs in the general class of grid-based differential time-domain numerical modeling methods. Maxwell's equations (in partial differential form) are modified to central-difference equations, discretized, and implemented in software. The equations are solved in a cyclic manner: the electric field is solved at a given instant in time, then the magnetic field is solved at the next instant in time, and the process is repeated over and over again.
The basic FDTD algorithm traces back to a seminal 1966 paper by Kane Yee in IEEE Transactions on Antennas and Propagation. Allen Taflove originated the descriptor ""Finite-difference time-domain"" and its corresponding ""FDTD"" acronym in a 1980 paper in IEEE Transactions on Electromagnetic Compatibility. Since about 1990, FDTD techniques have emerged as the primary means to model many scientific and engineering problems addressing electromagnetic wave interactions with material structures. An effective technique based on a time-domain finite-volume discretization procedure was introduced by Mohammadian et al. in 1991. Current FDTD modeling applications range from near-DC (ultralow-frequency geophysics involving the entire Earth-ionosphere waveguide) through microwaves (radar signature technology, antennas, wireless communications devices, digital interconnects, biomedical imaging/treatment) to visible light (photonic crystals, nanoplasmonics, solitons, and biophotonics). Approximately 30 commercial and university-developed software suites are available.


=== Multiresolution time-domain ===
MRTD is an adaptive alternative to the finite difference time domain method (FDTD) based on wavelet analysis.


=== Finite element method ===
The finite element method (FEM) is used to find approximate solution of partial differential equations (PDE) and integral equations. The solution approach is based either on eliminating the time derivatives completely (steady state problems), or rendering the PDE into an equivalent ordinary differential equation, which is then solved using standard techniques such as finite differences, etc.
In solving partial differential equations, the primary challenge is to create an equation which approximates the equation to be studied, but which is numerically stable, meaning that errors in the input data and intermediate calculations do not accumulate and destroy the meaning of the resulting output. There are many ways of doing this, with various advantages and disadvantages. The finite element method is a good choice for solving partial differential equations over complex domains or when the desired precision varies over the entire domain.


=== Finite integration technique ===
The finite integration technique (FIT) is a spatial discretization scheme to numerically solve electromagnetic field problems in time and frequency domain. It preserves basic topological properties of the continuous equations such as conservation of charge and energy. FIT was proposed in 1977 by Thomas Weiland and has been enhanced continually over the years. This method covers the full range of electromagnetics (from static up to high frequency) and optic applications and is the basis for commercial simulation tools.
The basic idea of this approach is to apply the Maxwell equations in integral form to a set of staggered grids. This method stands out due to high flexibility in geometric modeling and boundary handling as well as incorporation of arbitrary material distributions and material properties such as anisotropy, non-linearity and dispersion. Furthermore, the use of a consistent dual orthogonal grid (e.g. Cartesian grid) in conjunction with an explicit time integration scheme (e.g. leap-frog-scheme) leads to compute and memory-efficient algorithms, which are especially adapted for transient field analysis in radio frequency (RF) applications.


=== Pseudo-spectral time domain ===
This class of marching-in-time computational techniques for Maxwell's equations uses either discrete Fourier or discrete Chebyshev transforms to calculate the spatial derivatives of the electric and magnetic field vector components that are arranged in either a 2-D grid or 3-D lattice of unit cells. PSTD causes negligible numerical phase velocity anisotropy errors relative to FDTD, and therefore allows problems of much greater electrical size to be modeled.


=== Pseudo-spectral spatial domain ===
PSSD solves Maxwell's equations by propagating them forward in a chosen spatial direction. The fields are therefore held as a function of time, and (possibly) any transverse spatial dimensions. The method is pseudo-spectral because temporal derivatives are calculated in the frequency domain with the aid of FFTs. Because the fields are held as functions of time, this enables arbitrary dispersion in the propagation medium to be rapidly and accurately modelled with minimal effort. However, the choice to propagate forward in space (rather than in time) brings with it some subtleties, particularly if reflections are important.


=== Transmission line matrix ===
Transmission line matrix (TLM) can be formulated in several means as a direct set of lumped elements solvable directly by a circuit solver (ala SPICE, HSPICE, et al.), as a custom network of elements or via a scattering matrix approach. TLM is a very flexible analysis strategy akin to FDTD in capabilities, though more codes tend to be available with FDTD engines.


=== Locally one-dimensional ===
This is an implicit method. In this method, in two-dimensional case, Maxwell equations are computed in two steps, whereas in three-dimensional case Maxwell equations are divided into three spatial coordinate directions. Stability and dispersion analysis of the three-dimensional LOD-FDTD method have been discussed in detail.


== Other methods ==


=== EigenMode expansion ===
Eigenmode expansion (EME) is a rigorous bi-directional technique to simulate electromagnetic propagation which relies on the decomposition of the electromagnetic fields into a basis set of local eigenmodes. The eigenmodes are found by solving Maxwell's equations in each local cross-section. Eigenmode expansion can solve Maxwell's equations in 2D and 3D and can provide a fully vectorial solution provided that the mode solvers are vectorial. It offers very strong benefits compared with the FDTD method for the modelling of optical waveguides, and it is a popular tool for the modelling of fiber optics and silicon photonics devices.


=== Physical optics ===
Physical optics (PO) is the name of a high frequency approximation (short-wavelength approximation) commonly used in optics, electrical engineering and applied physics. It is an intermediate method between geometric optics, which ignores wave effects, and full wave electromagnetism, which is a precise theory. The word ""physical"" means that it is more physical than geometrical optics and not that it is an exact physical theory.
The approximation consists of using ray optics to estimate the field on a surface and then integrating that field over the surface to calculate the transmitted or scattered field. This resembles the Born approximation, in that the details of the problem are treated as a perturbation.


=== Uniform theory of diffraction ===
The uniform theory of diffraction (UTD) is a high frequency method for solving electromagnetic scattering problems from electrically small discontinuities or discontinuities in more than one dimension at the same point.
The uniform theory of diffraction approximates near field electromagnetic fields as quasi optical and uses ray diffraction to determine diffraction coefficients for each diffracting object-source combination. These coefficients are then used to calculate the field strength and phase for each direction away from the diffracting point. These fields are then added to the incident fields and reflected fields to obtain a total solution.


== Validation ==
Validation is one of the key issues facing electromagnetic simulation users. The user must understand and master the validity domain of its simulation. The measure is, ""how far from the reality are the results?""
Answering this question involves three steps: comparison between simulation results and analytical formulation, cross-comparison between codes, and comparison of simulation results with measurement.


=== Comparison between simulation results and analytical formulation ===
For example, assessing the value of the radar cross section of a plate with the analytical formula:

  
    
      
        
          
            RCS
          
          
            Plate
          
        
        =
        
          
            
              4
              π
              
                A
                
                  2
                
              
            
            
              λ
              
                2
              
            
          
        
        ,
      
    
    {\displaystyle {\text{RCS}}_{\text{Plate}}={\frac {4\pi A^{2}}{\lambda ^{2}}},}
  

where A is the surface of the plate and 
  
    
      
        λ
      
    
    {\displaystyle \lambda }
   is the wavelength. The next curve presenting the RCS of a plate computed at 35 GHz can be used as reference example.


=== Cross-comparison between codes ===
One example is the cross comparison of results from method of moments and asymptotic methods in their validity domains.


=== Comparison of simulation results with measurement ===
The final validation step is made by comparison between measurements and simulation. For example, the RCS calculation and the measurement of a complex metallic object at 35 GHz. The computation implements GO, PO and PTD for the edges.
Validation processes can clearly reveal that some differences can be explained by the differences between the experimental setup and its reproduction in the simulation environment.


== Light scattering codes ==
There are now many efficient codes for solving electromagnetic scattering problems. They are listed as discrete dipole approximation codes, codes for electromagnetic scattering by cylinders, codes for electromagnetic scattering by spheres. Solutions which are analytical, such as Mie solution for scattering by spheres or cylinders, can be used to validate more involved techniques.


== See also ==
EM simulation software
Analytical regularization
Electromagnetic field solver
Electromagnetic wave equation
Finite-difference time-domain method
Finite-difference frequency-domain
Mie theory
Physical optics
Rigorous coupled-wave analysis
Space mapping
Uniform theory of diffraction
https://en.wikipedia.org/wiki/HOBBIES_(electromagnetic_solver)


== References ==


== Further reading ==
Detailed and highly visual lecture notes and videos on Computational Electromagnetics
R. F. Harrington (1993). Field Computation by Moment Methods. Wiley-IEEE Press. ISBN 0-7803-1014-4. 
W. C. Chew; J.-M. Jin; E. Michielssen; J. Song (2001). Fast and Efficient Algorithms in Computational Electromagnetics. Artech House Publishers. ISBN 1-58053-152-0. 
J. Jin (2002). The Finite Element Method in Electromagnetics, 2nd. ed. Wiley-IEEE Press. ISBN 0-471-43818-9. 
Allen Taflove and Susan C. Hagness (2005). Computational Electrodynamics: The Finite-Difference Time-Domain Method, 3rd ed. Artech House Publishers. ISBN 1-58053-832-0. 


== External links ==
Computational electromagnetics at the Open Directory Project
Computational electromagnetics: a review"
31,Stylometry,2097760,28620,"Stylometry is the application of the study of linguistic style, usually to written language, but it has successfully been applied to music and to fine-art paintings as well.
Stylometry is often used to attribute authorship to anonymous or disputed documents. It has legal as well as academic and literary applications, ranging from the question of the authorship of Shakespeare's works to forensic linguistics.


== History ==
Stylometry grew out of earlier techniques of analyzing texts for evidence of authenticity, author identity, and other questions.
The modern practice of the discipline received major impetus from the study of authorship problems in English Renaissance drama. Researchers and readers observed that some playwrights of the era had distinctive patterns of language preferences, and attempted to use those patterns to identify authors in uncertain or collaborative works. Early efforts were not always successful: in 1901, one researcher attempted to use John Fletcher's preference for ""'em"", the contractional form of ""them"", as a marker to distinguish between Fletcher and Philip Massinger in their collaborations—but he mistakenly employed an edition of Massinger's works in which the editor had expanded all instances of ""'em"" to ""them"".
The basics of stylometry were set out by Polish philosopher Wincenty Lutosławski in Principes de stylométrie (1890). Lutosławski used this method to build a chronology of Plato's Dialogues.
The development of computers and their capacities for analyzing large quantities of data enhanced this type of effort by orders of magnitude. The great capacity of computers for data analysis, however, did not guarantee quality output. In the early 1960s, Rev. A. Q. Morton produced a computer analysis of the fourteen Epistles of the New Testament attributed to St. Paul, which showed that six different authors had written that body of work. A check of his method, applied to the works of James Joyce, gave the result that Ulysses, Joyce's multi-perspective, multi-style masterpiece, was written by five separate individuals; none of whom had any part in the crafting of Joyce's first novel, A Portrait of the Artist as a Young Man.
In time, however, and with practice, researchers and scholars have refined their approaches and methods, to yield better results. One notable early success was the resolution of disputed authorship in twelve of The Federalist Papers by Frederick Mosteller and David Wallace. While questions of initial assumptions and methodology still arise (and, perhaps, always will), few now dispute the basic premise that linguistic analysis of written texts can produce valuable information and insight. (Indeed, this was apparent even before the advent of computers: the successful application of a textual/linguistic approach to the Fletcher canon by Cyrus Hoy and others yielded clear results in the late 1950s and early '60s.)


== Applications ==
Applications of stylometry include literary studies, historical studies, social studies, gender studies, and many forensic cases and studies.


== Current research ==
Modern stylometry draws heavily on the aid of computers for statistical analysis, artificial intelligence and access to the growing corpus of texts available via the Internet. Software systems such as Signature (freeware produced by Dr Peter Millican of Oxford University), JGAAP (the Java Graphical Authorship Attribution Program—freeware produced by Dr Patrick Juola of Duquesne University), stylo (an open-source R package for a variety of stylometric analyses, including authorship attribution) and Stylene for Dutch (online freeware by Prof Walter Daelemans of University of Antwerp and Dr Véronique Hoste of University of Ghent) make


== Academic venues and events ==
Stylometric methods are discussed in several academic fields, mostly as a tangential field of application for e.g. machine learning, natural language processing, or lexicography.


=== Forensic linguistics ===
The International Association of Forensic Linguists (IAFL) organises the Biennial Conference of the International Association of Forensic Linguists (13th edition in 2016 in Porto) and publishes The International Journal of Speech, Language and the Law with forensic stylistics as one of its central topics.


=== AAAI ===
The Association for the Advancement of Artificial Intelligence (AAAI) has hosted several events on subjective and stylistic analysis of text.


=== PAN ===
PAN workshops (originally, plagiarism analysis, authorship identification, and near-duplicate detection, later more generally workshop on uncovering plagiarism, authorship, and social software misuse) organised since 2007 mainly in conjunction with information access conferences such as ACM SIGIR, FIRE, and CLEF. PAN formulates shared challenge tasks for plagiarism detection, authorship identification, author gender identification, author profiling, vandalism detection, and other related text analysis tasks, many of which hinge on stylometry.


== Case studies of interest ==
Around 1370–1070 BC, as recorded in the Book of Judges, one tribe identified members of another tribe in order to kill them by asking them to say the word Shibboleth which in the dialect of the intended victims sounded like ""sibboleth"".
In 1439, Lorenzo Valla showed that the Donation of Constantine was a forgery, an argument based partly on a comparison of the Latin with that used in authentic 4th-century documents.
In 1952, the Swedish bishop Dick Helander was elected bishop of Strängnäs. The campaign was competitive and Helander was accused of writing a series of a hundred-some anonymous libelous letters about other candidates to the electorate of the bishopric of Strängnäs. Helander was first convicted of writing the letters and lost his position as bishop but later partially exonerated. The letters were studied using a number of stylometric measures (and also typewriter characteristics) and the various court cases and further examinations, many contracted by Helander himself during the years up to his death in 1978 discussed stylometric methodology and its value as evidence in some detail.
In 1975, after Ronald Reagan had served as governor of California, he began giving weekly radio commentaries syndicated to hundreds of stations. After his personal notes were made public on his 90th birthday in 2001, a study to determine which of those talks were written by him and which were written by various aides used stylostatistical methods.
In 1996, the stylometric analysis of the controversial, pseudonymously authored book Primary Colors, performed by Vassar professor Donald Foster brought the field to the attention of a wider audience after correctly identifying the author as Joe Klein. (This case was only resolved after a handwriting analysis confirmed the authorship).
In 1996, stylometric methods were used to compare the Unabomber manifesto with letters written by one of the suspects, Theodor Kaczynski to his brother, which led to his apprehension and later conviction.
In April 2015, researchers using stylometry techniques identified a play, Double Falsehood, as being the work of William Shakespeare. Researchers analyzed 54 plays by Shakespeare and John Fletcher and compared average sentence length, studied the use of unusual words and quantified the complexity and psychological valence of its language.
In 2017, a group of linguists, computer scientists, and scholars analysed the authoship of Elena Ferrante. Based on a corpus created at University of Padua containing 150 novels written by 40 authors, they analyzed Ferrante's style based on seven of her novels. They were able to compare her writing style with 39 other novelists using, for example, stylo. The conclusion was the same for all of them: Domenico Starnone is the secret hand behind Elena Ferrante


== Data and methods ==
Since stylometry has both descriptive use cases, used to characterise the content of a collection, and identificatory use cases, e.g. identifying authors or categories of texts, the methods used to analyse the data and features above range from those built to classify items into sets or to distribute items in a space of feature variation. Most methods are statistical in nature, such as cluster analysis and discriminant analysis, are typically based on philological data and features, and are fruitful application domains for modern machine learning approaches.
Whereas in the past, stylometry emphasized the rarest or most striking elements of a text, contemporary techniques can isolate identifying patterns even in common parts of speech. Most systems are based on lexical statistics, i.e. using the frequencies of words and terms in the text to characterise the text (or its author). In this context, unlike in information retrieval, the observed occurrence patterns of the most common words are more interesting than the topical terms which are less frequent.
The primary stylometric method is the writer invariant: a property held in common by all texts, or at least all texts long enough to admit of analysis yielding statistically significant results, written by a given author. An example of a writer invariant is frequency of function words used by the writer.
In one such method, the text is analyzed to find the 50 most common words. The text is then broken into 5,000 word chunks and each of the chunks is analyzed to find the frequency of those 50 words in that chunk. This generates a unique 50-number identifier for each chunk. These numbers place each chunk of text into a point in a 50-dimensional space. This 50-dimensional space is flattened into a plane using principal components analysis (PCA). This results in a display of points that correspond to an author's style. If two literary works are placed on the same plane, the resulting pattern may show if both works were by the same author or different authors.


=== Neural networks ===
Neural networks, a special case of statistical machine learning methods, have been used to analyze authorship of texts. Text of undisputed authorship are used to train the neural network through processes such as backpropagation, where training error is calculated and used to update the process to increase accuracy. Through a process akin to non-linear regression, the network gains the ability to generalize its recognition ability to new texts to which it has not yet been exposed, classifying them to a stated degree of confidence. Such techniques were applied to the long-standing claims of collaboration of Shakespeare with his contemporaries Fletcher and Christopher Marlowe, and confirmed the view, based on more conventional scholarship, that such collaboration had indeed taken place.
A 1999 study showed that a neural network program reached 70% accuracy in determining authorship of poems it had not yet analyzed. This study from Vrije Universiteit examined identification of poems by three Dutch authors using only letter sequences such as ""den"".
A study used deep belief networks (DBN) for authorship verification model applicable for continuous authentication (CA).
One problem with this method of analysis is that the network can become biased based on its training set, possibly selecting authors the network has more often analyzed.


=== Genetic algorithms ===
The genetic algorithm is another machine learning technique used in stylometry. This involves a method that starts out with a set of rules. An example rule might be, ""If but appears more than 1.7 times in every thousand words, then the text is author X"". The program is presented with text and uses the rules to determine authorship. The rules are tested against a set of known texts and each rule is given a fitness score. The 50 rules with the lowest scores are thrown out. The remaining 50 rules are given small changes and 50 new rules are introduced. This is repeated until the evolved rules correctly attribute the texts.


=== Rare pairs ===
One method for identifying style is called ""rare pairs"", and relies upon individual habits of collocation. The use of certain words may, for a particular author, idiosyncratically entail the use of other, predictable words.


== Authorship attribution in instant messaging ==
The diffusion of Internet has shifted the authorship attribution attention towards online texts (web pages, blogs, etc.) electronic messages (e-mails, tweets, posts, etc.), and other types of written information that are far shorter than an average book, much less formal and more diverse in terms of expressive elements such as colors, layout, fonts, graphics, emoticons, etc. Efforts to take into account such aspects at the level of both structure and syntax were reported in. In addition, content-specific and idiosyncratic cues (e.g., topic models and grammar checking tools) were introduced to unveil deliberate stylistic choices.
Standard stylometric features have been employed to categorize the content of a chat over instant messaging, or the behavior of the participants, but attempts of identifying chat participants are still few and early. Furthermore, the similarity between spoken conversations and chat interactions has been neglected while being a key difference between chat data and any other type of written information.


== See also ==
Linguistics and the Book of Mormon, Stylometry (Wordprint Studies)
Moshe Koppel
Writeprint


== Notes ==


== References ==
Brocardo, Marcelo Luiz; Issa Traore; Sherif Saad; Isaac Woungang (2013). Authorship Verification for Short Messages Using Stylometry. IEEE Intl. Conference on Computer, Information and Telecommunication Systems (CITS). 
Can F, Patton JM (2004). ""Change of writing style with time"". Computers and the Humanities. 38 (1): 61–82. doi:10.1023/b:chum.0000009225.28847.77. 
Brennan, Michael Robert; Greenstadt, Rachel. ""Practical Attacks Against Authorship Recognition Techniques"". Innovative Applications of Artificial Intelligence. 
Hope, Jonathan (1994). The Authorship of Shakespeare's Plays. Cambridge: Cambridge University Press. 
Hoy C (1956–62). ""The Shares of Fletcher and His Collaborators in the Beaumont and Fletcher Canon"". Studies in Bibliography. 7–15. 
Juola, Patrick (2006). ""Authorship Attribution"" (PDF). Foundations and Trends in Information Retrieval. 1: 3. doi:10.1561/1500000005. 
Kenny, Anthony (1982). The Computation of Style: An Introduction to Statistics for Students of Literature and Humanities. Oxford: Pergamon Press. 
Romaine, Suzanne (1982). Socio-Historical Linguistics. Cambridge: Cambridge University Press. 
Samuels, M. L. (1972). Linguistic Evolution: With Special Reference to English. Cambridge: Cambridge University Press. 
Schoenbaum, Samuel (1966). Internal Evidence and Elizabethan Dramatic Authorship: An Essay in Literary History and Method. Evanston, IL, USA: Northwestern University Press. 
Van Droogenbroeck, Frans J. (2016) ""Handling the Zipf distribution in computerized authorship attribution""
Zenkov A.V. A Method of Text Attribution Based on the Statistics of Numerals // Journal of Quantitative Linguistics, 2017, https://dx.doi.org/10.1080/09296174.2017.1371915


=== Further reading ===
See also the academic journal Literary and Linguistic Computing (published by the University of Oxford) and the Language Resources and Evaluation journal.


== External links ==
Association for Computers and the Humanities
Literary and Linguistic Computing
Computational Stylistics Group
Signature Stylometric System
JGAAP Authorship Attribution Program
Uncovering the Mystery of J.K. Rowling's Latest Novel"
32,Financial modeling,2844974,28549,"Financial modeling is the task of building an abstract representation (a model) of a real world financial situation. This is a mathematical model designed to represent (a simplified version of) the performance of a financial asset or portfolio of a business, project, or any other investment. Financial modeling is a general term that means different things to different users; the reference usually relates either to accounting and corporate finance applications, or to quantitative finance applications. While there has been some debate in the industry as to the nature of financial modeling—whether it is a tradecraft, such as welding, or a science—the task of financial modeling has been gaining acceptance and rigor over the years. Typically, financial modeling is understood to mean an exercise in either asset pricing or corporate finance, of a quantitative nature. In other words, financial modelling is about translating a set of hypotheses about the behavior of markets or agents into numerical predictions; for example, a firm's decisions about investments (the firm will invest 20% of assets), or investment returns (returns on ""stock A"" will, on average, be 10% higher than the market's returns).


== Accounting ==
In corporate finance and the accounting profession, financial modeling typically entails financial statement forecasting; usually the preparation of detailed company-specific models used for decision making purposes and financial analysis.
Applications include:
Business valuation, especially discounted cash flow, but including other valuation problems
Scenario planning and management decision making (""what is""; ""what if""; ""what has to be done"")
Capital budgeting
Cost of capital (i.e. WACC) calculations
Financial statement analysis (including of operating- and finance leases, and R&D)
Project finance
To generalize as to the nature of these models: firstly, as they are built around financial statements, calculations and outputs are monthly, quarterly or annual; secondly, the inputs take the form of “assumptions”, where the analyst specifies the values that will apply in each period for external / global variables (exchange rates, tax percentage, etc.…; may be thought of as the model parameters), and for internal / company specific variables (wages, unit costs, etc.…). Correspondingly, both characteristics are reflected (at least implicitly) in the mathematical form of these models: firstly, the models are in discrete time; secondly, they are deterministic. For discussion of the issues that may arise, see below; for discussion as to more sophisticated approaches sometimes employed, see Corporate finance# Quantifying uncertainty, and Financial economics #Corporate finance theory.
Modelers are are often designated ""financial analyst"" (and are sometimes referred to (tongue in cheek) as ""number crunchers"") . Typically, the modeler will have completed an MBA or MSF with (optional) coursework in ""financial modeling"". Accounting qualifications and finance certifications such as the CIIA and CFA generally do not provide direct or explicit training in modeling. At the same time, numerous commercial training courses are offered, both through universities and privately.
Although purpose built software does exist, the vast proportion of the market is spreadsheet-based; this is largely since the models are almost always company specific. Also, analysts will each have their own criteria and methods for financial modeling. Microsoft Excel now has by far the dominant position, having overtaken Lotus 1-2-3 in the 1990s. Spreadsheet-based modelling can have its own problems, and several standardizations and ""best practices"" have been proposed. ""Spreadsheet risk"" is increasingly studied and managed.
One critique here, is that model outputs, i.e. line items, often incorporate “unrealistic implicit assumptions” and “internal inconsistencies”. (For example, a forecast for growth in revenue but without corresponding increases in working capital, fixed assets and the associated financing, may imbed unrealistic assumptions about asset turnover, leverage and / or equity financing.) What is required, but often lacking, is that all key elements are explicitly and consistently forecasted. Related to this, is that modellers often additionally ""fail to identify crucial assumptions"" relating to inputs, ""and to explore what can go wrong"". Here, in general, modellers ""use point values and simple arithmetic instead of probability distributions and statistical measures"" — i.e., as mentioned, the problems are treated as deterministic in nature — and thus calculate a single value for the asset or project, but without providing information on the range, variance and sensitivity of outcomes. Other critiques discuss the lack of basic computer programming concepts. More serious criticism, in fact, relates to the nature of budgeting itself, and its impact on the organization.
The Financial Modeling World Championships, known as ModelOff, have been held since 2012. ModelOff is a global online financial modeling competition which culminates in a Live Finals Event for top competitors. From 2012-2014 the Live Finals were held in New York City and in 2015, in London.


== Quantitative finance ==
In quantitative finance, financial modeling entails the development of a sophisticated mathematical model. Models here deal with asset prices, market movements, portfolio returns and the like. A general distinction is between: ""quantitative financial management"", models of the financial situation of a large, complex firm; ""quantitative asset pricing"", models of the returns of different stocks; ""financial engineering"", models of the price or returns of derivative securities; ""quantitative corporate finance"", models of the firm's financial decisions.
Relatedly, applications include:
Option pricing and calculation of their ""Greeks""
Other derivatives, especially interest rate derivatives, credit derivatives and exotic derivatives
Modeling the term structure of interest rates (Bootstrapping, short rate modelling, building ""curve sets"") and credit spreads
Credit scoring and provisioning
Corporate financing activity prediction problems
Portfolio optimization.
Real options
Risk modeling (Financial risk modeling) and value at risk
Dynamic financial analysis (DFA)
Credit valuation adjustment, CVA, as well as the various XVA
These problems are generally stochastic and continuous in nature, and models here thus require complex algorithms, entailing computer simulation, advanced numerical methods (such as numerical differential equations, numerical linear algebra, dynamic programming) and/or the development of optimization models. The general nature of these problems is discussed under Mathematical finance, while specific techniques are listed under Outline of finance# Mathematical tools. For further discussion here see also: Financial models with long-tailed distributions and volatility clustering; Brownian model of financial markets; Martingale pricing; Extreme value theory; Historical simulation (finance).
Modellers are generally referred to as ""quants"" (quantitative analysts), and typically have advanced (Ph.D. level) backgrounds in quantitative disciplines such as physics, engineering, computer science, mathematics or operations research. Alternatively, or in addition to their quantitative background, they complete a finance masters with a quantitative orientation, such as the Master of Quantitative Finance, or the more specialized Master of Computational Finance or Master of Financial Engineering; the CQF is increasingly common.
Although spreadsheets are widely used here also (almost always requiring extensive VBA), custom C++, Fortran or Python, or numerical analysis software such as MATLAB, are often preferred, particularly where stability or speed is a concern. MATLAB is often used at the research or prototyping stage because of its intuitive programming, graphical and debugging tools, but C++/Fortran are preferred for conceptually simple but high computational-cost applications where MATLAB is too slow; Python is increasingly used due to its simplicity and large standard library. Additionally, for many (of the standard) derivative and portfolio applications, commercial software is available, and the choice as to whether the model is to be developed in-house, or whether existing products are to be deployed, will depend on the problem in question.
The complexity of these models may result in incorrect pricing or hedging or both. This Model risk is the subject of ongoing research by finance academics, and is a topic of great, and growing, interest in the risk management arena.
Criticism of the discipline (often preceding the financial crisis of 2007–08 by several years) emphasizes the differences between the mathematical and physical sciences, and finance, and the resultant caution to be applied by modelers, and by traders and risk managers using their models. Notable here are Emanuel Derman and Paul Wilmott, authors of the Financial Modelers' Manifesto. Some go further and question whether mathematical- and statistical modeling may be applied to finance at all, at least with the assumptions usually made (for options; for portfolios). In fact, these may go so far as to question the ""empirical and scientific validity... of modern financial theory"". Notable here are Nassim Taleb and Benoit Mandelbrot. See also Mathematical finance #Criticism and Financial economics #Challenges and criticism.


== See also ==


== References ==


== Bibliography =="
33,Fat object,42247504,28269,"In geometry, a fat object is an object in two or more dimensions, whose lengths in the different dimensions are similar. For example, a square is fat because its length and width are identical. A 2-by-1 rectangle is thinner than a square, but it is fat relative to a 10-by-1 rectangle. Similarly, a circle is fatter than a 1-by-10 ellipse and an equilateral triangle is fatter than a very obtuse triangle.
Fat objects are especially important in computational geometry. Many algorithms in computational geometry can perform much better if their input consists of only fat objects; see the applications section below.


== Global fatness ==

Given a constant R≥1, an object o is called R-fat if its ""slimness factor"" is at most R. The ""slimness factor"" has different definitions in different papers. A common definition is:

  
    
      
        
          
            
              
                side of smallest cube enclosing
              
               
              o
            
            
              
                side of largest cube enclosed in
              
               
              o
            
          
        
      
    
    {\displaystyle {\frac {{\text{side of smallest cube enclosing}}\ o}{{\text{side of largest cube enclosed in}}\ o}}}
  
where o and the cubes are d-dimensional. A 2-dimensional cube is a square, so the slimness factor of a square is 1 (since its smallest enclosing square is the same as its largest enclosed disk). The slimness factor of a 10-by-1 rectangle is 10. The slimness factor of a circle is √2. Hence, by this definition, a square is 1-fat but a disk and a 10×1 rectangle are not 1-fat. A square is also 2-fat (since its slimness factor is less than 2), 3-fat, etc. A disk is also 2-fat (and also 3-fat etc.), but a 10×1 rectangle is not 2-fat. Every shape is ∞-fat, since by definition the slimness factor is always at most ∞.
The above definition can be termed two-cubes fatness since it is based on the ratio between the side-lengths of two cubes. Similarly, it is possible to define two-balls fatness, in which a d-dimensional ball is used instead. A 2-dimensional ball is a disk. According to this alternative definition, a disk is 1-fat but a square is not 1-fat, since its two-balls-slimness is √2.
An alternative definition, that can be termed enclosing-ball fatness (also called ""thickness"") is based on the following slimness factor:

  
    
      
        
          
            (
            
              
                
                  
                    volume of smallest ball enclosing
                  
                   
                  o
                
                
                  
                    volume of
                  
                   
                  o
                
              
            
            )
          
          
            1
            
              /
            
            d
          
        
      
    
    {\displaystyle \left({\frac {{\text{volume of smallest ball enclosing}}\ o}{{\text{volume of}}\ o}}\right)^{1/d}}
  
The exponent 1/d makes this definition a ratio of two lengths, so that it is comparable to the two-balls-fatness.
Here, too, a cube can be used instead of a ball.
Similarly it is possible to define the enclosed-ball fatness based on the following slimness factor:

  
    
      
        
          
            (
            
              
                
                  
                    volume of
                  
                   
                  o
                
                
                  
                    volume of largest  ball enclosed in
                  
                   
                  o
                
              
            
            )
          
          
            1
            
              /
            
            d
          
        
      
    
    {\displaystyle \left({\frac {{\text{volume of}}\ o}{{\text{volume of largest  ball enclosed in}}\ o}}\right)^{1/d}}
  


=== Enclosing-fatness vs. enclosed-fatness ===
The enclosing-ball/cube-slimness might be very different from the enclosed-ball/cube-slimness.
For example, consider a lollipop with a candy in the shape of a 1×1 square and a stick in the shape of a b×(1/b) rectangle (with b>1>(1/b)). As b increases, the area of the enclosing cube (≈b2) increases, but the area of the enclosed cube remains constant (=1) and the total area of the shape also remains constant (=2). Thus the enclosing-cube-slimness can grow arbitrarily while the enclosed-cube-slimness remains constant (=√2). See this GeoGebra page for a demonstration.
On the other hand, consider a rectilinear 'snake' with width 1/b and length b, that is entirely folded within a square of side length 1. As b increases, the area of the enclosed cube(≈1/b2) decreases, but the total areas of the snake and of the enclosing cube remain constant (=1). Thus the enclosed-cube-slimness can grow arbitrarily while the enclosing-cube-slimness remains constant (=1).
With both the lollipop and the snake, the two-cubes-slimness grows arbitrarily, since in general:

enclosing-ball-slimness ⋅ enclosed-ball-slimness = two-balls-slimness
enclosing-cube-slimness ⋅ enclosed-cube-slimness = two-cubes-slimness

Since all slimness factor are at least 1, it follows that if an object o is R-fat according to the two-balls/cubes definition, it is also R-fat according to the enclosing-ball/cube and enclosed-ball/cube definitions (but the opposite is not true, as exemplified above).


=== Balls vs. cubes ===
The volume of a d-dimensional ball of radius r is: 
  
    
      
        
          V
          
            d
          
        
        ⋅
        
          r
          
            d
          
        
      
    
    {\displaystyle V_{d}\cdot r^{d}}
  , where Vd is a dimension-dependent constant:

  
    
      
        
          V
          
            d
          
        
        =
        
          
            
              π
              
                d
                
                  /
                
                2
              
            
            
              Γ
              (
              
                
                  d
                  2
                
              
              +
              1
              )
            
          
        
      
    
    {\displaystyle V_{d}={\frac {\pi ^{d/2}}{\Gamma ({\frac {d}{2}}+1)}}}
  
A d-dimensional cube with side-length 2a has volume (2a)d. It is enclosed in a d-dimensional ball with radius a√d whose volume is Vd(a√d)d. Hence for every d-dimensional object:

enclosing-ball-slimness ≤ enclosing-cube-slimness ⋅ 
  
    
      
        
          
            
              V
              
                d
              
            
          
          
            1
            
              /
            
            d
          
        
        ⋅
        
          
            d
          
        
        
          /
        
        2
      
    
    {\displaystyle {V_{d}}^{1/d}\cdot {\sqrt {d}}/2}
  .

For even dimensions (d=2k), the factor simplifies to: 
  
    
      
        
          
            0.5
            π
            k
          
        
        
          /
        
        
          
            
              (
              k
              !
              )
            
            
              1
              
                /
              
              2
              k
            
          
        
      
    
    {\displaystyle {\sqrt {0.5\pi k}}/{{(k!)}^{1/2k}}}
  . In particular, for two-dimensional shapes V2=π and the factor is: √(0.5 π)≈1.25, so:

enclosing-disk-slimness ≤ enclosing-square-slimness ⋅ 1.25

From similar considerations:

enclosed-cube-slimness ≤ enclosed-ball-slimness ⋅ 
  
    
      
        
          
            
              V
              
                d
              
            
          
          
            1
            
              /
            
            d
          
        
        ⋅
        
          
            d
          
        
        
          /
        
        2
      
    
    {\displaystyle {V_{d}}^{1/d}\cdot {\sqrt {d}}/2}
  
enclosed-square-slimness ≤ enclosed-disk-slimness ⋅ 1.25

A d-dimensional ball with radius a is enclosed in a d-dimensional cube with side-length 2a. Hence for every d-dimensional object:

enclosing-cube-slimness ≤ enclosing-ball-slimness ⋅ 
  
    
      
        2
        
          /
        
        
          
            
              V
              
                d
              
            
          
          
            1
            
              /
            
            d
          
        
      
    
    {\displaystyle 2/{V_{d}}^{1/d}}
  

For even dimensions (d=2k), the factor simplifies to: 
  
    
      
        2
        
          /
        
        
          
            (
            k
            !
            )
          
          
            1
            
              /
            
            2
            k
          
        
        
          /
        
        
          
            π
          
        
      
    
    {\displaystyle 2/{(k!)}^{1/2k}/{\sqrt {\pi }}}
  . In particular, for two-dimensional shapes the factor is: 2/√π≈1.13, so:

enclosing-square-slimness ≤ enclosing-disk-slimness ⋅ 1.13

From similar considerations:

enclosed-ball-slimness ≤ enclosed-cube-slimness ⋅ 
  
    
      
        2
        
          /
        
        
          
            
              V
              
                d
              
            
          
          
            1
            
              /
            
            d
          
        
      
    
    {\displaystyle 2/{V_{d}}^{1/d}}
  
enclosed-disk-slimness ≤ enclosed-square-slimness ⋅ 1.13

Multiplying the above relations gives the following simple relations:

two-balls-slimness ≤ two-cubes-slimness ⋅ √d
two-cubes-slimness ≤ two-balls-slimness ⋅ √d

Thus, an R-fat object according to the either the two-balls or the two-cubes definition is at most R√d-fat according to the alternative definition.


== Local fatness ==
The above definitions are all global in the sense that they don't care about small thin areas that are part of a large fat object.
For example, consider a lollipop with a candy in the shape of a 1×1 square and a stick in the shape of a 1×(1/b) rectangle (with b>1>(1/b)). As b increases, the area of the enclosing cube (=4) and the area of the enclosed cube (=1) remain constant, while the total area of the shape changes only slightly (=1+1/b). Thus all three slimness factors are bounded: enclosing-cube-slimness≤2, enclosed-cube-slimness≤2, two-cube-slimness=2. Thus by all definitions the lollipop is 2-fat. However, the stick-part of the lollipop obviously becomes thinner and thinner.
In some applications, such thin parts are unacceptable, so local fatness, based on a local slimness factor, may be more appropriate. For every global slimness factor, it is possible to define a local version. For example, for the enclosing-ball-slimness, it is possible to define the local-enclosing-ball slimness factor of an object o by considering the set B of all balls whose center is inside o and whose boundary intersects the boundary of o (i.e. not entirely containing o). The local-enclosing-ball-slimness factor is defined as:

  
    
      
        
          
            1
            2
          
        
        ⋅
        
          sup
          
            b
            ∈
            B
          
        
        
          
            (
            
              
                
                  
                    volume of
                  
                   
                  B
                
                
                  
                    volume of
                  
                   
                  B
                  ∩
                  o
                
              
            
            )
          
          
            1
            
              /
            
            d
          
        
      
    
    {\displaystyle {\frac {1}{2}}\cdot \sup _{b\in B}\left({\frac {{\text{volume of}}\ B}{{\text{volume of}}\ B\cap o}}\right)^{1/d}}
  
The 1/2 is a normalization factor that makes the local-enclosing-ball-slimness of a ball equal to 1. The local-enclosing-ball-slimness of the lollipop-shape described above is dominated by the 1×(1/b) stick, and it goes to ∞ as b grows. Thus by the local definition the above lollipop is not 2-fat.


=== Global vs. local definitions ===
Local-fatness implies global-fatness. Here is a proof sketch for fatness based on enclosing balls. By definition, the volume of the smallest enclosing ball is ≤ the volume of any other enclosing ball. In particular, it is ≤ the volume of any enclosing ball whose center is inside o and whose boundary touches the boundary of o. But every such enclosing ball is in the set B considered by the definition of local-enclosing-ball slimness. Hence:

enclosing-ball-slimnessd =
= volume(smallest-enclosing-ball)/volume(o)
≤ volume(enclosing-ball-b-in-B)/volume(o)
= volume(enclosing-ball-b-in-B)/volume(b ∩ o)
≤ (2 local-enclosing-ball-slimness)d

Hence:

enclosing-ball-slimness ≤ 2⋅local-enclosing-ball-slimness

For a convex body, the opposite is also true: local-fatness implies global-fatness. The proof is based on the following lemma. Let o be a convex object. Let P be a point in o. Let b and B be two balls centered at P such that b is smaller than B. Then o intersects a larger portion of b than of B, i.e.:

  
    
      
        
          
            
              
                volume
              
               
              (
              b
              ∩
              o
              )
            
            
              
                volume
              
               
              (
              b
              )
            
          
        
        ≥
        
          
            
              
                volume
              
               
              (
              B
              ∩
              o
              )
            
            
              
                volume
              
               
              (
              B
              )
            
          
        
      
    
    {\displaystyle {\frac {{\text{volume}}\ (b\cap o)}{{\text{volume}}\ (b)}}\geq {\frac {{\text{volume}}\ (B\cap o)}{{\text{volume}}\ (B)}}}
  

Proof sketch: standing at the point P, we can look at different angles θ and measure the distance to the boundary of o. Because o is convex, this distance is a function, say r(θ). We can calculate the left-hand side of the inequality by integrating the following function (multiplied by some determinant function) over all angles:

  
    
      
        f
        (
        θ
        )
        =
        min
        
          (
          
            
              
                r
                (
                θ
                )
              
              
                
                  radius
                
                 
                (
                b
                )
              
            
          
          ,
          1
          )
        
      
    
    {\displaystyle f(\theta )=\min {({\frac {r(\theta )}{{\text{radius}}\ (b)}},1)}}
  

Similarly we can calculate the right-hand side of the inequality by integrating the following function:

  
    
      
        F
        (
        θ
        )
        =
        min
        
          (
          
            
              
                r
                (
                θ
                )
              
              
                
                  radius
                
                 
                (
                B
                )
              
            
          
          ,
          1
          )
        
      
    
    {\displaystyle F(\theta )=\min {({\frac {r(\theta )}{{\text{radius}}\ (B)}},1)}}
  

By checking all 3 possible cases, it is possible to show that always 
  
    
      
        f
        (
        θ
        )
        ≥
        F
        (
        θ
        )
      
    
    {\displaystyle f(\theta )\geq F(\theta )}
  . Thus the integral of f is at least the integral of F, and the lemma follows.
The definition of local-enclosing-ball slimness considers all balls that are centered in a point in o and intersect the boundary of o. However, when o is convex, the above lemma allows us to consider, for each point in o, only balls that are maximal in size, i.e., only balls that entirely contain o (and whose boundary intersects the boundary of o). For every such ball b:

  
    
      
        
          volume
        
         
        (
        b
        )
        ≤
        
          C
          
            d
          
        
        ⋅
        
          diameter
        
         
        (
        o
        
          )
          
            d
          
        
      
    
    {\displaystyle {\text{volume}}\ (b)\leq C_{d}\cdot {\text{diameter}}\ (o)^{d}}
  

where 
  
    
      
        
          C
          
            d
          
        
      
    
    {\displaystyle C_{d}}
   is some dimension-dependent constant.
The diameter of o is at most the diameter of the smallest ball enclosing o, and the volume of that ball is: 
  
    
      
        
          C
          
            d
          
        
        ⋅
        (
        
          diameter(smallest ball enclosing
        
         
        o
        )
        
          /
        
        2
        
          )
          
            d
          
        
      
    
    {\displaystyle C_{d}\cdot ({\text{diameter(smallest ball enclosing}}\ o)/2)^{d}}
  . Combining all inequalities gives that for every convex object:

local-enclosing-ball-slimness ≤ enclosing-ball-slimness

For non-convex objects, this inequality of course doesn't hold, as exemplified by the lollipop above.


== Examples ==
The following table shows the slimness factor of various shapes based on the different definitions. The two columns of the local definitions are filled with ""*"" when the shape is convex (in this case, the value of the local slimness equals the value of the corresponding global slimness):


== Fatness of a triangle ==
Slimness is invariant to scale, so the slimness factor of a triangle (as of any other polygon) can be presented as a function of its angles only. The three ball-based slimness factors can be calculated using well-known trigonometric identities.


=== Enclosed-ball slimness ===
The largest circle contained in a triangle is called its incircle. It is known that:

  
    
      
        Δ
        =
        
          r
          
            2
          
        
        ⋅
        (
        cot
        ⁡
        
          
            
              ∠
              A
            
            2
          
        
        +
        cot
        ⁡
        
          
            
              ∠
              B
            
            2
          
        
        +
        cot
        ⁡
        
          
            
              ∠
              C
            
            2
          
        
        )
      
    
    {\displaystyle \Delta =r^{2}\cdot (\cot {\frac {\angle A}{2}}+\cot {\frac {\angle B}{2}}+\cot {\frac {\angle C}{2}})}
  
where Δ is the area of a triangle and r is the radius of the incircle. Hence, the enclosed-ball slimness of a triangle is:

  
    
      
        
          
            
              
                cot
                ⁡
                
                  
                    
                      ∠
                      A
                    
                    2
                  
                
                +
                cot
                ⁡
                
                  
                    
                      ∠
                      B
                    
                    2
                  
                
                +
                cot
                ⁡
                
                  
                    
                      ∠
                      C
                    
                    2
                  
                
              
              π
            
          
        
      
    
    {\displaystyle {\sqrt {\frac {\cot {\frac {\angle A}{2}}+\cot {\frac {\angle B}{2}}+\cot {\frac {\angle C}{2}}}{\pi }}}}
  


=== Enclosing-ball slimness ===
The smallest containing circle for an acute triangle is its circumcircle, while for an obtuse triangle it is the circle having the triangle's longest side as a diameter.
It is known that:

  
    
      
        Δ
        =
        
          R
          
            2
          
        
        ⋅
        2
        sin
        ⁡
        A
        sin
        ⁡
        B
        sin
        ⁡
        C
      
    
    {\displaystyle \Delta =R^{2}\cdot 2\sin A\sin B\sin C}
  
where again Δ is the area of a triangle and R is the radius of the circumcircle. Hence, for an acute triangle, the enclosing-ball slimness factor is:

  
    
      
        
          
            
              π
              
                2
                sin
                ⁡
                A
                sin
                ⁡
                B
                sin
                ⁡
                C
              
            
          
        
      
    
    {\displaystyle {\sqrt {\frac {\pi }{2\sin A\sin B\sin C}}}}
  
It is also known that:

  
    
      
        Δ
        =
        
          
            
              c
              
                2
              
            
            
              2
              (
              cot
              ⁡
              ∠
              
                A
              
              +
              cot
              ⁡
              ∠
              
                B
              
              )
            
          
        
        =
        
          
            
              
                c
                
                  2
                
              
              (
              sin
              ⁡
              ∠
              
                A
              
              )
              (
              sin
              ⁡
              ∠
              
                B
              
              )
            
            
              2
              sin
              ⁡
              (
              ∠
              
                A
              
              +
              ∠
              
                B
              
              )
            
          
        
      
    
    {\displaystyle \Delta ={\frac {c^{2}}{2(\cot \angle {A}+\cot \angle {B})}}={\frac {c^{2}(\sin \angle {A})(\sin \angle {B})}{2\sin(\angle {A}+\angle {B})}}}
  
where c is any side of the triangle and A,B are the adjacent angles. Hence, for an obtuse triangle with acute angles A and B (and longest side c), the enclosing-ball slimness factor is:

  
    
      
        
          
            
              
                π
                ⋅
                (
                cot
                ⁡
                ∠
                
                  A
                
                +
                cot
                ⁡
                ∠
                
                  B
                
                )
              
              2
            
          
        
        =
        
          
            
              
                π
                ⋅
                sin
                ⁡
                (
                ∠
                
                  A
                
                +
                ∠
                
                  B
                
                )
              
              
                2
                (
                sin
                ⁡
                ∠
                
                  A
                
                )
                (
                sin
                ⁡
                ∠
                
                  B
                
                )
              
            
          
        
      
    
    {\displaystyle {\sqrt {\frac {\pi \cdot (\cot \angle {A}+\cot \angle {B})}{2}}}={\sqrt {\frac {\pi \cdot \sin(\angle {A}+\angle {B})}{2(\sin \angle {A})(\sin \angle {B})}}}}
  
Note that in a right triangle, 
  
    
      
        sin
        ⁡
        
          ∠
          
            C
          
        
        =
        sin
        ⁡
        
          ∠
          
            A
          
          +
          ∠
          
            B
          
        
        =
        1
      
    
    {\displaystyle \sin {\angle {C}}=\sin {\angle {A}+\angle {B}}=1}
  , so the two expressions coincide.


=== Two-balls slimness ===
The inradius r and the circumradius R are connected via a couple of formulae which provide two alternative expressions for the two-balls slimness of an acute triangle:

  
    
      
        
          
            R
            r
          
        
        =
        
          
            1
            
              4
              sin
              ⁡
              (
              
                
                  
                    ∠
                    
                      A
                    
                  
                  2
                
              
              )
              sin
              ⁡
              (
              
                
                  
                    ∠
                    
                      B
                    
                  
                  2
                
              
              )
              sin
              ⁡
              (
              
                
                  
                    ∠
                    
                      C
                    
                  
                  2
                
              
              )
            
          
        
        =
        
          
            1
            
              cos
              ⁡
              ∠
              
                A
              
              +
              cos
              ⁡
              ∠
              
                B
              
              +
              cos
              ⁡
              ∠
              
                C
              
              −
              1
            
          
        
      
    
    {\displaystyle {\frac {R}{r}}={\frac {1}{4\sin({\frac {\angle {A}}{2}})\sin({\frac {\angle {B}}{2}})\sin({\frac {\angle {C}}{2}})}}={\frac {1}{\cos \angle {A}+\cos \angle {B}+\cos \angle {C}-1}}}
  
For an obtuse triangle, c/2 should be used instead of R. By the Law of sines:

  
    
      
        
          
            c
            2
          
        
        =
        R
        sin
        ⁡
        
          ∠
          
            C
          
        
      
    
    {\displaystyle {\frac {c}{2}}=R\sin {\angle {C}}}
  
Hence the slimness factor of an obtuse triangle with obtuse angle C is:

  
    
      
        
          
            
              c
              
                /
              
              2
            
            r
          
        
        =
        
          
            
              sin
              ⁡
              
                ∠
                
                  C
                
              
            
            
              4
              sin
              ⁡
              (
              
                
                  
                    ∠
                    
                      A
                    
                  
                  2
                
              
              )
              sin
              ⁡
              (
              
                
                  
                    ∠
                    
                      B
                    
                  
                  2
                
              
              )
              sin
              ⁡
              (
              
                
                  
                    ∠
                    
                      C
                    
                  
                  2
                
              
              )
            
          
        
        =
        
          
            
              sin
              ⁡
              
                ∠
                
                  C
                
              
            
            
              cos
              ⁡
              ∠
              
                A
              
              +
              cos
              ⁡
              ∠
              
                B
              
              +
              cos
              ⁡
              ∠
              
                C
              
              −
              1
            
          
        
      
    
    {\displaystyle {\frac {c/2}{r}}={\frac {\sin {\angle {C}}}{4\sin({\frac {\angle {A}}{2}})\sin({\frac {\angle {B}}{2}})\sin({\frac {\angle {C}}{2}})}}={\frac {\sin {\angle {C}}}{\cos \angle {A}+\cos \angle {B}+\cos \angle {C}-1}}}
  
Note that in a right triangle, 
  
    
      
        sin
        ⁡
        
          ∠
          
            C
          
        
        =
        1
      
    
    {\displaystyle \sin {\angle {C}}=1}
  , so the two expressions coincide.
The two expressions can be combined in the following way to get a single expression for the two-balls slimness of any triangle with smaller angles A and B:

  
    
      
        
          
            
              sin
              ⁡
              
                max
                (
                ∠
                
                  A
                
                ,
                ∠
                
                  B
                
                ,
                ∠
                
                  C
                
                ,
                π
                
                  /
                
                2
                )
              
            
            
              4
              sin
              ⁡
              (
              
                
                  
                    ∠
                    
                      A
                    
                  
                  2
                
              
              )
              sin
              ⁡
              (
              
                
                  
                    ∠
                    
                      B
                    
                  
                  2
                
              
              )
              sin
              ⁡
              (
              
                
                  
                    π
                    −
                    ∠
                    
                      A
                    
                    −
                    ∠
                    
                      B
                    
                  
                  2
                
              
              )
            
          
        
        =
        
          
            
              sin
              ⁡
              
                max
                (
                ∠
                
                  A
                
                ,
                ∠
                
                  B
                
                ,
                ∠
                
                  C
                
                ,
                π
                
                  /
                
                2
                )
              
            
            
              cos
              ⁡
              ∠
              
                A
              
              +
              cos
              ⁡
              ∠
              
                B
              
              −
              cos
              ⁡
              (
              ∠
              
                A
              
              +
              ∠
              
                B
              
              )
              −
              1
            
          
        
      
    
    {\displaystyle {\frac {\sin {\max(\angle {A},\angle {B},\angle {C},\pi /2)}}{4\sin({\frac {\angle {A}}{2}})\sin({\frac {\angle {B}}{2}})\sin({\frac {\pi -\angle {A}-\angle {B}}{2}})}}={\frac {\sin {\max(\angle {A},\angle {B},\angle {C},\pi /2)}}{\cos \angle {A}+\cos \angle {B}-\cos(\angle {A}+\angle {B})-1}}}
  
To get a feeling of the rate of change in fatness, consider what this formula gives for an isosceles triangle with head angle θ when θ is small:

  
    
      
        
          
            
              sin
              ⁡
              
                max
                (
                θ
                ,
                π
                
                  /
                
                2
                )
              
            
            
              4
              
                sin
                
                  2
                
              
              ⁡
              (
              
                
                  
                    π
                    −
                    θ
                  
                  4
                
              
              )
              sin
              ⁡
              (
              
                
                  θ
                  2
                
              
              )
            
          
        
        ≈
        
          
            1
            
              4
              
                
                  
                    1
                    
                      /
                    
                    2
                  
                
                
                  2
                
              
              θ
              
                /
              
              2
            
          
        
        =
        
          
            1
            θ
          
        
      
    
    {\displaystyle {\frac {\sin {\max(\theta ,\pi /2)}}{4\sin ^{2}({\frac {\pi -\theta }{4}})\sin({\frac {\theta }{2}})}}\approx {\frac {1}{4{\sqrt {1/2}}^{2}\theta /2}}={\frac {1}{\theta }}}
  

The following graphs show the 2-balls slimness factor of a triangle:
Slimness of a general triangle when one angle (a) is a constant parameter while the other angle (x) changes.
Slimness of an isosceles triangle as a function of its head angle (x).


== Fatness of circles, ellipses and their parts ==
The ball-based slimness of a circle is of course 1 - the smallest possible value.

For a circular segment with central angle θ, the circumcircle diameter is the length of the chord and the incircle diameter is the height of the segment, so the two-balls slimness (and its approximation when θ is small) is:

  
    
      
        
          
            length of chord
            height of segment
          
        
        =
        
          
            
              2
              R
              sin
              ⁡
              
                
                  θ
                  2
                
              
            
            
              R
              
                (
                
                  1
                  −
                  cos
                  ⁡
                  
                    
                      θ
                      2
                    
                  
                
                )
              
            
          
        
        =
        
          
            
              2
              sin
              ⁡
              
                
                  θ
                  2
                
              
            
            
              (
              
                1
                −
                cos
                ⁡
                
                  
                    θ
                    2
                  
                
              
              )
            
          
        
        ≈
        
          
            θ
            
              
                θ
                
                  2
                
              
              
                /
              
              8
            
          
        
        =
        
          
            8
            θ
          
        
      
    
    {\displaystyle {\frac {\text{length of chord}}{\text{height of segment}}}={\frac {2R\sin {\frac {\theta }{2}}}{R\left(1-\cos {\frac {\theta }{2}}\right)}}={\frac {2\sin {\frac {\theta }{2}}}{\left(1-\cos {\frac {\theta }{2}}\right)}}\approx {\frac {\theta }{\theta ^{2}/8}}={\frac {8}{\theta }}}
  

For a circular sector with central angle θ (when θ is small), the circumcircle diameter is the radius of the circle and the incircle diameter is the chord length, so the two-balls slimness is:

  
    
      
        
          
            radius of circle
            length of chord
          
        
        =
        
          
            R
            
              2
              R
              sin
              ⁡
              
                
                  θ
                  2
                
              
            
          
        
        =
        
          
            1
            
              2
              sin
              ⁡
              
                
                  θ
                  2
                
              
            
          
        
        ≈
        
          
            1
            
              2
              θ
              
                /
              
              2
            
          
        
        =
        
          
            1
            θ
          
        
      
    
    {\displaystyle {\frac {\text{radius of circle}}{\text{length of chord}}}={\frac {R}{2R\sin {\frac {\theta }{2}}}}={\frac {1}{2\sin {\frac {\theta }{2}}}}\approx {\frac {1}{2\theta /2}}={\frac {1}{\theta }}}
  
For an ellipse, the slimness factors are different in different locations. For example, consider an ellipse with short axis a and long axis b. the length of a chord ranges between 
  
    
      
        2
        a
        sin
        ⁡
        
          
            θ
            2
          
        
      
    
    {\displaystyle 2a\sin {\frac {\theta }{2}}}
   at the narrow side of the ellipse and 
  
    
      
        2
        b
        sin
        ⁡
        
          
            θ
            2
          
        
      
    
    {\displaystyle 2b\sin {\frac {\theta }{2}}}
   at its wide side; similarly, the height of the segment ranges between 
  
    
      
        b
        
          (
          
            1
            −
            cos
            ⁡
            
              
                θ
                2
              
            
          
          )
        
      
    
    {\displaystyle b\left(1-\cos {\frac {\theta }{2}}\right)}
   at the narrow side and 
  
    
      
        a
        
          (
          
            1
            −
            cos
            ⁡
            
              
                θ
                2
              
            
          
          )
        
      
    
    {\displaystyle a\left(1-\cos {\frac {\theta }{2}}\right)}
   at its wide side. So the two-balls slimness ranges between:

  
    
      
        
          
            
              2
              a
              sin
              ⁡
              
                
                  θ
                  2
                
              
            
            
              b
              
                (
                
                  1
                  −
                  cos
                  ⁡
                  
                    
                      θ
                      2
                    
                  
                
                )
              
            
          
        
        ≈
        
          
            
              8
              a
            
            
              b
              θ
            
          
        
      
    
    {\displaystyle {\frac {2a\sin {\frac {\theta }{2}}}{b\left(1-\cos {\frac {\theta }{2}}\right)}}\approx {\frac {8a}{b\theta }}}
  
and:

  
    
      
        
          
            
              2
              b
              sin
              ⁡
              
                
                  θ
                  2
                
              
            
            
              a
              
                (
                
                  1
                  −
                  cos
                  ⁡
                  
                    
                      θ
                      2
                    
                  
                
                )
              
            
          
        
        ≈
        
          
            
              8
              b
            
            
              a
              θ
            
          
        
      
    
    {\displaystyle {\frac {2b\sin {\frac {\theta }{2}}}{a\left(1-\cos {\frac {\theta }{2}}\right)}}\approx {\frac {8b}{a\theta }}}
  
In general, when the secant starts at angle Θ the slimness factor can be approximated by:

  
    
      
        
          
            
              2
              sin
              ⁡
              
                
                  θ
                  2
                
              
            
            
              (
              
                1
                −
                cos
                ⁡
                
                  
                    θ
                    2
                  
                
              
              )
            
          
        
      
    
    {\displaystyle {\frac {2\sin {\frac {\theta }{2}}}{\left(1-\cos {\frac {\theta }{2}}\right)}}}
   
  
    
      
        
          (
          
            
              
                b
                a
              
            
            
              cos
              
                2
              
            
            ⁡
            (
            Θ
            +
            
              
                θ
                2
              
            
            )
            +
            
              
                a
                b
              
            
            
              sin
              
                2
              
            
            ⁡
            (
            Θ
            +
            
              
                θ
                2
              
            
            )
          
          )
        
      
    
    {\displaystyle \left({\frac {b}{a}}\cos ^{2}(\Theta +{\frac {\theta }{2}})+{\frac {a}{b}}\sin ^{2}(\Theta +{\frac {\theta }{2}})\right)}
  


== Fatness of a convex polygon ==
A convex polygon is called r-separated if the angle between each pair of edges (not necessarily adjacent) is at least r.
Lemma: The enclosing-ball-slimness of an r-separated convex polygon is at most 
  
    
      
        O
        (
        1
        
          /
        
        r
        )
      
    
    {\displaystyle O(1/r)}
  .
A convex polygon is called k,r-separated if:
It does not have parallel edges, except maybe two horizontal and two vertical.
Each non-axis-parallel edge makes an angle of at least r with any other edge, and with the x and y axes.
If there are two horizontal edges, then diameter/height is at most k.
If there are two vertical edges, then diameter/width is at most k.
Lemma: The enclosing-ball-slimness of a k,r-separated convex polygon is at most 
  
    
      
        O
        (
        max
        (
        k
        ,
        1
        
          /
        
        r
        )
        )
      
    
    {\displaystyle O(\max(k,1/r))}
  . improve the upper bound to 
  
    
      
        O
        (
        d
        )
      
    
    {\displaystyle O(d)}
  .


== Counting fat objects ==
If an object o has diameter 2a, then every ball enclosing o must have radius at least a and volume at least Vdad. Hence, by definition of enclosing-ball-fatness, the volume of an R-fat object with diameter 2a must be at least: Vdad/Rd. Hence:
Lemma 1: Let R≥1 and C≥0 be two constants. Consider a collection of non-overlapping d-dimensional objects that are all globally R-fat (i.e. with enclosing-ball-slimness ≤ R). The number of such objects of diameter at least 2a, contained in a ball of radius C⋅a, is at most:

  
    
      
        
          V
          
            d
          
        
        ⋅
        (
        C
        a
        
          )
          
            d
          
        
        
          /
        
        (
        
          V
          
            d
          
        
        ⋅
        
          a
          
            d
          
        
        
          /
        
        
          R
          
            d
          
        
        )
        =
        (
        R
        C
        
          )
          
            d
          
        
      
    
    {\displaystyle V_{d}\cdot (Ca)^{d}/(V_{d}\cdot a^{d}/R^{d})=(RC)^{d}}
  

For example (taking d=2, R=1 and C=3): The number of non-overlapping disks with radius at least 1 contained in a circle of radius 3 is at most 32=9. (Actually, it is at most 7).
If we consider local-fatness instead of global-fatness, we can get a stronger lemma:
Lemma 2: Let R≥1 and C≥0 be two constants. Consider a collection of non-overlapping d-dimensional objects that are all locally R-fat (i.e. with local-enclosing-ball-slimness ≤ R). Let o be a single object in that collection with diameter 2a. Then the number of objects in the collection with diameter larger than 2a that lie within distance 2C⋅a from object o is at most:

  
    
      
        (
        4
        R
        ⋅
        (
        C
        +
        1
        )
        
          )
          
            d
          
        
      
    
    {\displaystyle (4R\cdot (C+1))^{d}}
  

For example (taking d=2, R=1 and C=0): the number of non-overlapping disks with radius larger than 1 that touch a given unit disk is at most 42=16 (this is not a tight bound since in this case it is easy to prove an upper bound of 5).


== Generalizations ==
The following generalization of fatness were studied by  for 2-dimensional objects.
A triangle ∆ is a (β, δ)-triangle of a planar object o (0<β≤π/3, 0<δ< 1), if ∆ ⊆ o, each of the angles of ∆ is at least β, and the length of each of its edges is at least δ·diameter(o). An object o in the plane is (β,δ)-covered if for each point P ∈ o there exists a (β, δ)-triangle ∆ of o that contains P.
For convex objects, the two definitions are equivalent, in the sense that if o is α-fat, for some constant α, then it is also (β,δ)-covered, for appropriate constants β and δ, and vice versa. However, for non-convex objects the definition of being fat is more general than the definition of being (β, δ)-covered.


== Applications ==
Fat objects are used in various problems, for example:
Motion planning - planning a path for a robot moving amidst obstacles becomes easier when the obstacles are fat objects.
Fair cake-cutting - dividing a cake becomes more difficult when the pieces have to be fat objects. This requirement is common, for example, when the ""cake"" to be divided is a land-estate.
More applications can be found in the references below.


== References =="
34,Computational law,38358886,28154,"Computational law is a branch of legal informatics concerned with the mechanization of legal reasoning (whether done by humans or by computers). It emphasizes explicit behavioral constraints and eschews implicit rules of conduct. Importantly, there is a commitment to a level of rigor in specifying laws that is sufficient to support entirely mechanical processing.
Philosophically, computational law sits within the Legal Formalist school of jurisprudence. Given its emphasis on rigorously specified laws, computational law is most applicable in civil law settings, where laws are taken more or less literally. It is less applicable to legal systems based on common law, which provides more scope for unspecified normative considerations. However, even in common law systems, computational law still has relevance in the case of categorical statutes and in settings where the handling of cases has resulted in de facto rules.
From a pragmatic perspective, computational law is important as the basis for computer systems capable of doing useful legal calculations, such as compliance checking, legal planning, regulatory analysis, and so forth. Some systems of this sort already exist. TurboTax is a good example. And the potential is particularly significant now due to recent technological advances – including the prevalence of the Internet in human interaction and the proliferation of embedded computer systems (such as smart phones, self-driving cars, and robots).


== History ==
Speculation about potential benefits to legal practice through applying methods from computational science and AI research to automate parts of the law date back at least to the middle 1940s. Further, AI and law and computational law do not seem easily separable, as perhaps most of AI research focusing on the law and its automation appears to utilize computational methods. The forms that speculation took are multiple and not all related in ways to readily show closeness to one another. This history will sketch them as they were, attempting to show relationships where they can be found to have existed.
By 1949, a minor academic field aiming to incorporate electronic and computational methods to legal problems had been founded by American legal scholars, called jurimetrics. Though broadly said to be concerned with the application of the ""methods of science"" to the law, these methods were actually of a quite specifically defined scope. Jurimetrics was to be ""concerned with such matters as the quantitative analysis of judicial behavior, the application of communication and information theory to legal expression, the use of mathematical logic in law, the retrieval of legal data by electronic and mechanical means, and the formulation of a calculus of legal predictability"". These interests led in 1959 to the founding a journal, Modern Uses of Logic in Law, as a forum wherein articles would be published about the applications of techniques such as mathematical logic, engineering, statistics, etc. to the legal study and development. In 1966, this Journal was renamed as Jurimetrics. Today, however, the journal and meaning of jurimetrics seems to have broadened far beyond what would fit under the areas of applications of computers and computational methods to law. Today the journal not only publishes articles on such practices as found in computational law, but has broadened jurimetrical concerns to mean also things like the use of social science in law or the ""policy implications [of] and legislative and administrative control of science"".
Independently in 1958, at the Conference for the Mechanization of Thought held at the National Physical Laboratory in Teddington, Middlesex, UK, the French jurist Lucien Mehl presented a paper both on the benefits of using computational methods for law and on the potential means to use such methods to automate law for a discussion that included AI luminaries like Marvin Minsky. Mehl believed that the law could by automated by two basic distinct, though not wholly separable, types of machine. These were the ""documentary or information machine"", which would provide the legal researcher quick access to relevant case precedents and legal scholarship, and the ""consultation machine"", which would be ""capable of answering any question put to it over a vast field of law"". The latter type of machine would be able to basically do much of a lawyer's job by simply giving the ""exact answer to a [legal] problem put to it"".
By 1970, Mehl's first type of machine, one that would be able to retrieve information, had been accomplished but there seems to have been little consideration of further fruitful intersections between AI and legal research. There were, however, still hopes that computers could model the lawyer's thought processes through computational methods and then apply that capacity to solve legal problems, thus automating and improving legal services via increased efficiency as well as shedding light on the nature of legal reasoning. By the late 1970s, computer science and the affordability of computer technology had progressed enough that the retrieval of ""legal data by electronic and mechanical means"" had been achieved by machines fitting Mehl's first type and were in common use in American law firms. During this time, research focused on improving the goals of the early 1970s occurred, with programs like Taxman being worked on in order to both bring useful computer technology into the law as practical aids and to help specify the exact nature of legal concepts.
Nonetheless, progress on the second type of machine, one that would more fully automate the law, remained relatively inert. Research into machines that could answer questions in the way that Mehl's consultation machine would picked up somewhat in the late 1970s and 1980s. A 1979 convention in Swansea, Wales marked the first international effort solely to focus upon applying artificial intelligence research to legal problems in order to ""consider how computers can be used to discover and apply the legal norms embedded within the written sources of the law"". That said, little substantial progress seems to have been made in the following decade of the 1980s. In a 1988 review of Anne Gardner's book An Artificial Intelligence Approach to Legal Reasoning (1987), the Harvard academic legal scholar and computer scientist Edwina Rissland wrote that ""She plays, in part, the role of pioneer; artificial intelligence (""AI"") techniques have not yet been widely applied to perform legal tasks. Therefore, Gardner, and this review, first describe and define the field, then demonstrate a working model in the domain of contract offer and acceptance."" Eight years after the Swansea conference had passed, and still AI and law researchers merely trying to delineate the field could be described by their own kind as ""pioneer[s]"".
In the 1990s and early 2000s more progress occurred. Computational research generated insights for law. The First International Conference on AI and the Law occurred it 1987, but it is in the 1990s and 2000s that the biannual conference began to build up steam and to delve more deeply into the issues involved with work intersecting computational methods, AI, and law. Classes began to be taught to undergraduates on the uses of computational methods to automating, understanding, and obeying the law. Further, by 2005, a team largely composed of Stanford computer scientists from the Stanford Logic group had devoted themselves to studying the uses of computational techniques to the law. Computational methods in fact advanced enough that members of the legal profession began in the 2000s to both analyze, predict and worry about the potential future of computational law and a new academic field of computational legal studies seems to be now well established. As insight into what such scholars see in the law's future due in part to computational law, here is quote from a recent conference about the ""New Normal"" for the legal profession:
""Over the last 5 years, in the fallout of the Great Recession, the legal profession has entered the era of the New Normal. Notably, a series of forces related to technological change, globalization, and the pressure to do more with less (in both corporate America and law firms) has changed permanently the legal services industry. As one article put it, firms are cutting back on hiring ""in order to increase efficiency, improve profit margins, and reduce client costs."" Indeed, in its recently noted cutbacks, Weil Gotshal's leaders remarked that it had initially expected old work to return, but came ""around to the view that this is the ‘new normal.’""The New Normal provides lawyers with an opportunity to rethink—and reimagine—the role of lawyers in our economy and society. To the extent that law firms enjoyed, or still enjoy, the ability to bundle work together, that era is coming to an end, as clients unbundle legal services and tasks. Moreover, in other cases, automation and technology can change the roles of lawyers, both requiring them to oversee processes and use technology more aggressively as well as doing less of the work that is increasingly managed by computers (think: electronic discovery). The upside is not only greater efficiencies for society, but new possibilities for legal craftsmanship. The emerging craft of lawyering in the New Normal is likely to require lawyers to be both entrepreneurial and fluent with a range of competencies that will enable them to add value for clients. Apropos of the trends noted above, there are emerging opportunities for ""legal entrepreneurs"" in a range of roles from legal process management to developing technologies to manage legal operations (such as overseeing automated processes) to supporting online dispute resolution processes. In other cases, effective legal training as well as domain specific knowledge (finance, sales, IT, entrepreneurship, human resources, etc.) can form a powerful combination that prepares law school grads for a range of opportunities (business development roles, financial operations roles, HR roles, etc.). In both cases, traditional legal skills alone will not be enough to prepare law students for these roles. But the proper training, which builds on the traditional law school curriculum and goes well beyond it including practical skills, relevant domain knowledge (e.g., accounting), and professional skills (e.g., working in teams), will provide law school students a huge advantage over those with a one-dimensional skill set.""
Many see perks to oncoming changes brought about by the computational automation of law. For one thing, legal experts have predicted that it will aid legal self-help, especially in the areas of contract formation, enterprise planning, and the prediction of rule changes. For another thing, those with knowledge about computers see the potential for computational law to really fully bloom as eminent. In this vein, it seems that machines like Mehl's second type may come into existence. Stephen Wolfram has said that:
""So we're slowly moving toward people being educated in the kind of computational paradigm. Which is good, because the way I see it, computation is going to become central to almost every field. Let's talk about two examples—classic professions: law and medicine. It's funny, when Leibniz was first thinking about computation at the end of the 1600s, the thing he wanted to do was to build a machine that would effectively answer legal questions. It was too early then. But now we’re almost ready, I think, for computational law. Where for example contracts become computational. They explicitly become algorithms that decide what's possible and what's not.You know, some pieces of this have already happened. Like with financial derivatives, like options and futures. In the past these used to just be natural language contracts. But then they got codified and parametrized. So they’re really just algorithms, which of course one can do meta-computations on, which is what has launched a thousand hedge funds, and so on. Well, eventually one's going to be able to make computational all sorts of legal things, from mortgages to tax codes to perhaps even patents. Now to actually achieve that, one has to have ways to represent many aspects of the real world, in all its messiness. Which is what the whole knowledge-based computing of Wolfram|Alpha is about.""


== Approaches ==


=== Algorithmic law ===
There have also been many attempts to create a machine readable or machine executable legal code. A machine readable code would simplify the analysis of legal code, allowing the rapid construction and analysis of databases, without the need for advanced text processing techniques. A machine executable format would allow the specifics of a case to be input, and would return the decision based on the case.
Machine readable legal code is already quite common. METAlex, an XML-based standard proposed and developed by the Leibniz Center for Law of the University of Amsterdam, is used by the governments of both the United Kingdom and the Netherlands to encode their laws. In the United States, an executive order issued by President Barack Obama in the May 2013 mandated that all public government documentation be released in a machine readable format by default, although no specific format was mentioned.
Machine executable legal code is much less common. Notable among current efforts is the Hammurabi Project, an attempt to rewrite parts of the United States legal code in such a way that a law can take facts as input and return decisions. The Hammurabi Project currently focuses on the aspects of law that lend themselves to this type of specification, such as tax or immigration laws, although in the long-term the developers of the Hammurabi Project plan to include as many laws as possible.


=== Empirical analysis ===
Many current efforts in computational law are focused on the empirical analysis of legal decisions, and their relation to legislation. These efforts usually make use of citation analysis, which examines patterns in citations between works. Due to the widespread practice of legal citation, it is possible to construct citation indices and large graphs of legal precedent, called citation networks. Citation networks allow the use of graph traversal algorithms in order to relate cases to one another, as well as the use of various distance metrics to find mathematical relationships between them. These analyses can reveal important overarching patterns and trends in judicial proceedings and the way law is used.
There have been several breakthroughs in the analysis of judicial rulings in recent research on legal citation networks. These analyses have made use of citations in Supreme Court majority opinions to build citation networks, and analyzed the patterns in these networks to identify meta-information about individual decisions, such as the importance of the decision, as well as general trends in judicial proceedings, such as the role of precedent over time. These analyses have been used to predict which cases the Supreme Court will choose to consider.
Another effort has examined United States Tax Court decisions, compiling a publicly available database of Tax Court decisions, opinions, and citations between the years of 1990 and 2008, and constructing a citation network from this database. Analysis of this network revealed that large sections of the tax code were rarely, if ever, cited, and that other sections of code, such as those that dealt with ""divorce, dependents, nonprofits, hobby and business expenses and losses, and general definition of income,"" were involved the vast majority of disputes.
Some research has also been focused on hierarchical networks, in combination with citation networks, and the analysis of United States Code. This research has been used to analyze various aspects of the Code, including its size, the density of citations within and between sections of the Code, the type of language used in the Code, and how these features vary over time. This research has been used to provide commentary on the nature of the Code's change over time, which is characterized by an increase in size and in interdependence between sections.


=== Visualization ===
Visualization of legal code, and of the relationships between various laws and decisions, is also a hot topic in computational law. Visualizations allow both professionals and laypeople to see large-scale relationships and patterns, which may be difficult to see using standard legal analysis or empirical analysis.
Legal citation networks lend themselves to visualization, and many citation networks which are analyzed empirically also have sub-sections of the network that are represented visually as a result. However, there are still many technical problems in network visualization. The density of connections between nodes, and the sheer number of nodes in some cases, can make the visualization incomprehensible to humans. There are a variety of methods that can be used to reduce the complexity of the displayed information, for example by defining semantic sub-groups within the network, and then representing relationships between these semantic groups, rather than between every node. This allows the visualization to be human readable, but the reduction in complexity can obscure relationships. Despite this limitation, visualization of legal citation networks remains a popular field and practice.


== Examples of tools ==
OASIS Legal XML, UNDESA Akoma Ntoso, and CEN Metalex, which are standardizations created by legal and technical experts for the electronic exchange of legal data.
Creative Commons, which correspond to custom-generated copyright licenses for internet content.
Legal Analytics, which combines big data, critical expertise, and intuitive tools to deliver business intelligence and benchmarking solutions.
Legal visualizations. Examples include Katz's map of supreme court decisions and Starger's Opinion Lines for the commerce clause and stare decisis.


== Online legal resources and databases ==
PACER is an online repository of judicial rulings, maintained by the Federal Judiciary.
The Law Library of Congress maintains a comprehensive online repository of legal information, including legislation at the international, national, and state levels.
The Supreme Court Database is a comprehensive database containing detailed information about decisions made by the Supreme Court from 1946 to the present.
The United States Reports contained detailed information about every Supreme Court decision from 1791 to the near-present.


== See also ==
Artificial intelligence and law
Legal informatics
Legal expert systems
Robot lawyer


== References ==


== External links ==
CodeX Techindex, Stanford Law School Legal Tech List
LawBots.info Annual Lawbot Awards
LawSites List of Legal Tech Startups"
35,Maximum disjoint set,41701177,28122,"In computational geometry, a maximum disjoint set (MDS) is a largest set of non-overlapping geometric shapes selected from a given set of candidate shapes.
Finding an MDS is important in applications such as automatic label placement, VLSI circuit design, and cellular frequency division multiplexing.
Every set of non-overlapping shapes is an independent set in the intersection graph of the shapes. Therefore, the MDS problem is a special case of the maximum independent set (MIS) problem. Both problems are NP complete, but finding a MDS may be easier than finding a MIS in two respects:
For the general MIS problem, the best known exact algorithms are exponential. In some geometric intersection graphs, there are sub-exponential algorithms for finding a MDS.
The general MIS problem is hard to approximate and doesn't even have a constant-factor approximation. In some geometric intersection graphs, there are polynomial-time approximation schemes (PTAS) for finding a MDS.
The MDS problem can be generalized by assigning a different weight to each shape and searching for a disjoint set with a maximum total weight.
In the following text, MDS(C) denotes the maximum disjoint set in a set C.


== Greedy algorithms ==
Given a set C of shapes, an approximation to MDS(C) can be found by the following greedy algorithm:
INITIALIZATION: Initialize an empty set, S.
SEARCH: For every shape x in C:
Calculate N(x) - the subset of all shapes in C that intersect x (including x itself).
Calculate the largest independent set in this subset: MDS(N(x)).
Select an x such that |MDS(N(x))| is minimized.

Add x to S.
Remove x and N(x) from C.
If there are shapes in C, go back to Search.
END: return the set S.
For every shape x that we add to S, we lose the shapes in N(x), because they are intersected by x and thus cannot be added to S later on. However, some of these shapes themselves intersect each other, and thus in any case it is not possible that they all be in the optimal solution MDS(S). The largest subset of shapes that can all be in the optimal solution is MDS(N(x)). Therefore, selecting an x that minimizes |MDS(N(x))| minimizes the loss from adding x to S.
In particular, if we can guarantee that there is an x for which |MDS(N(x))| is bounded by a constant (say, M), then this greedy algorithm yields a constant M-factor approximation, as we can guarantee that:

  
    
      
        
          |
        
        S
        
          |
        
        ≥
        
          
            
              
                |
              
              M
              D
              S
              (
              C
              )
              
                |
              
            
            M
          
        
      
    
    {\displaystyle |S|\geq {\frac {|MDS(C)|}{M}}}
  
Such an upper bound M exists for several interesting cases:


=== 1-dimensional intervals: exact polynomial algorithm ===

When C is a set of intervals on a line, M=1, and thus the greedy algorithm finds the exact MDS. To see this, assume w.l.o.g. that the intervals are vertical, and let x be the interval with the highest bottom endpoint. All other intervals intersected by x must cross its bottom endpoint. Therefore, all intervals in N(x) intersect each other, and MDS(N(x)) has a size of at most 1 (see figure).
Therefore, in the 1-dimensional case, the MDS can be found exactly in time O(n log n):
Sort the intervals in ascending order of their bottom endpoints (this takes time O(n log n)).
Add an interval with the highest bottom endpoint, and delete all intervals intersecting it.
Continue until no intervals remain.
This algorithm is analogous to the earliest deadline first scheduling solution to the interval scheduling problem.
In contrast to the 1-dimensional case, in 2 or more dimensions the MDS problem becomes NP-complete, and thus has either exact super-polynomial algorithms or approximate polynomial algorithms.


=== Fat shapes: constant-factor approximations ===

When C is a set of unit disks, M=3, because the leftmost disk (the disk whose center has the smallest x coordinate) intersects at most 3 other disjoint disks (see figure). Therefore the greedy algorithm yields a 3-approximation, i.e., it finds a disjoint set with a size of at least MDS(C)/3.
Similarly, when C is a set of axis-parallel unit squares, M=2.

When C is a set of arbitrary-size disks, M=5, because the disk with the smallest radius intersects at most 5 other disjoint disks (see figure).
Similarly, when C is a set of arbitrary-size axis-parallel squares, M=4.
Other constants can be calculated for other regular polygons.


== Divide-and-conquer algorithms ==
The most common approach to finding a MDS is divide-and-conquer. A typical algorithm in this approach looks like the following:
Divide the given set of shapes into two or more subsets, such that the shapes in each subset cannot overlap the shapes in other subsets because of geometric considerations.
Recursively find the MDS in each subset separately.
Return the union of the MDSs from all subsets.
The main challenge with this approach is to find a geometric way to divide the set into subsets. This may require to discard a small number of shapes that do not fit into any one of the subsets, as explained in the following subsections.


=== Axis-parallel rectangles: Logarithmic-factor approximation ===
Let C be a set of n axis-parallel rectangles in the plane. The following algorithm finds a disjoint set with a size of at least 
  
    
      
        
          
            
              
                |
              
              M
              D
              S
              (
              C
              )
              
                |
              
            
            
              log
              ⁡
              
                n
              
            
          
        
      
    
    {\displaystyle {\frac {|MDS(C)|}{\log {n}}}}
   in time 
  
    
      
        O
        (
        n
        log
        ⁡
        
          n
        
        )
      
    
    {\displaystyle O(n\log {n})}
  :
INITIALIZATION: sort the horizontal edges of the given rectangles by their y-coordinate, and the vertical edges by their x-coordinate (this step takes time O(n log n)).
STOP CONDITION: If there are at most n ≤ 2 shapes, compute the MDS directly and return.
RECURSIVE PART:
Let 
  
    
      
        
          x
          
            
              m
              e
              d
            
          
        
      
    
    {\displaystyle x_{\mathrm {med} }}
   be the median x-coordinate.
Partition the input rectangles into three groups according to their relation to the line 
  
    
      
        x
        =
        
          x
          
            
              m
              e
              d
            
          
        
      
    
    {\displaystyle x=x_{\mathrm {med} }}
  : those entirely to its left (
  
    
      
        
          R
          
            
              l
              e
              f
              t
            
          
        
      
    
    {\displaystyle R_{\mathrm {left} }}
  ), those entirely to its right (
  
    
      
        
          R
          
            
              r
              i
              g
              h
              t
            
          
        
      
    
    {\displaystyle R_{\mathrm {right} }}
  ), and those intersected by it (
  
    
      
        
          R
          
            
              i
              n
              t
            
          
        
      
    
    {\displaystyle R_{\mathrm {int} }}
  ). By construction, the cardinalities of 
  
    
      
        
          R
          
            
              l
              e
              f
              t
            
          
        
      
    
    {\displaystyle R_{\mathrm {left} }}
   and 
  
    
      
        
          R
          
            
              r
              i
              g
              h
              t
            
          
        
      
    
    {\displaystyle R_{\mathrm {right} }}
   are at most n/2.
Recursively compute an approximate MDS in 
  
    
      
        
          R
          
            
              l
              e
              f
              t
            
          
        
      
    
    {\displaystyle R_{\mathrm {left} }}
   (
  
    
      
        
          M
          
            
              l
              e
              f
              t
            
          
        
      
    
    {\displaystyle M_{\mathrm {left} }}
  ) and in 
  
    
      
        
          R
          
            
              r
              i
              g
              h
              t
            
          
        
      
    
    {\displaystyle R_{\mathrm {right} }}
   (
  
    
      
        
          M
          
            
              r
              i
              g
              h
              t
            
          
        
      
    
    {\displaystyle M_{\mathrm {right} }}
  ), and calculate their union. By construction, the rectangles in 
  
    
      
        
          M
          
            
              l
              e
              f
              t
            
          
        
      
    
    {\displaystyle M_{\mathrm {left} }}
   and 
  
    
      
        
          M
          
            
              r
              i
              g
              h
              t
            
          
        
      
    
    {\displaystyle M_{\mathrm {right} }}
   are all disjoint, so 
  
    
      
        
          M
          
            
              l
              e
              f
              t
            
          
        
        ∪
        
          M
          
            
              r
              i
              g
              h
              t
            
          
        
      
    
    {\displaystyle M_{\mathrm {left} }\cup M_{\mathrm {right} }}
   is a disjoint set.
Compute an exact MDS in 
  
    
      
        
          R
          
            
              i
              n
              t
            
          
        
      
    
    {\displaystyle R_{\mathrm {int} }}
   (
  
    
      
        
          M
          
            
              i
              n
              t
            
          
        
      
    
    {\displaystyle M_{\mathrm {int} }}
  ). Since all rectangles in 
  
    
      
        
          R
          
            
              i
              n
              t
            
          
        
      
    
    {\displaystyle R_{\mathrm {int} }}
   intersect a single vertical line 
  
    
      
        x
        =
        
          x
          
            
              m
              e
              d
            
          
        
      
    
    {\displaystyle x=x_{\mathrm {med} }}
  , this computation is equivalent to finding an MDS from a set of intervals, and can be solved exactly in time O(n log n) (see above).

Return either 
  
    
      
        
          M
          
            
              l
              e
              f
              t
            
          
        
        ∪
        
          M
          
            
              r
              i
              g
              h
              t
            
          
        
      
    
    {\displaystyle M_{\mathrm {left} }\cup M_{\mathrm {right} }}
   or 
  
    
      
        
          M
          
            
              i
              n
              t
            
          
        
      
    
    {\displaystyle M_{\mathrm {int} }}
   – whichever of them is larger.
It is provable by induction that, at the last step, either 
  
    
      
        
          M
          
            
              l
              e
              f
              t
            
          
        
        ∪
        
          M
          
            
              r
              i
              g
              h
              t
            
          
        
      
    
    {\displaystyle M_{\mathrm {left} }\cup M_{\mathrm {right} }}
   or 
  
    
      
        
          M
          
            
              i
              n
              t
            
          
        
      
    
    {\displaystyle M_{\mathrm {int} }}
   have a cardinality of at least 
  
    
      
        
          
            
              
                |
              
              M
              D
              S
              (
              C
              )
              
                |
              
            
            
              log
              ⁡
              
                n
              
            
          
        
      
    
    {\displaystyle {\frac {|MDS(C)|}{\log {n}}}}
  .
The approximation factor has been reduced to 
  
    
      
        O
        (
        log
        ⁡
        
          log
          ⁡
          
            n
          
        
        )
      
    
    {\displaystyle O(\log {\log {n}})}
   and generalized to the case in which rectangles have different weights.


=== Axis-parallel rectangles with the same height: 2-approximation ===
Let C be a set of n axis-parallel rectangles in the plane, all with the same height H but with varying lengths. The following algorithm finds a disjoint set with a size of at least |MDS(C)|/2 in time O(n log n):
Draw m horizontal lines, such that:
The separation between two lines is strictly more than H.
Each line intersects at least one rectangle (hence m ≤ n).
Each rectangle is intersected by exactly one line.

Since the height of all rectangles is H, it is not possible that a rectangle is intersected by more than one line. Therefore the lines partition the set of rectangles into m subsets (
  
    
      
        
          R
          
            i
          
        
        ,
        …
        ,
        
          R
          
            m
          
        
      
    
    {\displaystyle R_{i},\ldots ,R_{m}}
  ) – each subset includes the rectangles intersected by a single line.
For each subset 
  
    
      
        
          R
          
            i
          
        
      
    
    {\displaystyle R_{i}}
  , compute an exact MDS 
  
    
      
        
          M
          
            i
          
        
      
    
    {\displaystyle M_{i}}
   using the one-dimensional greedy algorithm (see above).
By construction, the rectangles in (
  
    
      
        
          R
          
            i
          
        
      
    
    {\displaystyle R_{i}}
  ) can intersect only rectangles in 
  
    
      
        
          R
          
            i
            +
            1
          
        
      
    
    {\displaystyle R_{i+1}}
   or in 
  
    
      
        
          R
          
            i
            −
            1
          
        
      
    
    {\displaystyle R_{i-1}}
  . Therefore, each of the following two unions is a disjoint sets:
Union of odd MDSs: 
  
    
      
        
          M
          
            1
          
        
        ∪
        
          M
          
            3
          
        
        ∪
        ⋯
      
    
    {\displaystyle M_{1}\cup M_{3}\cup \cdots }
  
Union of even MDSs: 
  
    
      
        
          M
          
            2
          
        
        ∪
        
          M
          
            4
          
        
        ∪
        ⋯
      
    
    {\displaystyle M_{2}\cup M_{4}\cup \cdots }
  

Return the largest of these two unions. Its size must be at least |MDS|/2.


=== Axis-parallel rectangles with the same height: PTAS ===
Let C be a set of n axis-parallel rectangles in the plane, all with the same height but with varying lengths. There is an algorithm that finds a disjoint set with a size of at least |MDS(C)|/(1 + 1/k) in time O(n2k−1), for every constant k > 1.
The algorithm is an improvement of the above-mentioned 2-approximation, by combining dynamic programming with the shifting technique of.
This algorithm can be generalized to d dimensions. If the labels have the same size in all dimensions except one, it is possible to find a similar approximation by applying dynamic programming along one of the dimensions. This also reduces the time to n^O(1/e).


=== Fat objects with identical sizes: PTAS ===
Let C be a set of n squares or circles of identical size. There is a polynomial-time approximation scheme for finding an MDS using a simple shifted-grid strategy. It finds a solution within (1 − e) of the maximum in time nO(1/e2) time and linear space. The strategy generalizes to any collection of fat objects of roughly the same size (i.e., when the maximum-to-minimum size ratio is bounded by a constant).


=== Fat objects with arbitrary sizes: PTAS ===
Let C be a set of n fat objects (e.g. squares or circles) of arbitrary sizes. There is a PTAS for finding an MDS based on multi-level grid alignment. It has been discovered by two groups in approximately the same time, and described in two different ways.
Version 1 finds a disjoint set with a size of at least (1 − 1/k)2 · |MDS(C)| in time nO(k2), for every constant k > 1:
Scale the disks so that the smallest disk has diameter 1. Partition the disks to levels, based on the logarithm of their size. I.e., the j-th level contains all disks with diameter between (k + 1)j and (k + 1)j+1, for j ≤ 0 (the smallest disk is in level 0).
For each level j, impose a grid on the plane that consists of lines that are (k + 1)j+1 apart from each other. By construction, every disk can intersect at most one horizontal line and one vertical line from its level.
For every r, s between 0 and k, define D(r,s) as the subset of disks that are not intersected by any horizontal line whose index modulo k is r, nor by any vertical line whose index modulu k is s. By the pigeonhole principle, there is at least one pair (r,s) such that 
  
    
      
        
          |
        
        
          M
          D
          S
        
        (
        D
        (
        r
        ,
        s
        )
        )
        
          |
        
        ≥
        (
        1
        −
        
          
            1
            k
          
        
        
          )
          
            2
          
        
        ⋅
        
          |
        
        
          M
          D
          S
        
        
          |
        
      
    
    {\displaystyle |\mathrm {MDS} (D(r,s))|\geq (1-{\frac {1}{k}})^{2}\cdot |\mathrm {MDS} |}
  , i.e., we can find the MDS only in D(r,s) and miss only a small fraction of the disks in the optimal solution:
For all k2 possible values of r,s (0 ≤ r,s < k), calculate D(r,s) using dynamic programming.
Return the largest of these k2 sets.

Version 2 finds a disjoint set with a size of at least (1 − 2/k)·|MDS(C)| in time nO(k), for every constant k > 1.
The algorithm uses shifted quadtrees. The key concept of the algorithm is alignment to the quadtree grid. An object of size r is called k-aligned (where k ≥ 1 is a constant) if it is inside a quadtree cell of size at most kr (R ≤ kr).
By definition, a k-aligned object that intersects the boundary of a quatree cell of size R must have a size of at least R/k (r > R/k). The boundary of a cell of size R can be covered by 4k squares of size R/k; hence the number of disjoint fat objects intersecting the boundary of that cell is at most 4kc, where c is a constant measuring the fatness of the objects.
Therefore, if all objects are fat and k-aligned, it is possible to find the exact maximum disjoint set in time nO(kc) using a divide-and-conquer algorithm. Start with a quadtree cell that contains all objects. Then recursively divide it to smaller quadtree cells, find the maximum in each smaller cell, and combine the results to get the maximum in the larger cell. Since the number of disjoint fat objects intersecting the boundary of every quadtree cell is bounded by 4kc, we can simply ""guess"" which objects intersect the boundary in the optimal solution, and then apply divide-and-conquer to the objects inside.
If almost all objects are k-aligned, we can just discard the objects that are not k-aligned, and find a maximum disjoint set of the remaining objects in time nO(k). This results in a (1 − e) approximation, where e is the fraction of objects that are not k-aligned.
If most objects are not k-aligned, we can try to make them k-aligned by shifting the grid in multiples of (1/k,1/k). First, scale the objects such that they are all contained in the unit square. Then, consider k shifts of the grid: (0,0), (1/k,1/k), (2/k,2/k), ..., ((k − 1)/k,(k − 1)/k). I.e., for each j in {0,...,k − 1}, consider a shift of the grid in (j/k,j/k). It is possible to prove that every label will be 2k-aligned for at least k − 2 values of j. Now, for every j, discard the objects that are not k-aligned in the (j/k,j/k) shift, and find a maximum disjoint set of the remaining objects. Call that set A(j). Call the real maximum disjoint set is A*. Then:

  
    
      
        
          ∑
          
            j
            =
            0
            ,
            …
            ,
            k
            −
            1
          
        
        
          
            |
          
          A
          (
          j
          )
          
            |
          
        
        ≥
        (
        k
        −
        2
        )
        
          |
        
        A
        ∗
        
          |
        
      
    
    {\displaystyle \sum _{j=0,\ldots ,k-1}{|A(j)|}\geq (k-2)|A*|}
  
Therefore, the largest A(j) has a size of at least: (1 − 2/k)|A*|. The return value of the algorithm is the largest A(j); the approximation factor is (1 − 2/k), and the run time is nO(k). We can make the approximation factor as small as we want, so this is a PTAS.
Both versions can be generalized to d dimensions (with different approximation ratios) and to the weighted case.


== Geometric separator algorithms ==
Several divide-and-conquer algorithms are based on a certain geometric separator theorem. A geometric separator is a line or shape that separates a given set of shapes to two smaller subsets, such that the number of shapes lost during the division is relatively small. This allows both PTASs and sub-exponential exact algorithms, as explained below.


=== Fat objects with arbitrary sizes: PTAS using geometric separators ===
Let C be a set of n fat objects of arbitrary sizes. The following algorithm finds a disjoint set with a size of at least (1 − O(√b))·|MDS(C)| in time nO(b), for every constant b > 1.
The algorithm is based on the following geometric separator theorem, which can be proved similarly to the proof of the existence of geometric separator for disjoint squares:

For every set C of fat objects, there is a rectangle that partitions C into three subsets of objects – Cinside, Coutside and Cboundary, such that:
|MDS(Cinside)| ≤ a|MDS(C)|
|MDS(Coutside)| ≤ a|MDS(C)|
|MDS(Cboundary)| c√|MDS(C)|

where a and c are constants. If we could calculate MDS(C) exactly, we could make the constant a as low as 2/3 by a proper selection of the separator rectangle. But since we can only approximate MDS(C) by a constant factor, the constant a must be larger. Fortunately, a remains a constant independent of |C|.
This separator theorem allows to build the following PTAS:
Select a constant b. Check all possible combinations of up to b + 1 labels.
If |MDS(C)| has a size of at most b (i.e. all sets of b + 1 labels are not disjoint) then just return that MDS and exit. This step takes nO(b) time.
Otherwise, use a geometric separator to separate C to two subsets. Find the approximate MDS in Cinside and Coutside separately, and return their combination as the approximate MDS in C.
Let E(m) be the error of the above algorithm when the optimal MDS size is MDS(C) = m. When m ≤ b, the error is 0 because the maximum disjoint set is calculated exactly; when m > b, the error increases by at most c√m the number of labels intersected by the separator. The worst case for the algorithm is when the split in each step is in the maximum possible ratio which is a:(1 − a). Therefore the error function satisfies the following recurrence relation:

  
    
      
        E
        (
        m
        )
        =
        0
         
         
         
         
        
           if 
        
        m
        ≤
        b
      
    
    {\displaystyle E(m)=0\ \ \ \ {\text{ if }}m\leq b}
  

  
    
      
        E
        (
        m
        )
        =
        E
        (
        a
        ⋅
        m
        )
        +
        E
        (
        (
        1
        −
        a
        )
        ⋅
        m
        )
        +
        c
        ⋅
        
          
            m
          
        
        
           if 
        
        m
        >
        b
      
    
    {\displaystyle E(m)=E(a\cdot m)+E((1-a)\cdot m)+c\cdot {\sqrt {m}}{\text{ if }}m>b}
  
The solution to this recurrence is:

  
    
      
        E
        (
        m
        )
        =
        (
        
          
            0
            b
          
        
        +
        
          
            c
            
              
                
                  b
                
              
              (
              
                
                  a
                
              
              +
              
                
                  1
                  −
                  a
                
              
              −
              1
              )
            
          
        
        )
        ⋅
        m
        −
        
          
            c
            
              
                
                  a
                
              
              +
              
                
                  1
                  −
                  a
                
              
              −
              1
            
          
        
        ⋅
        
          
            m
          
        
        .
      
    
    {\displaystyle E(m)=({\frac {0}{b}}+{\frac {c}{{\sqrt {b}}({\sqrt {a}}+{\sqrt {1-a}}-1)}})\cdot m-{\frac {c}{{\sqrt {a}}+{\sqrt {1-a}}-1}}\cdot {\sqrt {m}}.}
  
i.e., 
  
    
      
        E
        (
        m
        )
        =
        O
        (
        m
        
          /
        
        
          
            b
          
        
        )
      
    
    {\displaystyle E(m)=O(m/{\sqrt {b}})}
  . We can make the approximation factor as small as we want by a proper selection of b.
This PTAS is more space-efficient than the PTAS based on quadtrees, and can handle a generalization where the objects may slide, but it cannot handle the weighted case.


=== Disks with a bounded size-ratio: exact sub-exponential algorithm ===
Let C be a set of n disks, such that the ratio between the largest radius and the smallest radius is at most r. The following algorithm finds MDS(C) exactly in time 
  
    
      
        
          2
          
            O
            (
            r
            ⋅
            
              
                n
              
            
            )
          
        
      
    
    {\displaystyle 2^{O(r\cdot {\sqrt {n}})}}
  .
The algorithm is based on a width-bounded geometric separator on the set Q of the centers of all disks in C. This separator theorem allows to build the following exact algorithm:
Find a separator line such that at most 2n/3 centers are to its right (Cright), at most 2n/3 centers are to its left (Cleft), and at most O(√n) centers are at a distance of less than r/2 from the line (Cint).
Consider all possible non-overlapping subsets of Cint. There are at most 
  
    
      
        
          2
          
            O
            (
            r
            ⋅
            
              
                n
              
            
            )
          
        
      
    
    {\displaystyle 2^{O(r\cdot {\sqrt {n}})}}
   such subsets. For each such subset, recursively compute the MDS of Cleft and the MDS of Cright, and return the largest combined set.
The run time of this algorithm satisfies the following recurrence relation:

  
    
      
        T
        (
        1
        )
        =
        1
      
    
    {\displaystyle T(1)=1}
  

  
    
      
        T
        (
        n
        )
        =
        
          2
          
            O
            (
            r
            ⋅
            
              
                n
              
            
            )
          
        
        T
        
          (
          
            
              
                2
                n
              
              3
            
          
          )
        
        
           if 
        
        n
        >
        1
      
    
    {\displaystyle T(n)=2^{O(r\cdot {\sqrt {n}})}T\left({\frac {2n}{3}}\right){\text{ if }}n>1}
  
The solution to this recurrence is:

  
    
      
        T
        (
        n
        )
        =
        
          2
          
            O
            (
            r
            ⋅
            
              
                n
              
            
            )
          
        
      
    
    {\displaystyle T(n)=2^{O(r\cdot {\sqrt {n}})}}
  


== Local search algorithms ==


=== Pseudo-disks: a PTAS ===
A pseudo-disks-set is a set of objects in which the boundaries of every pair of objects intersect at most twice. (Note that this definition relates to a whole collection, and does not say anything about the shapes of the specific objects in the collection). A pseudo-disks-set has a bounded union complexity, i.e., the number of intersection points on the boundary of the union of all objects is linear in the number of objects.
Let C be a pseudo-disks-set with n objects. The following local search algorithm finds a disjoint set of size at least 
  
    
      
        (
        1
        −
        O
        (
        
          
            1
            
              b
            
          
        
        )
        )
        ⋅
        
          |
        
        M
        D
        S
        (
        C
        )
        
          |
        
      
    
    {\displaystyle (1-O({\frac {1}{\sqrt {b}}}))\cdot |MDS(C)|}
   in time 
  
    
      
        O
        (
        
          n
          
            b
            +
            3
          
        
        )
      
    
    {\displaystyle O(n^{b+3})}
  , for every integer constant 
  
    
      
        b
        ≥
        0
      
    
    {\displaystyle b\geq 0}
  :
INITIALIZATION: Initialize an empty set, 
  
    
      
        S
      
    
    {\displaystyle S}
  .
SEARCH: Loop over all the subsets of 
  
    
      
        C
        −
        S
      
    
    {\displaystyle C-S}
   whose size is between 1 and 
  
    
      
        b
        +
        1
      
    
    {\displaystyle b+1}
  . For each such subset X:
Verify that X itself is independent (otherwise go to the next subset);
Calculate the set Y of objects in S that intersect X.
If 
  
    
      
        
          |
        
        Y
        
          |
        
        <
        
          |
        
        X
        
          |
        
      
    
    {\displaystyle |Y|<|X|}
  , then remove Y from S and insert X: 
  
    
      
        S
        :=
        S
        −
        Y
        +
        X
      
    
    {\displaystyle S:=S-Y+X}
  .

END: return the set S.
Every exchange in the search step increases the size of S by at least 1, and thus can happen at most n times.
The algorithm is very simple; the difficult part is to prove the approximation ratio.
See also.


== Linear programming relaxation algorithms ==


=== Pseudo-disks: a PTAS ===
Let C be a pseudo-disks-set with n objects and union complexity u. Using linear programming relaxation, it is possible to find a disjoint set of size at least 
  
    
      
        
          
            n
            u
          
        
        ⋅
        
          |
        
        M
        D
        S
        (
        C
        )
        
          |
        
      
    
    {\displaystyle {\frac {n}{u}}\cdot |MDS(C)|}
  . This is possible either with a randomized algorithm that has a high probability of success and run time 
  
    
      
        O
        (
        
          n
          
            3
          
        
        )
      
    
    {\displaystyle O(n^{3})}
  , or a deterministic algorithm with a slower run time (but still polynomial). This algorithm can be generalized to the weighted case.


== Other classes of shapes for which approximations are known ==
Line segments in the two-dimensional plane.
Arbitrary two-dimensional convex objects.
Curves with a bounded number of intersection points.


== External links ==
Approximation Algorithms for Maximum Independent Set of Pseudo-Disks - presentation by Sariel Har-Peled.
Javascript code for calculating exact maximum disjoint set of rectangles.


== Notes =="
36,Secondary School Mathematics Curriculum Improvement Study,40556829,27001,"The Secondary School Mathematics Curriculum Improvement Study (SSMCIS) was the name of an American mathematics education program that stood for both the name of a curriculum and the name of the project that was responsible for developing curriculum materials. It is considered part of the second round of initiatives in the ""New Math"" movement of the 1960s. The program was led by Howard F. Fehr, a professor at Columbia University Teachers College.
The program's signature goal was to create a unified treatment of mathematics and eliminate the traditional separate per-year studies of algebra, geometry, trigonometry, and so forth, that was typical of American secondary schools. Instead, the treatment unified those branches by studying fundamental concepts such as sets, relations, operations, and mappings, and fundamental structures such as groups, rings, fields, and vector spaces. The SSMCIS program produced six courses' worth of class material, intended for grades 7 through 12, in textbooks called Unified Modern Mathematics. Some 25,000 students took SSMCIS courses nationwide during the late 1960s and early 1970s.


== Background ==
The program was led by Howard F. Fehr, a professor at Columbia University Teachers College who was internationally known and had published numerous mathematics textbooks and hundreds of articles about mathematics teaching. In 1961 he had been the principal author of the 246-page report ""New Thinking in School Mathematics"", which held that traditional teaching of mathematics approaches did not meet the needs of the new technical society being entered into or of the current language of mathematicians and scientists. Fehr considered the separation of mathematical study into separate years of distinct subjects to be an American failing that followed an educational model two hundred years old.
The new curriculum was inspired by the seminar reports from the Organisation for Economic Co-operation and Development in the early 1960s and by the Cambridge Conference on School Mathematics (1963), which also inspired the Comprehensive School Mathematics Program. There were some interactions among these initiatives in the early stages, and the development of SSMCIS was part of a general wave of cooperation in the mathematics education reform movement between Europe and the U.S.


== Curriculum ==

Work on the SSMCIS program began in 1965 and took place mainly at Teachers College. Fehr was the director of the project from 1965 to 1973. The principal consultants in the initial stages and subsequent yearly planning sessions were Marshall H. Stone of the University of Chicago, Albert W. Tucker of Princeton University, Edgar Lorch of Columbia University, and Meyer Jordan of Brooklyn College. The program was targeted at the junior high and high school level and the 15–20 percent best students in a grade.
Funding for the initiative began with the U.S. Office of Education and covered the development first three courses produced; the last three courses produced, as well as teacher training, were funded by the National Science Foundation and by Teachers College itself. The scope and sequence of the curriculum was developed by eighteen mathematicians from the U.S. and Europe in 1966 and subsequently refined in experimental course material by mathematical educators with high school level teaching experience. By 1971, some thirty-eight contributors to course materials were identified, eight from Teachers College, four from Europe, one from Canada, and the rest from various other universities (and a couple of high schools) in the United States. Fehr did not do much curriculum development himself, but rather recruited and led the others and organized the whole process. Graduate students from the Department of Mathematical Education at Teachers College also served each year in various capacities on the SSMCIS program.
The central idea of the program was to organize mathematics not by algebra, geometry, etc., but rather to unify those branches by studying the fundamental concepts of sets, relations, operations, and mappings, and fundamental structures such as groups, rings, fields, and vector spaces. Other terms used for this approach included ""global"" or ""integrated""; Fehr himself spoke of a ""spiral"" or ""helical"" development, and wrote of ""the spirit of global organization that is at the heart of the SSMCIS curriculum – important mathematical systems unified by a core of fundamental concepts and structures common to all.
For example, as the courses progressed, the concept of mappings were used to describe, and visually illustrate, the traditionally disparate topics of translation, line reflection, probability of an event, trigonometric functions, isomorphism and complex numbers, and analysis and linear mappings. Traditional subjects were broken up, such that the course material for each year included some material related to algebra, some to geometry, and so forth.
Even when abstract concepts were being introduced, they were introduced in concrete, intuitive forms, especially at the younger levels. Logical proofs were introduced early on and built in importance as the years developed. At least one year of university-level mathematics education was incorporated into the later courses. Solving traditional applications problems was de-emphasized, especially in the earlier courses, but the intent of the project was to make up for that with its focus on real numbers in measurements, computer programming, and probability and statistics. In particular, the last of these was a pronounced element of SSMCIS, with substantial material on it present in all six courses, from measures of statistical dispersion to combinatorics to Bayes' theorem and more.
The curriculum that SSMCIS devised had influences from earlier reform work in Europe, going back to the Bourbaki group's work in France in the 1930s and the Synopses for Modern Secondary School Mathematics published in Paris in 1961. Indeed, most European secondary schools were teaching a more integrated approach. Also, this was one of several American efforts to address a particularly controversial issue, the teaching of a full year of Euclidean geometry in secondary school. Like many of the others, it did this by teaching geometric transformations as a unifying approach between algebra and geometry.
Regardless of all these influences and other projects, the SSMCIS study group considered its work unique in scope and breadth, and Fehr wrote that ""nowhere [else] had a total 7–12 unified mathematics program been designed, produced, and tested."" It was thus considered one of the more radical of the reform efforts lumped under the ""New Math"" label. Moreover, Fehr believed that the SSMCIS could not just improve students' thinking in mathematics, but in all subjects, by ""develop[ing] the capacity of the human mind for the observation, selection, generalization, abstraction, and construction of models for use in the other disciplines.""


== Materials ==

The course books put out by SSMCIS were titled Unified Modern Mathematics, and labeled as Course I through Course VI, with the two volumes in each year labeled as Part I and Part II. Materials for the next year's course were prepared each year, thus keeping up with the early adoption programs underway. Using largely formative evaluation methods for gaining teacher feedback, revised versions were put out after the first year's teaching experience. By 1973, the revised version of all six courses had been completed. The first three volumes were put into the public domain for any organization to use.
The pages of the books were formatted by typewriter, augmented by some mathematical symbols and inserted graphs, bound in paper, and published by Teachers College itself. A more polished hardcover version of Courses I through IV was put out in subsequent years by Addison-Wesley; these were adaptations made by Fehr and others and targeted to students with a broader range of mathematical ability.
Computer programming on time-shared computer systems was included in the curriculum both for its own importance and for understanding numerical methods. The first course introduced flow charts and the notion of algorithms. The beginning portion of the fourth-year course was devoted to introducing the BASIC programming language, with an emphasis on fundamental control flow statements, continued use of flow charts for design, and numerical programming applications. Interactive teletype interfaces on slow and erratic dial-up connections, with troublesome paper tape for offline storage, was the typical physical environment.


== Adoption ==
Starting in 1966, teachers from nine junior high and high schools, mostly in the New York metropolitan area, began getting training in the study program at Teachers College. Such training was crucial since few junior high or high school teachers knew all the material that would be introduced. They then returned to their schools and began teaching the experimental courses, two teachers per grade. For instance, Leonia High School, which incorporated grades 8–12 (since there was no middle school then), called the program ""Math X"" for experimental, with individual courses called Math 8X, Math 9X, etc. Hunter College High School used it as the basis for its Extended Honors Program; the school's description stated that the program ""includes many advanced topics and requires extensive preparation and a considerable commitment of time to the study of mathematics."" Students were periodically given standardized tests to make sure there was no decline in performance due to the unusual organization of material. Some 400 students were involved in this initial phase.
Because the program was so different from standard U.S. mathematics curricula, it was quite difficult to students to enter after the first year; students did, however, sometimes drop out of it and return to standard courses. As teaching the program was a specialized activity, teachers tended to move along from each grade to the next with their students, and so it was typical for students to have one of the same two teachers, or even the same teacher, for five or six years in a row.
More teachers were added in 1968 and 1969 and the University of Maryland and University of Arizona were added as teaching sites. Eighteen schools in Los Angeles adopted SSMCIS in what was called the Accelerated Mathematics Instruction program; some 2,500 gifted students took part. By 1971, teacher education programs were being conducted in places like Austin Peay State University in Tennessee, which was attended by junior high school teachers from seventeen states and one foreign country. By 1974, Fehr stated that 25,000 students were taking SSMCIS courses across the U.S.


== Results ==

The Secondary School Mathematics Curriculum Improvement Study program did show some success in its educational purpose. A study of the Los Angeles program found that SSMCIS-taught students had a better attitude toward their program than did students using School Mathematics Study Group courses (another ""New Math"" initiative) or traditional courses. In New York State schools, special examinations were given to tenth and eleventh grade SSMCIS students in lieu of the standard Regents Examinations due to a mismatch in curriculum. However, SSMCIS was one of the direct inspirations for the New York State Education Department, in the late 1970s and 1980s, adopting an integrated, three-year mathematics curriculum for all its students, combining algebra, geometry, and trigonometry with an increased emphasis in probability and statistics.
Given the differences in subject matter and approach, how SSMCIS-taught students would perform on College Entrance Examination Board tests became a major concern of parents and students and teachers. A 1973 report compared the test performance of such students with those from traditional mathematics curricula. It found that the SSMCIS students did better on the mathematics portion of the Preliminary Scholastic Aptitude Test (PSAT), even when matched for background and performance on the verbal portion. It also found that SSMCIS students did just as well on the Mathematics Level II Achievement Test as traditional students taking college preparatory courses or, indeed, as college freshmen taking introductory calculus courses. Another study found SSMCIS students well prepared for the mathematics portion of the regular Scholastic Aptitude Test.
However, SSMCIS developed slowly. Funding became an issue, and indeed it was never funded as well as some other mathematics curriculum efforts had been. Despite the federal funding source, there was no centralized, national focal point in the U.S. for curriculum changes – such as some European countries had – and that made adoption of SSMCIS innovations a harder task. By the mid-1970s there was a growing backlash against the ""New Math"" movement, spurred in part by a perceived decline in standardized test scores and by Morris Kline's critical book Why Johnny Can't Add: The Failure of the New Math. Many reform efforts had underestimated the difficulty of getting the public and the mathematics educational community to believe that major changes were really necessary, especially for secondary school programs where college entrance performance was always the key concern of administrators. Federal funding for curriculum development also came under attack from American conservatives in the U.S. Congress. As one of the participants in creating SSMCIS, James T. Fey of Teachers College, later wrote, ""Schools and societal expectations of schools appear to change very slowly."" In the end, SSMCIS never became widely adopted.


== Legacy ==
One SSMCIS student, Toomas Hendrik Ilves of Leonia High School, decades later became Foreign Minister and then President of Estonia. He credited the SSMCIS course, the early exposure it gave him to computer programming, and the teacher of the course, Christine Cummings, with his subsequent interest in computer infrastructure, which in part resulted in the country leaping over its Soviet-era technological backwardness; computer-accessible education became pervasive in Estonian schools, and the Internet in Estonia has one of the highest penetration rates in the world. As his tenure as president came to a close in 2016, Ilves visited his old school building with Cummings and said, ""I owe everything to her. Because of what she taught us, my country now uses it."" Cummings said that SSMCIS not only introduced beginning computer programming but also taught students ""how to think"".
SSMCIS did represent a productive exercise in thinking about mathematics curriculum, and the mathematics education literature would cite it in subsequent years, including references to it as a distinct, and the most radical, approach to teaching geometry; as using functions as a unifying element of teaching mathematics; and as its course materials having value when used as the vehicle for further research in mathematics education.


== References =="
37,Conformal geometric algebra,34629138,25636,"Conformal geometric algebra (CGA) is the geometric algebra constructed over the resultant space of a projective map from an n-dimensional base space ℝp,q into ℝp+1,q+1. This allows operations on the base space, including reflections, rotations and translations to be represented using versors of the geometric algebra; and it is found that points, lines, planes, circles and spheres gain particularly natural and computationally amenable representations.
The effect of the mapping is that generalized (i.e. including zero curvature) k-spheres in the base space map onto (k + 2)-blades, and so that the effect of a translation (or any conformal mapping) of the base space corresponds to a rotation in the higher-dimensional space. In the algebra of this space, based on the geometric product of vectors, such transformations correspond to the algebra's characteristic sandwich operations, similar to the use of quaternions for spatial rotation in 3D, which combine very efficiently. A consequence of rotors representing transformations is that the representations of spheres, planes, circles and other geometrical objects, and equations connecting them, all transform covariantly. A geometric object (a k-sphere) can be synthesized as the wedge product of k + 2 linearly independent vectors representing points on the object; conversely, the object can be decomposed as the repeated wedge product of vectors representing k + 2 distinct points in its surface. Some intersection operations also acquire a tidy algebraic form: for example, for the Euclidean base space ℝ3, applying the wedge product to the dual of the tetravectors representing two spheres produces the dual of the trivector representation of their circle of intersection.
As this algebraic structure lends itself directly to effective computation, it facilitates exploration of the classical methods of projective geometry and inversive geometry in a concrete, easy-to-manipulate setting. It has also been used as an efficient structure to represent and facilitate calculations in screw theory. CGA has particularly been applied in connection with the projective mapping of the everyday Euclidean space ℝ3 into a five-dimensional vector space ℝ4,1, which has been investigated for applications in robotics and computer vision. It can be applied generally to any pseudo-Euclidean space, and the mapping of Minkowski space ℝ3,1 to the space ℝ4,2 is being investigated for applications to relativistic physics.


== Construction of CGA ==


=== Notation and terminology ===
In this article, the focus is on the algebra 
  
    
      
        
          
            G
          
        
        (
        4
        ,
        1
        )
      
    
    {\displaystyle {\mathcal {G}}(4,1)}
   as it is this particular algebra that has been the subject of most attention over time; other cases are briefly covered in a separate section. The space containing the objects being modelled is referred to here as the base space, and the algebraic space used to model these objects as the representation or conformal space. A homogeneous subspace refers to a linear subspace of the algebraic space.
The terms for objects: point, line, circle, sphere, quasi-sphere etc. are used to mean either the geometric object in the base space, or the homogeneous subspace of the representation space that represents that object, with the latter generally being intended unless indicated otherwise. Algebraically, any nonzero null element of the homogeneous subspace will be used, with one element being referred to as normalized by some criterion.
Boldface lowercase Latin letters are used to represent position vectors from the origin to a point in the base space. Italic symbols are used for other elements of the representation space.


=== Base and representation spaces ===
The base space ℝ3 is represented by extending a basis for the displacements from a chosen origin and adding two basis vectors e− and e+ orthogonal to the base space and to each other, with e−2 = −1 and e+2 = +1, creating the representation space 
  
    
      
        
          
            G
          
        
        (
        4
        ,
        1
        )
      
    
    {\displaystyle {\mathcal {G}}(4,1)}
  .
It is convenient to use two null vectors no and n∞ as basis vectors in place of e+ and e−, where no = (e− − e+)/2, and n∞ = e− + e+. It can be verified, where x is in the base space, that:

  
    
      
        
          
            
              
                
                  
                    
                      n
                      
                        o
                      
                    
                  
                  
                    2
                  
                
              
              
                =
                0
                
                
                  n
                  
                    o
                  
                
                ⋅
                
                  n
                  
                    ∞
                  
                
              
              
                =
                −
                1
                
              
              
                
                  n
                  
                    o
                  
                
                ⋅
                
                  x
                
              
              
                =
                0
              
            
            
              
                
                  
                    
                      n
                      
                        ∞
                      
                    
                  
                  
                    2
                  
                
              
              
                =
                0
                
                
                  n
                  
                    o
                  
                
                ∧
                
                  n
                  
                    ∞
                  
                
              
              
                =
                
                  e
                  
                    −
                  
                
                
                  e
                  
                    +
                  
                
                
              
              
                
                  n
                  
                    ∞
                  
                
                ⋅
                
                  x
                
              
              
                =
                0
              
            
          
        
      
    
    {\displaystyle {\begin{array}{lllll}{n_{\text{o}}}^{2}&=0\qquad n_{\text{o}}\cdot n_{\infty }&=-1\qquad &n_{\text{o}}\cdot \mathbf {x} &=0\\{n_{\infty }}^{2}&=0\qquad n_{\text{o}}\wedge n_{\infty }&=e_{-}e_{+}\qquad &n_{\infty }\cdot \mathbf {x} &=0\end{array}}}
  
These properties lead to the following formulas for the basis vector coefficients of a general vector r in the representation space for a basis with elements ei orthogonal to every other basis element:
The coefficient of no for r is −n∞ ⋅ r
The coefficient of n∞ for r is −no ⋅ r
The coefficient of ei for r is ei−1 ⋅ r.


=== Mapping between the base space and the representation space ===
The mapping from a vector in the base space (being from the origin to a point in the affine space represented) is given by the formula:

  
    
      
        F
        :
        
          x
        
        ↦
        
          n
          
            o
          
        
        +
        
          x
        
        +
        
          
            
              1
              2
            
          
        
        
          
            x
          
          
            2
          
        
        
          n
          
            ∞
          
        
      
    
    {\displaystyle F:\mathbf {x} \mapsto n_{\text{o}}+\mathbf {x} +{\tfrac {1}{2}}\mathbf {x} ^{2}n_{\infty }}
  
Points and other objects that differ only by a nonzero scalar factor all map to the same object in the base space. When normalisation is desired, as for generating a simple reverse map of a point from the representation space to the base space or determining distances, the condition F(x) ⋅ n∞ = −1 may be used.

The forward mapping is equivalent to:
first conformally projecting x from e123 onto a unit 3-sphere in the space e+ ∧ e123 (in 5-D this is in the subspace r ⋅ (−no − 1/2n∞) = 0);
then lift this into a projective space, by adjoining e– = 1, and identifying all points on the same ray from the origin (in 5-D this is in the subspace r ⋅ (−no − 1/2n∞) = 1);
then change the normalisation, so the plane for the homogeneous projection is given by the no co-ordinate having a value 1, i.e. r ⋅ n∞ = −1.


=== Inverse mapping ===
An inverse mapping for X on the null cone is given (Perwass eqn 4.37) by

  
    
      
        X
        ↦
        
          
            
              P
            
          
          
            
              n
              
                ∞
              
            
            ∧
            
              n
              
                o
              
            
          
          
            ⊥
          
        
        
          (
          
            
              X
              
                −
                X
                ⋅
                
                  n
                  
                    ∞
                  
                
              
            
          
          )
        
      
    
    {\displaystyle X\mapsto {\mathcal {P}}_{n_{\infty }\wedge n_{\text{o}}}^{\perp }\left({\frac {X}{-X\cdot n_{\infty }}}\right)}
  
This first gives a stereographic projection from the light-cone onto the plane r ⋅ n∞ = −1, and then throws away the no and n∞ parts, so that the overall result is to map all of the equivalent points αX = α(no + x + 1/2x2n∞) to x.


=== Origin and point at infinity ===
The point x = 0 in ℝp,q maps to no in ℝp+1,q+1, so no is identified as the (representation) vector of the point at the origin.
A vector in ℝp+1,q+1 with a nonzero n∞ coefficient, but a zero no coefficient, must (considering the inverse map) be the image of an infinite vector in ℝp,q. The direction n∞ therefore represents the (conformal) point at infinity. This motivates the subscripts o and ∞ for identifying the null basis vectors.
The choice of the origin is arbitrary: any other point may be chosen, as the representation is of an affine space. The origin merely represents a reference point, and is algebraically equivalent to any other point. As with any translation, changing the origin corresponds to a rotation in the representation space.


== Geometrical objects ==


=== Basis ===
Together with 
  
    
      
        
          I
          
            5
          
        
        =
        
          e
          
            123
          
        
        E
      
    
    {\displaystyle I_{5}=e_{123}E}
   and 
  
    
      
        1
      
    
    {\displaystyle 1}
  , these are the 32 basis blades of the algebra. The Flat Point Origin is written as an outer product because the geometric product is of mixed grade.(
  
    
      
        E
        =
        
          e
          
            +
          
        
        
          e
          
            −
          
        
      
    
    {\displaystyle E=e_{+}e_{-}}
  ).


=== As the solution of a pair of equations ===
Given any nonzero blade A of the representing space, the set of vectors that are solutions to a pair of homogeneous equations of the form

  
    
      
        
          X
          
            2
          
        
        =
        0
      
    
    {\displaystyle X^{2}=0}
  

  
    
      
        X
        ∧
        A
        =
        0
      
    
    {\displaystyle X\wedge A=0}
  
is the union of homogeneous 1-d subspaces of null vectors, and is thus a representation of a set of points in the base space. This leads to the choice of a blade A as being a useful way to represent a particular class of geometric object. Specific cases for the blade A (independent of the number of dimensions of the space) when the base space is Euclidean space are:
a scalar: the empty set
a vector: a single point
a bivector: a pair of points
a trivector: a generalized circle
a 4-vector: a generalized sphere
etc.
These each may split into three cases according to whether A2 is positive, zero or negative, corresponding (in reversed order in some cases) to the object as listed, a degenerate case of a single point, or no points (where the nonzero solutions of X ∧ A exclude null vectors).
The listed geometric objects (generalized n-spheres) become quasi-spheres in the more general case of the base space being pseudo-Euclidean.
Flat objects may be identified by the point at infinity being included in the solutions. Thus, if n∞ ∧ A = 0, the object will be a line, plane, etc., for the blade A respectively being of grade 3, 4, etc.


=== As derived from points of the object ===
A blade A representing of one of this class of object may be found as the outer product of linearly independent vectors representing points on the object. In the base space, this linear independence manifests as each point lying outside the object defined by the other points. So, for example, a fourth point lying on the generalized circle defined by three distinct points cannot be used as a fourth point to define a sphere.


=== odds ===
Points in e123 map onto the null cone—the null parabola if we set r . n∞ = -1.
We can consider the locus of points in e123 s.t. in conformal space g(x) . A = 0, for various types of geometrical object A.
We start by observing that 
  
    
      
        g
        (
        
          a
        
        )
        .
        g
        (
        
          b
        
        )
        =
        −
        
          
            1
            2
          
        
        ‖
        
          a
        
        −
        
          b
        
        
          ‖
          
            2
          
        
      
    
    {\displaystyle g(\mathbf {a} ).g(\mathbf {b} )=-{\frac {1}{2}}\|\mathbf {a} -\mathbf {b} \|^{2}}
  
compare:
x. a = 0 => x perp a; x.(a∧b) = 0 => x perp a and x perp b
x∧a = 0 => x parallel to a; x∧(a∧b) = 0 => x parallel to a or to b (or to some linear combination)
the inner product and outer product representations are related by dualisation
x∧A = 0 <=> x . A* = 0 (check—works if x is 1-dim, A is n-1 dim)


==== g(x) . A = 0 ====

A point: the locus of x in R3 is a point if A in R4,1 is a vector on the null cone.

(N.B. that because it's a homogeneous projective space, vectors of any length on a ray through the origin are equivalent, so g(x).A =0 is equivalent to g(x).g(a) = 0).
*** warning: apparently wrong codimension—go to the sphere as the general case, then restrict to a sphere of size zero. Is the dual of the equation affected by being on the null cone?

A sphere: the locus of x is a sphere if A = S, a vector off the null cone.

If

  
    
      
        
          S
        
        =
        g
        (
        
          a
        
        )
        −
        
          
            1
            2
          
        
        
          ρ
          
            2
          
        
        
          
            e
          
          
            ∞
          
        
      
    
    {\displaystyle \mathbf {S} =g(\mathbf {a} )-{\frac {1}{2}}\rho ^{2}\mathbf {e} _{\infty }}
  

then S.X = 0 => 
  
    
      
        −
        
          
            1
            2
          
        
        (
        
          a
        
        −
        
          x
        
        
          )
          
            2
          
        
        +
        
          
            1
            2
          
        
        
          ρ
          
            2
          
        
        =
        0
      
    
    {\displaystyle -{\frac {1}{2}}(\mathbf {a} -\mathbf {x} )^{2}+{\frac {1}{2}}\rho ^{2}=0}
  
these are the points corresponding to a sphere

make pic to show hyperbolic orthogonality --> for a vector S off the null-cone, which directions are hyperbolically orthogonal? (cf Lorentz transformation pix)
in 2+1 D, if S is (1,a,b), (using co-ords e-, {e+, ei}), the points hyperbolically orthogonal to S are those euclideanly orthogonal to (-1,a,b)—i.e., a plane; or in n dimensions, a hyperplane through the origin. This would cut another plane not through the origin in a line (a hypersurface in an n-2 surface), and then the cone in two points (resp. some sort of n-3 conic surface). So it's going to probably look like some kind of conic. This is the surface that is the image of a sphere under g.

A plane: the locus of x is a plane if A = P, a vector with a zero no component. In a homogeneous projective space such a vector P represents a vector on the plane no=1 that would be infinitely far from the origin (ie infinitely far outside the null cone) , so g(x).P =0 corresponds to x on a sphere of infinite radius, a plane.
In particular:

  
    
      
        
          P
        
        =
        
          
            
              
                a
              
              ^
            
          
        
        +
        α
        
          
            e
          
          
            ∞
          
        
      
    
    {\displaystyle \mathbf {P} ={\hat {\mathbf {a} }}+\alpha \mathbf {e} _{\infty }}
   corresponds to x on a plane with normal 
  
    
      
        
          
            
              
                a
              
              ^
            
          
        
      
    
    {\displaystyle {\hat {\mathbf {a} }}}
   an orthogonal distance α from the origin.

  
    
      
        
          P
        
        =
        g
        (
        
          a
        
        )
        −
        g
        (
        
          b
        
        )
      
    
    {\displaystyle \mathbf {P} =g(\mathbf {a} )-g(\mathbf {b} )}
   corresponds to a plane half way between a and b, with normal a - b

circles
tangent planes
lines
lines at infinity
point pairs


== Transformations ==

reflections
It can be verified that forming P g(x) P gives a new direction on the null-cone, g(x' ), where x' corresponds to a reflection in the plane of points p in R3 that satisfy g(p) . P = 0.
g(x) . A = 0 => P g(x) . A P = 0 => P g(x) P . P A P (and similarly for the wedge product), so the effect of applying P sandwich-fashion to any the quantities A in the section above is similarly to reflect the corresponding locus of points x, so the corresponding circles, spheres, lines and planes corresponding to particular types of A are reflected in exactly the same way that applying P to g(x) reflects a point x.

This reflection operation can be used to build up general translations and rotations:

translations
Reflection in two parallel planes gives a translation,

  
    
      
        g
        (
        
          
            x
          
          
            ′
          
        
        )
        =
        
          
            P
          
          
            β
          
        
        
          
            P
          
          
            α
          
        
        
        g
        (
        
          x
        
        )
        
        
          
            P
          
          
            α
          
        
        
          
            P
          
          
            β
          
        
      
    
    {\displaystyle g(\mathbf {x} ^{\prime })=\mathbf {P} _{\beta }\mathbf {P} _{\alpha }\;g(\mathbf {x} )\;\mathbf {P} _{\alpha }\mathbf {P} _{\beta }}
  
If 
  
    
      
        
          
            P
          
          
            α
          
        
        =
        
          
            
              
                a
              
              ^
            
          
        
        +
        α
        
          
            e
          
          
            ∞
          
        
      
    
    {\displaystyle \mathbf {P} _{\alpha }={\hat {\mathbf {a} }}+\alpha \mathbf {e} _{\infty }}
   and 
  
    
      
        
          
            P
          
          
            β
          
        
        =
        
          
            
              
                a
              
              ^
            
          
        
        +
        β
        
          
            e
          
          
            ∞
          
        
      
    
    {\displaystyle \mathbf {P} _{\beta }={\hat {\mathbf {a} }}+\beta \mathbf {e} _{\infty }}
   then 
  
    
      
        
          
            x
          
          
            ′
          
        
        =
        
          x
        
        +
        2
        (
        β
        −
        α
        )
        
          
            
              
                a
              
              ^
            
          
        
      
    
    {\displaystyle \mathbf {x} ^{\prime }=\mathbf {x} +2(\beta -\alpha ){\hat {\mathbf {a} }}}
  

rotations

  
    
      
        g
        (
        
          
            x
          
          
            ′
          
        
        )
        =
        
          
            
              
                b
              
              ^
            
          
        
        
          
            
              
                a
              
              ^
            
          
        
        
        g
        (
        
          x
        
        )
        
        
          
            
              
                a
              
              ^
            
          
        
        
          
            
              
                b
              
              ^
            
          
        
      
    
    {\displaystyle g(\mathbf {x} ^{\prime })={\hat {\mathbf {b} }}{\hat {\mathbf {a} }}\;g(\mathbf {x} )\;{\hat {\mathbf {a} }}{\hat {\mathbf {b} }}}
   corresponds to an x' that is rotated about the origin by an angle 2 θ where θ is the angle between a and b -- the same effect that this rotor would have if applied directly to x.

general rotations
rotations about a general point can be achieved by first translating the point to the origin, then rotating around the origin, then translating the point back to its original position, i.e. a sandwiching by the operator 
  
    
      
        
          T
          R
          
            
              
                T
                ~
              
            
          
        
      
    
    {\displaystyle \mathbf {TR{\tilde {T}}} }
   so

  
    
      
        g
        (
        
          
            G
          
        
        x
        )
        =
        
          T
          R
          
            
              
                T
                ~
              
            
          
        
        
        g
        (
        
          x
        
        )
        
        
          T
          
            
              
                R
                ~
              
            
          
          
            
              
                T
                ~
              
            
          
        
      
    
    {\displaystyle g({\mathcal {G}}x)=\mathbf {TR{\tilde {T}}} \;g(\mathbf {x} )\;\mathbf {T{\tilde {R}}{\tilde {T}}} }
  

screws
the effect a screw, or motor, (a rotation about a general point, followed by a translation parallel to the axis of rotation) can be achieved by sandwiching g(x) by the operator 
  
    
      
        
          M
        
        =
        
          
            T
            
              2
            
          
          
            T
            
              1
            
          
          R
          
            
              
                
                  T
                  
                    1
                  
                
                ~
              
            
          
        
      
    
    {\displaystyle \mathbf {M} =\mathbf {T_{2}T_{1}R{\tilde {T_{1}}}} }
  .
M can also be parametrised 
  
    
      
        
          M
        
        =
        
          
            T
            
              ′
            
          
          
            R
            
              ′
            
          
        
      
    
    {\displaystyle \mathbf {M} =\mathbf {T^{\prime }R^{\prime }} }
   (Chasles' theorem)

inversions
an inversion is a reflection in a sphere – various operations that can be achieved using such inversions are discussed at inversive geometry. In particular, the combination of inversion together with the Euclidean transformations translation and rotation is sufficient to express any conformal mapping – i.e. any mapping that universally preserves angles. (Liouville's theorem).

dilations
two inversions with the same centre produce a dilation.


== Generalizations ==


== History ==


== Notes ==


== References ==


== Bibliography =="
38,Computational biology,149353,25169,"Computational biology involves the development and application of data-analytical and theoretical methods, mathematical modeling and computational simulation techniques to the study of biological, behavioral, and social systems. The field is broadly defined and includes foundations in biology, applied mathematics, statistics, biochemistry, chemistry, biophysics, molecular biology, genetics, genomics, computer science and evolution.
Computational biology is different from biological computation, which is a subfield of computer science and computer engineering using bioengineering and biology to build computers, but is similar to bioinformatics, which is an interdisciplinary science using computers to store and process biological data.


== Introduction ==
Computational Biology, which includes many aspects of bioinformatics, is the science of using biological data to develop algorithms or models to understand among various biological systems and relationships. Until recently, biologists did not have access to very large amounts of data which have become commonplace, particularly in molecular biology and genomics. Researchers were able to develop analytical methods for interpreting biological information, but were unable to share them quickly among colleagues.
Bioinformatics began to develop in the early 1970s. It was considered the science of analyzing informatics processes of various biological systems. At this time, research in artificial intelligence was using network models of the human brain in order to generate new algorithms. This use of biological data to develop other fields pushed biological researchers to revisit the idea of using computers to evaluate and compare large data sets. By 1982, information was being shared amongst researchers through the use of punch cards. The amount of data being shared began to grow exponentially by the end of the 1980s. This required the development of new computational methods in order to quickly analyze and interpret relevant information.
Since the late 1990s, computational biology has become an important part of developing emerging technologies for the field of biology. The terms computational biology and evolutionary computation have a similar name, but are not to be confused. Unlike computational biology, evolutionary computation is not concerned with modeling and analyzing biological data. It instead creates algorithms based on the ideas of evolution across species. Sometimes referred to as genetic algorithms, the research of this field can be applied to computational biology. While evolutionary computation is not inherently a part of computational biology, Computational evolutionary biology is a subfield of it.
Computational biology has been used to help sequence the human genome, create accurate models of the human brain, and assist in modeling biological systems.


== Subfields ==


=== Computational anatomy ===

Computational anatomy is a discipline focusing on the study of anatomical shape and form at the visible or gross anatomical 
  
    
      
        50
        −
        100
        μ
      
    
    {\displaystyle 50-100\mu }
   scale of morphology. It involves the development and application of computational, mathematical and data-analytical methods for modeling and simulation of biological structures. The field is broadly defined and includes foundations in anatomy, applied mathematics and pure mathematics, machine learning, computational mechanics, computational science, medical imaging, neuroscience, physics, probability, and statistics; it also has strong connections with fluid mechanics and geometric mechanics. It focuses on the anatomical structures being imaged, rather than the medical imaging devices. It is similar in spirit to the history of Computational linguistics, a discipline that focuses on the linguistic structures rather than the sensor acting as the transmission and communication medium(s).Due to the availability of dense 3D measurements via technologies such as magnetic resonance imaging (MRI), Computational anatomy has emerged as a subfield of medical imaging and bioengineering for extracting anatomical coordinate systems at the morphome scale in 3D.
In computational anatomy, the diffeomorphism group is used to study different coordinate systems via coordinate transformations as generated via the Lagrangian and Eulerian velocities of flow from one anatomical configuration in 
  
    
      
        
          
            
              R
            
          
          
            3
          
        
      
    
    {\displaystyle {\mathbb {R} }^{3}}
   to another. Computational anatomy intersects the study of Riemannian manifolds where groups of diffeomorphisms are the central focus, intersecting with emerging high-dimensional theories of shape emerging from the field of shape statistics. The metric structures in Computational anatomy are related in spirit to morphometrics, with the distinction that Computational anatomy focuses on an infinite-dimensional space of coordinate systems transformed by a diffeomorphism, hence the central use of the terminology diffeomorphometry, the metric space study of coordinate systems via diffeomorphisms. At Computational anatomy's heart is the comparison of shape by recognizing in one shape the other. This connects it to D'Arcy Wentworth Thompson's developments On Growth and Form which has led to scientific explanations of morphogenesis, the process by which patterns are formed in Biology. The original formulation of Computational anatomy is as a generative model of shape and form from exemplars acted upon via transformations.
The spirit of this discipline shares strong overlap with areas such as computer vision and kinematics of rigid bodies, where objects are studied by analysing the groups responsible for the movement in question. It is a branch of the image analysis and pattern theory school at Brown University pioneered by Ulf Grenander. Making spaces of anatomical patterns in Pattern Theory, into a metric space is one of the fundamental operations since being able to cluster and recognize anatomical configurations often requires a metric of close and far between shapes. The diffeomorphometry metric of Computational anatomy measures how far two diffeomorphic changes of coordinates are from each other, which in turn induces a metric on the shapes and images indexed to them. The models of metric pattern theory, in particular group action on the orbit of shapes and forms is a central tool to the formal definitions in Computational anatomy.


=== Computational biomodeling ===

Computational biomodeling is a field concerned with building computer models of biological systems. Computational biomodeling aims to develop and use visual simulations in order to assess the complexity of biological systems. This is accomplished through the use of specialized algorithms, and visualization software. These models allow for prediction of how systems will react under different environments. This is useful for determining if a system is robust. A robust biological system is one that “maintain their state and functions against external and internal perturbations”, which is essential for a biological system to survive. Computational biomodeling generates a large archive of such data, allowing for analysis from multiple users. While current techniques focus on small biological systems, researchers are working on approaches that will allow for larger networks to be analyzed and modeled. A majority of researchers believe that this will be essential in developing modern medical approaches to creating new drugs and gene therapy. A useful modelling approach is to use Petri nets via tools such as esyN 


=== Computational genomics (Computational genetics) ===

Computational genomics is a field within genomics which studies the genomes of cells and organisms. It is sometimes referred to as Computational and Statistical Genetics and encompasses much of Bioinformatics. The Human Genome Project is one example of computational genomics. This project looks to sequence the entire human genome into a set of data. Once fully implemented, this could allow for doctors to analyze the genome of an individual patient. This opens the possibility of personalized medicine, prescribing treatments based on an individual’s pre-existing genetic patterns. This project has created many similar programs. Researchers are looking to sequence the genomes of animals, plants, bacteria, and all other types of life.
One of the main ways that genomes are compared is by homology. Homology is the study of biological structures and nucleotide sequences in different organisms that come from a common ancestor. Research suggests that between 80 and 90% of genes in newly sequenced prokaryotic genomes can be identified this way.
This field is still in development. An untouched project in the development of computational genomics is the analysis of intergenic regions. Studies show that roughly 97% of the human genome consists of these regions. Researchers in computational genomics are working on understanding the functions of non-coding regions of the human genome through the development of computational and statistical methods and via large consortia projects such as ENCODE (The Encyclopedia of DNA Elements) and the Roadmap Epigenomics Project.


=== Computational neuroscience ===

Computational neuroscience is the study of brain function in terms of the information processing properties of the structures that make up the nervous system. It is a subset of the field of neuroscience, and looks to analyze brain data to create practical applications. It looks to model the brain in order to examine specific types aspects of the neurological system. Various types of models of the brain include:
Realistic Brain Models: These models look to represent every aspect of the brain, including as much detail at the cellular level as possible. Realistic models provide the most information about the brain, but also have the largest margin for error. More variables in a brain model create the possibility for more error to occur. These models do not account for parts of the cellular structure that scientists do not know about. Realistic brain models are the most computationally heavy and the most expensive to implement.
Simplifying Brain Models: These models look to limit the scope of a model in order to assess a specific physical property of the neurological system. This allows for the intensive computational problems to be solved, and reduces the amount of potential error from a realistic brain model.
It is the work of computational neuroscientists to improve the algorithms and data structures currently used to increase the speed of such calculations.


=== Computational pharmacology ===
Computational pharmacology (from a computational biology perspective) is “the study of the effects of genomic data to find links between specific genotypes and diseases and then screening drug data”. The pharmaceutical industry requires a shift in methods to analyze drug data. Pharmacologists were able to use Microsoft Excel to compare chemical and genomic data related to the effectiveness of drugs. However, the industry has reached what is referred to as the Excel barricade. This arises from the limited number of cells accessible on a spreadsheet. This development led to the need for computational pharmacology. Scientists and researchers develop computational methods to analyze these massive data sets. This allows for an efficient comparison between the notable data points and allows for more accurate drugs to be developed.
Analysts project that if major medications fail due to patents, that computational biology will be necessary to replace current drugs on the market. Doctoral students in computational biology are being encouraged to pursue careers in industry rather than take Post-Doctoral positions. This is a direct result of major pharmaceutical companies needing more qualified analysts of the large data sets required for producing new drugs.


=== Computational evolutionary biology ===
Computational biology has assisted the field of evolutionary biology in many capacities. This includes:
Using DNA data to reconstruct the tree of life with computational phylogenetics
Fitting population genetics models (either forward time or backward time) to DNA data to make inferences about demographic or selective history
Building population genetics models of evolutionary systems from first principles in order to predict what is likely to evolve.


=== Cancer computational biology ===
Cancer computational biology is a field that aims to determine the future mutations in cancer through an algorithmic approach to analyzing data. Research in this field has led to the use of high-throughput measurement. High throughput measurement allows for the gathering of millions of data points using robotics and other sensing devices. This data is collected from DNA, RNA, and other biological structures. Areas of focus include determining the characteristics of tumors, analyzing molecules that are deterministic in causing cancer, and understanding how the human genome relates to the causation of tumors and cancer.


=== Computational neuropsychiatry ===
Computational neuropsychiatry is the emerging field that uses mathematical and computer-assisted modeling of brain mechanisms involved in mental disorders. It was already demonstrated by several initiatives that computational modeling is an important contribution to understand neuronal circuits that could generate mental functions and dysfunctions.


== Software and tools ==
Computational Biologists use a wide range of software. These range from command line programs to graphical and web-based programs.


=== Open source software ===
Open source software provides a platform to develop computational biological methods. Specifically, open source means that every person and/or entity can access and benefit from software developed in research. PLOS cites four main reasons for the use of open source software including:
Reproducibility: This allows for researchers to use the exact methods used to calculate the relations between biological data.
Faster Development: developers and researchers do not have to reinvent existing code for minor tasks. Instead they can use pre-existing programs to save time on the development and implementation of larger projects.
Increased quality: Having input from multiple researchers studying the same topic provides a layer of assurance that errors will not be in the code.
Long-term availability: Open source programs are not tied to any businesses or patents. This allows for them to be posted to multiple web pages and ensure that they are available in the future.


== Conferences ==
There are several large conferences that are concerned with computational biology. Some notable examples are Intelligent Systems for Molecular Biology (ISMB), European Conference on Computational Biology (ECCB) and Research in Computational Molecular Biology (RECOMB).


== Journals ==
There are numerous journals dedicated to computational biology. Some notable examples include Journal of Computational Biology and PLOS Computational Biology. The PLOS computational biology journal is a peer-reviewed journal that has many notable research projects in the field of computational biology. They provide reviews on software, tutorials for open source software, and display information on upcoming computational biology conferences. PLOS Computational Biology is an open access journal. The publication may be openly used provided the author is cited. Recently a new open access journal Computational Molecular Biology was launched.


== Related fields ==
Computational biology, bioinformatics and mathematical biology are all interdisciplinary approaches to the life sciences that draw from quantitative disciplines such as mathematics and information science. The NIH describes computational/mathematical biology as the use of computational/mathematical approaches to address theoretical and experimental questions in biology and, by contrast, bioinformatics as the application of information science to understand complex life-sciences data.
Specifically, the NIH defines

Computational biology: The development and application of data-analytical and theoretical methods, mathematical modeling and computational simulation techniques to the study of biological, behavioral, and social systems.

Bioinformatics: Research, development, or application of computational tools and approaches for expanding the use of biological, medical, behavioral or health data, including those to acquire, store, organize, archive, analyze, or visualize such data.

While each field is distinct, there may be significant overlap at their interface.


== See also ==


== References ==


== External links ==
bioinformatics.org"
39,Aanderaa–Karp–Rosenberg conjecture,21681084,25145,"In theoretical computer science, the Aanderaa–Karp–Rosenberg conjecture (also known as the Aanderaa–Rosenberg conjecture or the evasiveness conjecture) is a group of related conjectures about the number of questions of the form ""Is there an edge between vertex u and vertex v?"" that have to be answered to determine whether or not an undirected graph has a particular property such as planarity or bipartiteness. They are named after Stål Aanderaa, Richard M. Karp, and Arnold L. Rosenberg. According to the conjecture, for a wide class of properties, no algorithm can guarantee that it will be able to skip any questions: any algorithm for determining whether the graph has the property, no matter how clever, might need to examine every pair of vertices before it can give its answer. A property satisfying this conjecture is called evasive.
More precisely, the Aanderaa–Rosenberg conjecture states that any deterministic algorithm must test at least a constant fraction of all possible pairs of vertices, in the worst case, to determine any non-trivial monotone graph property; in this context, a property is monotone if it remains true when edges are added (so planarity is not monotone, but non-planarity is monotone). A stronger version of this conjecture, called the evasiveness conjecture or the Aanderaa–Karp–Rosenberg conjecture, states that exactly n(n − 1)/2 tests are needed. Versions of the problem for randomized algorithms and quantum algorithms have also been formulated and studied.
The deterministic Aanderaa–Rosenberg conjecture was proven by Rivest & Vuillemin (1975), but the stronger Aanderaa–Karp–Rosenberg conjecture remains unproven. Additionally, there is a large gap between the conjectured lower bound and the best proven lower bound for randomized and quantum query complexity.


== Example ==
The property of being non-empty (that is, having at least one edge) is monotone, because adding another edge to a non-empty graph produces another non-empty graph. There is a simple algorithm for testing whether a graph is non-empty: loop through all of the pairs of vertices, testing whether each pair is connected by an edge. If an edge is ever found in this way, break out of the loop, and report that the graph is non-empty, and if the loop completes without finding any edges, then report that the graph is empty. On some graphs (for instance the complete graphs) this algorithm will terminate quickly, without testing every pair of vertices, but on the empty graph it tests all possible pairs before terminating. Therefore, the query complexity of this algorithm is n(n − 1)/2: in the worst case, the algorithm performs n(n − 1)/2 tests.
The algorithm described above is not the only possible method of testing for non-emptiness, but the Aanderaa–Karp–Rosenberg conjecture implies that every deterministic algorithm for testing non-emptiness has the same query complexity, n(n − 1)/2. That is, the property of being non-empty is evasive. For this property, the result is easy to prove directly: if an algorithm does not perform n(n − 1)/2 tests, it cannot distinguish the empty graph from a graph that has one edge connecting one of the untested pairs of vertices, and must give an incorrect answer on one of these two graphs.


== Definitions ==
In the context of this article, all graphs will be simple and undirected, unless stated otherwise. This means that the edges of the graph form a set (and not a multiset) and each edge is a pair of distinct vertices. Graphs are assumed to have an implicit representation in which each vertex has a unique identifier or label and in which it is possible to test the adjacency of any two vertices, but for which adjacency testing is the only allowed primitive operation.
Informally, a graph property is a property of a graph that is independent of labeling. More formally, a graph property is a mapping from the set of all graphs to {0,1} such that isomorphic graphs are mapped to the same value. For example, the property of containing at least 1 vertex of degree 2 is a graph property, but the property that the first vertex has degree 2 is not, because it depends on the labeling of the graph (in particular, it depends on which vertex is the ""first"" vertex). A graph property is called non-trivial if it doesn't assign the same value to all graphs. For instance, the property of being a graph is a trivial property, since all graphs possess this property. On the other hand, the property of being empty is non-trivial, because the empty graph possesses this property, but non-empty graphs do not. A graph property is said to be monotone if the addition of edges does not destroy the property. Alternately, if a graph possesses a monotone property, then every supergraph of this graph on the same vertex set also possesses it. For instance, the property of being nonplanar is monotone: a supergraph of a nonplanar graph is itself nonplanar. However, the property of being regular is not monotone.
The big O notation is often used for query complexity. In short, f(n) is O(g(n)) if for large enough n, f(n) ≤ c g(n) for some positive constant c. Similarly, f(n) is Ω(g(n)) if for large enough n, f(n) ≥ c g(n) for some positive constant c. Finally, f(n) is Θ(g(n)) if it is both O(g(n)) and Ω(g(n)).


== Query complexity ==
The deterministic query complexity of evaluating a function on n bits (x1, x2, ..., xn) is the number of bits xi that have to be read in the worst case by a deterministic algorithm to determine the value of the function. For instance, if the function takes value 0 when all bits are 0 and takes value 1 otherwise (this is the OR function), then the deterministic query complexity is exactly n. In the worst case, the first n − 1 bits read could all be 0, and the value of the function now depends on the last bit. If an algorithm doesn't read this bit, it might output an incorrect answer. (Such arguments are known as adversary arguments.) The number of bits read are also called the number of queries made to the input. One can imagine that the algorithm asks (or queries) the input for a particular bit and the input responds to this query.
The randomized query complexity of evaluating a function is defined similarly, except the algorithm is allowed to be randomized, i.e., it can flip coins and use the outcome of these coin flips to decide which bits to query. However, the randomized algorithm must still output the correct answer for all inputs: it is not allowed to make errors. Such algorithms are called Las Vegas algorithms, which distinguishes them from Monte Carlo algorithms which are allowed to make some error. Randomized query complexity can also be defined in the Monte Carlo sense, but the Aanderaa–Karp–Rosenberg conjecture is about the Las Vegas query complexity of graph properties.
Quantum query complexity is the natural generalization of randomized query complexity, of course allowing quantum queries and responses. Quantum query complexity can also be defined with respect to Monte Carlo algorithms or Las Vegas algorithms, but it is usually taken to mean Monte Carlo quantum algorithms.
In the context of this conjecture, the function to be evaluated is the graph property, and the input is a string of size n(n − 1)/2, which gives the locations of the edges on an n vertex graph, since a graph can have at most n(n − 1)/2 possible edges. The query complexity of any function is upper bounded by n(n − 1)/2, since the whole input is read after making n(n − 1)/2 queries, thus determining the input graph completely.


== Deterministic query complexity ==
For deterministic algorithms, Rosenberg (1973) originally conjectured that for all nontrivial graph properties on n vertices, deciding whether a graph possesses this property requires Ω(n2) queries. The non-triviality condition is clearly required because there are trivial properties like ""is this a graph?"" which can be answered with no queries at all.

The conjecture was disproved by Aanderaa, who exhibited a directed graph property (the property of containing a ""sink"") which required only O(n) queries to test. A sink, in a directed graph, is a vertex of indegree n-1 and outdegree 0. This property can be tested with less than 3n queries (Best, van Emde Boas & Lenstra 1974). An undirected graph property which can also be tested with O(n) queries is the property of being a scorpion graph, first described in Best, van Emde Boas & Lenstra (1974). A scorpion graph is a graph containing a three-vertex path, such that one endpoint of the path is connected to all remaining vertices, while the other two path vertices have no incident edges other than the ones in the path.
Then Aanderaa and Rosenberg formulated a new conjecture (the Aanderaa–Rosenberg conjecture) which says that deciding whether a graph possesses a non-trivial monotone graph property requires Ω(n2) queries. This conjecture was resolved by Rivest & Vuillemin (1975) by showing that at least n2/16 queries are needed to test for any nontrivial monotone graph property. The bound was further improved to n2/9 by Kleitman & Kwiatkowski (1980), then to n2/4 - o(n2) by Kahn, Saks & Sturtevant (1983), then to (8/25)n2 - o(n2) by Korneffel & Triesch (2010), and then to n2/3 - o(n2) by Scheidweiler & Triesch (2013).
Richard Karp conjectured the stronger statement (which is now called the evasiveness conjecture or the Aanderaa–Karp–Rosenberg conjecture) that ""every nontrivial monotone graph property for graphs on n vertices is evasive."" A property is called evasive if determining whether a given graph has this property sometimes requires all n(n − 1)/2 queries. This conjecture says that the best algorithm for testing any nontrivial monotone property must (in the worst case) query all possible edges. This conjecture is still open, although several special graph properties have shown to be evasive for all n. The conjecture has been resolved for the case where n is a prime power by Kahn, Saks & Sturtevant (1983) using a topological approach. The conjecture has also been resolved for all non-trivial monotone properties on bipartite graphs by Yao (1988). Minor-closed properties have also been shown to be evasive for large n (Chakrabarti, Khot & Shi 2001).


== Randomized query complexity ==
Richard Karp also conjectured that Ω(n2) queries are required for testing nontrivial monotone properties even if randomized algorithms are permitted. No nontrivial monotone property is known which requires less than n2/4 queries to test. A linear lower bound (i.e., Ω(n)) on all monotone properties follows from a very general relationship between randomized and deterministic query complexities. The first superlinear lower bound for all monotone properties was given by Yao (1991) who showed that Ω(n log1/12 n) queries are required. This was further improved by King (1988) to Ω(n5/4), and then by Hajnal (1991) to Ω(n4/3). This was subsequently improved to the current best known lower bound (among bounds that hold for all monotone properties) of Ω(n4/3 log1/3 n) by Chakrabarti & Khot (2001).
Some recent results give lower bounds which are determined by the critical probability p of the monotone graph property under consideration. The critical probability p is defined as the unique p such that a random graph G(n, p) possesses this property with probability equal to 1/2. A random graph G(n, p) is a graph on n vertices where each edge is chosen to be present with probability p independent of all the other edges. Friedgut, Kahn & Wigderson (2002) showed that any monotone property with critical probability p requires 
  
    
      
        Ω
        
          (
          
            min
            
              {
              
                
                  
                    n
                    
                      min
                      (
                      p
                      ,
                      1
                      −
                      p
                      )
                    
                  
                
                ,
                
                  
                    
                      n
                      
                        2
                      
                    
                    
                      log
                      ⁡
                      n
                    
                  
                
              
              }
            
          
          )
        
      
    
    {\displaystyle \Omega \left(\min \left\{{\frac {n}{\min(p,1-p)}},{\frac {n^{2}}{\log n}}\right\}\right)}
   queries. For the same problem, O'Donnell et al. (2005) showed a lower bound of Ω(n4/3/p1/3).
As in the deterministic case, there are many special properties for which an Ω(n2) lower bound is known. Moreover, better lower bounds are known for several classes of graph properties. For instance, for testing whether the graph has a subgraph isomorphic to any given graph (the so-called subgraph isomorphism problem), the best known lower bound is Ω(n3/2) due to Gröger (1992).


== Quantum query complexity ==
For bounded-error quantum query complexity, the best known lower bound is Ω(n2/3 log1/6 n) as observed by Andrew Yao. It is obtained by combining the randomized lower bound with the quantum adversary method. The best possible lower bound one could hope to achieve is Ω(n), unlike the classical case, due to Grover's algorithm which gives an O(n) query algorithm for testing the monotone property of non-emptiness. Similar to the deterministic and randomized case, there are some properties which are known to have an Ω(n) lower bound, for example non-emptiness (which follows from the optimality of Grover's algorithm) and the property of containing a triangle. More interestingly, there are some graph properties which are known to have an Ω(n3/2) lower bound, and even some properties with an Ω(n2) lower bound. For example, the monotone property of nonplanarity requires Θ(n3/2) queries (Ambainis et al. 2008) and the monotone property of containing more than half the possible number of edges (also called the majority function) requires Θ(n2) queries (Beals et al. 2001).


== Notes ==


== References ==


== Further reading ==
Bollobás, Béla (2004), ""Chapter VIII. Complexity and packing"", Extremal Graph Theory, New York: Dover Publications, pp. 401–437, ISBN 978-0-486-43596-1 .
László Lovász; Young, Neal E. (2002). ""Lecture Notes on Evasiveness of Graph Properties"". arXiv:cs/0205031v1  [cs.CC]. 
Chronaki, Catherine E (1990), A survey of Evasiveness: Lower Bounds on the Decision-Tree Complexity of Boolean Functions, CiteSeerX 10.1.1.37.1041 . 
Michael Saks. ""Decision Trees: Problems and Results, Old and New"" (PDF)."
40,Informatics,23997153,25130,"Informatics is a branch of information engineering. It involves the practice of information processing and the engineering of information systems, and as an academic field it is an applied form of information science. The field considers the interaction between humans and information alongside the construction of interfaces, organisations, technologies and systems. As such, the field of informatics has great breadth and encompasses many subspecialties, including disciplines of computer science, information systems, information technology and statistics. Since the advent of computers, individuals and organizations increasingly process information digitally. This has led to the study of informatics with computational, mathematical, biological, cognitive and social aspects, including study of the social impact of information technologies.


== Etymology ==

In 1956 the German computer scientist Karl Steinbuch coined the word Informatik by publishing a paper called Informatik: Automatische Informationsverarbeitung (""Informatics: Automatic Information Processing""). The English term Informatics is sometimes understood as meaning the same as computer science. The German word Informatik is usually translated to English as computer science.
The French term informatique was coined in 1962 by Philippe Dreyfus together with various translations—informatics (English), also proposed independently and simultaneously by Walter F. Bauer and associates who co-founded Informatics Inc., and informatica (Italian, Spanish, Romanian, Portuguese, Dutch), referring to the application of computers to store and process information.
The term was coined as a combination of ""information"" and ""automatic"" to describe the science of automating information interactions. The morphology—informat-ion + -ics—uses ""the accepted form for names of sciences, as conics, linguistics, optics, or matters of practice, as economics, politics, tactics"", and so, linguistically, the meaning extends easily to encompass both the science of information and the practice of information processing.


== History ==

The culture of library science promotes policies and procedures for managing information that fosters the relationship between library science and the development of information science to provide benefits for health informatics development; which is traced to the 1950s with the beginning of computer uses in healthcare (Nelson & Staggers p.4). Early practitioners interested in the field soon learned that there were no formal education programs set up to educate them on the informatics science until the late 1960s and early 1970s. Professional development began to emerge, playing a significant role in the development of health informatics (Nelson &Staggers p.7) According to Imhoff et al., 2001, healthcare informatics is not only the application of computer technology to problems in healthcare but covers all aspects of generation, handling, communication, storage, retrieval, management, analysis, discovery, and synthesis of data information and knowledge in the entire scope of healthcare. Furthermore, they stated that the primary goal of health informatics can be distinguished as follows: To provide solutions for problems related to data, information, and knowledge processing. To study general principles of processing data information and knowledge in medicine and healthcare.
Reference Imhoff, M., Webb. A,.&Goldschmidt, A., (2001). Health Informatics. Intensive Care Med, 27: 179-186. doi:10.1007//s001340000747.
Nelson, R. & Staggers, N. Health Informatics: An Interprofessional Approach. St. Louis: Mosby, 2013. Print. (p.4,7)

This new term was adopted across Western Europe, and, except in English, developed a meaning roughly translated by the English ‘computer science’, or ‘computing science’. Mikhailov advocated the Russian term informatika (1966), and the English informatics (1967), as names for the theory of scientific information, and argued for a broader meaning, including study of the use of information technology in various communities (for example, scientific) and of the interaction of technology and human organizational structures.
Informatics is the discipline of science which investigates the structure and properties (not specific content) of scientific information, as well as the regularities of scientific information activity, its theory, history, methodology and organization.
Usage has since modified this definition in three ways. First, the restriction to scientific information is removed, as in business informatics or legal informatics. Second, since most information is now digitally stored, computation is now central to informatics. Third, the representation, processing and communication of information are added as objects of investigation, since they have been recognized as fundamental to any scientific account of information. Taking information as the central focus of study distinguishes informatics from computer science. Informatics includes the study of biological and social mechanisms of information processing whereas computer science focuses on the digital computation. Similarly, in the study of representation and communication, informatics is indifferent to the substrate that carries information. For example, it encompasses the study of communication using gesture, speech and language, as well as digital communications and networking.
In the English-speaking world the term informatics was first widely used in the compound medical informatics, taken to include ""the cognitive, information processing, and communication tasks of medical practice, education, and research, including information science and the technology to support these tasks"". Many such compounds are now in use; they can be viewed as different areas of ""applied informatics"". Indeed, ""In the U.S., however, informatics is linked with applied computing, or computing in the context of another domain.""
Informatics encompasses the study of systems that represent, process, and communicate information. However, the theory of computation in the specific discipline of theoretical computer science, which evolved from Alan Turing, studies the notion of a complex system regardless of whether or not information actually exists. Since both fields process information, there is some disagreement among scientists as to field hierarchy; for example Arizona State University attempted to adopt a broader definition of informatics to even encompass cognitive science at the launch of its School of Computing and Informatics in September 2006.
A broad interpretation of informatics, as ""the study of the structure, algorithms, behaviour, and interactions of natural and artificial computational systems,"" was introduced by the University of Edinburgh in 1994 when it formed the grouping that is now its School of Informatics. This meaning is now (2006) increasingly used in the United Kingdom.
The 2008 Research Assessment Exercise, of the UK Funding Councils, includes a new, Computer Science and Informatics, unit of assessment (UoA), whose scope is described as follows:
The UoA includes the study of methods for acquiring, storing, processing, communicating and reasoning about information, and the role of interactivity in natural and artificial systems, through the implementation, organisation and use of computer hardware, software and other resources. The subjects are characterised by the rigorous application of analysis, experimentation and design.


== Academic schools and departments ==
Academic research in the informatics area can be found in a number of disciplines such as computer science, information technology, Information and Computer Science, information system, business information management and health informatics.
In France, the first degree level qualifications in Informatics (computer science) appeared in the mid-1960s.
In English-speaking countries, the first example of a degree level qualification in Informatics occurred in 1982 when Plymouth Polytechnic (now the University of Plymouth) offered a four-year BSc(Honours) degree in Computing and Informatics – with an initial intake of only 35 students. The course still runs today  making it the longest available qualification in the subject.
At the Indiana University School of Informatics (Bloomington, Indianapolis and Southeast), informatics is defined as ""the art, science and human dimensions of information technology"" and ""the study, application, and social consequences of technology."" It is also defined in Informatics 101, Introduction to Informatics as ""the application of information technology to the arts, sciences, and professions."" These definitions are widely accepted in the United States, and differ from British usage in omitting the study of natural computation.
Texas Woman's University places its informatics degrees in its department of Mathematics and Computer Science within the College of Arts & Sciences, though it offers interdisciplinary Health Informatics degrees. Informatics is presented in a generalist framework, as evidenced by their definition of informatics (""Using technology and data analytics to derive meaningful information from data for data and decision driven practice in user centered systems""), though TWU is also known for its nursing and health informatics programs.
At the University of California, Irvine Department of Informatics, informatics is defined as ""the interdisciplinary study of the design, application, use and impact of information technology. The discipline of informatics is based on the recognition that the design of this technology is not solely a technical matter, but must focus on the relationship between the technology and its use in real-world settings. That is, informatics designs solutions in context, and takes into account the social, cultural and organizational settings in which computing and information technology will be used.""
At the University of Michigan, Ann Arbor Informatics interdisciplinary major, informatics is defined as ""the study of information and the ways information is used by and affects human beings and social systems. The major involves coursework from the College of Literature, Science and the Arts, where the Informatics major is housed, as well as the School of Information and the College of Engineering. Key to this growing field is that it applies both technological and social perspectives to the study of information. Michigan's interdisciplinary approach to teaching Informatics gives a solid grounding in contemporary computer programming, mathematics, and statistics, combined with study of the ethical and social science aspects of complex information systems. Experts in the field help design new information technology tools for specific scientific, business, and cultural needs."" Michigan offers four curricular tracks within the informatics degree to provide students with increased expertise. These four track topics include:
Internet Informatics: An applied track in which students experiment with technologies behind Internet-based information systems and acquire skills to map problems to deployable Internet-based solutions. This track will replace Computational Informatics in Fall 2013.
Data Mining & Information Analysis: Integrates the collection, analysis, and visualization of complex data and its critical role in research, business, and government to provide students with practical skills and a theoretical basis for approaching challenging data analysis problems.
Life Science Informatics: Examines artificial information systems, which has helped scientists make great progress in identifying core components of organisms and ecosystems.
Social Computing: Advances in computing have created opportunities for studying patterns of social interaction and developing systems that act as introducers, recommenders, coordinators, and record-keepers. Students, in this track, craft, evaluate, and refine social software computer applications for engaging technology in unique social contexts. This track will be phased out in Fall 2013 in favor of the new bachelor of science in information. This will be the first undergraduate degree offered by the School of Information since its founding in 1996. The School of Information already contains a Master's program, Doctorate program, and a professional master's program in conjunction with the School of Public Health. The BS in Information at the University of Michigan will be the first curriculum program of its kind in the United States, with the first graduating class to emerge in 2015. Students will be able to apply for this unique degree in 2013 for the 2014 Fall semester; the new degree will be a stem off of the most popular Social Computing track in the current Informatics interdisciplinary major in LSA. Applications will be open to upper-classmen, juniors and seniors, along with a variety of information classes available for first and second year students to gauge interest and value in the specific sector of study. The degree was approved by the University on June 11, 2012. Along with a new degree in the School of Information, there has also been the first and only chapter of an Informatics Professional Fraternity, Kappa Theta Pi, chartered in Fall 2012.
At the University of Washington, Seattle Informatics Undergraduate Program, Informatics is an undergraduate program offered by the Information School. Bachelor of Science in Informatics is described as ""[a] program that focuses on computer systems from a user-centered perspective and studies the structure, behavior and interactions of natural and artificial systems that store, process and communicate information. Includes instruction in information sciences, human computer interaction, information system analysis and design, telecommunications structure and information architecture and management."" Washington offers three degree options as well as a custom track.
Data Science Option: Data Science is an emerging interdisciplinary field that works to extract knowledge or insight from data. It combines fields such as information science, computer science, statistics, design, and social science.
Human-Computer Interaction: The iSchool’s work in human-computer interaction (HCI) strives to make information and computing useful, usable, and accessible to all. The Informatics HCI option allows one to blend your technical skills and expertise with a broader perspective on how design and development work impacts users. Courses explore the design, construction, and evaluation of interactive technologies for use by individuals, groups, and organizations, and the social implications of these systems. This work encompasses user interfaces, accessibility concerns, new design techniques and methods for interactive systems and collaboration. Coursework also examines the values implicit in the design and development of technology.
Information Architecture: Information architecture (IA) is a crucial component in the development of successful Web sites, software, intranets, and online communities. Architects structure the underlying information and its presentation in a logical and intuitive way so that people can put information to use. As an Informatics major with an IA option, one will master the skills needed to organize and label information for improved navigation and search. One will build frameworks to effectively collect, store and deliver information. One will also learn to design the databases and XML storehouses that drive complex and interactive websites, including the navigation, content layout, personalization, and transactional features of the site.
Information Assurance and Cybersecurity: Information Assurance and Cybersecurity (IAC) is the practice of creating and managing safe and secure systems. It is crucial for organizations public and private, large and small. In the IAC option, one will be equipped with the knowledge to create, deploy, use, and manage systems that preserve individual and organizational privacy and security. This tri-campus concentration leverages the strengths of the Information School, the Computing and Software Systems program at UW Bothell, and the Institute of Technology at UW Tacoma. After a course in the technical, policy, and management foundations of IAC, one may take electives at any campus to learn such specialties as information assurance policy, secure coding, or networking and systems administration.
Custom (Student-Designed Concentration): Students may choose to develop their own concentration, with approval from the academic adviser. Student-designed concentrations are created out of a list of approved courses and also result in the Bachelor of Science degree.


== Applied disciplines ==


=== Organizational informatics ===

One of the most significant areas of application of informatics is that of organizational informatics. Organizational informatics is fundamentally interested in the application of information, information systems and ICT within organisations of various forms including private sector, public sector and voluntary sector organisations. As such, organisational informatics can be seen to be a sub-category of social informatics and a super-category of business informatics. Organizational informatics are also present in the computer science and information technology industry. 


== See also ==
Artificial intelligence
Behavior informatics
Biomimetics
Cognitive science
Computer science
Communication studies
Information science
Information systems
Information theory
Information technology
Knowledge Management
Robotics
Urban informatics


== Notes ==


== External links ==
informatics: entry from International Encyclopedia of Information and Library Science
Informatics Studies: Journal of Centre for Informatics Research and Development
Software History Center: First usage of informatics in the US
What is Informatics? : Indiana University
Q&A about informatics
Prior Art Database: Informatics: An Early Software Company
Informatics Europe
The Council of European Professional Informatics Societies (CEPIS)
Informatics Department, College of Computing and Information, University at Albany - State University of New York
Department of Informatics, King's College London
An Informatics Education: What and who is it for?, from Northern Kentucky University
Texas Woman's University's Informatics on Facebook
Institution of Mechanical Engineers - Mechatronics, Informatics and Control Group (MICG)
https://www.coursehero.com/file/23728173/Academic-schools-and-departments-on-literature-infomatics/
Informatics: 10 Years Back, 10 years Ahead"
41,Computational science,1181008,24110,"Computational science (also scientific computing or scientific computation (SC)) is a rapidly growing multidisciplinary field that uses advanced computing capabilities to understand and solve complex problems. It is an area of science which spans many disciplines, but at its core it involves the development of models and simulations to understand natural systems.
Algorithms (numerical and non-numerical), mathematical and computational modeling and simulation developed to solve science (e.g., biological, physical, and social), engineering, and humanities problems
Computer and information science that develops and optimizes the advanced system hardware, software, networking, and data management components needed to solve computationally demanding problems
The computing infrastructure that supports both the science and engineering problem solving and the developmental computer and information science
In practical use, it is typically the application of computer simulation and other forms of computation from numerical analysis and theoretical computer science to solve problems in various scientific disciplines. The field is different from theory and laboratory experiment which are the traditional forms of science and engineering. The scientific computing approach is to gain understanding, mainly through the analysis of mathematical models implemented on computers. Scientists and engineers develop computer programs, application software, that model systems being studied and run these programs with various sets of input parameters. The essence of computational science is the application of numerical algorithms and/or computational mathematics. In some cases, these models require massive amounts of calculations (usually floating-point) and are often executed on supercomputers or distributed computing platforms.


== The computational scientist ==

The term computational scientist is used to describe someone skilled in scientific computing. This person is usually a scientist, an engineer or an applied mathematician who applies high-performance computing in different ways to advance the state-of-the-art in their respective applied disciplines in physics, chemistry or engineering.
Computational science is now commonly considered a third mode of science, complementing and adding to experimentation/observation and theory (see image on the right). Here, we define a system as a potential source of data, a experiment as a process of extracting data from a system by exerting it through its inputs and a model (M) for a system (S) and an experiment (E) as anything to which E can be applied in order to answer questions about S. A computational scientist should be capable of:
recognizing complex problems
adequately conceptualise the system containing these problems
design a framework of algorithms suitable for studying this system: the simulation
choose a suitable computing infrastructure (parallel computing/grid computing/supercomputers)
hereby, maximising the computational power of the simulation
assessing to what level the output of the simulation resembles the systems: the model is validated
adjust the conceptualisation of the system accordingly
repeat cycle until a suitable level of validation is obtained: the computational scientists trusts that the simulation generates adequately realistic results for the system, under the studied conditions
In fact, substantial effort in computational sciences has been devoted to the development of algorithms, the efficient implementation in programming languages, and validation of computational results. A collection of problems and solutions in computational science can be found in Steeb, Hardy, Hardy and Stoop (2004).
Philosophers of science addressed the question to what degree computational science qualifies as science, among them Humphreys and Gelfert They address the general question of epistemology: how do we gain insight from such computational science approaches. Tolk uses these insights to show the epistemological constraints of computer-based simulation research. As computational science uses mathematical models representing the underlying theory in executable form, in essence they apply modeling (theory building) and simulation (implementation and execution). While simulation and computational science are our most sophisticated way to express our knowledge and understanding, they also come with all constraints and limits already known for computational solutions.


== Applications of computational science ==
Problem domains for computational science/scientific computing include:


=== Urban complex systems ===
Now in 2015 over half the worlds population live in cities. By the middle of the 21st century, it is estimated that 75% of the world’s population will be urban. This urban growth is focused in the urban populations of developing counties where cities dwellers will more than double, increasing from 2.5 billion in 2009 to almost 5.2 billion in 2050. Cities are massive complex systems created by humans, made up of humans and governed by humans. Trying to predict, understand and somehow shape the development of cities in the future requires complexity thinking, and requires computational models and simulations to help mitigate challenges and possible disasters. The focus of research in urban complex systems is, through modelling and simulation, build greater understanding of city dynamics and help prepare for the coming urbanisation.


=== Computational finance ===

In today’s financial markets huge volumes of interdependent assets are traded by a large number of interacting market participants in different locations and time zones. Their behavior is of unprecedented complexity and the characterization and measurement of the risk inherent to these highly diverse set of instruments is typically based on complicated mathematical and computational models. Solving these models exactly in closed form, even at a single instrument level, is typically not possible, and therefore we have to look for efficient numerical algorithms. This has become even more urgent and complex recently, as the credit crisis has clearly demonstrated the role of cascading effects going from single instruments through portfolios of single institutions to even the interconnected trading network. Understanding this requires a multi-scale and holistic approach where interdependent risk factors such as market, credit and liquidity risk are modelled simultaneously and at different interconnected scales.


=== Computational biology ===

Exciting new developments in biotechnology are now revolutionizing biology and biomedical research. Examples of these techniques are high-throughput sequencing, high-throughput quantitative PCR, intra-cellular imaging, in-situ hybridization of gene expression, three-dimensional imaging techniques like Light Sheet Fluorescence Microscopy and Optical Projection, (micro)-Computer Tomography. Given the massive amounts of complicated data that is generated by these techniques, their meaningful interpretation, and even their storage, form major challenges calling for new approaches. Going beyond current bioinformatics approaches, computational biology needs to develop new methods to discover meaningful patterns in these large data sets. Model-based reconstruction of gene networks can be used to organize the gene expression data in systematic way and to guide future data collection. A major challenge here is to understand how gene regulation is controlling fundamental biological processes like biomineralisation and embryogenesis. The sub-processes like gene regulation, organic molecules interacting with the mineral deposition process, cellular processes, physiology and other processes at the tissue and environmental levels are linked. Rather than being directed by a central control mechanism, biomineralisation and embryogenesis can be viewed as an emergent behavior resulting from a complex system in which several sub-processes on very different temporal and spatial scales (ranging from nanometer and nanoseconds to meters and years) are connected into a multi-scale system. One of the few available options to understand such systems is by developing a multi-scale model of the system.


=== Complex systems theory ===

Using information theory, non-equilibrium dynamics and explicit simulations computational systems theory tries to uncover the true nature of complex adaptive systems.


=== Computational science in engineering ===

Computational science and engineering (CSE) is a relatively new discipline that deals with the development and application of computational models and simulations, often coupled with high-performance computing, to solve complex physical problems arising in engineering analysis and design (computational engineering) as well as natural phenomena (computational science). CSE has been described as the ""third mode of discovery"" (next to theory and experimentation). In many fields, computer simulation is integral and therefore essential to business and research. Computer simulation provides the capability to enter fields that are either inaccessible to traditional experimentation or where carrying out traditional empirical inquiries is prohibitively expensive. CSE should neither be confused with pure computer science, nor with computer engineering, although a wide domain in the former is used in CSE (e.g., certain algorithms, data structures, parallel programming, high performance computing) and some problems in the latter can be modeled and solved with CSE methods (as an application area).


== Methods and algorithms ==
Algorithms and mathematical methods used in computational science are varied. Commonly applied methods include:

Both historically and today, Fortran remains popular for most applications of scientific computing. Other programming languages and computer algebra systems commonly used for the more mathematical aspects of scientific computing applications include GNU Octave, Haskell, Julia, Maple, Mathematica, MATLAB, Python (with third-party SciPy library), Perl (with third-party PDL library), R, SciLab, and TK Solver. The more computationally intensive aspects of scientific computing will often use some variation of C or Fortran and optimized algebra libraries such as BLAS or LAPACK.
Computational science application programs often model real-world changing conditions, such as weather, air flow around a plane, automobile body distortions in a crash, the motion of stars in a galaxy, an explosive device, etc. Such programs might create a 'logical mesh' in computer memory where each item corresponds to an area in space and contains information about that space relevant to the model. For example, in weather models, each item might be a square kilometer; with land elevation, current wind direction, humidity, temperature, pressure, etc. The program would calculate the likely next state based on the current state, in simulated time steps, solving equations that describe how the system operates; and then repeat the process to calculate the next state.


== Conferences and journals ==
In the year 2001, the International Conference on Computational Science (ICCS) was first organised. Since then it has been organised yearly. ICCS is an A-rank conference in CORE classification.
The international Journal of Computational Science published its first issue in May 2010. A new initiative was launched in 2012, the Journal of Open Research Software. In 2015, ReSciencededicated to the replication of computational results has been started on GitHub.


== Education ==
At some institutions a specialization in scientific computation can be earned as a ""minor"" within another program (which may be at varying levels). However, there are increasingly many bachelor's, master's and doctoral programs in computational science. The joint degree programme master program computational science at the University of Amsterdam and the Vrije Universiteit was the first full academic degree offered in computational science, and started in 2004. In this programme, students:
learn to build computational models from real-life observations;
develop skills in turning these models into computational structures and in performing large-scale simulations;
learn theory that will give a firm basis for the analysis of complex systems;
learn to analyse the results of simulations in a virtual laboratory using advanced numerical algorithms.


== Related fields ==


== See also ==

Computer simulations in science
Computational science and engineering
Comparison of computer algebra systems
List of molecular modeling software
List of numerical analysis software
List of statistical packages
Timeline of scientific computing
Simulated reality
Extensions for Scientific Computation (XSC)


== References ==


== Additional sources ==
E. Gallopoulos and A. Sameh, ""CSE: Content and Product"". IEEE Computational Science and Engineering Magazine, 4(2):39–43 (1997)
G. Hager and G. Wellein, Introduction to High Performance Computing for Scientists and Engineers, Chapman and Hall (2010)
A.K. Hartmann, Practical Guide to Computer Simulations, World Scientific (2009)
Journal Computational Methods in Science and Technology (open access), Polish Academy of Sciences
Journal Computational Science and Discovery, Institute of Physics
R.H. Landau, C.C. Bordeianu, and M. Jose Paez, A Survey of Computational Physics: Introductory Computational Science, Princeton University Press (2008)


== External links ==
John von Neumann-Institut for Computing (NIC) at Juelich (Germany)
The National Center for Computational Science at Oak Ridge National Laboratory
Educational Materials for Undergraduate Computational Studies
Computational Science at the National Laboratories
Bachelor in Computational Science, University of Medellin, Colombia, South America
Simulation Optimization Systems (SOS) Research Laboratory, McMaster University, Hamilton, ON"
42,Gödel Prize,643342,23764,"The Gödel Prize is an annual prize for outstanding papers in the area of theoretical computer science, given jointly by European Association for Theoretical Computer Science (EATCS) and the Association for Computing Machinery Special Interest Group on Algorithms and Computational Theory (ACM SIGACT). The award is named in honor of Kurt Gödel. Gödel's connection to theoretical computer science is that he was the first to mention the ""P versus NP"" question, in a 1956 letter to John von Neumann in which Gödel asked whether a certain NP-complete problem could be solved in quadratic or linear time.
The Gödel Prize has been awarded since 1993. The prize is awarded either at STOC (ACM Symposium on Theory of Computing, one of the main North American conferences in theoretical computer science) or ICALP (International Colloquium on Automata, Languages and Programming, one of the main European conferences in the field). To be eligible for the prize, a paper must be published in a refereed journal within the last 14 (formerly 7) years. The prize includes a reward of US$5000.
The winner of the Prize is selected by a committee of six members. The EATCS President and the SIGACT Chair each appoint three members to the committee, to serve staggered three-year terms. The committee is chaired alternately by representatives of EATCS and SIGACT.


== Recipients ==


== Winning papers ==


== References ==
Prize website with list of winners"
43,Mexican International Conference on Artificial Intelligence,36494971,23605,"The Mexican International Conference on Artificial Intelligence (MICAI) is the name of an annual conference covering all areas of Artificial Intelligence (AI), held in Mexico. The first MICAI conference was held in 2000. The conference is attended every year by about two hundred of AI researchers and PhD students and 500−1000 local graduate students.


== Overview ==
MICAI is a high-level peer-reviewed international conference covering all areas of Artificial Intelligence. All editions of MICAI have been published in Springer Springer LNAI (N 1793, 2313, 2972, 3789, 4293, 4827, 5317, 5845, 6437–6438). Recent MICAI events (2006, 2007, 2008, 2009, and 2010) received over 300 submissions from over 40 countries each. The conference's scientific program includes keynote lectures, paper presentations, tutorials, panels, posters, and workshops. MICAI is organized by the Mexican Society for Artificial Intelligence (SMIA) in cooperation with various national institutions.
Their topics of interest include, but are not limited to: Applications of artificial intelligence, Automated theorem proving, Belief revision, Bioinformatics and Medical applications of artificial intelligence, Case-based reasoning, Common-sense reasoning, Computer vision and image processing, Constraint programming, Data mining, Expert systems and knowledge-based systems, Fuzzy logic, Genetic algorithms, Hybrid intelligent systems, Intelligent interfaces: multimedia, virtual reality, Intelligent organizations, Intelligent tutoring systems, Knowledge acquisition, Knowledge representation and knowledge management, Logic programming, Machine learning, Model-based reasoning, Multiagent systems and distributed artificial intelligence, Natural Language Processing, Neural Networks, Non-monotonic Reasoning, Ontologies, Pattern Recognition, Philosophical and methodological issues of artificial intelligence, Planning and scheduling, Qualitative reasoning, Robotics, Spatial and remporal reasoning, Uncertainty reasoning and probabilistic reasoning.


== Specific MICAI conferences ==
In the table below, the figures for the number of accepted papers and acceptance rate refer to the main proceedings volume and do not include supplemental proceedings volumes. The number of countries corresponds to submissions, not to accepted papers.


== Keynote speakers and program chairs ==
The following persons were honored by being selected by the organizers as keynote speakers or program chairs:


== Awards ==
The authors of the following papers received the Best Paper Award:


== See also ==
The list of computer science conferences contains other academic conferences in computer science.


== References ==


== External links ==
MICAI series website
Mexican Society for Artificial Intelligence (SMIA)"
44,Bentley–Ottmann algorithm,16329810,23334,"In computational geometry, the Bentley–Ottmann algorithm is a sweep line algorithm for listing all crossings in a set of line segments, i.e. it finds the intersection points (or, simply, intersections) of line segments. It extends the Shamos–Hoey algorithm, a similar previous algorithm for testing whether or not a set of line segments has any crossings. For an input consisting of 
  
    
      
        n
      
    
    {\displaystyle n}
   line segments with 
  
    
      
        k
      
    
    {\displaystyle k}
   crossings (or intersections), the Bentley–Ottmann algorithm takes time 
  
    
      
        
          
            O
          
        
        (
        (
        n
        +
        k
        )
        log
        ⁡
        n
        )
      
    
    {\displaystyle {\mathcal {O}}((n+k)\log n)}
  . In cases where 
  
    
      
        k
        =
        
          
            o
          
        
        
          (
          
            
              
                n
                
                  2
                
              
              
                log
                ⁡
                n
              
            
          
          )
        
      
    
    {\displaystyle k={\mathcal {o}}\left({\frac {n^{2}}{\log n}}\right)}
  , this is an improvement on a naïve algorithm that tests every pair of segments, which takes 
  
    
      
        Θ
        (
        
          n
          
            2
          
        
        )
      
    
    {\displaystyle \Theta (n^{2})}
  .
The algorithm was initially developed by Jon Bentley and Thomas Ottmann (1979); it is described in more detail in the textbooks Preparata & Shamos (1985), O'Rourke (1998), and de Berg et al. (2000). Although asymptotically faster algorithms are now known, the Bentley–Ottmann algorithm remains a practical choice due to its simplicity and low memory requirements.


== Overall strategy ==
The main idea of the Bentley–Ottmann algorithm is to use a sweep line approach, in which a vertical line L moves from left to right (or, e.g., from top to bottom) across the plane, intersecting the input line segments in sequence as it moves. The algorithm is described most easily in its general position, meaning:
No two line segment endpoints or crossings have the same x-coordinate
No line segment endpoint lies upon another line segment
No three line segments intersect at a single point.
In such a case, L will always intersect the input line segments in a set of points whose vertical ordering changes only at a finite set of discrete events. Thus, the continuous motion of L can be broken down into a finite sequence of steps, and simulated by an algorithm that runs in a finite amount of time.
There are two types of events that may happen during the course of this simulation. When L sweeps across an endpoint of a line segment s, the intersection of L with s is added to or removed from the vertically ordered set of intersection points. These events are easy to predict, as the endpoints are known already from the input to the algorithm. The remaining events occur when L sweeps across a crossing between (or intersection of) two line segments s and t. These events may also be predicted from the fact that, just prior to the event, the points of intersection of L with s and t are adjacent in the vertical ordering of the intersection points.
The Bentley–Ottmann algorithm itself maintains data structures representing the current vertical ordering of the intersection points of the sweep line with the input line segments, and a collection of potential future events formed by adjacent pairs of intersection points. It processes each event in turn, updating its data structures to represent the new set of intersection points.


== Data structures ==
In order to efficiently maintain the intersection points of the sweep line L with the input line segments and the sequence of future events, the Bentley–Ottmann algorithm maintains two data structures:
A binary search tree (the ""sweep line status tree""), containing the set of input line segments that cross L, ordered by the y-coordinates of the points where these segments cross L. The crossing points themselves are not represented explicitly in the binary search tree. The Bentley–Ottmann algorithm will insert a new segment s into this data structure when the sweep line L crosses the left endpoint p of this segment (i.e. the endpoint of the segment with the smallest x-coordinate, provided the sweep line L starts from the left, as explained above in this article). The correct position of segment s in the binary search tree may be determined by a binary search, each step of which tests whether p is above or below some other segment that is crossed by L. Thus, an insertion may be performed in logarithmic time. The Bentley–Ottmann algorithm will also delete segments from the binary search tree, and use the binary search tree to determine the segments that are immediately above or below other segments; these operations may be performed using only the tree structure itself without reference to the underlying geometry of the segments.
A priority queue (the ""event queue""), used to maintain a sequence of potential future events in the Bentley–Ottmann algorithm. Each event is associated with a point p in the plane, either a segment endpoint or a crossing point, and the event happens when line L sweeps over p. Thus, the events may be prioritized by the x-coordinates of the points associated with each event. In the Bentley–Ottmann algorithm, the potential future events consist of line segment endpoints that have not yet been swept over, and the points of intersection of pairs of lines containing pairs of segments that are immediately above or below each other.
The algorithm does not need to maintain explicitly a representation of the sweep line L or its position in the plane. Rather, the position of L is represented indirectly: it is the vertical line through the point associated with the most recently processed event.
The binary search tree may be any balanced binary search tree data structure, such as a red-black tree; all that is required is that insertions, deletions, and searches take logarithmic time. Similarly, the priority queue may be a binary heap or any other logarithmic-time priority queue; more sophisticated priority queues such as a Fibonacci heap are not necessary. Note that the space complexity of the priority queue depends on the data structure used to implement it.


== Detailed algorithm ==
The Bentley–Ottmann algorithm performs the following steps.
Initialize a priority queue Q of potential future events, each associated with a point in the plane and prioritized by the x-coordinate of the point. So, initially, Q contains an event for each of the endpoints of the input segments.
Initialize a self-balancing binary search tree T of the line segments that cross the sweep line L, ordered by the y-coordinates of the crossing points. Initially, T is empty. (Even though the line sweep L is not explicitly represented, it may be helpful to imagine it as a vertical line which, initially, is at the left of all input segments.)
While Q is nonempty, find and remove the event from Q associated with a point p with minimum x-coordinate. Determine what type of event this is and process it according to the following case analysis:
If p is the left endpoint of a line segment s, insert s into T. Find the segments r and t that are respectively immediately below and above s in T (if they exist); if their crossing forms a potential future event in the event queue, remove it. If s crosses r or t, add those crossing points as potential future events in the event queue.
If p is the right endpoint of a line segment s, remove s from T. Find the segments r and t that (prior to the removal of s) were respectively immediately above and below it in T (if they exist). If r and t cross, add that crossing point as a potential future event in the event queue.
If p is the crossing point of two segments s and t (with s below t to the left of the crossing), swap the positions of s and t in T. Find the segments r and u (if they exist) that are immediately below and above t and s respectively (after the swap). Remove any crossing points rs and tu from the event queue, and, if r and t cross or s and u cross, add those crossing points to the event queue.


== Analysis ==
The algorithm processes one event per segment endpoint or crossing point, in the sorted order of the 
  
    
      
        x
      
    
    {\displaystyle x}
  -coordinates of these points, as may be proven by induction. This follows because, once the 
  
    
      
        i
      
    
    {\displaystyle i}
  th event has been processed, the next event (if it is a crossing point) must be a crossing of two segments that are adjacent in the ordering of the segments represented by 
  
    
      
        T
      
    
    {\displaystyle T}
  , and because the algorithm maintains all crossings between adjacent segments as potential future events in the event queue; therefore, the correct next event will always be present in the event queue. As a consequence, it correctly finds all crossings of input line segments, the problem it was designed to solve.
The Bentley–Ottmann algorithm processes a sequence of 
  
    
      
        2
        n
        +
        k
      
    
    {\displaystyle 2n+k}
   events, where 
  
    
      
        n
      
    
    {\displaystyle n}
   denotes the number of input line segments and 
  
    
      
        k
      
    
    {\displaystyle k}
   denotes the number of crossings. Each event is processed by a constant number of operations in the binary search tree and the event queue, and (because it contains only segment endpoints and crossings between adjacent segments) the event queue never contains more than 
  
    
      
        3
        n
      
    
    {\displaystyle 3n}
   events. All operations take time 
  
    
      
        
          
            O
          
        
        (
        log
        ⁡
        n
        )
      
    
    {\displaystyle {\mathcal {O}}(\log n)}
  . Hence the total time for the algorithm is 
  
    
      
        
          
            O
          
        
        (
        (
        n
        +
        k
        )
        log
        ⁡
        n
        )
      
    
    {\displaystyle {\mathcal {O}}((n+k)\log n)}
  .
If the crossings found by the algorithm do not need to be stored once they have been found, the space used by the algorithm at any point in time is 
  
    
      
        
          
            O
          
        
        (
        n
        )
      
    
    {\displaystyle {\mathcal {O}}(n)}
  : each of the 
  
    
      
        n
      
    
    {\displaystyle n}
   input line segments corresponds to at most one node of the binary search tree T, and as stated above the event queue contains at most 
  
    
      
        3
        n
      
    
    {\displaystyle 3n}
   elements. This space bound is due to Brown (1981); the original version of the algorithm was slightly different (it did not remove crossing events from 
  
    
      
        Q
      
    
    {\displaystyle Q}
   when some other event causes the two crossing segments to become non-adjacent) causing it to use more space.
Chen & Chan (2003) described a highly space-efficient version of the Bentley–Ottmann algorithm that encodes most of its information in the ordering of the segments in an array representing the input, requiring only 
  
    
      
        
          
            O
          
        
        (
        
          log
          
            2
          
        
        ⁡
        n
        )
      
    
    {\displaystyle {\mathcal {O}}(\log ^{2}n)}
   additional memory cells. However, in order to access the encoded information, the algorithm is slowed by a logarithmic factor.


== Special position ==
The algorithm description above assumes that line segments are not vertical, that line segment endpoints do not lie on other line segments, that crossings are formed by only two line segments, and that no two event points have the same x-coordinate. In other words, it doesn't take into account corner cases, i.e. it assumes general position of the endpoints of the input segments. However, these general position assumptions are not reasonable for most applications of line segment intersection. Bentley & Ottmann (1979) suggested perturbing the input slightly to avoid these kinds of numerical coincidences, but did not describe in detail how to perform these perturbations. de Berg et al. (2000) describe in more detail the following measures for handling special-position inputs:
Break ties between event points with the same x-coordinate by using the y-coordinate. Events with different y-coordinates are handled as before. This modification handles both the problem of multiple event points with the same x-coordinate, and the problem of vertical line segments: the left endpoint of a vertical segment is defined to be the one with the lower y-coordinate, and the steps needed to process such a segment are essentially the same as those needed to process a non-vertical segment with a very high slope.
Define a line segment to be a closed set, containing its endpoints. Therefore, two line segments that share an endpoint, or a line segment that contains an endpoint of another segment, both count as an intersection of two line segments.
When multiple line segments intersect at the same point, create and process a single event point for that intersection. The updates to the binary search tree caused by this event may involve removing any line segments for which this is the right endpoint, inserting new line segments for which this is the left endpoint, and reversing the order of the remaining line segments containing this event point. The output from the version of the algorithm described by de Berg et al. (2000) consists of the set of intersection points of line segments, labeled by the segments they belong to, rather than the set of pairs of line segments that intersect.
A similar approach to degeneracies was used in the LEDA implementation of the Bentley–Ottmann algorithm.


== Numerical precision issues ==
For the correctness of the algorithm, it is necessary to determine without approximation the above-below relations between a line segment endpoint and other line segments, and to correctly prioritize different event points. For this reason it is standard to use integer coordinates for the endpoints of the input line segments, and to represent the rational number coordinates of the intersection points of two segments exactly, using arbitrary-precision arithmetic. However, it may be possible to speed up the calculations and comparisons of these coordinates by using floating point calculations and testing whether the values calculated in this way are sufficiently far from zero that they may be used without any possibility of error. The exact arithmetic calculations required by a naïve implementation of the Bentley–Ottmann algorithm may require five times as many bits of precision as the input coordinates, but Boissonat & Preparata (2000) describe modifications to the algorithm that reduce the needed amount of precision to twice the number of bits as the input coordinates.


== Faster algorithms ==
The O(n log n) part of the time bound for the Bentley–Ottmann algorithm is necessary, as there are matching lower bounds for the problem of detecting intersecting line segments in algebraic decision tree models of computation. However, the dependence on k, the number of crossings, can be improved. Clarkson (1988) and Mulmuley (1988) both provided randomized algorithms for constructing the planar graph whose vertices are endpoints and crossings of line segments, and whose edges are the portions of the segments connecting these vertices, in expected time O(n log n + k), and this problem of arrangement construction was solved deterministically in the same O(n log n + k) time bound by Chazelle & Edelsbrunner (1992). However, constructing this arrangement as a whole requires space O(n + k), greater than the O(n) space bound of the Bentley–Ottmann algorithm; Balaban (1995) described a different algorithm that lists all intersections in time O(n log n + k) and space O(n).
If the input line segments and their endpoints form the edges and vertices of a connected graph (possibly with crossings), the O(n log n) part of the time bound for the Bentley–Ottmann algorithm may also be reduced. As Clarkson, Cole & Tarjan (1992) show, in this case there is a randomized algorithm for solving the problem in expected time O(n log* n + k), where log* denotes the iterated logarithm, a function much more slowly growing than the logarithm. A closely related randomized algorithm of Eppstein, Goodrich & Strash (2009) solves the same problem in time O(n + k log(i)n) for any constant i, where log(i) denotes the function obtained by iterating the logarithm function i times. The first of these algorithms takes linear time whenever k is larger than n by a log(i)n factor, for any constant i, while the second algorithm takes linear time whenever k is smaller than n by a log(i)n factor. Both of these algorithms involve applying the Bentley–Ottmann algorithm to small random samples of the input.


== Notes ==


== References ==
Balaban, I. J. (1995), ""An optimal algorithm for finding segments intersections"", Proc. 11th ACM Symp. Computational Geometry, pp. 211–219, doi:10.1145/220279.220302 .
Bartuschka, U.; Mehlhorn, K.; Näher, S. (1997), ""A robust and efficient implementation of a sweep line algorithm for the straight line segment intersection problem"", in Italiano, G. F.; Orlando, S., Proc. Worksh. Algorithm Engineering .
Bentley, J. L.; Ottmann, T. A. (1979), ""Algorithms for reporting and counting geometric intersections"", IEEE Transactions on Computers, C–28 (9): 643–647, doi:10.1109/TC.1979.1675432 .
de Berg, Mark; van Kreveld, Marc; Overmars, Mark; Schwarzkopf, Otfried (2000), ""Chapter 2: Line segment intersection"", Computational Geometry (2nd ed.), Springer-Verlag, pp. 19–44, ISBN 978-3-540-65620-3 .
Boissonat, J.-D.; Preparata, F. P. (2000), ""Robust plane sweep for intersecting segments"" (PDF), SIAM Journal on Computing, 29 (5): 1401–1421, doi:10.1137/S0097539797329373 .
Brown, K. Q. (1981), ""Comments on ""Algorithms for Reporting and Counting Geometric Intersections"""", IEEE Transactions on Computers, C–30 (2): 147, doi:10.1109/tc.1981.6312179 .
Chazelle, Bernard; Edelsbrunner, Herbert (1992), ""An optimal algorithm for intersecting line segments in the plane"", Journal of the ACM, 39 (1): 1–54, doi:10.1145/147508.147511 .
Chen, E. Y.; Chan, T. M. (2003), ""A space-efficient algorithm for segment intersection"", Proc. 15th Canadian Conference on Computational Geometry (PDF) .
Clarkson, K. L. (1988), ""Applications of random sampling in computational geometry, II"", Proc. 4th ACM Symp. Computational Geometry, pp. 1–11, doi:10.1145/73393.73394 .
Clarkson, K. L.; Cole, R.; Tarjan, R. E. (1992), ""Randomized parallel algorithms for trapezoidal diagrams"", International Journal of Computational Geometry and Applications, 2 (2): 117–133, doi:10.1142/S0218195992000081 . Corrigendum, 2 (3): 341–343.
Eppstein, D.; Goodrich, M.; Strash, D. (2009), ""Linear-time algorithms for geometric graphs with sublinearly many crossings"", Proc. 20th ACM-SIAM Symp. Discrete Algorithms (SODA 2009), pp. 150–159, arXiv:0812.0893  .
Mulmuley, K. (1988), ""A fast planar partition algorithm, I"", Proc. 29th IEEE Symp. Foundations of Computer Science (FOCS 1988), pp. 580–589, doi:10.1109/SFCS.1988.21974 .
O'Rourke, J. (1998), ""Section 7.7: Intersection of segments"", Computational Geometry in C (2nd ed.), Cambridge University Press, pp. 263–265, ISBN 978-0-521-64976-6 .
Preparata, F. P.; Shamos, M. I. (1985), ""Section 7.2.3: Intersection of line segments"", Computational Geometry: An Introduction, Springer-Verlag, pp. 278–287 .
Pach, J.; Sharir, M. (1991), ""On vertical visibility in arrangements of segments and the queue size in the Bentley–Ottmann line sweeping algorithm"", SIAM Journal on Computing, 20 (3): 460–470, doi:10.1137/0220029, MR 1094525 .
Shamos, M. I.; Hoey, Dan (1976), ""Geometric intersection problems"", 17th IEEE Conf. Foundations of Computer Science (FOCS 1976), pp. 208–215, doi:10.1109/SFCS.1976.16 .


== External links ==
Smid, Michiel (2003), Computing intersections in a set of line segments: the Bentley–Ottmann algorithm (PDF) ."
45,British Machine Vision Conference,42973483,21498,"The British Machine Vision Conference (BMVC) is the British Machine Vision Association (BMVA) annual conference on machine vision, image processing, and pattern recognition. It is one of the major international conferences on computer vision and related areas, held in UK. Particularly, BMVC is ranked as A2 by Qualis, and B by ERA. The upcoming 27th BMVC will be hosted by Imperial College London in September 2017.
BMVC is a successor of the older British Alvey Vision Conference (AVC), which had run in years 1985 (University of Sussex), 1987 (University of Cambridge), 1988 (University of Manchester) and 1989 (University of Reading). The British Machine Vision Conference has replaced AVC in 1990, when BMVA was founded. Despite starting as a national conference, it is now a prestigious major international venue with high level of foreign participation (in 2013, 84% of accepted papers were completely from outside the UK and another 4% with mixed authorships) and high stress on quality of publications (in 2013, the acceptance rate was only 30%). BMVC is a relatively small conference, with the number of accepted publications (and therefore number of talks and posters) around 100.


== Usual programme ==
BMVC is a single-track conference held usually over the course of one week in early September. On Monday, there are usually one or several tutorials, followed by the main conference in the following three days. A typical conference day consists of a keynote talk, two or three oral sessions and a poster session. Thursday's programme tends to be shorter. The conference usually includes a banquet and a reception. The main conference is followed by a one-day student workshop on Friday, which provides an opportunity for doctoral students to present their work and interact with their peers.


== Awards ==
At BMVC, there are several awards given. Besides the Best Scientific Paper Award (formerly known as Science Prize), there is Best Industrial Paper Award (formerly known as Industry Prize), Best Poster Award and others. The awards recipients are tabulated below. Additionally, other BMVA prizes such as BMVA Distinguished Fellowship or Sullivan Prize are awarded during BMVC.
Other awards
1998 Demonstration Prize. Active object recognition in parametric eigenspace. M. Prantl.
2000 Demonstration Prize. A hierarchical model of dynamics for tracking people with a single video camera. I.A. Karaulova, P.M. Hall and A.D. Marshall.
2001 Demonstration Prize. Video image enhancement for terrestrial, aerial and underwater environments. J. Oakley
2001 Best Model Based Vision Paper. An Information Theoretic Approach to Statistical Shape Modelling. R.H. Davies, T.F. Cootes, C.J. Twining and C.J. Taylor.
2001 Presentation Prize. Robust Registration of 2D and 3D Point Sets. A. Fitzgibbon.
2002 Demonstration Prize. Real time robust template matching. F. Jurie and M. Dhome.
2002 Best Model Based Vision Paper. Real time gesture recognition using deterministic boosting. R. Lockton and A. Fitzgibbon.
2002 Work that most deserves help with exploitation. Orientation correlation. A.J. Fitch, A. Kadyrov, W.J. Christmas and J. Kittler.
2003 Demonstration Prize. Visual golf club tracking for enhanced swing analysis. N. Gehrig, V. Lepetit and P. Fua.
2003 Best Model Based Vision Paper. Modelling talking head behaviour. C.A. Hack and C.J. Taylor.
2004 Demonstration Prize. Interactions between hand and wearable camera in 2D and 3D environments. A. Davison.
2004 Best Model Based Vision Paper. A Bayesian Occlusion Model for Sequential Object Matching. T. Tamminen and J. Lampinen.
2007 Best Security Paper Prize. Gender Classification using Shape from Shading. J. Wu, W.A.P. Smith and E.R. Hancock.
2008 Best Security Paper Prize. Crowd Detection from Still Images. O. Arandjelovic.
2008 Highly Commended Reviewers. J.-M. Geusebroek, B. Leibe, A. Shahrokni, J. Sivic, J. Starck.
2010 Best Student Paper Prize. Motion Coherent Tracking with Multi-label MRF optimization. D. Tsai, M. Flagg and J. Rehg.
2010 Best Supplementary Material Prize. Manifold Learning for ToF-based Human Body Tracking and Activity Recognition. L. Schwarz, D. Mateus, V. Castaneda and N. Navab.
2011 Best Impact Paper Prize. Branch and rank: non-linear object detection. A. Lehmann, P. Gehler and L. Van Gool.
2011 Best Supplementary Material Prize. Skeletal graph based human pose estimation in real-time. M. Straka, S. Hauswiesner, M. Rüther and H. Bischof.
2011 Student Workshop Prize. Model Constraints for Non-Rigid Structure from Motion. L. Tao, B. Matuszewski and S. Mein.
2012 Best Impact Paper Prize. PMBP: PatchMatch Belief Propagation for Correspondence Field Estimation. F. Besse, C. Rother, A. Fitzgibbon and J. Kautz.
2012 Mark Everingham Prize for Rigorous Evaluation. Tom-vs.-Pete Classifiers and Identity-Preserving Alignment for Face Verification. T. Berg and P. Belhumeu.
2012 Best Demonstration Prize. Online Feedback for Structure-from-Motion Image Acquisition. C. Hoppe, M. Klopschitz, M. Rumpler, A. Wendel, S. Kluckner, H. Bischof and G. Reitmayr.
2012 Best Video Prize. Automatic and Efficient Long Term Arm and Hand Tracking for Continuous Sign Language TV Broadcasts. Tomas Pfister, J. Charles, M. Everingham and A. Zisserman.
2013 Maria Petrou Prize for Invariance in Computer Vision. The Complete Rank Transform: A Tool for Accurate and Morphologically Invariant Matching of Structures. O. Demetz, D. Hafner and J. Weickert.
2014 Best Student Workshop Paper Prize. Gong Interactive Shadow Removal and Ground Truth for Variable Scene Categories. H. Gong and D. Cosker.


== See also ==
BMVA
BMVA Summer School
ICCV
CVPR
ECCV


== References =="
46,Integer factorization,15491,21146,"In number theory, integer factorization is the decomposition of a composite number into a product of smaller integers. If these integers are further restricted to prime numbers, the process is called prime factorization.
When the numbers are sufficiently large, no efficient, non-quantum integer factorization algorithm is known. An effort by several researchers, concluded in 2009, to factor a 232-digit number (RSA-768) utilizing hundreds of machines took two years and the researchers estimated that a 1024-bit RSA modulus would take about a thousand times as long. However, it has not been proven that no efficient algorithm exists. The presumed difficulty of this problem is at the heart of widely used algorithms in cryptography such as RSA. Many areas of mathematics and computer science have been brought to bear on the problem, including elliptic curves, algebraic number theory, and quantum computing.
Not all numbers of a given length are equally hard to factor. The hardest instances of these problems (for currently known techniques) are semiprimes, the product of two prime numbers. When they are both large, for instance more than two thousand bits long, randomly chosen, and about the same size (but not too close, e.g., to avoid efficient factorization by Fermat's factorization method), even the fastest prime factorization algorithms on the fastest computers can take enough time to make the search impractical; that is, as the number of digits of the primes being factored increases, the number of operations required to perform the factorization on any computer increases drastically.
Many cryptographic protocols are based on the difficulty of factoring large composite integers or a related problem—for example, the RSA problem. An algorithm that efficiently factors an arbitrary integer would render RSA-based public-key cryptography insecure.


== Prime decomposition ==

By the fundamental theorem of arithmetic, every positive integer has a unique prime factorization. (By convention 1 is the empty product.) If the integer is prime then it can be recognized as such in polynomial time. If composite however, the theorem gives no insight into how to obtain the factors.
Given a general algorithm for integer factorization, any integer can be factored down to its constituent prime factors simply by repeated application of this algorithm. The situation is more complicated with special-purpose factorization algorithms, whose benefits may not be realized as well or even at all with the factors produced during decomposition. For example, if N = 10 × p × q where p < q are very large primes, trial division will quickly produce the factors 2 and 5 but will take p divisions to find the next factor. As a contrasting example, if N is the product of the primes 13729, 1372933, and 18848997161, where 13729 × 1372933 = 18848997157, Fermat's factorization method will start out with a = ⌈√N⌉ = 18848997159 which immediately yields b = √a2 − N = √4 = 2 and hence the factors a − b = 18848997157 and a + b = 18848997161. While these are easily recognized as respectively composite and prime, Fermat's method will take much longer to factorize the composite one because the starting value of ⌈√18848997157⌉ = 137292 for a is nowhere near 1372933.


== Current state of the art ==

Among the b-bit numbers, the most difficult to factor in practice using existing algorithms are those that are products of two primes of similar size. For this reason, these are the integers used in cryptographic applications. The largest such semiprime yet factored was RSA-768, a 768-bit number with 232 decimal digits, on December 12, 2009. This factorization was a collaboration of several research institutions, spanning two years and taking the equivalent of almost 2000 years of computing on a single-core 2.2 GHz AMD Opteron. Like all recent factorization records, this factorization was completed with a highly optimized implementation of the general number field sieve run on hundreds of machines.


=== Difficulty and complexity ===
No algorithm has been published that can factor all integers in polynomial time, i.e., that can factor b-bit numbers in time O(bk) for some constant k. Neither the existence nor non-existence of such algorithms has been proved, but it is generally suspected that they do not exist and hence that the problem is not in class P. The problem is clearly in class NP but has not been proved to be or not be NP-complete. It is generally suspected not to be NP-complete.
There are published algorithms that are faster than O((1+ε)b) for all positive ε, i.e., sub-exponential. The best published asymptotic running time is for the general number field sieve (GNFS) algorithm, which, for a b-bit number n, is:

  
    
      
        O
        
          (
          
            exp
            ⁡
            
              
                
                  
                    
                      64
                      9
                    
                  
                  b
                  (
                  log
                  ⁡
                  b
                  
                    )
                    
                      2
                    
                  
                
                
                  3
                
              
            
          
          )
        
        .
      
    
    {\displaystyle O\left(\exp {\sqrt[{3}]{{\frac {64}{9}}b(\log b)^{2}}}\right).}
  
For current computers, GNFS is the best published algorithm for large n (more than about 100 digits). For a quantum computer, however, Peter Shor discovered an algorithm in 1994 that solves it in polynomial time. This will have significant implications for cryptography if quantum computation becomes scalable. Shor's algorithm takes only O(b3) time and O(b) space on b-bit number inputs. In 2001, the first seven-qubit quantum computer became the first to run Shor's algorithm. It factored the number 15.
When discussing what complexity classes the integer factorization problem falls into, it is necessary to distinguish two slightly different versions of the problem:
The function problem version: given an integer N, find an integer d with 1 < d < N that divides N (or conclude that N is prime). This problem is trivially in FNP and it's not known whether it lies in FP or not. This is the version solved by practical implementations.
The decision problem version: given an integer N and an integer M with 1 < M < N, does N have a factor d with 1 < d ≤ M? This version is useful because most well studied complexity classes are defined as classes of decision problems, not function problems.
For √N ≤ M < N, the decision problem is equivalent to asking if N is not prime.
An algorithm for either version provides one for the other. Repeated application of the function problem (applied to d and N/d, and their factors, if needed) will eventually provide either a factor of N no larger than M or a factorization into primes all greater than M. All known algorithms for the decision problem work in this way. Hence it is only of theoretical interest that, with at most log N queries using an algorithm for the decision problem, one would isolate a factor of N (or prove it prime) by binary search.
It is not known exactly which complexity classes contain the decision version of the integer factorization problem. It is known to be in both NP and co-NP. This is because both YES and NO answers can be verified in polynomial time. An answer of YES can be certified by exhibiting a factorization N = d(N/d) with d ≤ M. An answer of NO can be certified by exhibiting the factorization of N into distinct primes, all larger than M. We can verify their primality using the AKS primality test and that their product is N by multiplication. The fundamental theorem of arithmetic guarantees that there is only one possible string that will be accepted (providing the factors are required to be listed in order), which shows that the problem is in both UP and co-UP. It is known to be in BQP because of Shor's algorithm. It is suspected to be outside of all three of the complexity classes P, NP-complete, and co-NP-complete. It is therefore a candidate for the NP-intermediate complexity class. If it could be proved that it is in either NP-Complete or co-NP-Complete, that would imply NP = co-NP. That would be a very surprising result, and therefore integer factorization is widely suspected to be outside both of those classes. Many people have tried to find classical polynomial-time algorithms for it and failed, and therefore it is widely suspected to be outside P.
In contrast, the decision problem ""is N a composite number?"" (or equivalently: ""is N a prime number?"") appears to be much easier than the problem of actually finding the factors of N. Specifically, the former can be solved in polynomial time (in the number n of digits of N) with the AKS primality test. In addition, there are a number of probabilistic algorithms that can test primality very quickly in practice if one is willing to accept the vanishingly small possibility of error. The ease of primality testing is a crucial part of the RSA algorithm, as it is necessary to find large prime numbers to start with.


== Factoring algorithms ==


=== Special-purpose ===
A special-purpose factoring algorithm's running time depends on the properties of the number to be factored or on one of its unknown factors: size, special form, etc. Exactly what the running time depends on varies between algorithms.
An important subclass of special-purpose factoring algorithms is the Category 1 or First Category algorithms, whose running time depends on the size of smallest prime factor. Given an integer of unknown form, these methods are usually applied before general-purpose methods to remove small factors. For example, trial division is a Category 1 algorithm.
Trial division
Wheel factorization
Pollard's rho algorithm
Algebraic-group factorisation algorithms, among which are Pollard's p − 1 algorithm, Williams' p + 1 algorithm, and Lenstra elliptic curve factorization
Fermat's factorization method
Euler's factorization method
Special number field sieve


=== General-purpose ===
A general-purpose factoring algorithm, also known as a Category 2, Second Category, or Kraitchik family algorithm (after Maurice Kraitchik), has a running time which depends solely on the size of the integer to be factored. This is the type of algorithm used to factor RSA numbers. Most general-purpose factoring algorithms are based on the congruence of squares method.
Dixon's algorithm
Continued fraction factorization (CFRAC)
Quadratic sieve
Rational sieve
General number field sieve
Shanks' square forms factorization (SQUFOF)


=== Other notable algorithms ===
Shor's algorithm, for quantum computers


== Heuristic running time ==
In number theory, there are many integer factoring algorithms that heuristically have expected running time

  
    
      
        
          L
          
            n
          
        
        
          [
          
            
              
                
                  1
                  2
                
              
            
            ,
            1
            +
            o
            (
            1
            )
          
          ]
        
        =
        
          e
          
            (
            1
            +
            o
            (
            1
            )
            )
            
              
                (
                log
                ⁡
                n
                )
                (
                log
                ⁡
                log
                ⁡
                n
                )
              
            
          
        
      
    
    {\displaystyle L_{n}\left[{\tfrac {1}{2}},1+o(1)\right]=e^{(1+o(1)){\sqrt {(\log n)(\log \log n)}}}}
  
in big O and L-notation. Some examples of those algorithms are the elliptic curve method and the quadratic sieve. Another such algorithm is the class group relations method proposed by Schnorr, Seysen, and Lenstra, that is proved under the assumption of the Generalized Riemann Hypothesis (GRH).


== Rigorous running time ==
The Schnorr-Seysen-Lenstra probabilistic algorithm has been rigorously proven by Lenstra and Pomerance to have expected running time 
  
    
      
        
          L
          
            n
          
        
        
          [
          
            
              
                
                  1
                  2
                
              
            
            ,
            1
            +
            o
            (
            1
            )
          
          ]
        
      
    
    {\displaystyle L_{n}\left[{\tfrac {1}{2}},1+o(1)\right]}
   by replacing the GRH assumption with the use of multipliers. The algorithm uses the class group of positive binary quadratic forms of discriminant Δ denoted by GΔ. GΔ is the set of triples of integers (a, b, c) in which those integers are relative prime.


=== Schnorr-Seysen-Lenstra Algorithm ===
Given an integer n that will be factored, where n is an odd positive integer greater than a certain constant. In this factoring algorithm the discriminant Δ is chosen as a multiple of n, Δ = −dn, where d is some positive multiplier. The algorithm expects that for one d there exist enough smooth forms in GΔ. Lenstra and Pomerance show that the choice of d can be restricted to a small set to guarantee the smoothness result.
Denote by PΔ the set of all primes q with Kronecker symbol 
  
    
      
        
          (
          
            
              
                Δ
                q
              
            
          
          )
        
        =
        1
      
    
    {\displaystyle \left({\tfrac {\Delta }{q}}\right)=1}
  . By constructing a set of generators of GΔ and prime forms fq of GΔ with q in PΔ a sequence of relations between the set of generators and fq are produced. The size of q can be bounded by 
  
    
      
        
          c
          
            0
          
        
        (
        log
        ⁡
        
          |
        
        Δ
        
          |
        
        
          )
          
            2
          
        
      
    
    {\displaystyle c_{0}(\log |\Delta |)^{2}}
   for some constant 
  
    
      
        
          c
          
            0
          
        
      
    
    {\displaystyle c_{0}}
  .
The relation that will be used is a relation between the product of powers that is equal to the neutral element of GΔ. These relations will be used to construct a so-called ambiguous form of GΔ, which is an element of GΔ of order dividing 2. By calculating the corresponding factorization of Δ and by taking a gcd, this ambiguous form provides the complete prime factorization of n. This algorithm has these main steps:
Let n be the number to be factored.
Let Δ be a negative integer with Δ = −dn, where d is a multiplier and Δ is the negative discriminant of some quadratic form.
Take the t first primes 
  
    
      
        
          p
          
            1
          
        
        =
        2
        ,
        
          p
          
            2
          
        
        =
        3
        ,
        
          p
          
            3
          
        
        =
        5
        ,
        …
        ,
        
          p
          
            t
          
        
      
    
    {\displaystyle p_{1}=2,p_{2}=3,p_{3}=5,\dots ,p_{t}}
  , for some 
  
    
      
        t
        ∈
        
          
            N
          
        
      
    
    {\displaystyle t\in {\mathbb {N} }}
  .
Let 
  
    
      
        
          f
          
            q
          
        
      
    
    {\displaystyle f_{q}}
   be a random prime form of GΔ with 
  
    
      
        
          (
          
            
              
                Δ
                q
              
            
          
          )
        
        =
        1
      
    
    {\displaystyle \left({\tfrac {\Delta }{q}}\right)=1}
  .
Find a generating set X of GΔ
Collect a sequence of relations between set X and {fq : q ∈ PΔ} satisfying: 
  
    
      
        
          (
          
            
              ∏
              
                x
                ∈
                
                  X
                  

                  
                
              
            
            
              x
              
                r
                (
                x
                )
              
            
          
          )
        
        .
        
          (
          
            
              ∏
              
                q
                ∈
                
                  P
                  
                    Δ
                  
                
              
            
            
              f
              
                q
              
              
                t
                (
                q
                )
              
            
          
          )
        
        =
        1
      
    
    {\displaystyle \left(\prod _{x\in X_{}}x^{r(x)}\right).\left(\prod _{q\in P_{\Delta }}f_{q}^{t(q)}\right)=1}
  
Construct an ambiguous form 
  
    
      
        (
        a
        ,
        b
        ,
        c
        )
      
    
    {\displaystyle (a,b,c)}
   that is an element f ∈ GΔ of order dividing 2 to obtain a coprime factorization of the largest odd divisor of Δ in which 
  
    
      
        Δ
        =
        −
        4
        a
        c
        
           or 
        
        a
        (
        a
        −
        4
        c
        )
        
           or 
        
        (
        b
        −
        2
        a
        )
        (
        b
        +
        2
        a
        )
      
    
    {\displaystyle \Delta =-4ac{\text{ or }}a(a-4c){\text{ or }}(b-2a)(b+2a)}
  
If the ambiguous form provides a factorization of n then stop, otherwise find another ambiguous form until the factorization of n is found. In order to prevent useless ambiguous forms from generating, build up the 2-Sylow group Sll2(Δ) of G(Δ).
To obtain an algorithm for factoring any positive integer, it is necessary to add a few steps to this algorithm such as trial division, and the Jacobi sum test.


=== Expected running time ===
The algorithm as stated is a probabilistic algorithm as it makes random choices. Its expected running time is at most 
  
    
      
        
          L
          
            n
          
        
        
          [
          
            
              
                
                  1
                  2
                
              
            
            ,
            1
            +
            o
            (
            1
            )
          
          ]
        
      
    
    {\displaystyle L_{n}\left[{\tfrac {1}{2}},1+o(1)\right]}
  .


== See also ==
Canonical representation of a positive integer
Factorization
Multiplicative partition
Partition (number theory) – a way of writing a number as a sum of positive integers.


== Notes ==


== References ==
Richard Crandall and Carl Pomerance (2001). Prime Numbers: A Computational Perspective. Springer. ISBN 0-387-94777-9.  Chapter 5: Exponential Factoring Algorithms, pp. 191–226. Chapter 6: Subexponential Factoring Algorithms, pp. 227–284. Section 7.4: Elliptic curve method, pp. 301–313.
Donald Knuth. The Art of Computer Programming, Volume 2: Seminumerical Algorithms, Third Edition. Addison-Wesley, 1997. ISBN 0-201-89684-2. Section 4.5.4: Factoring into Primes, pp. 379–417.
Samuel S. Wagstaff, Jr. (2013). The Joy of Factoring. Providence, RI: American Mathematical Society. ISBN 978-1-4704-1048-3. .
Warren Jr., Henry S. (2013). Hacker's Delight (2 ed.). Addison Wesley - Pearson Education, Inc. ISBN 978-0-321-84268-8. 


== External links ==
msieve - SIQS and NFS - has helped complete some of the largest public factorizations known
Richard P. Brent, ""Recent Progress and Prospects for Integer Factorisation Algorithms"", Computing and Combinatorics"", 2000, pp. 3–22. download
Manindra Agrawal, Neeraj Kayal, Nitin Saxena, ""PRIMES is in P."" Annals of Mathematics 160(2): 781-793 (2004). August 2005 version PDF
Eric W. Weisstein, “RSA-640 Factored” MathWorld Headline News, November 8, 2005"
47,Computable topology,36075414,20748,"Computable topology is a discipline in mathematics that studies the topological and algebraic structure of computation. Computable topology is not to be confused with algorithmic or computational topology, which studies the application of computation to topology.


== Topology of lambda calculus ==
As shown by Alan Turing and Alonzo Church, the λ-calculus is strong enough to describe all mechanically computable functions (see Church–Turing thesis). Lambda-calculus is thus effectively a programming language, from which other languages can be built. For this reason when considering the topology of computation it is common to focus on the topology of λ-calculus. Note that this is not necessarily a complete description of the topology of computation, since functions which are equivalent in the Church-Turing sense may still have different topologies.
The topology of λ-calculus is the Scott topology, and when restricted to continuous functions the type free λ-calculus amounts to a topological space reliant on the tree topology. Both the Scott and Tree topologies exhibit continuity with respect to the binary operators of application ( f applied to a = fa ) and abstraction ((λx.t(x))a = t(a)) with a modular equivalence relation based on a congruency. The λ-algebra describing the algebraic structure of the lambda-calculus is found to be an extension of the combinatory algebra, with an element introduced to accommodate abstraction.
Type free λ-calculus treats functions as rules and does not differentiate functions and the objects which they are applied to, meaning λ-calculus is type free. A by-product of type free λ-calculus is an effective computability equivalent to general recursion and Turing machines. The set of λ-terms can be considered a functional topology in which a function space can be embedded, meaning λ mappings within the space X are such that λ:X → X. Introduced November 1969, Dana Scott's untyped set theoretic model constructed a proper topology for any λ-calculus model whose function space is limited to continuous functions. The result of a Scott continuous λ-calculus topology is a function space built upon a programming semantic allowing fixed point combinatorics, such as the Y combinator, and data types. By 1971, λ-calculus was equipped to define any sequential computation and could be easily adapted to parallel computations. The reducibility of all computations to λ-calculus allows these λ-topological properties to become adopted by all programming languages.


== Computational algebra from λ-calculus algebra ==
Based on the operators within lambda calculus, application and abstraction, it is possible to develop an algebra whose group structure uses application and abstraction as binary operators. Application is defined as an operation between lambda terms producing a λ-term, e.g. the application of λ onto the lambda term a produces the lambda term λa. Abstraction incorporates undefined variables by denoting λx.t(x) as the function assigning the variable a to the lambda term with value t(a) via the operation ((λ x.t(x))a = t(a)). Lastly, an equivalence relation emerges which identifies λ-terms modulo convertible terms, an example being beta normal form.


== Scott topology ==
The Scott topology is essential in understanding the topological structure of computation as expressed through the λ-calculus. Scott found that after constructing a function space using λ-calculus one obtains a Kolmogorov space, a 
  
    
      
        
          T
          
            o
          
        
      
    
    {\displaystyle T_{o}}
   topological space which exhibits pointwise convergence, in short the product topology. It is the ability of self homeomorphism as well as the ability to embed every space into such a space, denoted Scott continuous, as previously described which allows Scott's topology to be applicable to logic and recursive function theory. Scott approaches his derivation using a complete lattice, resulting in a topology dependent on the lattice structure. It is possible to generalise Scott's theory with the use of complete partial orders. For this reason a more general understanding of the computational topology is provided through complete partial orders. We will re-iterate to familiarize ourselves with the notation to be used during the discussion of Scott topology.
Complete partial orders are defined as follows:
First, given the partially ordered set D=(D,≤), a nonempty subset X ⊆ D is directed if ∀ x,y ∈ X ∃ z ∈ X where x≤ z & y ≤ z.
D is a complete partial order (cpo) if:

· Every directed X ⊆D has a supremum, and:

∃ bottom element ⊥ ∈ D such that ∀ x ∈ D ⊥ ≤ x.

We are now able to define the Scott topology over a cpo (D, ≤ ).
O ⊆ D is open if:

for x ∈ O, and x ≤ y, then y ∈ O, i.e. O is an upper set.
for a directed set X ⊆ D, and supremum(X) ∈ O, then X ∩ O ≠ ∅.

Using the Scott topological definition of open it is apparent that all topological properties are met.

·∅ and D, i.e. the empty set and whole space, are open.

·Arbitrary unions of open sets are open:

Proof: Assume 
  
    
      
        
          U
          
            i
          
        
      
    
    {\displaystyle U_{i}}
   is open where i ∈ I, I being the index set. We define U = ∪{ 
  
    
      
        
          U
          
            i
          
        
      
    
    {\displaystyle U_{i}}
   ; i ∈ I}. Take b as an element of the upper set of U, therefore a ≤ b for some a ∈ U It must be that a ∈ 
  
    
      
        
          U
          
            i
          
        
      
    
    {\displaystyle U_{i}}
   for some i, likewise b ∈ upset(
  
    
      
        
          U
          
            i
          
        
      
    
    {\displaystyle U_{i}}
  ). U must therefore be upper as well since 
  
    
      
        
          U
          
            i
          
        
      
    
    {\displaystyle U_{i}}
   ∈ U.

Likewise, if D is a directed set with a supremum in U, then by assumption sup(D) ∈ 
  
    
      
        
          U
          
            i
          
        
      
    
    {\displaystyle U_{i}}
  where 
  
    
      
        
          U
          
            i
          
        
      
    
    {\displaystyle U_{i}}
  is open. Thus there is a b ∈ D where b ∈ 
  
    
      
        
          U
          
            i
          
        
        ∩
        D
        ⊆
        U
        ∩
        D
      
    
    {\displaystyle U_{i}\cap D\subseteq U\cap D}
  . The union of open sets 
  
    
      
        
          U
          
            i
          
        
      
    
    {\displaystyle U_{i}}
  is therefore open.

·Open sets under intersection are open:

Proof: Given two open sets, U and V, we define W = U∩V. If W = ∅ then W is open. If non-empty say b ∈ upset(W) (the upper set of W), then for some a ∈ W, a ≤ b. Since a ∈ U∩V, and b an element of the upper set of both U and V, then b ∈ W.

Finally, if D is a directed set with a supremum in W, then by assumption sup(D) ∈ 
  
    
      
        U
        ∩
        V
      
    
    {\displaystyle U\cap V}
  . So there is a ∈ 
  
    
      
        U
        ∩
        D
      
    
    {\displaystyle U\cap D}
   and b ∈ 
  
    
      
        V
        ∩
        D
      
    
    {\displaystyle V\cap D}
  . Since D is directed there is c ∈ D with 
  
    
      
        a
        ≤
        c
        ,
        b
        ≤
        c
      
    
    {\displaystyle a\leq c,b\leq c}
  ; and since U and V are upper sets, c ∈ 
  
    
      
        U
        ∩
        V
      
    
    {\displaystyle U\cap V}
   as well.

Though not shown here, it is the case that the map 
  
    
      
        f
        :
        D
        →
        
          D
          
            
              
              ′
            
          
        
      
    
    {\displaystyle f:D\rightarrow D^{'}}
   is continuous if and only if f(sup(X)) = sup(f(X)) for all directed X⊆D, where f(X) = {f(x) | x ∈ X} and the second supremum in 
  
    
      
        
          D
          
            
              
              ′
            
          
        
      
    
    {\displaystyle D^{'}}
  .
Before we begin explaining that application as common to λ-calculus is continuous within the Scott topology we require a certain understanding of the behavior of supremums over continuous functions as well as the conditions necessary for the product of spaces to be continuous namely

With 
  
    
      
        
          
            
              f
              
                i
              
            
          
          
            i
          
        
        ⊆
        [
        D
        →
        
          D
          
            
              
              ′
            
          
        
        ]
      
    
    {\displaystyle {f_{i}}_{i}\subseteq [D\rightarrow D^{'}]}
   be a directed family of maps, then 
  
    
      
        f
        (
        x
        )
        =
        
          ∪
          
            i
          
        
        
          f
          
            i
          
        
        (
        x
        )
      
    
    {\displaystyle f(x)=\cup _{i}f_{i}(x)}
   if well defined and continuous.
If F 
  
    
      
        ⊆
        [
        D
        →
        
          D
          
            
              
              ′
            
          
        
        ]
      
    
    {\displaystyle \subseteq [D\rightarrow D^{'}]}
   is directed and cpo and 
  
    
      
        [
        D
        →
        
          D
          
            
              
              ′
            
          
        
        ]
      
    
    {\displaystyle [D\rightarrow D^{'}]}
   a cpo where sup({f(x) | f ∈ F}).

We now show the continuity of application. Using the definition of application as follows:

Ap: 
  
    
      
        [
        D
        →
        
          D
          
            
              
              ′
            
          
        
        ]
        ×
        D
        →
        
          D
          
            
              
              ′
            
          
        
      
    
    {\displaystyle [D\rightarrow D^{'}]\times D\rightarrow D^{'}}
   where Ap(f,x) = f(x).

Ap is continuous with respect to the Scott topology on the product (
  
    
      
        [
        D
        →
        
          D
          
            
              
              ′
            
          
        
        ]
        ×
        D
        →
        
          D
          
            
              
              ′
            
          
        
      
    
    {\displaystyle [D\rightarrow D^{'}]\times D\rightarrow D^{'}}
  ) :

Proof: λx.f(x) = f is continuous. Let h = λ f.f(x). For directed F
  
    
      
        ⊆
        [
        D
        →
        
          D
          
            
              
              ′
            
          
        
        ]
      
    
    {\displaystyle \subseteq [D\rightarrow D^{'}]}
  

h(sup(F)) = sup(F)(x)

= sup( {f(x) | f ∈ F} )

= sup( {h(f) | f ∈ F} )

= sup( h(F) )

By definition of Scott continuity h has been shown continuous. All that is now required to prove is that application is continuous when it's separate arguments are continuous, i.e. 
  
    
      
        [
        D
        →
        
          D
          
            
              
              ′
            
          
        
        ]
      
    
    {\displaystyle [D\rightarrow D^{'}]}
  and 
  
    
      
        D
        →
        
          D
          
            
              
              ′
            
          
        
      
    
    {\displaystyle D\rightarrow D^{'}}
  are continuous, in our case f and h.

Now abstracting our argument to show 
  
    
      
        f
        :
        D
        ×
        
          D
          
            
              
              ′
            
          
        
        →
        
          D
          
            
              
              ″
            
          
        
      
    
    {\displaystyle f:D\times D^{'}\rightarrow D^{''}}
   with 
  
    
      
        g
        =
        λ
        x
        .
        f
        (
        x
        ,
        
          x
          
            0
          
        
        )
      
    
    {\displaystyle g=\lambda x.f(x,x_{0})}
   and 
  
    
      
        d
        =
        λ
        
          x
          
            
              
              ′
            
          
        
        .
        f
        (
        
          x
          
            0
          
        
        ,
        
          x
          
            
              
              ′
            
          
        
        )
      
    
    {\displaystyle d=\lambda x^{'}.f(x_{0},x^{'})}
   as the arguments for D and 
  
    
      
        
          D
          
            
              
              ′
            
          
        
      
    
    {\displaystyle D^{'}}
   respectively, then for a directed X ⊆ D

  
    
      
        g
        (
        sup
        (
        X
        )
        )
        =
        f
        (
        sup
        (
        X
        )
        ,
        
          x
          
            0
          
          
            
              
              ′
            
          
        
        )
        )
      
    
    {\displaystyle g(\sup(X))=f(\sup(X),x_{0}^{'}))}
  

= f( sup( (x,
  
    
      
        
          x
          
            0
          
          
            
              
              ′
            
          
        
      
    
    {\displaystyle x_{0}^{'}}
  ) | x ∈ X} ))

(since f is continuous and {(x,
  
    
      
        
          x
          
            0
          
          
            
              
              ′
            
          
        
      
    
    {\displaystyle x_{0}^{'}}
  ) | x ∈ X}) is directed):

= sup( {f(x,
  
    
      
        
          x
          
            0
          
          
            
              
              ′
            
          
        
      
    
    {\displaystyle x_{0}^{'}}
  ) | x ∈ X} )

= sup(g(X))

g is therefore continuous. The same process can be taken to show d is continuous.
It has now been shown application is continuous under the Scott topology.

In order to demonstrate the Scott topology is a suitable fit for λ-calculus it is necessary to prove abstraction remains continuous over the Scott topology. Once completed it will have been shown that the mathematical foundation of λ-calculus is a well defined and suitable candidate functional paradigm for the Scott topology.
With 
  
    
      
        f
        ∈
        [
        D
        ×
        
          D
          
            
              
              ′
            
          
        
        →
        
          D
          
            
              
              ″
            
          
        
        ]
      
    
    {\displaystyle f\in [D\times D^{'}\rightarrow D^{''}]}
   we define 
  
    
      
        
          
            
              f
              ˇ
            
          
        
      
    
    {\displaystyle {\check {f}}}
   (x) =λ y ∈ 
  
    
      
        
          D
          
            
              
              ′
            
          
        
      
    
    {\displaystyle D^{'}}
  f(x,y)We will show:
(i) 
  
    
      
        
          
            
              f
              ˇ
            
          
        
      
    
    {\displaystyle {\check {f}}}
   is continuous, meaning 
  
    
      
        
          
            
              f
              ˇ
            
          
        
      
    
    {\displaystyle {\check {f}}}
   ∈ 
  
    
      
        [
        D
        →
        [
        
          D
          
            
              
              ′
            
          
        
        →
        
          D
          
            
              
              ″
            
          
        
        ]
      
    
    {\displaystyle [D\rightarrow [D^{'}\rightarrow D^{''}]}
  
(ii) λ 
  
    
      
        f
        .
        
          
            
              f
              ˇ
            
          
        
        :
        [
        D
        ×
        
          D
          
            
              
              ′
            
          
        
        →
        
          D
          
            
              
              ″
            
          
        
        ]
        →
        [
        D
        →
        [
        
          D
          
            
              
              ′
            
          
        
        →
        
          D
          
            
              
              ″
            
          
        
        ]
      
    
    {\displaystyle f.{\check {f}}:[D\times D^{'}\rightarrow D^{''}]\rightarrow [D\rightarrow [D^{'}\rightarrow D^{''}]}
   is continuous.

Proof (i): Let X ⊆ D be directed, then

  
    
      
        
          
            
              f
              ˇ
            
          
        
      
    
    {\displaystyle {\check {f}}}
  (sup(X)) = λ y.f( sup(X),y )

= λ y.
  
    
      
        
          sup
          
            x
            ∈
            X
          
        
      
    
    {\displaystyle \sup _{x\in X}}
  ( f(x,y) )

= 
  
    
      
        
          sup
          
            x
            ∈
            X
          
        
      
    
    {\displaystyle \sup _{x\in X}}
  ( λy.f(x,y) )

= sup(
  
    
      
        
          
            
              f
              ˇ
            
          
        
      
    
    {\displaystyle {\check {f}}}
  (X))

Proof (ii): Defining L = λ 
  
    
      
        f
        .
        
          
            
              f
              ˇ
            
          
        
      
    
    {\displaystyle f.{\check {f}}}
   then for F 
  
    
      
        ⊆
        [
        D
        ×
        
          D
          
            
              
              ′
            
          
        
        →
        
          D
          
            
              
              ″
            
          
        
        ]
      
    
    {\displaystyle \subseteq [D\times D^{'}\rightarrow D^{''}]}
   directed

L(sup(F)) = λ x λ y. (sup(F))(x,y))

= λ x λ y. 
  
    
      
        
          sup
          
            y
            ∈
            F
          
        
      
    
    {\displaystyle \sup _{y\in F}}
  f(x,y)

= 
  
    
      
        
          sup
          
            y
            ∈
            F
          
        
      
    
    {\displaystyle \sup _{y\in F}}
  λx λy.f(x,y)

= sup(L(F))

It has not been demonstrated how and why the λ-calculus defines the Scott topology.


== Böhm trees and computational topology ==
Böhm trees, easily represented graphically, express the computational behavior of a lambda term. It is possible to predict the functionality of a given lambda expression from reference to its correlating Böhm tree. Böhm trees can be seen somewhat analogous to 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
   where the Böhm tree of a given set is similar to the continued fraction of a real number, and what is more, the Böhm tree corresponding to a sequence in normal form is finite, similar to the rational subset of the Reals.
Böhm trees are defined by a mapping of elements within a sequence of numbers with ordering (≤, lh) and a binary operator * to a set of symbols. The Böhm tree is then a relation among a set of symbols through a partial mapping ψ.
Informally Böhm trees may be conceptualized as follows:
Given: Σ = 
  
    
      
        ⊥
        ∪
      
    
    {\displaystyle \perp \cup }
   { λ x_{1} 
  
    
      
        ⋯
      
    
    {\displaystyle \cdots }
  x_{n} . y | n ∈ 
  
    
      
        
          N
        
        ,
        
          x
          
            1
          
        
        .
        .
        .
        
          x
          
            n
          
        
      
    
    {\displaystyle \mathbb {N} ,x_{1}...x_{n}}
  y are variables and denoting BT(M) as the Böhm tree for a lambda term M we then have:
BT(M) = ⊥ if M is unsolvable (therefore a single node)

More formally:
Σ is defined as a set of symbols. The Böhm tree of a λ term M, denoted BT(M), is the Σ labelled tree defined as follows:

If M is unsolvable:

  
    
      
        B
        T
        (
        M
        )
        (
        ⟨
         
        ⟩
        )
        =⊥
        ,
      
    
    {\displaystyle BT(M)(\langle \ \rangle )=\perp ,}
  

BT(M)(
  
    
      
        ⟨
        k
        ⟩
        ∗
        α
      
    
    {\displaystyle \langle k\rangle *\alpha }
  ) is unsolvable 
  
    
      
        ∀
        k
        ,
        α
      
    
    {\displaystyle \forall k,\alpha }
  

If M is solvable, where M = λ x_{1}
  
    
      
        ⋯
        
          x
          
            n
          
        
        .
        y
        
          M
          
            0
          
        
        ⋯
        
          M
          
            m
            −
            1
          
        
      
    
    {\displaystyle \cdots x_{n}.yM_{0}\cdots M_{m-1}}
  :

BT(M)(< >) = λ x_{1} 
  
    
      
        ⋯
        
          x
          
            n
          
        
        .
        y
      
    
    {\displaystyle \cdots x_{n}.y}
  

BT(M)(
  
    
      
        ⟨
        k
        ⟩
        ∗
        α
      
    
    {\displaystyle \langle k\rangle *\alpha }
  ) = BT(M_k)(
  
    
      
        α
      
    
    {\displaystyle \alpha }
  ) 
  
    
      
        ∀
        α
      
    
    {\displaystyle \forall \alpha }
   and k < m

= undefined 
  
    
      
        ∀
        α
      
    
    {\displaystyle \forall \alpha }
   and k ≥ m

We may now move on to show that Böhm trees act as suitable mappings from the tree topology to the scott topology. Allowing one to see computational constructs, be it within the Scott or tree topology, as Böhm tree formations.


=== Böhm tree and tree topology ===
It is found that Böhm tree's allow for a continuous mapping from the tree topology to the Scott topology. More specifically:
We begin with the cpo B = (B,⊆) on the Scott topology, with ordering of Böhm tree's denoted M⊆ N, meaning M, N are trees and M results from N. The tree topology on the set Γ is the smallest set allowing for a continuous map

BT:
  
    
      
        Γ
        →
      
    
    {\displaystyle \Gamma \rightarrow }
  B.

An equivalent definition would be to say the open sets of Γ are the image of the inverse Böhm tree 
  
    
      
        B
        
          T
          
            −
            1
          
        
      
    
    {\displaystyle BT^{-1}}
   (O) where O is Scott open in B.
The applicability of the Bömh trees and the tree topology has many interesting consequences to λ-terms expressed topologically:
Normal forms are found to exist as isolated points.
Unsolvable λ-terms are compactification points.
Application and abstraction, similar to the Scott topology, are continuous on the tree topology.


== Algebraic structure of computation ==
New methods of interpretation of the λ-calculus are not only interesting in themselves but allow new modes of thought concerning the behaviors of computer science. The binary operator within the λ-algebra A is application. Application is denoted · and is said to give structure 
  
    
      
        A
        =
        (
        X
        ,
        ⋅
        )
      
    
    {\displaystyle A=(X,\cdot )}
  . A combinatory algebra allows for the application operator and acts as a useful starting point but remains insufficient for the λ-calculus in being unable to express abstraction. The λ algebra becomes a combinatory algebra M combined with a syntactic operator λ* that transforms a term B(x,y), with constants in M, into C(
  
    
      
        
          
            
              y
              ^
            
          
        
      
    
    {\displaystyle {\hat {y}}}
  )≡ λ* x.B(x,
  
    
      
        
          
            
              y
              ^
            
          
        
      
    
    {\displaystyle {\hat {y}}}
  ). It is also possible to define an extension model to circumvent the need of the λ* operator by allowing ∀x (fx =gx) ⇒ f =g . The construction of the λ-algebra through the introduction of an abstraction operator proceeds as follows:
We must construct an algebra which allows for solutions to equations such as axy = xyy such that a = λ xy.xyy there is need for the combinatory algebra. Relevant attributes of the combinatory algebra are:
Within combinatory algebra there exists applicative structures. An applicative structure W is a combinatory algebra provided:

·W is non-trival, meaning W has cardinality > 1
·W exhibits combinatory completeness (see completeness of the S-K basis). More specifically: for every term A ∈ the set of terms of W, and 
  
    
      
        
          x
          
            1
          
        
        ,
        .
        .
        .
        ,
        
          x
          
            n
          
        
      
    
    {\displaystyle x_{1},...,x_{n}}
   with the free variables of A within 
  
    
      
        
          
            x
            
              1
            
          
          ,
          .
          .
          .
          ,
          
            x
            
              n
            
          
        
      
    
    {\displaystyle {x_{1},...,x_{n}}}
   then:

  
    
      
        ∃
        f
        ∀
        
          x
          
            1
          
        
        ⋅
        ⋅
        ⋅
        
          x
          
            n
          
        
      
    
    {\displaystyle \exists f\forall x_{1}\cdot \cdot \cdot x_{n}}
   where 
  
    
      
        f
        
          x
          
            1
          
        
        ⋅
        ⋅
        ⋅
        
          x
          
            n
          
        
        =
        A
      
    
    {\displaystyle fx_{1}\cdot \cdot \cdot x_{n}=A}
  

The combinatory algebra is:
Never commutative
Not associative.
Never finite.
Never recursive.
Combinatory algebras remain unable to act as the algebraic structure for λ-calculus, the lack of recursion being a major disadvantage. However the existence of an applicative term 
  
    
      
        A
        (
        x
        ,
        
          
            
              y
              →
            
          
        
      
    
    {\displaystyle A(x,{\vec {y}}}
  ) provides a good starting point to build a λ-calculus algebra. What is needed is the introduction of a lambda term, i.e. include λx.A(x, 
  
    
      
        
          
            
              y
              →
            
          
        
      
    
    {\displaystyle {\vec {y}}}
  ).
We begin by exploiting the fact that within a combinatory algebra M, with A(x, 
  
    
      
        
          
            
              y
              →
            
          
        
      
    
    {\displaystyle {\vec {y}}}
  ) within the set of terms, then:

  
    
      
        ∀
        
          
            
              y
              →
            
          
        
      
    
    {\displaystyle \forall {\vec {y}}}
   
  
    
      
        ∃
        b
      
    
    {\displaystyle \exists b}
   s.t. bx = A(x, 
  
    
      
        
          
            
              y
              →
            
          
        
      
    
    {\displaystyle {\vec {y}}}
  ).

We then require b have a dependence on 
  
    
      
        
          
            
              y
              →
            
          
        
      
    
    {\displaystyle {\vec {y}}}
   resulting in:

  
    
      
        ∀
        x
      
    
    {\displaystyle \forall x}
   B(
  
    
      
        
          
            
              y
              →
            
          
        
      
    
    {\displaystyle {\vec {y}}}
  )x = A(x, 
  
    
      
        
          
            
              y
              →
            
          
        
      
    
    {\displaystyle {\vec {y}}}
  ).

B(
  
    
      
        
          
            
              y
              →
            
          
        
      
    
    {\displaystyle {\vec {y}}}
  ) becomes equivalent to a λ term, and is therefore suitably defined as follows: B(
  
    
      
        
          
            
              y
              →
            
          
        
        )
        ≡
      
    
    {\displaystyle {\vec {y}})\equiv }
   λ*.
A pre-λ-algebra (pλA) can now be defined.

pλA is an applicative structure W = (X,·) such that for each term A within the set of terms within W and for every x there is a term λ*x.A ∈ T(W) (T(W) ≡ the terms of W) where (the set of free variables of λ*x.A) = (the set of free variables of A) - {x}. W must also demonstrate:

  
    
      
        (
        β
        )
      
    
    {\displaystyle (\beta )}
   (λ*x.A)x = A

  
    
      
        
          α
          
            1
          
        
      
    
    {\displaystyle \alpha _{1}}
  λ*x.A≡ λ*x.A[x:=y] provided y is not a free variable of A

  
    
      
        
          α
          
            2
          
        
      
    
    {\displaystyle \alpha _{2}}
  (λ*x.A)[y:=z]≡λ*x.A[x:=y] provided y,z ≠ x and z is not a free variable of A

Before defining the full λ-algebra we must introduce the following definition for the set of λ-terms within W denoted 
  
    
      
        Γ
        (
        W
        )
      
    
    {\displaystyle \Gamma (W)}
   with the following requirements:

a ∈ W 
  
    
      
        ⇒
        
          c
          
            a
          
        
        ∈
        Γ
        (
        W
        )
      
    
    {\displaystyle \Rightarrow c_{a}\in \Gamma (W)}
  
x ∈ 
  
    
      
        Γ
        (
        W
        )
      
    
    {\displaystyle \Gamma (W)}
   for x ∈ (
  
    
      
        
          v
          
            0
          
        
        ,
        
          v
          
            1
          
        
        ,
        .
        .
        .
      
    
    {\displaystyle v_{0},v_{1},...}
  )
M,N ∈ 
  
    
      
        Γ
        (
        W
        )
        ⇒
      
    
    {\displaystyle \Gamma (W)\Rightarrow }
   (MN) ∈ 
  
    
      
        Γ
        (
        W
        )
      
    
    {\displaystyle \Gamma (W)}
  
M ∈ 
  
    
      
        Γ
        (
        W
        )
        ⇒
      
    
    {\displaystyle \Gamma (W)\Rightarrow }
   (λx.M) ∈ 
  
    
      
        Γ
        (
        W
        )
      
    
    {\displaystyle \Gamma (W)}
  

A mapping from the terms within 
  
    
      
        Γ
        (
        W
        )
      
    
    {\displaystyle \Gamma (W)}
   to all λ terms within W, denoted * : 
  
    
      
        Γ
        (
        W
        )
        →
        
          T
        
        (
        W
        )
      
    
    {\displaystyle \Gamma (W)\rightarrow \mathrm {T} (W)}
  , can then be designed as follows:

  
    
      
        
          v
          
            i
          
          
            ∗
          
        
        =
        
          w
          
            i
          
        
        ,
        
          c
          
            a
          
          
            ∗
          
        
        =
        
          c
          
            a
          
        
      
    
    {\displaystyle v_{i}^{*}=w_{i},c_{a}^{*}=c_{a}}
  
(MN)* = M* N*
(λx.M)* = λ* x*.M*

We now define λ(M) to denote the extension after evaluating the terms within 
  
    
      
        Γ
        (
        W
        )
      
    
    {\displaystyle \Gamma (W)}
  .

λx.(λy.yx)
  
    
      
        
          c
          
            a
          
        
      
    
    {\displaystyle c_{a}}
   = λx.
  
    
      
        
          c
          
            a
          
        
      
    
    {\displaystyle c_{a}}
  x in λ(W).

Finally we obtain the full λ-algebra through the following definition:

(1) A λ-algebra is a pλA W such that for M,N ∈ Γ(W):
λ(W) ⊢ M = N ⇒ W ⊨ M = N.

Though arduous, the foundation has been set for a proper algebraic framework for which the λ-calculus, and therefore computation, may be investigated in a group theoretic manner.


== References =="
48,Loebner Prize,238725,20692,"The Loebner Prize is an annual competition in artificial intelligence that awards prizes to the computer programs considered by the judges to be the most human-like. The format of the competition is that of a standard Turing test. In each round, a human judge simultaneously holds textual conversations with a computer program and a human being via computer. Based upon the responses, the judge must decide which is which.
The contest was launched in 1990 by Hugh Loebner in conjunction with the Cambridge Center for Behavioral Studies, Massachusetts, United States. Since 2014 it has been organised by the AISB at Bletchley Park. It has also been associated with Flinders University, Dartmouth College, the Science Museum in London, University of Reading and Ulster University, Magee Campus, Derry, UK City of Culture. In 2004 and 2005, it was held in Loebner's apartment in New York City. Within the field of artificial intelligence, the Loebner Prize is somewhat controversial; the most prominent critic, Marvin Minsky, called it a publicity stunt that does not help the field along.


== Prizes ==
Originally, $2,000 was awarded for the most human-seeming program in the competition. The prize was $3,000 in 2005 and $2,250 in 2006. In 2008, $3,000 was awarded.
In addition, there are two one-time-only prizes that have never been awarded. $25,000 is offered for the first program that judges cannot distinguish from a real human and which can convince judges that the human is the computer program. $100,000 is the reward for the first program that judges cannot distinguish from a real human in a Turing test that includes deciphering and understanding text, visual, and auditory input. Once this is achieved, the annual competition will end.


== Competition rules and restrictions ==
The rules have varied over the years and early competitions featured restricted conversation Turing tests but since 1995 the discussion has been unrestricted.
For the three entries in 2007, Robert Medeksza, Noah Duncan and Rollo Carpenter, some basic ""screening questions"" were used by the sponsor to evaluate the state of the technology. These included simple questions about the time, what round of the contest it is, etc.; general knowledge (""What is a hammer for?""); comparisons (""Which is faster, a train or a plane?""); and questions demonstrating memory for preceding parts of the same conversation. ""All nouns, adjectives and verbs will come from a dictionary suitable for children or adolescents under the age of 12."" Entries did not need to respond ""intelligently"" to the questions to be accepted.
For the first time in 2008 the sponsor allowed introduction of a preliminary phase to the contest opening up the competition to previously disallowed web-based entries judged by a variety of invited interrogators. The available rules do not state how interrogators are selected or instructed. Interrogators (who judge the systems) have limited time: 5 minutes per entity in the 2003 competition, 20+ per pair in 2004–2007 competitions, 5 minutes to conduct simultaneous conversations with a human and the program in 2008-2009, increased to 25 minutes of simultaneous conversation since 2010.


== Criticisms ==
The prize has long been scorned by experts in the field, for a variety of reasons.
It is regarded by many as a publicity stunt. Marvin Minsky scathingly offered a ""prize"" to anyone who could stop the competition. Loebner responded by jokingly observing that Minsky's offering a prize to stop the competition effectively made him a co-sponsor.
The rules of the competition have encouraged poorly qualified judges to make rapid judgements. Interactions between judges and competitors was originally very brief, for example effectively 2.5 mins of questioning, which permitted only a few questions. Questioning was initially restricted to ""whimsical conversation"", a domain suiting standard chatbot tricks.
Competition entrants do not aim at understanding or intelligence but resort to basic ELIZA style tricks, and successful entrants find deception and pretense is rewarded.
Reporting of the annual competition often confuses the imitation test with intelligence, a typical example being Brian Christian's introduction to his article ""Mind vs. Machine"" in The Atlantic, March 2011, stating that ""in the race to build computers that can think like humans, the proving ground is the Turing Test"".


== Contests ==


=== 2006 ===
In 2006, the contest was organised by Tim Child (CEO of Televirtual) and Huma Shah. On August 30, the four finalists were announced:
Rollo Carpenter
Richard Churchill and Marie-Claire Jenkins
Noah Duncan
Robert Medeksza
The contest was held on 17 September in the VR theatre, Torrington Place campus of University College London. The judges included the University of Reading's cybernetics professor, Kevin Warwick, a professor of artificial intelligence, John Barnden (specialist in metaphor research at the University of Birmingham), a barrister, Victoria Butler-Cole and a journalist, Graham Duncan-Rowe. The latter's experience of the event can be found in an article in Technology Review. The winner was 'Joan', based on Jabberwacky, both created by Rollo Carpenter.


=== 2007 ===
The 2007 competition was held on October 21 in New York City. The judges were: computer science professor Russ Abbott, philosophy professor Hartry Field, psychology assistant professor Clayton Curtis and English lecturer Scott Hutchins.
No bot passed the Turing test, but the judges ranked the three contestants as follows:
1st: Robert Medeksza from Zabaware, creator of Ultra Hal Assistant
2nd: Noah Duncan, a private entry, creator of Cletus
3rd: Rollo Carpenter from Icogno, creator of Jabberwacky
The winner received $2,250 and the annual medal. The runners-up received $250 each.


=== 2008 ===
The 2008 competition was organised by professor Kevin Warwick, coordinated by Huma Shah and held on October 12 at the University of Reading, UK. After testing by over one hundred judges during the preliminary phase, in June and July 2008, six finalists were selected from thirteen original entrants - artificial conversational entity (ACE). Five of those invited competed in the finals:
Brother Jerome, Peter Cole and Benji Adams
Elbot, Fred Roberts / Artificial Solutions
Eugene Goostman, Vladimir Veselov, Eugene Demchenko and Sergey Ulasen
Jabberwacky, Rollo Carpenter
Ultra Hal, Robert Medeksza
In the finals, each of the judges was given five minutes to conduct simultaneous, split-screen conversations with two hidden entities. Elbot of Artificial Solutions won the 2008 Loebner Prize bronze award, for most human-like artificial conversational entity, through fooling three of the twelve judges who interrogated it (in the human-parallel comparisons) into believing it was human. This is coming very close to the 30% traditionally required to consider that a program has actually passed the Turing test. Eugene Goostman and Ultra Hal both deceived one judge each that it was the human.
Will Pavia, a journalist for The Times, has written about his experience; a Loebner finals' judge, he was deceived by Elbot and Eugene. Kevin Warwick and Huma Shah have reported on the parallel-paired Turing tests.


=== 2009 ===
The 2009 Loebner Prize Competition was held September 6, 2009 at the Brighton Centre, Brighton UK in conjunction with the Interspeech 2009 conference. The prize amount for 2009 was $3,000.
Entrants were David Levy, Rollo Carpenter, and Mohan Embar, who finished in that order.
The writer Brian Christian participated in the 2009 Loebner Prize Competition as a human confederate, and described his experiences at the competition in his book The Most Human Human.


=== 2010 ===
The 2010 Loebner Prize Competition was held on October 23 at California State University, Los Angeles. The 2010 competition was the 20th running of the contest. The winner was Bruce Wilcox with Suzette.


=== 2011 ===
The 2011 Loebner Prize Competition was held on October 19 at the University of Exeter, Devon, United Kingdom. The prize amount for 2011 was $4,000.
The four finalists and their chatterbots were Bruce Wilcox (Rosette), Adeena Mignogna (Zoe), Mohan Embar (Chip Vivant) and Ron Lee (Tutor), who finished in that order.
That year there was an addition of a panel of junior judges, namely Jean-Paul Astal-Stain, William Dunne, Sam Keat and Kirill Jerdev. The results of the junior contest were markedly different from the main contest, with chatterbots Tutor and Zoe tying for first place and Chip Vivant and Rosette coming in third and fourth place, respectively.


=== 2012 ===
The 2012 Loebner Prize Competition was held on the 15th of May in Bletchley Park in Bletchley, Buckinghamshire, England, in honor of the Alan Turing centenary celebrations. The prize amount for 2012 was $5,000. The local arrangements organizer was David Levy, who won the Loebner Prize in 1997 and 2009.
The four finalists and their chatterbots were Mohan Embar (Chip Vivant), Bruce Wilcox (Angela), Daniel Burke (Adam), M. Allan (Linguo), who finished in that order.
That year, a team from the University of Exeter's computer science department (Ed Keedwell, Max Dupenois and Kent McClymont) conducted the first-ever live webcast of the conversations.


=== 2013 ===
The 2013 Loebner Prize Competition was held, for the first time on the Island of Ireland, on September 14 at the Ulster University, Magee College, Derry, Northern Ireland, UK.
The four finalists and their chatbots were Steve Worswick (Mitsuku), Dr. Ron C. Lee (Tutor), Bruce Wilcox (Rose) and Brian Rigsby (Izar), who finished in that order.
The judges were Professor Roger Schank (Socratic Arts), Professor Noel Sharkey (Sheffield University), Professor Minhua (Eunice) Ma (Huddersfield University, then University of Glasgow) and Professor Mike McTear (Ulster University).
For the 2013 Junior Loebner Prize Competition the chatbots Mitsuku and Tutor tied for first place with Rose and Izar in 3rd and 4th place respectively.


=== 2014 ===
The 2014 Loebner Prize Competition was held at Bletchley Park, England, on Saturday 15 November 2014. The event was filmed live by Sky News. The guest judge was television presenter and broadcaster James May.
After 2 hours of judging, 'Rose' by Bruce Wilcox was declared the winner. Bruce will receive a cheque for $4000 and a bronze medal. The ranks were as follows:
Rose - Rank 1 ($4000 & Bronze Medal); Izar - Rank 2.25 ($1500); Uberbot - Rank 3.25 ($1000); and Mitsuku - Rank 3.5 ($500).
The Judges were Dr Ian Hocking, Writer & Senior Lecturer in Psychology, Christ Church College, Canterbury; Dr Ghita Kouadri-Mostefaoui, Lecturer in Computer Science and Technology, University of Bedfordshire; Mr James May, Television Presenter and Broadcaster; and Dr Paul Sant, Dean of UCMK, University of Bedfordshire.


=== 2015 ===
The 2015 Loebner Prize Competition was again won by 'Rose' by Bruce Wilcox.
The judges were Jacob Aaron, Physical sciences reporter for New Scientist; Rory Cellan-Jones, Technology correspondent for the BBC; Brett Marty, Film Director and Photographer; Ariadne Tampion, Writer.


=== 2016 ===
The 2016 Loebner Prize was held at Bletchley Park on 17 September 2016. After 2 hours of judging the final results were announced. The ranks were as follows:
1st place: Mitsuku
2nd place: Tutor
3rd place: Rose


== Winners ==
Official list of winners.


== See also ==
Artificial intelligence
Glossary of artificial intelligence
Robot
Artificial general intelligence
Confederate effect
Computer game bot Turing Test


== References ==


== External links ==
Official website
Markoff, John (Jan 10, 1993). ""Cocktail-Party Conversation -- With a Computer"". New York Times. Conversation with the 1992 winner; topic: men and women 
Platt, Charles (April 1995). ""What's It Mean to be Human, Anyway?"". Wired. 
Shah, Huma (Oct 2008). ""2008 Loebner Prize: myths and misconceptions"". 
Christian, Brian (March 2011). ""Mind vs. Machine"". The Atlantic."
49,Biodiversity informatics,6900845,20428,"Biodiversity Informatics is the application of informatics techniques to biodiversity information for improved management, presentation, discovery, exploration and analysis. It typically builds on a foundation of taxonomic, biogeographic, or ecological information stored in digital form, which, with the application of modern computer techniques, can yield new ways to view and analyse existing information, as well as predictive models for information that does not yet exist (see niche modelling). Biodiversity informatics is a relatively young discipline (the term was coined in or around 1992) but has hundreds of practitioners worldwide, including the numerous individuals involved with the design and construction of taxonomic databases. The term ""Biodiversity Informatics"" is generally used in the broad sense to apply to computerized handling of any biodiversity information; the somewhat broader term ""bioinformatics"" is often used synonymously with the computerized handling of data in the specialized area of molecular biology.


== Overview ==
Biodiversity informatics (different but linked to bioinformatics) is the application of information technology methods to the problems of organizing, accessing, visualizing and analyzing primary biodiversity data. Primary biodiversity data is composed of names, observations and records of specimens, and genetic and morphological data associated to a specimen. Biodiversity informatics may also have to cope with managing information from unnamed taxa such as that produced by environmental sampling and sequencing of mixed-field samples. The term biodiversity informatics is also used to cover the computational problems specific to the names of biological entities, such as the development of algorithms to cope with variant representations of identifiers such as species names and authorities, and the multiple classification schemes within which these entities may reside according to the preferences of different workers in the field, as well as the syntax and semantics by which the content in taxonomic databases can be made machine queryable and interoperable for biodiversity informatics purposes...


== History of the discipline ==
Biodiversity Informatics can be considered to have commenced with the construction of the first computerized taxonomic databases in the early 1970s, and progressed through subsequent developing of distributed search tools towards the late 1990s including the Species Analyst from Kansas University, the North American Biodiversity Information Network NABIN, CONABIO in Mexico, and others, the establishment of the Global Biodiversity Information Facility in 2001, and the parallel development of a variety of niche modelling and other tools to operate on digitized biodiversity data from the mid-1980s onwards (e.g. see ). In September 2000, the U.S. journal Science devoted a special issue to ""Bioinformatics for Biodiversity"", the journal ""Biodiversity Informatics"" commenced publication in 2004, and several international conferences through the 2000s have brought together Biodiversity Informatics practitioners, including the London e-Biosphere conference in June 2009. A supplement to the journal BMC Bioinformatics (Volume 10 Suppl 14) published in November 2009 also deals with Biodiversity Informatics.


== History of the term ==
According to correspondence reproduced by Walter Berendsohn, the term ""Biodiversity Informatics"" was coined by John Whiting in 1992 to cover the activities of an entity known as the Canadian Biodiversity Informatics Consortium, a group involved with fusing basic biodiversity information with environmental economics and geospatial information in the form of GPS and GIS. Subsequently, it appears to have lost any obligate connection with the GPS/GIS world and be associated with the computerized management of any aspects of biodiversity information (e.g. see )


== Current Biodiversity Informatics issues ==


=== Global list of all species ===
One major issue for biodiversity informatics at a global scale is the current absence of a complete master list of currently recognised species of the world, although this is an aim of the Catalogue of Life project which has ca. 1.65 million species of an estimated 1.9 million described species in its 2016 Annual Checklist. A similar effort for fossil taxa, the Paleobiology Database documents some 100,000+ names for fossil species, out of an unknown total number.


=== Genus and species scientific names as unique identifiers ===
Application of the Linnaean system of binomial nomenclature for species, and uninomials for genera and higher ranks, has led to many advantages but also problems with homonyms (the same name being used for multiple taxa, either inadvertently or legitimately across multiple kingdoms), synonyms (multiple names for the same taxon), as well as variant representations of the same name due to orthographic differences, minor spelling errors, variation in the manner of citation of author names and dates, and more. In addition, names can change through time on account of changing taxonomic opinions (for example, the correct generic placement of a species, or the elevation of a subspecies to species rank or vice versa), and also the circumscription of a taxon can change according to different authors' taxonomic concepts. One proposed solution to this problem is the usage of Life Science Identifiers (LSIDs) for machine-machine communication purposes, although there are both proponents and opponents of this approach.


=== A consensus classification of organisms ===
Organisms can be classified in a multitude of ways (see main page Biological classification), which can create design problems for Biodiversity Informatics systems aimed at incorporating either a single or multiple classification to suit the needs of users, or to guide them towards a single ""preferred"" system. Whether a single consensus classification system can ever be achieved is probably an open question, however the Catalogue of Life has commissioned activity in this area which has been succeeded by a published system proposed in 2015 by M. Ruggiero and co-workers.


== Mobilizing primary biodiversity information ==
""Primary"" biodiversity information can be considered the basic data on the occurrence and diversity of species (or indeed, any recognizable taxa), commonly in association with information regarding their distribution in either space, time, or both. Such information may be in the form of retained specimens and associated information, for example as assembled in the natural history collections of museums and herbaria, or as observational records, for example either from formal faunal or floristic surveys undertaken by professional biologists and students, or as amateur and other planned or unplanned observations including those increasingly coming under the scope of citizen science. Providing online, coherent digital access to this vast collection of disparate primary data is a core Biodiversity Informatics function that is at the heart of regional and global biodiversity data networks, examples of the latter including OBIS and GBIF.
As a secondary source of biodiversity data, relevant scientific literature can be parsed either by humans or (potentially) by specialized information retrieval algorithms to extract the relevant primary biodiversity information that is reported therein, sometimes in aggregated / summary form but frequently as primary observations in narrative or tabular form. Elements of such activity (such as extracting key taxonomic identifiers, keywording / index terms, etc.) have been practiced for many years at a higher level by selected academic databases and search engines. However, for the maximum Biodiversity Informatics value, the actual primary occurrence data should ideally be retrieved and then made available in a standardized form or forms; for example both the Plazi and INOTAXA projects are transforming taxonomic literature into XML formats that can then be read by client applications, the former using TaxonX-XML and the latter using the taXMLit format. The Biodiversity Heritage Library is also making significant progress in its aim to digitize substantial portions of the out-of-copyright taxonomic literature, which is then subjected to OCR (optical character recognition) so as to be amenable to further processing using Biodiversity Informatics tools.


== Standards and protocols ==
In common with other data-related disciplines, Biodiversity Informatics benefits from the adoption of appropriate standards and protocols in order to support machine-machine transmission and interoperability of information within its particular domain. Examples of relevant standards include the Darwin Core XML schema for specimen- and observation-based biodiversity data developed from 1998 onwards, plus extensions of the same, Taxonomic Concept Transfer Schema, plus standards for Structured Descriptive Data and Access to Biological Collection Data (ABCD); while data retrieval and transfer protocols include DiGIR (now mostly superseded) and TAPIR (TDWG Access Protocol for Information Retrieval). Many of these standards and protocols are currently maintained, and their development overseen, by the Taxonomic Databases Working Group (TDWG).


== Current activities ==
At the 2009 e-Biosphere conference in the U.K., the following themes were adopted, which is indicative of a broad range of current Biodiversity Informatics activities and how they might be categorized:
Application: Conservation / Agriculture / Fisheries / Industry / Forestry
Application: Invasive Alien Species
Application: Systematic and Evolutionary Biology
Application: Taxonomy and Identification Systems
New Tools, Services and Standards for Data Management and Access
New Modeling Tools
New Tools for Data Integration
New Approaches to Biodiversity Infrastructure
New Approaches to Species Identification
New Approaches to Mapping Biodiversity

National and Regional Biodiversity Databases and Networks
A post-conference workshop of key persons with current significant Biodiversity Informatics roles also resulted in a Workshop Resolution that stressed, among other aspects, the need to create durable, global registries for the resources that are basic to biodiversity informatics (e.g., repositories, collections); complete the construction of a solid taxonomic infrastructure; and create ontologies for biodiversity data.


== Example Biodiversity Informatics projects ==
Global:
The Global Biodiversity Information Facility (GBIF), and the Ocean Biogeographic Information System (OBIS) (for marine species)
The Species 2000, ITIS (Integrated Taxonomic Information System), and Catalogue of Life projects
Global Names
EOL, The Encyclopedia of Life project
The Consortium for the Barcode of Life project
The Map of Life project
The uBio Universal Biological Indexer and Organizer, from the Woods Hole Marine Biological Laboratory
The Index to Organism Names (ION) from Thomson Reuters, providing access to scientific names of taxa from numerous journals as indexed in the Zoological Record
ZooBank, the registry for nomenclatural acts and relevant systematic literature in zoology
The Index Nominum Genericorum, compilation of generic names published for organisms covered by the International Code of Botanical Nomenclature, maintained at the Smithsonian Institution in the U.S.A.
The International Plant Names Index
MycoBank, documenting new names and combinations for fungi
The List of Prokaryotic names with Standing in Nomenclature (LPSN) - Official register of valid names for bacteria and archaea, as governed by the International Code of Nomenclature of Bacteria
The Biodiversity Heritage Library project - digitising biodiversity literature
Wikispecies, open source (community-editable) compilation of taxonomic information, companion project to Wikipedia
TaxonConcept.org, a Linked Data project that connects disparate species databases
Instituto de Ciencias Naturales. Universidad Nacional de Colombia. Virtual Collections and Biodiversity Informatics Unit
ANTABIF. The Antarctic Biodiversity Information Facility gives free and open access to Antarctic Biodiversity data, in the spirit of the Antarctic Treaty.
Genesys (website), database of plant genetic resources maintained in national, regional and international gene banks
VertNet, Access to vertebrate primary occurrence data from data sets worldwide.
Regional / national projects:
Fauna Europaea
Atlas of Living Australia
A Pan-European Species-directories Infrastructure (PESI)
Symbiota
i4Life project
Sistema de Información sobre Biodiversidad de Colombia
India Biodiversity Portal (IBP)
Bhutan Biodiversity Portal (BBP)
Weed Identification and Knowledge in the Western Indian Ocean (WIKWIO)
LifeWatch is proposed by ESFRI as a pan-European research (e-)infrastructure to support Biodiversity research and policy-making.
A listing of over 600 current biodiversity informatics related activities can be found at the TDWG ""Biodiversity Information Projects of the World"" database.


== See also ==
Biodiversity
Global biodiversity
Taxonomic database
Web-based taxonomy
List of biodiversity databases


== References ==


== Further reading ==
OECD Megascience Forum Working Group on Biological Informatics (1999). Final Report of the OECD Megascience Forum Working Group on Biological Informatics, January 1999. pp. 1–74. 
Canhos, V.P.; Souza, S.; Giovanni, R. & Canhos, D.A.L. (2004). ""Global biodiversity informatics: setting the scene for a ""new world"" of ecological modeling"". Biodiversity Informatics. 1: 1–13. 
Soberón, J. & Peterson, A.T. (2004). ""Biodiversity informatics: managing and applying primary biodiversity data"". Phil. Trans. R. Soc. Lond. B359: 689–698. doi:10.1098/rstb.2003.1439. 
Chapman, A.D. (2005). Uses of Primary Species-Occurrence Data (PDF). Copenhagen: Global Biodiversity Information Facility. pp. 1–106. 
Johnson, N.F. (2007). ""Biodiversity informatics"". Annual Review of Entomology. 52: 421–438. doi:10.1146/annurev.ento.52.110405.091259. PMID 16956323. 
Sarkar, I.N. (2007). ""Biodiversity informatics: organizing and linking information across the spectrum of life"". Briefings in Bioinformatics. 8 (5): 347–357. doi:10.1093/bib/bbm037. PMID 17704120. 
Guralnick, R.P.; Hill, A (2009). ""Biodiversity Informatics: Automated Approaches for Documenting Global Biodiversity Patterns and Processes"". Bioinformatics. 25 (4): 421–428. doi:10.1093/bioinformatics/btn659. PMID 19129210. 


== External links ==
Biodiversity Informatics (journal)
Website of the 2009 e-Biosphere International Conference on Biodiversity Informatics"
